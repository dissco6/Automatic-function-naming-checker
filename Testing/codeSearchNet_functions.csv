name;code
read_requirements;"def read_requirements(fh, resolve=False):
    """"""
    Reads requirements from a file like object and (optionally) from referenced files.
    :param fh: file like object to read from
    :param resolve: boolean. resolves referenced files.
    :return: generator
    """"""
    is_temp_file = not hasattr(fh, 'name')
    for num, line in enumerate(iter_lines(fh)):
        line = line.strip()
        if not line:
            # skip empty lines
            continue
        if line.startswith('#') or \
            line.startswith('-i') or \
            line.startswith('--index-url') or \
            line.startswith('--extra-index-url') or \
            line.startswith('-f') or line.startswith('--find-links') or \
            line.startswith('--no-index') or line.startswith('--allow-external') or \
            line.startswith('--allow-unverified') or line.startswith('-Z') or \
            line.startswith('--always-unzip'):
            # skip unsupported lines
            continue
        elif line.startswith('-r') or line.startswith('--requirement'):
            # got a referenced file here, try to resolve the path
            # if this is a tempfile, skip
            if is_temp_file:
                continue
            filename = line.strip(""-r "").strip(""--requirement"").strip()
            # if there is a comment, remove it
            if "" #"" in filename:
                filename = filename.split("" #"")[0].strip()
            req_file_path = os.path.join(os.path.dirname(fh.name), filename)
            if resolve:
                # recursively yield the resolved requirements
                if os.path.exists(req_file_path):
                    with open(req_file_path) as _fh:
                        for req in read_requirements(_fh, resolve=True):
                            yield req
            else:
                yield RequirementFile(path=req_file_path)
        else:
            try:
                parseable_line = line
                # multiline requirements are not parseable
                if ""\\"" in line:
                    parseable_line = line.replace(""\\"", """")
                    for next_line in iter_lines(fh, num + 1):
                        parseable_line += next_line.strip().replace(""\\"", """")
                        line += ""\n"" + next_line
                        if ""\\"" in next_line:
                            continue
                        break
                req, = parse_line(parseable_line)
                if len(req.specifier._specs) == 1 and \
                        next(iter(req.specifier._specs))._spec[0] == ""=="":
                    yield Package(key=req.name, version=next(iter(req.specifier._specs))._spec[1])
                else:
                    try:
                        fname = fh.name
                    except AttributeError:
                        fname = line

                    click.secho(
                        ""Warning: unpinned requirement '{req}' found in {fname}, ""
                        ""unable to check."".format(req=req.name,
                                                  fname=fname),
                        fg=""yellow"",
                        file=sys.stderr
                    )
            except ValueError:
                continue"
user_addmedia;"def user_addmedia(userids, active, mediatypeid, period, sendto, severity, **kwargs):
    '''
    Add new media to multiple users.

    .. versionadded:: 2016.3.0

    :param userids: ID of the user that uses the media
    :param active: Whether the media is enabled (0 enabled, 1 disabled)
    :param mediatypeid: ID of the media type used by the media
    :param period: Time when the notifications can be sent as a time period
    :param sendto: Address, user name or other identifier of the recipient
    :param severity: Trigger severities to send notifications about
    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)
    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)
    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)

    :return: IDs of the created media.

    CLI Example:
    .. code-block:: bash

        salt '*' zabbix.user_addmedia 4 active=0 mediatypeid=1 period='1-7,00:00-24:00' sendto='support2@example.com'
        severity=63

    '''
    conn_args = _login(**kwargs)
    ret = {}
    try:
        if conn_args:
            method = 'user.addmedia'
            params = {""users"": []}
            # Users
            if not isinstance(userids, list):
                userids = [userids]
            for user in userids:
                params['users'].append({""userid"": user})
            # Medias
            params['medias'] = [{""active"": active, ""mediatypeid"": mediatypeid, ""period"": period,
                                 ""sendto"": sendto, ""severity"": severity}, ]

            ret = _query(method, params, conn_args['url'], conn_args['auth'])
            return ret['result']['mediaids']
        else:
            raise KeyError
    except KeyError:
        return ret"
last_valid_index;"def last_valid_index(self):
        """"""Returns index of last non-NaN/NULL value.

        Return:
            Scalar of index name.
        """"""

        def last_valid_index_builder(df):
            df.index = pandas.RangeIndex(len(df.index))
            return df.apply(lambda df: df.last_valid_index())

        func = self._build_mapreduce_func(last_valid_index_builder)
        # We get the maximum from each column, then take the max of that to get
        # last_valid_index. The `to_pandas()` here is just for a single value and
        # `squeeze` will convert it to a scalar.
        first_result = self._full_axis_reduce(0, func).max(axis=1).to_pandas().squeeze()
        return self.index[first_result]"
get_selected_filenames;"def get_selected_filenames(self):
        """"""Return selected filenames""""""
        if self.selectionMode() == self.ExtendedSelection:
            if self.selectionModel() is None:
                return []
            return [self.get_filename(idx) for idx in 
                    self.selectionModel().selectedRows()]
        else:
            return [self.get_filename(self.currentIndex())]"
receive_trial_result;"def receive_trial_result(self, parameter_id, parameters, value):
        """"""receive_trial_result
       
        Parameters
        ----------
        parameter_id: int
            parameter id
        parameters:
            parameters
        value:
            value
        
        Raises
        ------
        RuntimeError
            Received parameter id not in total_data
        """"""
        reward = extract_scalar_reward(value)
        if self.optimize_mode is OptimizeMode.Maximize:
            reward = -reward

        if parameter_id not in self.total_data:
            raise RuntimeError('Received parameter_id not in total_data.')
        if self.first_one:
            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)
            self.first_one = False
        else:
            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)"
get_gpu_ids;"def get_gpu_ids():
    """"""Get the IDs of the GPUs that are available to the worker.

    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker
    started up, then the IDs returned by this method will be a subset of the
    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range
    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.

    Returns:
        A list of GPU IDs.
    """"""
    if _mode() == LOCAL_MODE:
        raise Exception(""ray.get_gpu_ids() currently does not work in PYTHON ""
                        ""MODE."")

    all_resource_ids = global_worker.raylet_client.resource_ids()
    assigned_ids = [
        resource_id for resource_id, _ in all_resource_ids.get(""GPU"", [])
    ]
    # If the user had already set CUDA_VISIBLE_DEVICES, then respect that (in
    # the sense that only GPU IDs that appear in CUDA_VISIBLE_DEVICES should be
    # returned).
    if global_worker.original_gpu_ids is not None:
        assigned_ids = [
            global_worker.original_gpu_ids[gpu_id] for gpu_id in assigned_ids
        ]

    return assigned_ids"
dns_check;"def dns_check(addr, port=80, safe=False, ipv6=None, attempt_connect=True):
    '''
    Return the ip resolved by dns, but do not exit on failure, only raise an
    exception. Obeys system preference for IPv4/6 address resolution - this
    can be overridden by the ipv6 flag.
    Tries to connect to the address before considering it useful. If no address
    can be reached, the first one resolved is used as a fallback.
    '''
    error = False
    lookup = addr
    seen_ipv6 = False
    family = socket.AF_INET6 if ipv6 else socket.AF_INET if ipv6 is False else socket.AF_UNSPEC

    hostnames = []
    try:
        refresh_dns()
        hostnames = socket.getaddrinfo(addr, port, family, socket.SOCK_STREAM)
    except TypeError:
        err = ('Attempt to resolve address \'{0}\' failed. Invalid or unresolveable address').format(lookup)
        raise SaltSystemExit(code=42, msg=err)
    except socket.error:
        error = True

    # If ipv6 is set to True, attempt another lookup using the IPv4 family,
    # just in case we're attempting to lookup an IPv4 IP
    # as an IPv6 hostname.
    if error and ipv6:
        try:
            refresh_dns()
            hostnames = socket.getaddrinfo(addr, port,
                                           socket.AF_INET,
                                           socket.SOCK_STREAM)
        except TypeError:
            err = ('Attempt to resolve address \'{0}\' failed. Invalid or unresolveable address').format(lookup)
            raise SaltSystemExit(code=42, msg=err)
        except socket.error:
            error = True

    try:
        if not hostnames:
            error = True
        else:
            resolved = False
            candidates = []
            for h in hostnames:
                # Input is IP address, passed through unchanged, just return it
                if h[4][0] == addr:
                    resolved = salt.utils.zeromq.ip_bracket(addr)
                    break

                candidate_addr = salt.utils.zeromq.ip_bracket(h[4][0])

                # sometimes /etc/hosts contains ::1 localhost
                if not ipv6 and candidate_addr == '[::1]':
                    continue

                candidates.append(candidate_addr)

                if attempt_connect:
                    try:
                        s = socket.socket(h[0], socket.SOCK_STREAM)
                        s.settimeout(2)
                        s.connect((candidate_addr.strip('[]'), h[4][1]))
                        s.close()

                        resolved = candidate_addr
                        break
                    except socket.error:
                        pass
            if not resolved:
                if candidates:
                    resolved = candidates[0]
                else:
                    error = True
    except TypeError:
        err = ('Attempt to resolve address \'{0}\' failed. Invalid or unresolveable address').format(lookup)
        raise SaltSystemExit(code=42, msg=err)
    except socket.error:
        error = True

    if error:
        err = ('DNS lookup or connection check of \'{0}\' failed.').format(addr)
        if safe:
            if salt.log.is_console_configured():
                # If logging is not configured it also means that either
                # the master or minion instance calling this hasn't even
                # started running
                log.error(err)
            raise SaltClientError()
        raise SaltSystemExit(code=42, msg=err)
    return resolved"
install_toolset;"def install_toolset(self, toolset):
        '''
        Installs specific toolset on CI system.
        '''
        info = toolset_info[toolset]
        if sys.platform.startswith('linux'):
            os.chdir(self.work_dir)
            if 'ppa' in info:
                for ppa in info['ppa']:
                    utils.check_call(
                        'sudo','add-apt-repository','--yes',ppa)
            if 'deb' in info:
                utils.make_file('sources.list',
                    ""deb %s""%(' '.join(info['deb'])),
                    ""deb-src %s""%(' '.join(info['deb'])))
                utils.check_call('sudo','bash','-c','cat sources.list >> /etc/apt/sources.list')
            if 'apt-key' in info:
                for key in info['apt-key']:
                    utils.check_call('wget',key,'-O','apt.key')
                    utils.check_call('sudo','apt-key','add','apt.key')
            utils.check_call(
                'sudo','apt-get','update','-qq')
            utils.check_call(
                'sudo','apt-get','install','-qq',info['package'])
            if 'debugpackage' in info and info['debugpackage']:
                utils.check_call(
                    'sudo','apt-get','install','-qq',info['debugpackage'])"
unset_config_value;"def unset_config_value(self, name, quiet=False):
        """"""unset a configuration value
            Parameters
           ==========
           name: the name of the value to unset (remove key in dictionary)
           quiet: disable verbose output if True (default is False)
        """"""

        config_data = self._read_config_file()

        if name in config_data:

            del config_data[name]

            self._write_config_file(config_data)

            if not quiet:
                self.print_config_value(name, separator=' is now set to: ')"
check_master;"def check_master(self):
        '''
        Log if the master is not running

        :rtype: bool
        :return: Whether or not the master is running
        '''
        if not os.path.exists(
                os.path.join(
                    self.opts['sock_dir'],
                    'publish_pull.ipc'
                    )
                ):
            return False
        return True"
get_focus_widget_properties;"def get_focus_widget_properties(self):
        """"""Get properties of focus widget
        Returns tuple (widget, properties) where properties is a tuple of
        booleans: (is_console, not_readonly, readwrite_editor)""""""
        from spyder.plugins.editor.widgets.editor import TextEditBaseWidget
        from spyder.plugins.ipythonconsole.widgets import ControlWidget
        widget = QApplication.focusWidget()

        textedit_properties = None
        if isinstance(widget, (TextEditBaseWidget, ControlWidget)):
            console = isinstance(widget, ControlWidget)
            not_readonly = not widget.isReadOnly()
            readwrite_editor = not_readonly and not console
            textedit_properties = (console, not_readonly, readwrite_editor)
        return widget, textedit_properties"
save_metadata;"def save_metadata(self, data_dir, feature_name=None):
    """"""See base class for details.""""""
    # Save names if defined
    if self._str2int is not None:
      names_filepath = _get_names_filepath(data_dir, feature_name)
      _write_names_to_file(names_filepath, self.names)"
add_button_box;"def add_button_box(self, stdbtns):
        """"""Create dialog button box and add it to the dialog layout""""""
        bbox = QDialogButtonBox(stdbtns)
        run_btn = bbox.addButton(_(""Run""), QDialogButtonBox.AcceptRole)
        run_btn.clicked.connect(self.run_btn_clicked)
        bbox.accepted.connect(self.accept)
        bbox.rejected.connect(self.reject)
        btnlayout = QHBoxLayout()
        btnlayout.addStretch(1)
        btnlayout.addWidget(bbox)
        self.layout().addLayout(btnlayout)"
status_charge;"def status_charge():
    '''
    Return battery charge

    CLI Example:

    .. code-block:: bash

        salt '*' apcups.status_charge
    '''
    data = status()
    if 'BCHARGE' in data:
        charge = data['BCHARGE'].split()
        if charge[1].lower() == 'percent':
            return float(charge[0])

    return {'Error': 'Load not available.'}"
apply_plugin_settings;"def apply_plugin_settings(self, options):
        """"""Apply configuration file's plugin settings""""""
        if self.editorstacks is not None:
            # --- syntax highlight and text rendering settings
            color_scheme_n = 'color_scheme_name'
            color_scheme_o = self.get_color_scheme()
            currentline_n = 'highlight_current_line'
            currentline_o = self.get_option(currentline_n)
            currentcell_n = 'highlight_current_cell'
            currentcell_o = self.get_option(currentcell_n)
            occurrence_n = 'occurrence_highlighting'
            occurrence_o = self.get_option(occurrence_n)
            occurrence_timeout_n = 'occurrence_highlighting/timeout'
            occurrence_timeout_o = self.get_option(occurrence_timeout_n)
            focus_to_editor_n = 'focus_to_editor'
            focus_to_editor_o = self.get_option(focus_to_editor_n)

            for editorstack in self.editorstacks:
                if color_scheme_n in options:
                    editorstack.set_color_scheme(color_scheme_o)
                if currentline_n in options:
                    editorstack.set_highlight_current_line_enabled(
                                                                currentline_o)
                if currentcell_n in options:
                    editorstack.set_highlight_current_cell_enabled(
                                                                currentcell_o)
                if occurrence_n in options:
                    editorstack.set_occurrence_highlighting_enabled(occurrence_o)
                if occurrence_timeout_n in options:
                    editorstack.set_occurrence_highlighting_timeout(
                                                           occurrence_timeout_o)
                if focus_to_editor_n in options:
                    editorstack.set_focus_to_editor(focus_to_editor_o)

            # --- everything else
            tabbar_n = 'show_tab_bar'
            tabbar_o = self.get_option(tabbar_n)
            classfuncdropdown_n = 'show_class_func_dropdown'
            classfuncdropdown_o = self.get_option(classfuncdropdown_n)
            linenb_n = 'line_numbers'
            linenb_o = self.get_option(linenb_n)
            blanks_n = 'blank_spaces'
            blanks_o = self.get_option(blanks_n)
            scrollpastend_n = 'scroll_past_end'
            scrollpastend_o = self.get_option(scrollpastend_n)
            edgeline_n = 'edge_line'
            edgeline_o = self.get_option(edgeline_n)
            edgelinecols_n = 'edge_line_columns'
            edgelinecols_o = self.get_option(edgelinecols_n)
            wrap_n = 'wrap'
            wrap_o = self.get_option(wrap_n)
            indentguides_n = 'indent_guides'
            indentguides_o = self.get_option(indentguides_n)
            tabindent_n = 'tab_always_indent'
            tabindent_o = self.get_option(tabindent_n)
            ibackspace_n = 'intelligent_backspace'
            ibackspace_o = self.get_option(ibackspace_n)
            removetrail_n = 'always_remove_trailing_spaces'
            removetrail_o = self.get_option(removetrail_n)
            converteol_n = 'convert_eol_on_save'
            converteol_o = self.get_option(converteol_n)
            converteolto_n = 'convert_eol_on_save_to'
            converteolto_o = self.get_option(converteolto_n)
            runcellcopy_n = 'run_cell_copy'
            runcellcopy_o = self.get_option(runcellcopy_n)
            closepar_n = 'close_parentheses'
            closepar_o = self.get_option(closepar_n)
            close_quotes_n = 'close_quotes'
            close_quotes_o = self.get_option(close_quotes_n)
            add_colons_n = 'add_colons'
            add_colons_o = self.get_option(add_colons_n)
            autounindent_n = 'auto_unindent'
            autounindent_o = self.get_option(autounindent_n)
            indent_chars_n = 'indent_chars'
            indent_chars_o = self.get_option(indent_chars_n)
            tab_stop_width_spaces_n = 'tab_stop_width_spaces'
            tab_stop_width_spaces_o = self.get_option(tab_stop_width_spaces_n)
            help_n = 'connect_to_oi'
            help_o = CONF.get('help', 'connect/editor')
            todo_n = 'todo_list'
            todo_o = self.get_option(todo_n)

            finfo = self.get_current_finfo()


            for editorstack in self.editorstacks:
                if tabbar_n in options:
                    editorstack.set_tabbar_visible(tabbar_o)
                if linenb_n in options:
                    editorstack.set_linenumbers_enabled(linenb_o,
                                                        current_finfo=finfo)
                if edgeline_n in options:
                    editorstack.set_edgeline_enabled(edgeline_o)
                if edgelinecols_n in options:
                    editorstack.set_edgeline_columns(edgelinecols_o)
                if wrap_n in options:
                    editorstack.set_wrap_enabled(wrap_o)
                if tabindent_n in options:
                    editorstack.set_tabmode_enabled(tabindent_o)
                if ibackspace_n in options:
                    editorstack.set_intelligent_backspace_enabled(ibackspace_o)
                if removetrail_n in options:
                    editorstack.set_always_remove_trailing_spaces(removetrail_o)
                if converteol_n in options:
                    editorstack.set_convert_eol_on_save(converteol_o)
                if converteolto_n in options:
                    editorstack.set_convert_eol_on_save_to(converteolto_o)
                if runcellcopy_n in options:
                    editorstack.set_run_cell_copy(runcellcopy_o)
                if closepar_n in options:
                    editorstack.set_close_parentheses_enabled(closepar_o)
                if close_quotes_n in options:
                    editorstack.set_close_quotes_enabled(close_quotes_o)
                if add_colons_n in options:
                    editorstack.set_add_colons_enabled(add_colons_o)
                if autounindent_n in options:
                    editorstack.set_auto_unindent_enabled(autounindent_o)
                if indent_chars_n in options:
                    editorstack.set_indent_chars(indent_chars_o)
                if tab_stop_width_spaces_n in options:
                    editorstack.set_tab_stop_width_spaces(tab_stop_width_spaces_o)
                if help_n in options:
                    editorstack.set_help_enabled(help_o)
                if todo_n in options:
                    editorstack.set_todolist_enabled(todo_o,
                                                     current_finfo=finfo)

            for name, action in self.checkable_actions.items():
                if name in options:
                    state = self.get_option(name)
                    action.setChecked(state)
                    action.trigger()

            # Multiply by 1000 to convert seconds to milliseconds
            self.autosave.interval = (
                    self.get_option('autosave_interval') * 1000)
            self.autosave.enabled = self.get_option('autosave_enabled')

            # We must update the current editor after the others:
            # (otherwise, code analysis buttons state would correspond to the
            #  last editor instead of showing the one of the current editor)
            if finfo is not None:
                # TODO: Connect this to the LSP
                if todo_n in options and todo_o:
                    finfo.run_todo_finder()"
delete_collection_namespaced_job;"def delete_collection_namespaced_job(self, namespace, **kwargs):
        """"""
        delete collection of Job
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_collection_namespaced_job(namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_collection_namespaced_job_with_http_info(namespace, **kwargs)
        else:
            (data) = self.delete_collection_namespaced_job_with_http_info(namespace, **kwargs)
            return data"
parsed_pipfile;"def parsed_pipfile(self):
        """"""Parse Pipfile into a TOMLFile and cache it

        (call clear_pipfile_cache() afterwards if mutating)""""""
        contents = self.read_pipfile()
        # use full contents to get around str/bytes 2/3 issues
        cache_key = (self.pipfile_location, contents)
        if cache_key not in _pipfile_cache:
            parsed = self._parse_pipfile(contents)
            _pipfile_cache[cache_key] = parsed
        return _pipfile_cache[cache_key]"
rename_file;"def rename_file(self, fname):
        """"""Rename file""""""
        path, valid = QInputDialog.getText(self, _('Rename'),
                              _('New name:'), QLineEdit.Normal,
                              osp.basename(fname))
        if valid:
            path = osp.join(osp.dirname(fname), to_text_string(path))
            if path == fname:
                return
            if osp.exists(path):
                if QMessageBox.warning(self, _(""Rename""),
                         _(""Do you really want to rename <b>%s</b> and ""
                           ""overwrite the existing file <b>%s</b>?""
                           ) % (osp.basename(fname), osp.basename(path)),
                         QMessageBox.Yes|QMessageBox.No) == QMessageBox.No:
                    return
            try:
                misc.rename_file(fname, path)
                if osp.isfile(fname):
                    self.sig_renamed.emit(fname, path)
                else:
                    self.sig_renamed_tree.emit(fname, path)
                return path
            except EnvironmentError as error:
                QMessageBox.critical(self, _(""Rename""),
                            _(""<b>Unable to rename file <i>%s</i></b>""
                              ""<br><br>Error message:<br>%s""
                              ) % (osp.basename(fname), to_text_string(error)))"
multilevel_roi_align;"def multilevel_roi_align(features, rcnn_boxes, resolution):
    """"""
    Args:
        features ([tf.Tensor]): 4 FPN feature level 2-5
        rcnn_boxes (tf.Tensor): nx4 boxes
        resolution (int): output spatial resolution
    Returns:
        NxC x res x res
    """"""
    assert len(features) == 4, features
    # Reassign rcnn_boxes to levels
    level_ids, level_boxes = fpn_map_rois_to_levels(rcnn_boxes)
    all_rois = []

    # Crop patches from corresponding levels
    for i, boxes, featuremap in zip(itertools.count(), level_boxes, features):
        with tf.name_scope('roi_level{}'.format(i + 2)):
            boxes_on_featuremap = boxes * (1.0 / cfg.FPN.ANCHOR_STRIDES[i])
            all_rois.append(roi_align(featuremap, boxes_on_featuremap, resolution))

    # this can fail if using TF<=1.8 with MKL build
    all_rois = tf.concat(all_rois, axis=0)  # NCHW
    # Unshuffle to the original order, to match the original samples
    level_id_perm = tf.concat(level_ids, axis=0)  # A permutation of 1~N
    level_id_invert_perm = tf.invert_permutation(level_id_perm)
    all_rois = tf.gather(all_rois, level_id_invert_perm)
    return all_rois"
kernels_initialize;"def kernels_initialize(self, folder):
        """""" create a new kernel in a specified folder from template, including
            json metadata that grabs values from the configuration.
             Parameters
            ==========
            folder: the path of the folder
        """"""
        if not os.path.isdir(folder):
            raise ValueError('Invalid folder: ' + folder)

        resources = []
        resource = {'path': 'INSERT_SCRIPT_PATH_HERE'}
        resources.append(resource)

        username = self.get_config_value(self.CONFIG_NAME_USER)
        meta_data = {
            'id': username + '/INSERT_KERNEL_SLUG_HERE',
            'title': 'INSERT_TITLE_HERE',
            'code_file': 'INSERT_CODE_FILE_PATH_HERE',
            'language': 'INSERT_LANGUAGE_HERE',
            'kernel_type': 'INSERT_KERNEL_TYPE_HERE',
            'is_private': 'true',
            'enable_gpu': 'false',
            'enable_internet': 'false',
            'dataset_sources': [],
            'competition_sources': [],
            'kernel_sources': [],
        }
        meta_file = os.path.join(folder, self.KERNEL_METADATA_FILE)
        with open(meta_file, 'w') as f:
            json.dump(meta_data, f, indent=2)

        return meta_file"
get_img_data;"def get_img_data(f, maxsize = (1200, 850), first = False):
    """"""Generate image data using PIL
    """"""
    img = Image.open(f)
    img.thumbnail(maxsize)
    if first:                     # tkinter is inactive the first time
        bio = io.BytesIO()
        img.save(bio, format = ""PNG"")
        del img
        return bio.getvalue()
    return ImageTk.PhotoImage(img)"
shutdown_abort;"def shutdown_abort():
    '''
    Abort a shutdown. Only available while the dialog box is being
    displayed to the user. Once the shutdown has initiated, it cannot be
    aborted.

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    CLI Example:

    .. code-block:: bash

        salt 'minion-id' system.shutdown_abort
    '''
    try:
        win32api.AbortSystemShutdown('127.0.0.1')
        return True
    except pywintypes.error as exc:
        (number, context, message) = exc.args
        log.error('Failed to abort system shutdown')
        log.error('nbr: %s', number)
        log.error('ctx: %s', context)
        log.error('msg: %s', message)
        return False"
get_available_translations;"def get_available_translations():
    """"""
    List available translations for spyder based on the folders found in the
    locale folder. This function checks if LANGUAGE_CODES contain the same
    information that is found in the 'locale' folder to ensure that when a new
    language is added, LANGUAGE_CODES is updated.
    """"""
    locale_path = get_module_data_path(""spyder"", relpath=""locale"",
                                       attr_name='LOCALEPATH')
    listdir = os.listdir(locale_path)
    langs = [d for d in listdir if osp.isdir(osp.join(locale_path, d))]
    langs = [DEFAULT_LANGUAGE] + langs

    # Remove disabled languages
    langs = list( set(langs) - set(DISABLED_LANGUAGES) )

    # Check that there is a language code available in case a new translation
    # is added, to ensure LANGUAGE_CODES is updated.
    for lang in langs:
        if lang not in LANGUAGE_CODES:
            error = ('Update LANGUAGE_CODES (inside config/base.py) if a new '
                     'translation has been added to Spyder')
            print(error)  # spyder: test-skip
            return ['en']
    return langs"
option_group_exists;"def option_group_exists(name, tags=None, region=None, key=None, keyid=None,
                        profile=None):
    '''
    Check to see if an RDS option group exists.

    CLI example::

        salt myminion boto_rds.option_group_exists myoptiongr region=us-east-1
    '''
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)

    try:
        rds = conn.describe_option_groups(OptionGroupName=name)
        return {'exists': bool(rds)}
    except ClientError as e:
        return {'error': __utils__['boto3.get_error'](e)}"
write_entries;"def write_entries(self, entries, logger_name=None, resource=None, labels=None):
        """"""API call:  log an entry resource via a POST request

        See
        https://cloud.google.com/logging/docs/reference/v2/rest/v2/entries/write

        :type entries: sequence of mapping
        :param entries: the log entry resources to log.

        :type logger_name: str
        :param logger_name: name of default logger to which to log the entries;
                            individual entries may override.

        :type resource: mapping
        :param resource: default resource to associate with entries;
                         individual entries may override.

        :type labels: mapping
        :param labels: default labels to associate with entries;
                       individual entries may override.
        """"""
        data = {""entries"": list(entries)}

        if logger_name is not None:
            data[""logName""] = logger_name

        if resource is not None:
            data[""resource""] = resource

        if labels is not None:
            data[""labels""] = labels

        self.api_request(method=""POST"", path=""/entries:write"", data=data)"
create_route_table;"def create_route_table(vpc_id=None, vpc_name=None, route_table_name=None,
                       tags=None, region=None, key=None, keyid=None, profile=None):
    '''
    Creates a route table.

    .. versionchanged:: 2015.8.0
        Added vpc_name argument

    CLI Examples:

    .. code-block:: bash

        salt myminion boto_vpc.create_route_table vpc_id='vpc-6b1fe402' \\
                route_table_name='myroutetable'
        salt myminion boto_vpc.create_route_table vpc_name='myvpc' \\
                route_table_name='myroutetable'
    '''
    vpc_id = check_vpc(vpc_id, vpc_name, region, key, keyid, profile)
    if not vpc_id:
        return {'created': False, 'error': {'message': 'VPC {0} does not exist.'.format(vpc_name or vpc_id)}}

    return _create_resource('route_table', route_table_name, tags=tags,
                            vpc_id=vpc_id, region=region, key=key,
                            keyid=keyid, profile=profile)"
find_source;"def find_source(self, source):
        """"""
        Given a source, find it.

        source can be a url or an index name.
        """"""
        if not is_valid_url(source):
            try:
                source = self.get_source(name=source)
            except SourceNotFound:
                source = self.get_source(url=source)
        else:
            source = self.get_source(url=source)
        return source"
convert_upsample;"def convert_upsample(builder, layer, input_names, output_names, keras_layer):
    """"""Convert convolution layer from keras to coreml.

    Parameters
    ----------
    keras_layer: layer
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_name, output_name = (input_names[0], output_names[0])

    if isinstance(keras_layer, keras.layers.convolutional.UpSampling1D):
        fh, fw = 1, keras_layer.length
    else: # 2D
        fh, fw = keras_layer.size

    builder.add_upsample(name = layer,
             scaling_factor_h = fh,
             scaling_factor_w = fw,
             input_name = input_name,
             output_name = output_name)"
generate_multiple_parameters;"def generate_multiple_parameters(self, parameter_id_list):
        """"""Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.
        Call 'generate_parameters()' by 'count' times by default.
        User code must override either this function or 'generate_parameters()'.
        If there's no more trial, user should raise nni.NoMoreTrialError exception in generate_parameters().
        If so, this function will only return sets of trial (hyper-)parameters that have already been collected.
        parameter_id_list: list of int
        """"""
        result = []
        for parameter_id in parameter_id_list:
            try:
                _logger.debug(""generating param for {}"".format(parameter_id))
                res = self.generate_parameters(parameter_id)
            except nni.NoMoreTrialError:
                return result
            result.append(res)
        return result"
select_directory;"def select_directory(self, edit):
        """"""Select directory""""""
        basedir = to_text_string(edit.text())
        if not osp.isdir(basedir):
            basedir = getcwd_or_home()
        title = _(""Select directory"")
        directory = getexistingdirectory(self, title, basedir)
        if directory:
            edit.setText(directory)"
list_sinks;"def list_sinks(self, page_size=None, page_token=None):
        """"""List sinks for the project associated with this client.

        See
        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/list

        :type page_size: int
        :param page_size:
            Optional. The maximum number of sinks in each page of results from
            this request. Non-positive values are ignored. Defaults to a
            sensible value set by the API.

        :type page_token: str
        :param page_token:
            Optional. If present, return the next batch of sinks, using the
            value, which must correspond to the ``nextPageToken`` value
            returned in the previous response.  Deprecated: use the ``pages``
            property of the returned iterator instead of manually passing the
            token.

        :rtype: :class:`~google.api_core.page_iterator.Iterator`
        :returns: Iterator of
                  :class:`~google.cloud.logging.sink.Sink`
                  accessible to the current client.
        """"""
        return self.sinks_api.list_sinks(self.project, page_size, page_token)"
has_data_for_dates;"def has_data_for_dates(series_or_df, first_date, last_date):
    """"""
    Does `series_or_df` have data on or before first_date and on or after
    last_date?
    """"""
    dts = series_or_df.index
    if not isinstance(dts, pd.DatetimeIndex):
        raise TypeError(""Expected a DatetimeIndex, but got %s."" % type(dts))
    first, last = dts[[0, -1]]
    return (first <= first_date) and (last >= last_date)"
select_header_content_type;"def select_header_content_type(self, content_types):
        """"""
        Returns `Content-Type` based on an array of content_types provided.

        :param content_types: List of content-types.
        :return: Content-Type (e.g. application/json).
        """"""
        if not content_types:
            return 'application/json'

        content_types = [x.lower() for x in content_types]

        if 'application/json' in content_types or '*/*' in content_types:
            return 'application/json'
        else:
            return content_types[0]"
get_jid;"def get_jid(jid):
    '''
    Return the information returned when the specified job id was executed
    '''
    serv = _get_serv(ret=None)

    sql = ""select id, full_ret from returns where jid = '{0}'"".format(jid)

    data = serv.query(sql)
    ret = {}
    if data:
        points = data[0]['points']
        for point in points:
            ret[point[3]] = salt.utils.json.loads(point[2])

    return ret"
print_class_histogram;"def print_class_histogram(roidbs):
    """"""
    Args:
        roidbs (list[dict]): the same format as the output of `load_training_roidbs`.
    """"""
    dataset = DetectionDataset()
    hist_bins = np.arange(dataset.num_classes + 1)

    # Histogram of ground-truth objects
    gt_hist = np.zeros((dataset.num_classes,), dtype=np.int)
    for entry in roidbs:
        # filter crowd?
        gt_inds = np.where(
            (entry['class'] > 0) & (entry['is_crowd'] == 0))[0]
        gt_classes = entry['class'][gt_inds]
        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]
    data = [[dataset.class_names[i], v] for i, v in enumerate(gt_hist)]
    data.append(['total', sum(x[1] for x in data)])
    # the first line is BG
    table = tabulate(data[1:], headers=['class', '#box'], tablefmt='pipe')
    logger.info(""Ground-Truth Boxes:\n"" + colored(table, 'cyan'))"
handle_minute_close;"def handle_minute_close(self, dt, data_portal):
        """"""
        Handles the close of the given minute in minute emission.

        Parameters
        ----------
        dt : Timestamp
            The minute that is ending

        Returns
        -------
        A minute perf packet.
        """"""
        self.sync_last_sale_prices(dt, data_portal)

        packet = {
            'period_start': self._first_session,
            'period_end': self._last_session,
            'capital_base': self._capital_base,
            'minute_perf': {
                'period_open': self._market_open,
                'period_close': dt,
            },
            'cumulative_perf': {
                'period_open': self._first_session,
                'period_close': self._last_session,
            },
            'progress': self._progress(self),
            'cumulative_risk_metrics': {},
        }
        ledger = self._ledger
        ledger.end_of_bar(self._session_count)
        self.end_of_bar(
            packet,
            ledger,
            dt,
            self._session_count,
            data_portal,
        )
        return packet"
set_current_client_working_directory;"def set_current_client_working_directory(self, directory):
        """"""Set current client working directory.""""""
        shellwidget = self.get_current_shellwidget()
        if shellwidget is not None:
            shellwidget.set_cwd(directory)"
random_normal;"def random_normal(attrs, inputs, proto_obj):
    """"""Draw random samples from a Gaussian distribution.""""""
    try:
        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE
    except ImportError:
        raise ImportError(""Onnx and protobuf need to be installed. ""
                          ""Instructions to install - https://github.com/onnx/onnx"")
    new_attr = translation_utils._remove_attributes(attrs, ['seed'])
    new_attr = translation_utils._fix_attribute_names(new_attr, {'mean': 'loc'})
    new_attr['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attr.get('dtype', 1))]
    return 'random_normal', new_attr, inputs"
run_activate_this;"def run_activate_this(self):
        """"""Runs the environment's inline activation script""""""
        if self.is_venv:
            activate_this = os.path.join(self.scripts_dir, ""activate_this.py"")
            if not os.path.isfile(activate_this):
                raise OSError(""No such file: {0!s}"".format(activate_this))
            with open(activate_this, ""r"") as f:
                code = compile(f.read(), activate_this, ""exec"")
                exec(code, dict(__file__=activate_this))"
order_shares;"def order_shares(id_or_ins, amount, price=None, style=None):
    """"""
    落指定股数的买/卖单，最常见的落单方式之一。如有需要落单类型当做一个参量传入，如果忽略掉落单类型，那么默认是市价单（market order）。

    :param id_or_ins: 下单标的物
    :type id_or_ins: :class:`~Instrument` object | `str`

    :param int amount: 下单量, 正数代表买入，负数代表卖出。将会根据一手xx股来向下调整到一手的倍数，比如中国A股就是调整成100股的倍数。

    :param float price: 下单价格，默认为None，表示 :class:`~MarketOrder`, 此参数主要用于简化 `style` 参数。

    :param style: 下单类型, 默认是市价单。目前支持的订单类型有 :class:`~LimitOrder` 和 :class:`~MarketOrder`
    :type style: `OrderStyle` object

    :return: :class:`~Order` object | None

    :example:

    .. code-block:: python

        #购买Buy 2000 股的平安银行股票，并以市价单发送：
        order_shares('000001.XSHE', 2000)
        #卖出2000股的平安银行股票，并以市价单发送：
        order_shares('000001.XSHE', -2000)
        #购买1000股的平安银行股票，并以限价单发送，价格为￥10：
        order_shares('000001.XSHG', 1000, style=LimitOrder(10))
    """"""
    if amount == 0:
        # 如果下单量为0，则认为其并没有发单，则直接返回None
        user_system_log.warn(_(u""Order Creation Failed: Order amount is 0.""))
        return None
    style = cal_style(price, style)
    if isinstance(style, LimitOrder):
        if style.get_limit_price() <= 0:
            raise RQInvalidArgument(_(u""Limit order price should be positive""))
    order_book_id = assure_stock_order_book_id(id_or_ins)
    env = Environment.get_instance()

    price = env.get_last_price(order_book_id)
    if not is_valid_price(price):
        user_system_log.warn(
            _(u""Order Creation Failed: [{order_book_id}] No market data"").format(order_book_id=order_book_id))
        return

    if amount > 0:
        side = SIDE.BUY
        position_effect = POSITION_EFFECT.OPEN
    else:
        amount = abs(amount)
        side = SIDE.SELL
        position_effect = POSITION_EFFECT.CLOSE

    if side == SIDE.BUY:
        # 卖出不再限制 round_lot, order_shares 不再依赖 portfolio
        round_lot = int(env.get_instrument(order_book_id).round_lot)
        try:
            amount = int(Decimal(amount) / Decimal(round_lot)) * round_lot
        except ValueError:
            amount = 0

    r_order = Order.__from_create__(order_book_id, amount, side, style, position_effect)

    if amount == 0:
        # 如果计算出来的下单量为0, 则不生成Order, 直接返回None
        # 因为很多策略会直接在handle_bar里面执行order_target_percent之类的函数，经常会出现下一个量为0的订单，如果这些订单都生成是没有意义的。
        user_system_log.warn(_(u""Order Creation Failed: 0 order quantity""))
        return
    if r_order.type == ORDER_TYPE.MARKET:
        r_order.set_frozen_price(price)
    if env.can_submit_order(r_order):
        env.broker.submit_order(r_order)
        return r_order"
split_args;"def split_args(line):
    """"""Version of shlex.split that silently accept incomplete strings.

    Parameters
    ----------
    line : str
        The string to split

    Returns
    -------
    [str]
        The line split in separated arguments
    """"""
    lex = shlex.shlex(line, posix=True)
    lex.whitespace_split = True
    lex.commenters = ''
    res = []
    try:
        while True:
            res.append(next(lex))
    except ValueError:  # No closing quotation
        pass
    except StopIteration:  # End of loop
        pass
    if lex.token:
        res.append(lex.token)
    return res"
list_arguments;"def list_arguments(self):
        """"""Lists all the arguments in the symbol.

        Example
        -------
        >>> a = mx.sym.var('a')
        >>> b = mx.sym.var('b')
        >>> c = a + b
        >>> c.list_arguments
        ['a', 'b']

        Returns
        -------
        args : list of string
            List containing the names of all the arguments required to compute the symbol.
        """"""
        size = ctypes.c_uint()
        sarr = ctypes.POINTER(ctypes.c_char_p)()
        check_call(_LIB.MXSymbolListArguments(
            self.handle, ctypes.byref(size), ctypes.byref(sarr)))
        return [py_str(sarr[i]) for i in range(size.value)]"
gen_stack_patches;"def gen_stack_patches(patch_list,
                      nr_row=None, nr_col=None, border=None,
                      max_width=1000, max_height=1000,
                      bgcolor=255, viz=False, lclick_cb=None):
    """"""
    Similar to :func:`stack_patches` but with a generator interface.
    It takes a much-longer list and yields stacked results one by one.
    For example, if ``patch_list`` contains 1000 images and ``nr_row==nr_col==10``,
    this generator yields 10 stacked images.

    Args:
        nr_row(int), nr_col(int): rows and cols of each result.
        max_width(int), max_height(int): Maximum allowed size of the
            stacked image. If ``nr_row/nr_col`` are None, this number
            will be used to infer the rows and cols. Otherwise the option is
            ignored.
        patch_list, border, viz, lclick_cb: same as in :func:`stack_patches`.

    Yields:
        np.ndarray: the stacked image.
    """"""
    # setup parameters
    patch_list = _preprocess_patch_list(patch_list)
    if lclick_cb is not None:
        viz = True
    ph, pw = patch_list.shape[1:3]

    if border is None:
        border = int(0.05 * min(ph, pw))
    if nr_row is None:
        nr_row = int(max_height / (ph + border))
    if nr_col is None:
        nr_col = int(max_width / (pw + border))
    canvas = Canvas(ph, pw, nr_row, nr_col, patch_list.shape[-1], border, bgcolor)

    nr_patch = nr_row * nr_col
    start = 0

    if lclick_cb is not None:
        def lclick_callback(img, x, y):
            idx = canvas.get_patchid_from_coord(x, y)
            idx = idx + start
            if idx < end:
                lclick_cb(patch_list[idx], idx)
    else:
        lclick_callback = None

    while True:
        end = start + nr_patch
        cur_list = patch_list[start:end]
        if not len(cur_list):
            return
        canvas.draw_patches(cur_list)
        if viz:
            interactive_imshow(canvas.canvas, lclick_cb=lclick_callback)
        yield canvas.canvas
        start = end"
schema_get;"def schema_get(name, profile=None):
    '''
    Known valid names of schemas are:
      - image
      - images
      - member
      - members

    CLI Example:

    .. code-block:: bash

        salt '*' glance.schema_get name=f16-jeos
    '''
    g_client = _auth(profile)
    schema_props = {}
    for prop in g_client.schemas.get(name).properties:
        schema_props[prop.name] = prop.description
    log.debug(
        'Properties of schema %s:\n%s',
        name, pprint.PrettyPrinter(indent=4).pformat(schema_props)
    )
    return {name: schema_props}"
attach_cluster;"def attach_cluster(config_file, start, use_tmux, override_cluster_name, new):
    """"""Attaches to a screen for the specified cluster.

    Arguments:
        config_file: path to the cluster yaml
        start: whether to start the cluster if it isn't up
        use_tmux: whether to use tmux as multiplexer
        override_cluster_name: set the name of the cluster
        new: whether to force a new screen
    """"""

    if use_tmux:
        if new:
            cmd = ""tmux new""
        else:
            cmd = ""tmux attach || tmux new""
    else:
        if new:
            cmd = ""screen -L""
        else:
            cmd = ""screen -L -xRR""

    exec_cluster(config_file, cmd, False, False, False, False, start,
                 override_cluster_name, None)"
should_stop;"def should_stop(self, result):
        """"""Whether the given result meets this trial's stopping criteria.""""""

        if result.get(DONE):
            return True

        for criteria, stop_value in self.stopping_criterion.items():
            if criteria not in result:
                raise TuneError(
                    ""Stopping criteria {} not provided in result {}."".format(
                        criteria, result))
            if result[criteria] >= stop_value:
                return True

        return False"
choice_complete;"def choice_complete(self, ctx, incomplete):
    """"""Returns the completion results for click.core.Choice

    Parameters
    ----------
    ctx : click.core.Context
        The current context
    incomplete :
        The string to complete

    Returns
    -------
    [(str, str)]
        A list of completion results
    """"""
    return [
        (c, None) for c in self.choices
        if completion_configuration.match_incomplete(c, incomplete)
    ]"
cmd_async;"def cmd_async(self, low):
        '''
        Execute a runner function asynchronously; eauth is respected

        This function requires that :conf_master:`external_auth` is configured
        and the user is authorized to execute runner functions: (``@runner``).

        .. code-block:: python

            runner.eauth_async({
                'fun': 'jobs.list_jobs',
                'username': 'saltdev',
                'password': 'saltdev',
                'eauth': 'pam',
            })
        '''
        reformatted_low = self._reformat_low(low)

        return mixins.AsyncClientMixin.cmd_async(self, reformatted_low)"
save_data;"def save_data(self, filename=None):
        """"""Save data""""""
        if filename is None:
            filename = self.filename
            if filename is None:
                filename = getcwd_or_home()
            filename, _selfilter = getsavefilename(self, _(""Save data""),
                                                   filename,
                                                   iofunctions.save_filters)
            if filename:
                self.filename = filename
            else:
                return False
        QApplication.setOverrideCursor(QCursor(Qt.WaitCursor))
        QApplication.processEvents()

        error_message = self.shellwidget.save_namespace(self.filename)
        self.shellwidget._kernel_reply = None

        QApplication.restoreOverrideCursor()
        QApplication.processEvents()
        if error_message is not None:
            if 'Some objects could not be saved:' in error_message:
                save_data_message = (
                    _('<b>Some objects could not be saved:</b>')
                    + '<br><br><code>{obj_list}</code>'.format(
                        obj_list=error_message.split(': ')[1]))
            else:
                save_data_message = _(
                    '<b>Unable to save current workspace</b>'
                    '<br><br>Error message:<br>') + error_message
            QMessageBox.critical(self, _(""Save data""), save_data_message)
        self.save_button.setEnabled(self.filename is not None)"
patch_namespaced_endpoints;"def patch_namespaced_endpoints(self, name, namespace, body, **kwargs):
        """"""
        partially update the specified Endpoints
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.patch_namespaced_endpoints(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Endpoints (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param object body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).
        :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.
        :return: V1Endpoints
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.patch_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.patch_namespaced_endpoints_with_http_info(name, namespace, body, **kwargs)
            return data"
create_dataset;"def create_dataset(
        self,
        parent,
        dataset,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates a dataset.

        Example:
            >>> from google.cloud import automl_v1beta1
            >>>
            >>> client = automl_v1beta1.AutoMlClient()
            >>>
            >>> parent = client.location_path('[PROJECT]', '[LOCATION]')
            >>>
            >>> # TODO: Initialize `dataset`:
            >>> dataset = {}
            >>>
            >>> response = client.create_dataset(parent, dataset)

        Args:
            parent (str): The resource name of the project to create the dataset for.
            dataset (Union[dict, ~google.cloud.automl_v1beta1.types.Dataset]): The dataset to create.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.automl_v1beta1.types.Dataset`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.automl_v1beta1.types.Dataset` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_dataset"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_dataset""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_dataset,
                default_retry=self._method_configs[""CreateDataset""].retry,
                default_timeout=self._method_configs[""CreateDataset""].timeout,
                client_info=self._client_info,
            )

        request = service_pb2.CreateDatasetRequest(parent=parent, dataset=dataset)
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""create_dataset""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
list_traces;"def list_traces(
        self,
        project_id=None,
        view=None,
        page_size=None,
        start_time=None,
        end_time=None,
        filter_=None,
        order_by=None,
        page_token=None,
    ):
        """"""
        Returns of a list of traces that match the filter conditions.

        Args:
            project_id (Optional[str]): ID of the Cloud project where the trace
                data is stored.

            view (Optional[~google.cloud.trace_v1.gapic.enums.
                ListTracesRequest.ViewType]): Type of data returned for traces
                in the list. Default is ``MINIMAL``.

            page_size (Optional[int]): Maximum number of traces to return. If
                not specified or <= 0, the implementation selects a reasonable
                value. The implementation may return fewer traces than the
                requested page size.

            start_time (Optional[~datetime.datetime]): Start of the time
                interval (inclusive) during which the trace data was collected
                from the application.

            end_time (Optional[~datetime.datetime]): End of the time interval
                (inclusive) during which the trace data was collected from the
                application.

            filter_ (Optional[str]): An optional filter for the request.

            order_by (Optional[str]): Field used to sort the returned traces.

            page_token (Optional[str]): opaque marker for the next ""page"" of
                entries. If not passed, the API will return the first page of
                entries.

        Returns:
            A  :class:`~google.api_core.page_iterator.Iterator` of traces that
            match the specified filter conditions.
        """"""
        if project_id is None:
            project_id = self.project

        if start_time is not None:
            start_time = _datetime_to_pb_timestamp(start_time)

        if end_time is not None:
            end_time = _datetime_to_pb_timestamp(end_time)

        return self.trace_api.list_traces(
            project_id=project_id,
            view=view,
            page_size=page_size,
            start_time=start_time,
            end_time=end_time,
            filter_=filter_,
            order_by=order_by,
            page_token=page_token,
        )"
blade_idrac;"def blade_idrac(name, idrac_password=None, idrac_ipmi=None,
                idrac_ip=None, idrac_netmask=None, idrac_gateway=None,
                idrac_dnsname=None,
                idrac_dhcp=None):
    '''
    Set parameters for iDRAC in a blade.

    :param idrac_password: Password to use to connect to the iDRACs directly
        (idrac_ipmi and idrac_dnsname must be set directly on the iDRAC.  They
        can't be set through the CMC.  If this password is present, use it
        instead of the CMC password)
    :param idrac_ipmi: Enable/Disable IPMI over LAN
    :param idrac_ip: Set IP address for iDRAC
    :param idrac_netmask: Set netmask for iDRAC
    :param idrac_gateway: Set gateway for iDRAC
    :param idrac_dhcp: Turn on DHCP for iDRAC (True turns on, False does
        nothing becaause setting a static IP will disable DHCP).

    :return: A standard Salt changes dictionary

    NOTE: If any of the IP address settings is configured, all of ip, netmask,
    and gateway must be present
    '''

    ret = {'name': name,
           'result': True,
           'changes': {},
           'comment': ''}

    if not idrac_password:
        (username, password) = __salt__['chassis.chassis_credentials']()
    else:
        password = idrac_password

    module_network = __salt__['chassis.cmd']('network_info', module=name)
    current_idrac_ip = module_network['Network']['IP Address']

    if idrac_ipmi is not None:
        if idrac_ipmi is True or idrac_ipmi == 1:
            idrac_ipmi = '1'
        if idrac_ipmi is False or idrac_ipmi == 0:
            idrac_ipmi = '0'
        current_ipmi = __salt__['dracr.get_general']('cfgIpmiLan', 'cfgIpmiLanEnable',
                                                     host=current_idrac_ip, admin_username='root',
                                                     admin_password=password)

        if current_ipmi != idrac_ipmi:
            ch = {'Old': current_ipmi, 'New': idrac_ipmi}
            ret['changes']['IPMI'] = ch

    if idrac_dnsname is not None:
        dnsret = __salt__['dracr.get_dns_dracname'](host=current_idrac_ip,
                                                    admin_username='root',
                                                    admin_password=password)
        current_dnsname = dnsret['[Key=iDRAC.Embedded.1#NIC.1]']['DNSRacName']
        if current_dnsname != idrac_dnsname:
            ch = {'Old': current_dnsname,
                  'New': idrac_dnsname}
            ret['changes']['DNSRacName'] = ch

    if idrac_dhcp is not None or idrac_ip or idrac_netmask or idrac_gateway:
        if idrac_dhcp is True or idrac_dhcp == 1:
            idrac_dhcp = 1
        else:
            idrac_dhcp = 0
        if six.text_type(module_network['Network']['DHCP Enabled']) == '0' and idrac_dhcp == 1:
            ch = {'Old': module_network['Network']['DHCP Enabled'],
                  'New': idrac_dhcp}
            ret['changes']['DRAC DHCP'] = ch

        if idrac_dhcp == 0 and all([idrac_ip, idrac_netmask, idrac_netmask]):
            current_network = __salt__['chassis.cmd']('network_info',
                                                      module=name)
            old_ipv4 = {}
            new_ipv4 = {}
            if current_network['Network']['IP Address'] != idrac_ip:
                old_ipv4['ip'] = current_network['Network']['IP Address']
                new_ipv4['ip'] = idrac_ip
            if current_network['Network']['Subnet Mask'] != idrac_netmask:
                old_ipv4['netmask'] = current_network['Network']['Subnet Mask']
                new_ipv4['netmask'] = idrac_netmask
            if current_network['Network']['Gateway'] != idrac_gateway:
                old_ipv4['gateway'] = current_network['Network']['Gateway']
                new_ipv4['gateway'] = idrac_gateway

            if new_ipv4 != {}:
                ret['changes']['Network'] = {}
                ret['changes']['Network']['Old'] = old_ipv4
                ret['changes']['Network']['New'] = new_ipv4

    if ret['changes'] == {}:
        ret['comment'] = 'iDRAC on blade is already in the desired state.'
        return ret

    if __opts__['test'] and ret['changes'] != {}:
        ret['result'] = None
        ret['comment'] = 'iDRAC on blade will change.'
        return ret

    if 'IPMI' in ret['changes']:
        ipmi_result = __salt__['dracr.set_general']('cfgIpmiLan',
                                                    'cfgIpmiLanEnable',
                                                    idrac_ipmi,
                                                    host=current_idrac_ip,
                                                    admin_username='root',
                                                    admin_password=password)
        if not ipmi_result:
            ret['result'] = False
            ret['changes']['IPMI']['success'] = False

    if 'DNSRacName' in ret['changes']:
        dnsracname_result = __salt__['dracr.set_dns_dracname'](idrac_dnsname,
            host=current_idrac_ip,
            admin_username='root',
            admin_password=password)
        if dnsracname_result['retcode'] == 0:
            ret['changes']['DNSRacName']['success'] = True
        else:
            ret['result'] = False
            ret['changes']['DNSRacName']['success'] = False
            ret['changes']['DNSRacName']['return'] = dnsracname_result

    if 'DRAC DHCP' in ret['changes']:
        dhcp_result = __salt__['chassis.cmd']('set_niccfg', dhcp=idrac_dhcp)
        if dhcp_result['retcode']:
            ret['changes']['DRAC DHCP']['success'] = True
        else:
            ret['result'] = False
            ret['changes']['DRAC DHCP']['success'] = False
            ret['changes']['DRAC DHCP']['return'] = dhcp_result

    if 'Network' in ret['changes']:
        network_result = __salt__['chassis.cmd']('set_niccfg', ip=idrac_ip,
                                                 netmask=idrac_netmask,
                                                 gateway=idrac_gateway,
                                                 module=name)
        if network_result['retcode'] == 0:
            ret['changes']['Network']['success'] = True
        else:
            ret['result'] = False
            ret['changes']['Network']['success'] = False
            ret['changes']['Network']['return'] = network_result

    return ret"
set_geometry;"def set_geometry(self, crect):
        """"""Set geometry for floating panels.

        Normally you don't need to override this method, you should override
        `geometry` instead.
        """"""
        x0, y0, width, height = self.geometry()

        if width is None:
            width = crect.width()
        if height is None:
            height = crect.height()

        # Calculate editor coordinates with their offsets
        offset = self.editor.contentOffset()
        x = self.editor.blockBoundingGeometry(self.editor.firstVisibleBlock())\
            .translated(offset.x(), offset.y()).left() \
            + self.editor.document().documentMargin() \
            + self.editor.panels.margin_size(Panel.Position.LEFT)
        y = crect.top() + self.editor.panels.margin_size(Panel.Position.TOP)

        self.setGeometry(QRect(x+x0, y+y0, width, height))"
collect_filtered_models;"def collect_filtered_models(discard, *input_values):
    ''' Collect a duplicate-free list of all other Bokeh models referred to by
    this model, or by any of its references, etc, unless filtered-out by the
    provided callable.

    Iterate over ``input_values`` and descend through their structure
    collecting all nested ``Models`` on the go.

    Args:
        *discard (Callable[[Model], bool])
            a callable which accepts a *Model* instance as its single argument
            and returns a boolean stating whether to discard the instance. The
            latter means that the instance will not be added to collected
            models nor will its references be explored.

        *input_values (Model)
            Bokeh models to collect other models from

    Returns:
        None

    '''

    ids = set([])
    collected = []
    queued = []

    def queue_one(obj):
        if obj.id not in ids and not (callable(discard) and discard(obj)):
            queued.append(obj)

    for value in input_values:
        _visit_value_and_its_immediate_references(value, queue_one)

    while queued:
        obj = queued.pop(0)
        if obj.id not in ids:
            ids.add(obj.id)
            collected.append(obj)
            _visit_immediate_value_references(obj, queue_one)

    return collected"
get_versions;"def get_versions(reporev=True):
    """"""Get version information for components used by Spyder""""""
    import sys
    import platform

    import qtpy
    import qtpy.QtCore

    revision = None
    if reporev:
        from spyder.utils import vcs
        revision, branch = vcs.get_git_revision(os.path.dirname(__dir__))

    if not sys.platform == 'darwin':  # To avoid a crash with our Mac app
        system = platform.system()
    else:
        system = 'Darwin'

    return {
        'spyder': __version__,
        'python': platform.python_version(),  # ""2.7.3""
        'bitness': 64 if sys.maxsize > 2**32 else 32,
        'qt': qtpy.QtCore.__version__,
        'qt_api': qtpy.API_NAME,      # PyQt5
        'qt_api_ver': qtpy.PYQT_VERSION,
        'system': system,   # Linux, Windows, ...
        'release': platform.release(),  # XP, 10.6, 2.2.0, etc.
        'revision': revision,  # '9fdf926eccce'
    }"
register_event;"def register_event(self):
        """"""
        注册事件
        """"""
        event_bus = Environment.get_instance().event_bus
        event_bus.prepend_listener(EVENT.PRE_BEFORE_TRADING, self._pre_before_trading)
        event_bus.prepend_listener(EVENT.POST_SETTLEMENT, self._post_settlement)"
create_cluster;"def create_cluster(
        self,
        parent,
        cluster_id,
        cluster,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates a cluster within an instance.

        Example:
            >>> from google.cloud import bigtable_admin_v2
            >>>
            >>> client = bigtable_admin_v2.BigtableInstanceAdminClient()
            >>>
            >>> parent = client.instance_path('[PROJECT]', '[INSTANCE]')
            >>>
            >>> # TODO: Initialize `cluster_id`:
            >>> cluster_id = ''
            >>>
            >>> # TODO: Initialize `cluster`:
            >>> cluster = {}
            >>>
            >>> response = client.create_cluster(parent, cluster_id, cluster)
            >>>
            >>> def callback(operation_future):
            ...     # Handle result.
            ...     result = operation_future.result()
            >>>
            >>> response.add_done_callback(callback)
            >>>
            >>> # Handle metadata.
            >>> metadata = response.metadata()

        Args:
            parent (str): The unique name of the instance in which to create the new cluster.
                Values are of the form ``projects/<project>/instances/<instance>``.
            cluster_id (str): The ID to be used when referring to the new cluster within its instance,
                e.g., just ``mycluster`` rather than
                ``projects/myproject/instances/myinstance/clusters/mycluster``.
            cluster (Union[dict, ~google.cloud.bigtable_admin_v2.types.Cluster]): The cluster to be created. Fields marked ``OutputOnly`` must be left
                blank.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.bigtable_admin_v2.types.Cluster`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.bigtable_admin_v2.types._OperationFuture` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_cluster"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_cluster""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_cluster,
                default_retry=self._method_configs[""CreateCluster""].retry,
                default_timeout=self._method_configs[""CreateCluster""].timeout,
                client_info=self._client_info,
            )

        request = bigtable_instance_admin_pb2.CreateClusterRequest(
            parent=parent, cluster_id=cluster_id, cluster=cluster
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        operation = self._inner_api_calls[""create_cluster""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )
        return google.api_core.operation.from_gapic(
            operation,
            self.transport._operations_client,
            instance_pb2.Cluster,
            metadata_type=bigtable_instance_admin_pb2.CreateClusterMetadata,
        )"
text_channels;"def text_channels(self):
        """"""List[:class:`TextChannel`]: A list of text channels that belongs to this guild.

        This is sorted by the position and are in UI order from top to bottom.
        """"""
        r = [ch for ch in self._channels.values() if isinstance(ch, TextChannel)]
        r.sort(key=lambda c: (c.position, c.id))
        return r"
add_layer;"def add_layer(self, layer, input_node_id):
        """"""Add a layer to the Graph.
        Args:
            layer: An instance of the subclasses of StubLayer in layers.py.
            input_node_id: An integer. The ID of the input node of the layer.
        Returns:
            output_node_id: An integer. The ID of the output node of the layer.
        """"""
        if isinstance(input_node_id, Iterable):
            layer.input = list(map(lambda x: self.node_list[x], input_node_id))
            output_node_id = self._add_node(Node(layer.output_shape))
            for node_id in input_node_id:
                self._add_edge(layer, node_id, output_node_id)

        else:
            layer.input = self.node_list[input_node_id]
            output_node_id = self._add_node(Node(layer.output_shape))
            self._add_edge(layer, input_node_id, output_node_id)

        layer.output = self.node_list[output_node_id]
        return output_node_id"
get_history;"def get_history(self, start, end):
        """"""返回历史成交

        Arguments:
            start {str} -- [description]
            end {str]} -- [description]
        """"""
        return self.history_table.set_index(
            'datetime',
            drop=False
        ).loc[slice(pd.Timestamp(start),
                    pd.Timestamp(end))]"
logical_and;"def logical_and(lhs, rhs):
    """"""Returns the result of element-wise **logical and** comparison
    operation with broadcasting.

    For each element in input arrays, return 1(true) if lhs elements and rhs elements
    are true, otherwise return 0(false).

    Equivalent to ``lhs and rhs`` and ``mx.nd.broadcast_logical_and(lhs, rhs)``.

    .. note::

       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
       then the arrays are broadcastable to a common shape.

    Parameters
    ----------
    lhs : scalar or mxnet.ndarray.array
        First input of the function.
    rhs : scalar or mxnet.ndarray.array
         Second input of the function. If ``lhs.shape != rhs.shape``, they must be
        broadcastable to a common shape.

    Returns
    -------
    NDArray
        Output array of boolean values.

    Examples
    --------
    >>> x = mx.nd.ones((2,3))
    >>> y = mx.nd.arange(2).reshape((2,1))
    >>> z = mx.nd.arange(2).reshape((1,2))
    >>> x.asnumpy()
    array([[ 1.,  1.,  1.],
           [ 1.,  1.,  1.]], dtype=float32)
    >>> y.asnumpy()
    array([[ 0.],
           [ 1.]], dtype=float32)
    >>> z.asnumpy()
    array([[ 0.,  1.]], dtype=float32)
    >>> mx.nd.logical_and(x, 1).asnumpy()
    array([[ 1.,  1.,  1.],
           [ 1.,  1.,  1.]], dtype=float32)
    >>> mx.nd.logical_and(x, y).asnumpy()
    array([[ 0.,  0.,  0.],
           [ 1.,  1.,  1.]], dtype=float32)
    >>> mx.nd.logical_and(z, y).asnumpy()
    array([[ 0.,  0.],
           [ 0.,  1.]], dtype=float32)
    """"""
    # pylint: disable= no-member, protected-access
    return _ufunc_helper(
        lhs,
        rhs,
        op.broadcast_logical_and,
        lambda x, y: 1 if x and y else 0,
        _internal._logical_and_scalar,
        None)"
confirmation_option;"def confirmation_option(*param_decls, **attrs):
    """"""Shortcut for confirmation prompts that can be ignored by passing
    ``--yes`` as parameter.

    This is equivalent to decorating a function with :func:`option` with
    the following parameters::

        def callback(ctx, param, value):
            if not value:
                ctx.abort()

        @click.command()
        @click.option('--yes', is_flag=True, callback=callback,
                      expose_value=False, prompt='Do you want to continue?')
        def dropdb():
            pass
    """"""
    def decorator(f):
        def callback(ctx, param, value):
            if not value:
                ctx.abort()
        attrs.setdefault('is_flag', True)
        attrs.setdefault('callback', callback)
        attrs.setdefault('expose_value', False)
        attrs.setdefault('prompt', 'Do you want to continue?')
        attrs.setdefault('help', 'Confirm the action without prompting.')
        return option(*(param_decls or ('--yes',)), **attrs)(f)
    return decorator"
parse_expr;"def parse_expr(tokens, options):
    """"""expr ::= seq ( '|' seq )* ;""""""
    seq = parse_seq(tokens, options)
    if tokens.current() != '|':
        return seq
    result = [Required(*seq)] if len(seq) > 1 else seq
    while tokens.current() == '|':
        tokens.move()
        seq = parse_seq(tokens, options)
        result += [Required(*seq)] if len(seq) > 1 else seq
    return [Either(*result)] if len(result) > 1 else result"
scalars_impl;"def scalars_impl(self, tag, run, experiment, output_format):
    """"""Result of the form `(body, mime_type)`.""""""
    if self._db_connection_provider:
      db = self._db_connection_provider()
      # We select for steps greater than -1 because the writer inserts
      # placeholder rows en masse. The check for step filters out those rows.
      cursor = db.execute('''
        SELECT
          Tensors.step,
          Tensors.computed_time,
          Tensors.data,
          Tensors.dtype
        FROM Tensors
        JOIN Tags
          ON Tensors.series = Tags.tag_id
        JOIN Runs
          ON Tags.run_id = Runs.run_id
        WHERE
          /* For backwards compatibility, ignore the experiment id
             for matching purposes if it is empty. */
          (:exp == '' OR Runs.experiment_id == CAST(:exp AS INT))
          AND Runs.run_name = :run
          AND Tags.tag_name = :tag
          AND Tags.plugin_name = :plugin
          AND Tensors.shape = ''
          AND Tensors.step > -1
        ORDER BY Tensors.step
      ''', dict(exp=experiment, run=run, tag=tag, plugin=metadata.PLUGIN_NAME))
      values = [(wall_time, step, self._get_value(data, dtype_enum))
                for (step, wall_time, data, dtype_enum) in cursor]
    else:
      tensor_events = self._multiplexer.Tensors(run, tag)
      values = [(tensor_event.wall_time,
                 tensor_event.step,
                 tensor_util.make_ndarray(tensor_event.tensor_proto).item())
                for tensor_event in tensor_events]

    if output_format == OutputFormat.CSV:
      string_io = StringIO()
      writer = csv.writer(string_io)
      writer.writerow(['Wall time', 'Step', 'Value'])
      writer.writerows(values)
      return (string_io.getvalue(), 'text/csv')
    else:
      return (values, 'application/json')"
insert_text;"def insert_text(self, text, keep_position=True):
        """"""
        Inserts text at the cursor position.

        :param text: text to insert
        :param keep_position: Flag that specifies if the cursor position must
            be kept. Pass False for a regular insert (the cursor will be at
            the end of the inserted text).
        """"""
        text_cursor = self._editor.textCursor()
        if keep_position:
            s = text_cursor.selectionStart()
            e = text_cursor.selectionEnd()
        text_cursor.insertText(text)
        if keep_position:
            text_cursor.setPosition(s)
            text_cursor.setPosition(e, text_cursor.KeepAnchor)
        self._editor.setTextCursor(text_cursor)"
write_docstring_for_shortcut;"def write_docstring_for_shortcut(self):
        """"""Write docstring to editor by shortcut of code editor.""""""
        # cursor placed below function definition
        result = self.get_function_definition_from_below_last_line()
        if result is not None:
            __, number_of_lines_of_function = result
            cursor = self.code_editor.textCursor()
            for __ in range(number_of_lines_of_function):
                cursor.movePosition(QTextCursor.PreviousBlock)

            self.code_editor.setTextCursor(cursor)

        cursor = self.code_editor.textCursor()
        self.line_number_cursor = cursor.blockNumber() + 1

        self.write_docstring_at_first_line_of_function()"
list_outputs;"def list_outputs(self):
        """"""Lists all the outputs in the symbol.

        Example
        -------
        >>> a = mx.sym.var('a')
        >>> b = mx.sym.var('b')
        >>> c = a + b
        >>> c.list_outputs()
        ['_plus12_output']

        Returns
        -------
        list of str
            List of all the outputs.
            For most symbols, this list contains only the name of this symbol.
            For symbol groups, this is a list with the names of all symbols
            in the group.
        """"""
        size = ctypes.c_uint()
        sarr = ctypes.POINTER(ctypes.c_char_p)()
        check_call(_LIB.MXSymbolListOutputs(
            self.handle, ctypes.byref(size), ctypes.byref(sarr)))
        return [py_str(sarr[i]) for i in range(size.value)]"
start_interpreter;"def start_interpreter(self, namespace):
        """"""Start Python interpreter""""""
        self.clear()
        
        if self.interpreter is not None:
            self.interpreter.closing()
        self.interpreter = Interpreter(namespace, self.exitfunc,
                                       SysOutput, WidgetProxy,
                                       get_debug_level())
        self.interpreter.stdout_write.data_avail.connect(self.stdout_avail)
        self.interpreter.stderr_write.data_avail.connect(self.stderr_avail)
        self.interpreter.widget_proxy.sig_set_readonly.connect(self.setReadOnly)
        self.interpreter.widget_proxy.sig_new_prompt.connect(self.new_prompt)
        self.interpreter.widget_proxy.sig_edit.connect(self.edit_script)
        self.interpreter.widget_proxy.sig_wait_input.connect(self.wait_input)
        if self.multithreaded:
            self.interpreter.start()
        
        # Interpreter banner
        banner = create_banner(self.message)
        self.write(banner, prompt=True)

        # Initial commands
        for cmd in self.commands:
            self.run_command(cmd, history=False, new_prompt=False)
                
        # First prompt
        self.new_prompt(self.interpreter.p1)
        self.refresh.emit()

        return self.interpreter"
get_aggregation;"def get_aggregation(self):
        """"""
        Return the aggregation object.
        """"""
        agg = A(self.agg_type, **self._params)
        if self._metric:
            agg.metric('metric', self._metric)
        return agg"
define_params;"def define_params(self):
        '''
        Define parameters.
        '''
        input_dim = self.input_dim
        hidden_dim = self.hidden_dim
        prefix = self.name
        self.w_matrix = tf.Variable(tf.random_normal([input_dim, 3 * hidden_dim], stddev=0.1),
                                    name='/'.join([prefix, 'W']))
        self.U = tf.Variable(tf.random_normal([hidden_dim, 3 * hidden_dim], stddev=0.1),
                             name='/'.join([prefix, 'U']))
        self.bias = tf.Variable(tf.random_normal([1, 3 * hidden_dim], stddev=0.1),
                                name='/'.join([prefix, 'b']))
        return self"
destroy_multiprocessing;"def destroy_multiprocessing(parallel_data, queue=None):
    '''
    This function will be called from another process when running a map in
    parallel mode. The result from the destroy is always a json object.
    '''
    salt.utils.crypt.reinit_crypto()

    parallel_data['opts']['output'] = 'json'
    clouds = salt.loader.clouds(parallel_data['opts'])

    try:
        fun = clouds['{0}.destroy'.format(parallel_data['driver'])]
        with salt.utils.context.func_globals_inject(
            fun,
            __active_provider_name__=':'.join([
                parallel_data['alias'],
                parallel_data['driver']
            ])
        ):
            output = fun(parallel_data['name'])

    except SaltCloudException as exc:
        log.error(
            'Failed to destroy %s. Error: %s',
            parallel_data['name'], exc, exc_info_on_loglevel=logging.DEBUG
        )
        return {parallel_data['name']: {'Error': str(exc)}}

    return {
        parallel_data['name']: salt.utils.data.simple_types_filter(output)
    }"
get_service_account_email;"def get_service_account_email(self, project=None):
        """"""Get the email address of the project's GCS service account

        :type project: str
        :param project:
            (Optional) Project ID to use for retreiving GCS service account
            email address.  Defaults to the client's project.

        :rtype: str
        :returns: service account email address
        """"""
        if project is None:
            project = self.project
        path = ""/projects/%s/serviceAccount"" % (project,)
        api_response = self._base_connection.api_request(method=""GET"", path=path)
        return api_response[""email_address""]"
doc_type;"def doc_type(self, *doc_type, **kwargs):
        """"""
        Set the type to search through. You can supply a single value or
        multiple. Values can be strings or subclasses of ``Document``.

        You can also pass in any keyword arguments, mapping a doc_type to a
        callback that should be used instead of the Hit class.

        If no doc_type is supplied any information stored on the instance will
        be erased.

        Example:

            s = Search().doc_type('product', 'store', User, custom=my_callback)
        """"""
        # .doc_type() resets
        s = self._clone()
        if not doc_type and not kwargs:
            s._doc_type = []
            s._doc_type_map = {}
        else:
            s._doc_type.extend(doc_type)
            s._doc_type.extend(kwargs.keys())
            s._doc_type_map.update(kwargs)
        return s"
num_dml_affected_rows;"def num_dml_affected_rows(self):
        """"""Return the number of DML rows affected by the job.

        See:
        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.numDmlAffectedRows

        :rtype: int or None
        :returns: number of DML rows affected by the job, or None if job is not
                  yet complete.
        """"""
        result = self._job_statistics().get(""numDmlAffectedRows"")
        if result is not None:
            result = int(result)
        return result"
explain_instance;"def explain_instance(self,
                         text_instance,
                         classifier_fn,
                         labels=(1,),
                         top_labels=None,
                         num_features=10,
                         num_samples=5000,
                         distance_metric='cosine',
                         model_regressor=None):
        """"""Generates explanations for a prediction.

        First, we generate neighborhood data by randomly hiding features from
        the instance (see __data_labels_distance_mapping). We then learn
        locally weighted linear models on this neighborhood data to explain
        each of the classes in an interpretable way (see lime_base.py).

        Args:
            text_instance: raw text string to be explained.
            classifier_fn: classifier prediction probability function, which
                takes a list of d strings and outputs a (d, k) numpy array with
                prediction probabilities, where k is the number of classes.
                For ScikitClassifiers , this is classifier.predict_proba.
            labels: iterable with labels to be explained.
            top_labels: if not None, ignore labels and produce explanations for
                the K labels with highest prediction probabilities, where K is
                this parameter.
            num_features: maximum number of features present in explanation
            num_samples: size of the neighborhood to learn the linear model
            distance_metric: the distance metric to use for sample weighting,
                defaults to cosine similarity
            model_regressor: sklearn regressor to use in explanation. Defaults
            to Ridge regression in LimeBase. Must have model_regressor.coef_
            and 'sample_weight' as a parameter to model_regressor.fit()
        Returns:
            An Explanation object (see explanation.py) with the corresponding
            explanations.
        """"""

        indexed_string = IndexedCharacters(
            text_instance, bow=self.bow) if self.char_level else IndexedString(
            text_instance, bow=self.bow, split_expression=self.split_expression)
        domain_mapper = TextDomainMapper(indexed_string)
        data, yss, distances = self.__data_labels_distances(
            indexed_string, classifier_fn, num_samples,
            distance_metric=distance_metric)
        if self.class_names is None:
            self.class_names = [str(x) for x in range(yss[0].shape[0])]
        ret_exp = explanation.Explanation(domain_mapper=domain_mapper,
                                          class_names=self.class_names,
                                          random_state=self.random_state)
        ret_exp.predict_proba = yss[0]
        if top_labels:
            labels = np.argsort(yss[0])[-top_labels:]
            ret_exp.top_labels = list(labels)
            ret_exp.top_labels.reverse()
        for label in labels:
            (ret_exp.intercept[label],
             ret_exp.local_exp[label],
             ret_exp.score, ret_exp.local_pred) = self.base.explain_instance_with_data(
                data, yss, distances, label, num_features,
                model_regressor=model_regressor,
                feature_selection=self.feature_selection)
        return ret_exp"
convert_padding;"def convert_padding(builder, layer, input_names, output_names, keras_layer):
    """"""Convert padding layer from keras to coreml.
    Keras only supports zero padding at this time.
    Parameters
    ----------
    keras_layer: layer
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_name, output_name = (input_names[0], output_names[0])

    if isinstance(keras_layer, keras.layers.convolutional.ZeroPadding1D):
        left, right = keras_layer.padding
        top, bottom = (0, 0)
    else: # 2D
        top, left = keras_layer.padding
        bottom, right = keras_layer.padding

    # Now add the layer
    builder.add_padding(name = layer,
        left = left, right=right, top=top, bottom=bottom, value = 0,
        input_name = input_name, output_name=output_name
        )"
check_order_triggers;"def check_order_triggers(self, current_price):
        """"""
        Given an order and a trade event, return a tuple of
        (stop_reached, limit_reached).
        For market orders, will return (False, False).
        For stop orders, limit_reached will always be False.
        For limit orders, stop_reached will always be False.
        For stop limit orders a Boolean is returned to flag
        that the stop has been reached.

        Orders that have been triggered already (price targets reached),
        the order's current values are returned.
        """"""
        if self.triggered:
            return (self.stop_reached, self.limit_reached, False)

        stop_reached = False
        limit_reached = False
        sl_stop_reached = False

        order_type = 0

        if self.amount > 0:
            order_type |= BUY
        else:
            order_type |= SELL

        if self.stop is not None:
            order_type |= STOP

        if self.limit is not None:
            order_type |= LIMIT

        if order_type == BUY | STOP | LIMIT:
            if current_price >= self.stop:
                sl_stop_reached = True
                if current_price <= self.limit:
                    limit_reached = True
        elif order_type == SELL | STOP | LIMIT:
            if current_price <= self.stop:
                sl_stop_reached = True
                if current_price >= self.limit:
                    limit_reached = True
        elif order_type == BUY | STOP:
            if current_price >= self.stop:
                stop_reached = True
        elif order_type == SELL | STOP:
            if current_price <= self.stop:
                stop_reached = True
        elif order_type == BUY | LIMIT:
            if current_price <= self.limit:
                limit_reached = True
        elif order_type == SELL | LIMIT:
            # This is a SELL LIMIT order
            if current_price >= self.limit:
                limit_reached = True

        return (stop_reached, limit_reached, sl_stop_reached)"
find_undeclared;"def find_undeclared(nodes, names):
    """"""Check if the names passed are accessed undeclared.  The return value
    is a set of all the undeclared names from the sequence of names found.
    """"""
    visitor = UndeclaredNameVisitor(names)
    try:
        for node in nodes:
            visitor.visit(node)
    except VisitorExit:
        pass
    return visitor.undeclared"
graph_route;"def graph_route(self, request):
    """"""Given a single run, return the graph definition in protobuf format.""""""
    run = request.args.get('run')
    tag = request.args.get('tag', '')
    conceptual_arg = request.args.get('conceptual', False)
    is_conceptual = True if conceptual_arg == 'true' else False

    if run is None:
      return http_util.Respond(
          request, 'query parameter ""run"" is required', 'text/plain', 400)

    limit_attr_size = request.args.get('limit_attr_size', None)
    if limit_attr_size is not None:
      try:
        limit_attr_size = int(limit_attr_size)
      except ValueError:
        return http_util.Respond(
            request, 'query parameter `limit_attr_size` must be an integer',
            'text/plain', 400)

    large_attrs_key = request.args.get('large_attrs_key', None)

    try:
      result = self.graph_impl(run, tag, is_conceptual, limit_attr_size, large_attrs_key)
    except ValueError as e:
      return http_util.Respond(request, e.message, 'text/plain', code=400)
    else:
      if result is not None:
        (body, mime_type) = result  # pylint: disable=unpacking-non-sequence
        return http_util.Respond(request, body, mime_type)
      else:
        return http_util.Respond(request, '404 Not Found', 'text/plain',
                                 code=404)"
replace_namespaced_replica_set_scale;"def replace_namespaced_replica_set_scale(self, name, namespace, body, **kwargs):
        """"""
        replace scale of the specified ReplicaSet
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_replica_set_scale(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Scale (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1Scale body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1Scale
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_replica_set_scale_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.replace_namespaced_replica_set_scale_with_http_info(name, namespace, body, **kwargs)
            return data"
list_traces;"def list_traces(
        self,
        project_id,
        view=None,
        page_size=None,
        start_time=None,
        end_time=None,
        filter_=None,
        order_by=None,
        page_token=None,
    ):
        """"""
        Returns of a list of traces that match the filter conditions.

        Args:
            project_id (Optional[str]): ID of the Cloud project where the trace
                data is stored.

            view (Optional[~google.cloud.trace_v1.gapic.enums.
                ListTracesRequest.ViewType]): Type of data returned for traces
                in the list. Default is ``MINIMAL``.

            page_size (Optional[int]): Maximum number of traces to return. If
                not specified or <= 0, the implementation selects a reasonable
                value. The implementation may return fewer traces than the
                requested page size.

            start_time (Optional[~datetime.datetime]): Start of the time
                interval (inclusive) during which the trace data was collected
                from the application.

            end_time (Optional[~datetime.datetime]): End of the time interval
                (inclusive) during which the trace data was collected from the
                application.

            filter_ (Optional[str]): An optional filter for the request.

            order_by (Optional[str]): Field used to sort the returned traces.

            page_token (Optional[str]): opaque marker for the next ""page"" of
                entries. If not passed, the API will return the first page of
                entries.

        Returns:
            A  :class:`~google.api_core.page_iterator.Iterator` of traces that
            match the specified filter conditions.
        """"""
        page_iter = self._gapic_api.list_traces(
            project_id=project_id,
            view=view,
            page_size=page_size,
            start_time=start_time,
            end_time=end_time,
            filter_=filter_,
            order_by=order_by,
        )
        page_iter.item_to_value = _item_to_mapping
        page_iter.next_page_token = page_token
        return page_iter"
optimize_linear;"def optimize_linear(grad, eps, ord=np.inf):
  """"""
  Solves for the optimal input to a linear function under a norm constraint.

  Optimal_perturbation = argmax_{eta, ||eta||_{ord} < eps} dot(eta, grad)

  :param grad: Tensor, shape (N, d_1, ...). Batch of gradients
  :param eps: float. Scalar specifying size of constraint region
  :param ord: np.inf, 1, or 2. Order of norm constraint.
  :returns: Tensor, shape (N, d_1, ...). Optimal perturbation
  """"""

  red_ind = list(range(1, len(grad.size())))
  avoid_zero_div = torch.tensor(1e-12, dtype=grad.dtype, device=grad.device)
  if ord == np.inf:
    # Take sign of gradient
    optimal_perturbation = torch.sign(grad)
  elif ord == 1:
    abs_grad = torch.abs(grad)
    sign = torch.sign(grad)
    red_ind = list(range(1, len(grad.size())))
    abs_grad = torch.abs(grad)
    ori_shape = [1]*len(grad.size())
    ori_shape[0] = grad.size(0)

    max_abs_grad, _ = torch.max(abs_grad.view(grad.size(0), -1), 1)
    max_mask = abs_grad.eq(max_abs_grad.view(ori_shape)).to(torch.float)
    num_ties = max_mask
    for red_scalar in red_ind:
      num_ties = torch.sum(num_ties, red_scalar, keepdim=True)
    optimal_perturbation = sign * max_mask / num_ties
    # TODO integrate below to a test file
    # check that the optimal perturbations have been correctly computed
    opt_pert_norm = optimal_perturbation.abs().sum(dim=red_ind)
    assert torch.all(opt_pert_norm == torch.ones_like(opt_pert_norm))
  elif ord == 2:
    square = torch.max(
        avoid_zero_div,
        torch.sum(grad ** 2, red_ind, keepdim=True)
        )
    optimal_perturbation = grad / torch.sqrt(square)
    # TODO integrate below to a test file
    # check that the optimal perturbations have been correctly computed
    opt_pert_norm = optimal_perturbation.pow(2).sum(dim=red_ind, keepdim=True).sqrt()
    one_mask = (square <= avoid_zero_div).to(torch.float) * opt_pert_norm + \
            (square > avoid_zero_div).to(torch.float)
    assert torch.allclose(opt_pert_norm, one_mask, rtol=1e-05, atol=1e-08)
  else:
    raise NotImplementedError(""Only L-inf, L1 and L2 norms are ""
                              ""currently implemented."")

  # Scale perturbation to be the solution for the norm=eps rather than
  # norm=1 problem
  scaled_perturbation = eps * optimal_perturbation
  return scaled_perturbation"
vectorized_beta;"def vectorized_beta(dependents, independent, allowed_missing, out=None):
    """"""
    Compute slopes of linear regressions between columns of ``dependents`` and
    ``independent``.

    Parameters
    ----------
    dependents : np.array[N, M]
        Array with columns of data to be regressed against ``independent``.
    independent : np.array[N, 1]
        Independent variable of the regression
    allowed_missing : int
        Number of allowed missing (NaN) observations per column. Columns with
        more than this many non-nan observations in both ``dependents`` and
        ``independents`` will output NaN as the regression coefficient.

    Returns
    -------
    slopes : np.array[M]
        Linear regression coefficients for each column of ``dependents``.
    """"""
    # Cache these as locals since we're going to call them multiple times.
    nan = np.nan
    isnan = np.isnan
    N, M = dependents.shape

    if out is None:
        out = np.full(M, nan)

    # Copy N times as a column vector and fill with nans to have the same
    # missing value pattern as the dependent variable.
    #
    # PERF_TODO: We could probably avoid the space blowup by doing this in
    # Cython.

    # shape: (N, M)
    independent = np.where(
        isnan(dependents),
        nan,
        independent,
    )

    # Calculate beta as Cov(X, Y) / Cov(X, X).
    # https://en.wikipedia.org/wiki/Simple_linear_regression#Fitting_the_regression_line  # noqa
    #
    # NOTE: The usual formula for covariance is::
    #
    #    mean((X - mean(X)) * (Y - mean(Y)))
    #
    # However, we don't actually need to take the mean of both sides of the
    # product, because of the folllowing equivalence::
    #
    # Let X_res = (X - mean(X)).
    # We have:
    #
    #     mean(X_res * (Y - mean(Y))) = mean(X_res * (Y - mean(Y)))
    #                             (1) = mean((X_res * Y) - (X_res * mean(Y)))
    #                             (2) = mean(X_res * Y) - mean(X_res * mean(Y))
    #                             (3) = mean(X_res * Y) - mean(X_res) * mean(Y)
    #                             (4) = mean(X_res * Y) - 0 * mean(Y)
    #                             (5) = mean(X_res * Y)
    #
    #
    # The tricky step in the above derivation is step (4). We know that
    # mean(X_res) is zero because, for any X:
    #
    #     mean(X - mean(X)) = mean(X) - mean(X) = 0.
    #
    # The upshot of this is that we only have to center one of `independent`
    # and `dependent` when calculating covariances. Since we need the centered
    # `independent` to calculate its variance in the next step, we choose to
    # center `independent`.

    # shape: (N, M)
    ind_residual = independent - nanmean(independent, axis=0)

    # shape: (M,)
    covariances = nanmean(ind_residual * dependents, axis=0)

    # We end up with different variances in each column here because each
    # column may have a different subset of the data dropped due to missing
    # data in the corresponding dependent column.
    # shape: (M,)
    independent_variances = nanmean(ind_residual ** 2, axis=0)

    # shape: (M,)
    np.divide(covariances, independent_variances, out=out)

    # Write nans back to locations where we have more then allowed number of
    # missing entries.
    nanlocs = isnan(independent).sum(axis=0) > allowed_missing
    out[nanlocs] = nan

    return out"
revoke_auth;"def revoke_auth(preserve_minion_cache=False):
    '''
    The minion sends a request to the master to revoke its own key.
    Note that the minion session will be revoked and the minion may
    not be able to return the result of this command back to the master.

    If the 'preserve_minion_cache' flag is set to True, the master
    cache for this minion will not be removed.

    CLI Example:

    .. code-block:: bash

        salt '*' saltutil.revoke_auth
    '''
    masters = list()
    ret = True
    if 'master_uri_list' in __opts__:
        for master_uri in __opts__['master_uri_list']:
            masters.append(master_uri)
    else:
        masters.append(__opts__['master_uri'])

    for master in masters:
        channel = salt.transport.client.ReqChannel.factory(__opts__, master_uri=master)
        tok = channel.auth.gen_token(b'salt')
        load = {'cmd': 'revoke_auth',
                'id': __opts__['id'],
                'tok': tok,
                'preserve_minion_cache': preserve_minion_cache}
        try:
            channel.send(load)
        except SaltReqTimeoutError:
            ret = False
        finally:
            channel.close()
    return ret"
contains_cursor;"def contains_cursor(self, cursor):
        """"""
        Checks if the textCursor is in the decoration.

        :param cursor: The text cursor to test
        :type cursor: QtGui.QTextCursor

        :returns: True if the cursor is over the selection
        """"""
        start = self.cursor.selectionStart()
        end = self.cursor.selectionEnd()
        if cursor.atBlockEnd():
            end -= 1
        return start <= cursor.position() <= end"
get_function_for_aws_event;"def get_function_for_aws_event(self, record):
        """"""
        Get the associated function to execute for a triggered AWS event

        Support S3, SNS, DynamoDB, kinesis and SQS events
        """"""
        if 's3' in record:
            if ':' in record['s3']['configurationId']:
                return record['s3']['configurationId'].split(':')[-1]

        arn = None
        if 'Sns' in record:
            try:
                message = json.loads(record['Sns']['Message'])
                if message.get('command'):
                    return message['command']
            except ValueError:
                pass
            arn = record['Sns'].get('TopicArn')
        elif 'dynamodb' in record or 'kinesis' in record:
            arn = record.get('eventSourceARN')
        elif 'eventSource' in record and record.get('eventSource') == 'aws:sqs':
            arn = record.get('eventSourceARN')
        elif 's3' in record:
            arn = record['s3']['bucket']['arn']

        if arn:
            return self.settings.AWS_EVENT_MAPPING.get(arn)

        return None"
post_copy_metacolums;"def post_copy_metacolums(self, cursor):
        """"""
        Performs post-copy to fill metadata columns.
        """"""
        logger.info('Executing post copy metadata queries')
        for query in self.metadata_queries:
            cursor.execute(query)"
set_style;"def set_style(self, input_feeds):
    """"""Set target style variables.
    
    Expected usage: 
      style_loss = StyleLoss(style_layers)
      ...
      init_op = tf.global_variables_initializer()
      init_op.run()
      
      feeds = {... session.run() 'feeds' argument that will make 'style_layers'
               tensors evaluate to activation values of style image...}
      style_loss.set_style(feeds)  # this must be called after 'init_op.run()'
    """"""
    sess = tf.get_default_session()
    computed = sess.run(self.input_grams, input_feeds)
    for v, g in zip(self.target_vars, computed):
      v.load(g)"
convert_duration;"def convert_duration(duration):
    '''
    Convert the a duration string into XXhYYmZZs format

    duration
        Duration to convert

    Returns: duration_string
        String representation of duration in XXhYYmZZs format
    '''

    # durations must be specified in days, weeks or hours

    if duration.endswith('h'):
        hours = int(duration.split('h'))

    elif duration.endswith('d'):
        days = duration.split('d')
        hours = int(days[0]) * 24

    elif duration.endswith('w'):
        weeks = duration.split('w')
        hours = int(weeks[0]) * 24 * 7

    duration_string = str(hours)+'h0m0s'
    return duration_string"
download_software;"def download_software(name, version=None, synch=False, check=False):
    '''
    Ensures that a software version is downloaded.

    name: The name of the module function to execute.

    version(str): The software version to check. If this version is not already downloaded, it will attempt to download
    the file from Palo Alto.

    synch(bool): If true, after downloading the file it will be synched to its peer.

    check(bool): If true, the PANOS device will first attempt to pull the most recent software inventory list from Palo
    Alto.

    SLS Example:

    .. code-block:: yaml

        panos/version8.0.0:
            panos.download_software:
              - version: 8.0.0
              - synch: False
              - check: True

    '''
    ret = _default_ret(name)

    if check is True:
        __salt__['panos.check_software']()

    versions = __salt__['panos.get_software_info']()['result']

    if 'sw-updates' not in versions \
        or 'versions' not in versions['sw-updates'] \
        or 'entry' not in versions['sw-updates']['versions']:
        ret.update({
            'comment': 'Software version is not found in the local software list.',
            'result': False
        })
        return ret

    for entry in versions['sw-updates']['versions']['entry']:
        if entry['version'] == version and entry['downloaded'] == ""yes"":
            ret.update({
                'comment': 'Software version is already downloaded.',
                'result': True
            })
        return ret

    ret.update({
        'changes': __salt__['panos.download_software_version'](version=version, synch=synch)
    })

    versions = __salt__['panos.get_software_info']()['result']

    if 'sw-updates' not in versions \
        or 'versions' not in versions['sw-updates'] \
        or 'entry' not in versions['sw-updates']['versions']:
        ret.update({
            'result': False
        })
        return ret

    for entry in versions['sw-updates']['versions']['entry']:
        if entry['version'] == version and entry['downloaded'] == ""yes"":
            ret.update({
                'result': True
            })
        return ret

    return ret"
urlencoded_processor;"def urlencoded_processor(entity):
    '''
    Accept x-www-form-urlencoded data (run through CherryPy's formatter)
    and reformat it into a Low State data structure.

    Since we can't easily represent complicated data structures with
    key-value pairs, any more complicated requirements (e.g. compound
    commands) must instead be delivered via JSON or YAML.

    For example::

    .. code-block:: bash

        curl -si localhost:8000 -d client=local -d tgt='*' \\
                -d fun='test.kwarg' -d arg='one=1' -d arg='two=2'

    :param entity: raw POST data
    '''
    # First call out to CherryPy's default processor
    cherrypy._cpreqbody.process_urlencoded(entity)
    cherrypy._cpreqbody.process_urlencoded(entity)
    cherrypy.serving.request.unserialized_data = entity.params
    cherrypy.serving.request.raw_body = ''"
get_active_project_path;"def get_active_project_path(self):
        """"""Get path of the active project""""""
        active_project_path = None
        if self.current_active_project:
            active_project_path = self.current_active_project.root_path
        return active_project_path"
set_item_data;"def set_item_data(self, item, filename, line_number):
        """"""Set tree item user data: filename (string) and line_number (int)""""""
        set_item_user_text(item, '%s%s%d' % (filename, self.SEP, line_number))"
get_image;"def get_image(roi_rec, short, max_size, mean, std):
    """"""
    read, resize, transform image, return im_tensor, im_info, gt_boxes
    roi_rec should have keys: [""image"", ""boxes"", ""gt_classes"", ""flipped""]
    0 --- x (width, second dim of im)
    |
    y (height, first dim of im)
    """"""
    im = imdecode(roi_rec['image'])
    if roi_rec[""flipped""]:
        im = im[:, ::-1, :]
    im, im_scale = resize(im, short, max_size)
    height, width = im.shape[:2]
    im_info = np.array([height, width, im_scale], dtype=np.float32)
    im_tensor = transform(im, mean, std)

    # gt boxes: (x1, y1, x2, y2, cls)
    if roi_rec['gt_classes'].size > 0:
        gt_inds = np.where(roi_rec['gt_classes'] != 0)[0]
        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)
        gt_boxes[:, 0:4] = roi_rec['boxes'][gt_inds, :]
        gt_boxes[:, 4] = roi_rec['gt_classes'][gt_inds]
        # scale gt_boxes
        gt_boxes[:, 0:4] *= im_scale
    else:
        gt_boxes = np.empty((0, 5), dtype=np.float32)

    return im_tensor, im_info, gt_boxes"
shap_values;"def shap_values(self, X, y=None, tree_limit=None, approximate=False):
        """""" Estimate the SHAP values for a set of samples.

        Parameters
        ----------
        X : numpy.array, pandas.DataFrame or catboost.Pool (for catboost)
            A matrix of samples (# samples x # features) on which to explain the model's output.

        y : numpy.array
            An array of label values for each sample. Used when explaining loss functions.

        tree_limit : None (default) or int 
            Limit the number of trees used by the model. By default None means no use the limit of the
            original model, and -1 means no limit.

        approximate : bool
            Run fast, but only roughly approximate the Tree SHAP values. This runs a method
            previously proposed by Saabas which only considers a single feature ordering. Take care
            since this does not have the consistency guarantees of Shapley values and places too
            much weight on lower splits in the tree.

        Returns
        -------
        For models with a single output this returns a matrix of SHAP values
        (# samples x # features). Each row sums to the difference between the model output for that
        sample and the expected value of the model output (which is stored in the expected_value
        attribute of the explainer when it is constant). For models with vector outputs this returns
        a list of such matrices, one for each output.
        """"""

        # see if we have a default tree_limit in place.
        if tree_limit is None:
            tree_limit = -1 if self.model.tree_limit is None else self.model.tree_limit

        # shortcut using the C++ version of Tree SHAP in XGBoost, LightGBM, and CatBoost
        if self.feature_dependence == ""tree_path_dependent"" and self.model.model_type != ""internal"" and self.data is None:
            phi = None
            if self.model.model_type == ""xgboost"":
                assert_import(""xgboost"")
                if not str(type(X)).endswith(""xgboost.core.DMatrix'>""):
                    X = xgboost.DMatrix(X)
                if tree_limit == -1:
                    tree_limit = 0
                phi = self.model.original_model.predict(
                    X, ntree_limit=tree_limit, pred_contribs=True,
                    approx_contribs=approximate, validate_features=False
                )
            
            elif self.model.model_type == ""lightgbm"":
                assert not approximate, ""approximate=True is not supported for LightGBM models!""
                phi = self.model.original_model.predict(X, num_iteration=tree_limit, pred_contrib=True)
                if phi.shape[1] != X.shape[1] + 1:
                    phi = phi.reshape(X.shape[0], phi.shape[1]//(X.shape[1]+1), X.shape[1]+1)
            
            elif self.model.model_type == ""catboost"": # thanks to the CatBoost team for implementing this...
                assert not approximate, ""approximate=True is not supported for CatBoost models!""
                assert tree_limit == -1, ""tree_limit is not yet supported for CatBoost models!""
                if type(X) != catboost.Pool:
                    X = catboost.Pool(X)
                phi = self.model.original_model.get_feature_importance(data=X, fstr_type='ShapValues')

            # note we pull off the last column and keep it as our expected_value
            if phi is not None:
                if len(phi.shape) == 3:
                    self.expected_value = [phi[0, i, -1] for i in range(phi.shape[1])]
                    return [phi[:, i, :-1] for i in range(phi.shape[1])]
                else:
                    self.expected_value = phi[0, -1]
                    return phi[:, :-1]

        # convert dataframes
        orig_X = X
        if str(type(X)).endswith(""pandas.core.series.Series'>""):
            X = X.values
        elif str(type(X)).endswith(""pandas.core.frame.DataFrame'>""):
            X = X.values
        flat_output = False
        if len(X.shape) == 1:
            flat_output = True
            X = X.reshape(1, X.shape[0])
        if X.dtype != self.model.dtype:
            X = X.astype(self.model.dtype)
        X_missing = np.isnan(X, dtype=np.bool)
        assert str(type(X)).endswith(""'numpy.ndarray'>""), ""Unknown instance type: "" + str(type(X))
        assert len(X.shape) == 2, ""Passed input data matrix X must have 1 or 2 dimensions!""

        if tree_limit < 0 or tree_limit > self.model.values.shape[0]:
            tree_limit = self.model.values.shape[0]
        
        if self.model_output == ""logloss"":
            assert y is not None, ""Both samples and labels must be provided when explaining the loss (i.e. `explainer.shap_values(X, y)`)!""
            assert X.shape[0] == len(y), ""The number of labels (%d) does not match the number of samples to explain (%d)!"" % (len(y), X.shape[0])
        transform = self.model.get_transform(self.model_output)

        if self.feature_dependence == ""tree_path_dependent"":
            assert self.model.fully_defined_weighting, ""The background dataset you provided does not cover all the leaves in the model, "" \
                                                       ""so TreeExplainer cannot run with the feature_dependence=\""tree_path_dependent\"" option! "" \
                                                       ""Try providing a larger background dataset, or using feature_dependence=\""independent\"".""
 
        # run the core algorithm using the C extension
        assert_import(""cext"")
        phi = np.zeros((X.shape[0], X.shape[1]+1, self.model.n_outputs))
        if not approximate:
            _cext.dense_tree_shap(
                self.model.children_left, self.model.children_right, self.model.children_default,
                self.model.features, self.model.thresholds, self.model.values, self.model.node_sample_weight,
                self.model.max_depth, X, X_missing, y, self.data, self.data_missing, tree_limit,
                self.model.base_offset, phi, feature_dependence_codes[self.feature_dependence],
                output_transform_codes[transform], False
            )
        else:
            _cext.dense_tree_saabas(
                self.model.children_left, self.model.children_right, self.model.children_default,
                self.model.features, self.model.thresholds, self.model.values,
                self.model.max_depth, tree_limit, self.model.base_offset, output_transform_codes[transform], 
                X, X_missing, y, phi
            )

        # note we pull off the last column and keep it as our expected_value
        if self.model.n_outputs == 1:
            if self.model_output != ""logloss"":
                self.expected_value = phi[0, -1, 0]
            if flat_output:
                return phi[0, :-1, 0]
            else:
                return phi[:, :-1, 0]
        else:
            if self.model_output != ""logloss"":
                self.expected_value = [phi[0, -1, i] for i in range(phi.shape[2])]
            if flat_output:
                return [phi[0, :-1, i] for i in range(self.model.n_outputs)]
            else:
                return [phi[:, :-1, i] for i in range(self.model.n_outputs)]"
apply_zoom_factor;"def apply_zoom_factor(self):
        """"""Apply zoom factor""""""
        if hasattr(self, 'setZoomFactor'):
            # Assuming Qt >=v4.5
            self.setZoomFactor(self.zoom_factor)
        else:
            # Qt v4.4
            self.setTextSizeMultiplier(self.zoom_factor)"
init_population;"def init_population(self, population_size, graph_max_layer, graph_min_layer):
        """"""
        initialize populations for evolution tuner
        """"""
        population = []
        graph = Graph(max_layer_num=graph_max_layer, min_layer_num=graph_min_layer,
                      inputs=[Layer(LayerType.input.value, output=[4, 5], size='x'), Layer(LayerType.input.value, output=[4, 5], size='y')],
                      output=[Layer(LayerType.output.value, inputs=[4], size='x'), Layer(LayerType.output.value, inputs=[5], size='y')],
                      hide=[Layer(LayerType.attention.value, inputs=[0, 1], output=[2]),
                            Layer(LayerType.attention.value, inputs=[1, 0], output=[3])])
        for _ in range(population_size):
            graph_tmp = copy.deepcopy(graph)
            graph_tmp.mutation()
            population.append(Individual(indiv_id=self.generate_new_id(), graph_cfg=graph_tmp, result=None))
        return population"
add_filter;"def add_filter(self, filter_values):
        """"""
        Construct a filter.
        """"""
        if not filter_values:
            return

        f = self.get_value_filter(filter_values[0])
        for v in filter_values[1:]:
            f |= self.get_value_filter(v)
        return f"
partition_query;"def partition_query(
        self,
        sql,
        params=None,
        param_types=None,
        partition_size_bytes=None,
        max_partitions=None,
    ):
        """"""Perform a ``ParitionQuery`` API request.

        :type sql: str
        :param sql: SQL query statement

        :type params: dict, {str -> column value}
        :param params: values for parameter replacement.  Keys must match
                       the names used in ``sql``.

        :type param_types: dict[str -> Union[dict, .types.Type]]
        :param param_types:
            (Optional) maps explicit types for one or more param values;
            required if parameters are passed.

        :type partition_size_bytes: int
        :param partition_size_bytes:
            (Optional) desired size for each partition generated.  The service
            uses this as a hint, the actual partition size may differ.

        :type max_partitions: int
        :param max_partitions:
            (Optional) desired maximum number of partitions generated. The
            service uses this as a hint, the actual number of partitions may
            differ.

        :rtype: iterable of bytes
        :returns: a sequence of partition tokens

        :raises ValueError:
            for single-use snapshots, or if a transaction ID is
            already associtated with the snapshot.
        """"""
        if not self._multi_use:
            raise ValueError(""Cannot use single-use snapshot."")

        if self._transaction_id is None:
            raise ValueError(""Transaction not started."")

        if params is not None:
            if param_types is None:
                raise ValueError(""Specify 'param_types' when passing 'params'."")
            params_pb = Struct(
                fields={key: _make_value_pb(value) for key, value in params.items()}
            )
        else:
            params_pb = None

        database = self._session._database
        api = database.spanner_api
        metadata = _metadata_with_prefix(database.name)
        transaction = self._make_txn_selector()
        partition_options = PartitionOptions(
            partition_size_bytes=partition_size_bytes, max_partitions=max_partitions
        )

        response = api.partition_query(
            session=self._session.name,
            sql=sql,
            transaction=transaction,
            params=params_pb,
            param_types=param_types,
            partition_options=partition_options,
            metadata=metadata,
        )

        return [partition.partition_token for partition in response.partitions]"
generate_rpn_proposals;"def generate_rpn_proposals(boxes, scores, img_shape,
                           pre_nms_topk, post_nms_topk=None):
    """"""
    Sample RPN proposals by the following steps:
    1. Pick top k1 by scores
    2. NMS them
    3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output.

    Args:
        boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already
        scores: n float, the logits
        img_shape: [h, w]
        pre_nms_topk, post_nms_topk (int): See above.

    Returns:
        boxes: kx4 float
        scores: k logits
    """"""
    assert boxes.shape.ndims == 2, boxes.shape
    if post_nms_topk is None:
        post_nms_topk = pre_nms_topk

    topk = tf.minimum(pre_nms_topk, tf.size(scores))
    topk_scores, topk_indices = tf.nn.top_k(scores, k=topk, sorted=False)
    topk_boxes = tf.gather(boxes, topk_indices)
    topk_boxes = clip_boxes(topk_boxes, img_shape)

    topk_boxes_x1y1x2y2 = tf.reshape(topk_boxes, (-1, 2, 2))
    topk_boxes_x1y1, topk_boxes_x2y2 = tf.split(topk_boxes_x1y1x2y2, 2, axis=1)
    # nx1x2 each
    wbhb = tf.squeeze(topk_boxes_x2y2 - topk_boxes_x1y1, axis=1)
    valid = tf.reduce_all(wbhb > cfg.RPN.MIN_SIZE, axis=1)  # n,
    topk_valid_boxes_x1y1x2y2 = tf.boolean_mask(topk_boxes_x1y1x2y2, valid)
    topk_valid_scores = tf.boolean_mask(topk_scores, valid)

    # TODO not needed
    topk_valid_boxes_y1x1y2x2 = tf.reshape(
        tf.reverse(topk_valid_boxes_x1y1x2y2, axis=[2]),
        (-1, 4), name='nms_input_boxes')
    nms_indices = tf.image.non_max_suppression(
        topk_valid_boxes_y1x1y2x2,
        topk_valid_scores,
        max_output_size=post_nms_topk,
        iou_threshold=cfg.RPN.PROPOSAL_NMS_THRESH)

    topk_valid_boxes = tf.reshape(topk_valid_boxes_x1y1x2y2, (-1, 4))
    proposal_boxes = tf.gather(topk_valid_boxes, nms_indices)
    proposal_scores = tf.gather(topk_valid_scores, nms_indices)
    tf.sigmoid(proposal_scores, name='probs')  # for visualization
    return tf.stop_gradient(proposal_boxes, name='boxes'), tf.stop_gradient(proposal_scores, name='scores')"
kernels_status;"def kernels_status(self, kernel):
        """""" call to the api to get the status of a kernel.
             Parameters
            ==========
            kernel: the kernel to get the status for
        """"""
        if kernel is None:
            raise ValueError('A kernel must be specified')
        if '/' in kernel:
            self.validate_kernel_string(kernel)
            kernel_url_list = kernel.split('/')
            owner_slug = kernel_url_list[0]
            kernel_slug = kernel_url_list[1]
        else:
            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)
            kernel_slug = kernel
        response = self.process_response(
            self.kernel_status_with_http_info(owner_slug, kernel_slug))
        return response"
convert_embedding;"def convert_embedding(builder, layer, input_names, output_names, keras_layer):
    """"""Convert a dense layer from keras to coreml.

    Parameters
    keras_layer: layer
    ----------
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_name, output_name = (input_names[0], output_names[0])

    # Get the weights from keras
    W = keras_layer.get_weights ()[0].T

    # assuming keras embedding layers don't have biases
    builder.add_embedding(name = layer,
                          W = W,
                          b = None,
                          input_dim = keras_layer.input_dim,
                          output_channels = keras_layer.output_dim,
                          has_bias = False,
                          input_name = input_name,
                          output_name = output_name)"
get_lifecycle;"def get_lifecycle(self, policy=None, params=None):
        """"""
        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_

        :arg policy: The name of the index lifecycle policy
        """"""
        return self.transport.perform_request(
            ""GET"", _make_path(""_ilm"", ""policy"", policy), params=params
        )"
setup_figcanvas;"def setup_figcanvas(self):
        """"""Setup the FigureCanvas.""""""
        self.figcanvas = FigureCanvas(background_color=self.background_color)
        self.figcanvas.installEventFilter(self)
        self.setWidget(self.figcanvas)"
set_classifier_interface_params;"def set_classifier_interface_params(spec, features, class_labels,
        model_accessor_for_class_labels, output_features = None):
    """"""
    Common utilities to set the regression interface params.
    """"""
    # Normalize the features list.
    features = _fm.process_or_validate_features(features)

    if class_labels is None:
        raise ValueError(""List of class labels must be provided."")

    n_classes = len(class_labels)

    output_features = _fm.process_or_validate_classifier_output_features(output_features, class_labels)

    if len(output_features) == 1:
        predicted_class_output, pred_cl_type = output_features[0]
        score_output = None
    elif len(output_features) == 2:
        predicted_class_output, pred_cl_type = output_features[0]
        score_output, score_output_type = output_features[1]
    else:
        raise ValueError(""Provided output classes for a classifier must be ""
                ""a list of features, predicted class and (optionally) class_score."")

    spec.description.predictedFeatureName = predicted_class_output

    # Are they out of order?
    if not (pred_cl_type == datatypes.Int64() or pred_cl_type == datatypes.String()):
        raise ValueError(""Provided predicted class output type not Int64 or String (%s).""
                % repr(pred_cl_type))

    if score_output is not None:
        if not isinstance(score_output_type, datatypes.Dictionary):
            raise ValueError(""Provided class score output type not a Dictionary (%s).""
                    % repr(score_output_type))

        if score_output_type.key_type != pred_cl_type:
            raise ValueError((""Provided class score output (%s) key_type (%s) does not ""
                    ""match type of class prediction (%s)."")
                    % (score_output, repr(score_output_type.key_type), repr(pred_cl_type)))

        spec.description.predictedProbabilitiesName = score_output

    # add input
    for index, (cur_input_name, input_type) in enumerate(features):
        input_ = spec.description.input.add()
        input_.name = cur_input_name
        datatypes._set_datatype(input_.type, input_type)

    # add output
    for index, (cur_output_name, output_type) in enumerate(output_features):
        output_ = spec.description.output.add()
        output_.name = cur_output_name
        datatypes._set_datatype(output_.type, output_type)

    # Worry about the class labels
    if pred_cl_type == datatypes.String():
        try:
            for c in class_labels:
                getattr(spec, model_accessor_for_class_labels).stringClassLabels.vector.append(str(c))
        # Not all the classifiers have class labels; in particular the pipeline
        # classifier.  Thus it's not an error if we can't actually set them.
        except AttributeError:
            pass

    else:
        for c in class_labels:
            conv_error = False
            try:
                if not (int(c) == c):
                    conv_error = True
            except:
                conv_error = True

            if conv_error:
                raise TypeError((""Cannot cast '%s' class to an int type "" % str(c))
                + ""(class type determined by type of first class)."")

            try:
                getattr(spec, model_accessor_for_class_labels).int64ClassLabels.vector.append(int(c))
            # Not all the classifiers have class labels; in particular the pipeline
            # classifier.  Thus it's not an error if we can't actually set them.
            except AttributeError:
                break

    # And we are done!
    return spec"
assign_issue;"def assign_issue(issue_key,
                 assignee,
                 server=None,
                 username=None,
                 password=None):
    '''
    Assign the issue to an existing user. Return ``True`` when the issue has
    been properly assigned.

    issue_key
        The JIRA ID of the ticket to manipulate.

    assignee
        The name of the user to assign the ticket to.

    CLI Example:

        salt '*' jira.assign_issue NET-123 example_user
    '''
    jira_ = _get_jira(server=server,
                      username=username,
                      password=password)
    assigned = jira_.assign_issue(issue_key, assignee)
    return assigned"
get_loc;"def get_loc(data, attr={'lr_mult':'0.01'}):
    """"""
    the localisation network in lenet-stn, it will increase acc about more than 1%,
    when num-epoch >=15
    """"""
    loc = mx.symbol.Convolution(data=data, num_filter=30, kernel=(5, 5), stride=(2,2))
    loc = mx.symbol.Activation(data = loc, act_type='relu')
    loc = mx.symbol.Pooling(data=loc, kernel=(2, 2), stride=(2, 2), pool_type='max')
    loc = mx.symbol.Convolution(data=loc, num_filter=60, kernel=(3, 3), stride=(1,1), pad=(1, 1))
    loc = mx.symbol.Activation(data = loc, act_type='relu')
    loc = mx.symbol.Pooling(data=loc, global_pool=True, kernel=(2, 2), pool_type='avg')
    loc = mx.symbol.Flatten(data=loc)
    loc = mx.symbol.FullyConnected(data=loc, num_hidden=6, name=""stn_loc"", attr=attr)
    return loc"
add_periodic_callback;"def add_periodic_callback(self, callback, period_milliseconds):
        ''' Add a callback to be invoked on a session periodically.

        Args:
            callback (callable) :
                A callback function to execute periodically

            period_milliseconds (int) :
                Number of milliseconds between each callback execution.

        Returns:
            PeriodicCallback : can be used with ``remove_periodic_callback``

        .. note::
            Periodic callbacks only work within the context of a Bokeh server
            session. This function will no effect when Bokeh outputs to
            standalone HTML or Jupyter notebook cells.

        '''
        from ..server.callbacks import PeriodicCallback
        cb = PeriodicCallback(self,
                              None,
                              period_milliseconds)
        return self._add_session_callback(cb, callback, one_shot=False, originator=self.add_periodic_callback)"
delete_certificate_signing_request;"def delete_certificate_signing_request(self, name, **kwargs):
        """"""
        delete a CertificateSigningRequest
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_certificate_signing_request(name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the CertificateSigningRequest (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param V1DeleteOptions body:
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \""orphan\"" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.
        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_certificate_signing_request_with_http_info(name, **kwargs)
        else:
            (data) = self.delete_certificate_signing_request_with_http_info(name, **kwargs)
            return data"
lowdata_fmt;"def lowdata_fmt():
    '''
    Validate and format lowdata from incoming unserialized request data

    This tool requires that the hypermedia_in tool has already been run.
    '''

    if cherrypy.request.method.upper() != 'POST':
        return

    data = cherrypy.request.unserialized_data

    # if the data was sent as urlencoded, we need to make it a list.
    # this is a very forgiving implementation as different clients set different
    # headers for form encoded data (including charset or something similar)
    if data and isinstance(data, collections.Mapping):
        # Make the 'arg' param a list if not already
        if 'arg' in data and not isinstance(data['arg'], list):
            data['arg'] = [data['arg']]

        # Finally, make a Low State and put it in request
        cherrypy.request.lowstate = [data]
    else:
        cherrypy.serving.request.lowstate = data"
start_order_threading;"def start_order_threading(self):
        """"""开启查询子线程(实盘中用)
        """"""

        self.if_start_orderthreading = True

        self.order_handler.if_start_orderquery = True
        self.trade_engine.create_kernel('ORDER', daemon=True)
        self.trade_engine.start_kernel('ORDER')
        self.sync_order_and_deal()"
undeclared_query_parameters;"def undeclared_query_parameters(self):
        """"""Return undeclared query parameters from job statistics, if present.

        See:
        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.undeclaredQueryParameters

        :rtype:
            list of
            :class:`~google.cloud.bigquery.ArrayQueryParameter`,
            :class:`~google.cloud.bigquery.ScalarQueryParameter`, or
            :class:`~google.cloud.bigquery.StructQueryParameter`
        :returns: undeclared parameters, or an empty list if the query has
                  not yet completed.
        """"""
        parameters = []
        undeclared = self._job_statistics().get(""undeclaredQueryParameters"", ())

        for parameter in undeclared:
            p_type = parameter[""parameterType""]

            if ""arrayType"" in p_type:
                klass = ArrayQueryParameter
            elif ""structTypes"" in p_type:
                klass = StructQueryParameter
            else:
                klass = ScalarQueryParameter

            parameters.append(klass.from_api_repr(parameter))

        return parameters"
api_version;"def api_version(profile=None, **connection_args):
    '''
    Returns the API version derived from endpoint's response.

    CLI Example:

    .. code-block:: bash

        salt '*' keystone.api_version
    '''
    kwargs = _get_kwargs(profile=profile, **connection_args)
    auth_url = kwargs.get('auth_url', kwargs.get('endpoint', None))
    try:
        return salt.utils.http.query(auth_url, decode=True, decode_type='json',
                                     verify_ssl=False)['dict']['version']['id']
    except KeyError:
        return None"
set_project_dir;"def set_project_dir(self, directory):
        """"""Set the project directory""""""
        if directory is not None:
            self.treewidget.set_root_path(osp.dirname(directory))
            self.treewidget.set_folder_names([osp.basename(directory)])
        self.treewidget.setup_project_view()
        try:
            self.treewidget.setExpanded(self.treewidget.get_index(directory),
                                        True)
        except TypeError:
            pass"
format_version;"def format_version(major, minor, patch, prerelease=None, build=None):
    """"""Format a version according to the Semantic Versioning specification

    :param str major: the required major part of a version
    :param str minor: the required minor part of a version
    :param str patch: the required patch part of a version
    :param str prerelease: the optional prerelease part of a version
    :param str build: the optional build part of a version
    :return: the formatted string
    :rtype: str

    >>> import semver
    >>> semver.format_version(3, 4, 5, 'pre.2', 'build.4')
    '3.4.5-pre.2+build.4'
    """"""
    version = ""%d.%d.%d"" % (major, minor, patch)
    if prerelease is not None:
        version = version + ""-%s"" % prerelease

    if build is not None:
        version = version + ""+%s"" % build

    return version"
variable_summaries;"def variable_summaries(var):
  """"""Attach a lot of summaries to a Tensor (for TensorBoard visualization).""""""
  with tf.name_scope('summaries'):
    mean = tf.reduce_mean(var)
    tf.summary.scalar('mean', mean)
    with tf.name_scope('stddev'):
      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
    tf.summary.scalar('stddev', stddev)
    tf.summary.scalar('max', tf.reduce_max(var))
    tf.summary.scalar('min', tf.reduce_min(var))
    tf.summary.histogram('histogram', var)"
get_sessions;"def get_sessions(name, backend, socket=DEFAULT_SOCKET_URL):
    '''
    .. versionadded:: 2016.11.0

    Get number of current sessions on server in backend (scur)

    name
        Server name

    backend
        haproxy backend

    socket
        haproxy stats socket, default ``/var/run/haproxy.sock``

    CLI Example:

    .. code-block:: bash

        salt '*' haproxy.get_sessions web1.example.com www
    '''
    class getStats(haproxy.cmds.Cmd):
        p_args = [""backend"", ""server""]
        cmdTxt = ""show stat\r\n""
        helpText = ""Fetch all statistics""

    ha_conn = _get_conn(socket)
    ha_cmd = getStats(server=name, backend=backend)
    result = ha_conn.sendCmd(ha_cmd)
    for line in result.split('\n'):
        if line.startswith(backend):
            outCols = line.split(',')
            if outCols[1] == name:
                return outCols[4]"
add_cog;"def add_cog(self, cog):
        """"""Adds a ""cog"" to the bot.

        A cog is a class that has its own event listeners and commands.

        Parameters
        -----------
        cog: :class:`.Cog`
            The cog to register to the bot.

        Raises
        -------
        TypeError
            The cog does not inherit from :class:`.Cog`.
        CommandError
            An error happened during loading.
        """"""

        if not isinstance(cog, Cog):
            raise TypeError('cogs must derive from Cog')

        cog = cog._inject(self)
        self.__cogs[cog.__cog_name__] = cog"
extract_labels;"def extract_labels(filename):
    """"""Extract the labels into a 1D uint8 numpy array [index].""""""
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2049:
            raise ValueError(
                'Invalid magic number %d in MNIST label file: %s' %
                (magic, filename))
        num_items = _read32(bytestream)
        buf = bytestream.read(num_items)
        labels = numpy.frombuffer(buf, dtype=numpy.uint8)
        return labels"
run_commands;"def run_commands(*commands, **kwargs):
    '''
    Sends the commands over the transport to the device.

    This function sends the commands to the device using the nodes
    transport.  This is a lower layer function that shouldn't normally
    need to be used, preferring instead to use ``config()`` or ``enable()``.

    transport: ``https``
        Specifies the type of connection transport to use. Valid values for the
        connection are ``socket``, ``http_local``, ``http``, and  ``https``.

        .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    host: ``localhost``
        The IP address or DNS host name of the connection device.

        .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    username: ``admin``
        The username to pass to the device to authenticate the eAPI connection.

         .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    password
        The password to pass to the device to authenticate the eAPI connection.

        .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    port
        The TCP port of the endpoint for the eAPI connection. If this keyword is
        not specified, the default value is automatically determined by the
        transport type (``80`` for ``http``, or ``443`` for ``https``).

        .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    enablepwd
        The enable mode password if required by the destination node.

        .. note::

            This argument does not need to be specified when running in a
            :mod:`pyeapi <salt.proxy.arista_pyeapi>` Proxy Minion.

    CLI Example:

    .. code-block:: bash

        salt '*' pyeapi.run_commands 'show version'
        salt '*' pyeapi.run_commands 'show version' encoding=text
        salt '*' pyeapi.run_commands 'show version' encoding=text host=cr1.thn.lon username=example password=weak

    Output example:

    .. code-block:: text

      veos1:
          |_
            ----------
            architecture:
                i386
            bootupTimestamp:
                1527541728.53
            hardwareRevision:
            internalBuildId:
                63d2e89a-220d-4b8a-a9b3-0524fa8f9c5f
            internalVersion:
                4.18.1F-4591672.4181F
            isIntlVersion:
                False
            memFree:
                501468
            memTotal:
                1893316
            modelName:
                vEOS
            serialNumber:
            systemMacAddress:
                52:54:00:3f:e6:d0
            version:
                4.18.1F
    '''
    encoding = kwargs.pop('encoding', 'json')
    send_enable = kwargs.pop('send_enable', True)
    output = call('run_commands',
                  commands,
                  encoding=encoding,
                  send_enable=send_enable,
                  **kwargs)
    if encoding == 'text':
        ret = []
        for res in output:
            ret.append(res['output'])
        return ret
    return output"
update_metric;"def update_metric(self, metric, labels, pre_sliced=False):
        """"""Update evaluation metric with label and current outputs.""""""
        for current_exec, (texec, islice) in enumerate(zip(self.train_execs, self.slices)):
            if not pre_sliced:
                labels_slice = [label[islice] for label in labels]
            else:
                labels_slice = labels[current_exec]
            metric.update(labels_slice, texec.outputs)"
key_ring_path;"def key_ring_path(cls, project, location, key_ring):
        """"""Return a fully-qualified key_ring string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/locations/{location}/keyRings/{key_ring}"",
            project=project,
            location=location,
            key_ring=key_ring,
        )"
connect_pub;"def connect_pub(self, timeout=None):
        '''
        Establish the publish connection
        '''
        if self.cpub:
            return True

        if self._run_io_loop_sync:
            with salt.utils.asynchronous.current_ioloop(self.io_loop):
                if self.subscriber is None:
                    self.subscriber = salt.transport.ipc.IPCMessageSubscriber(
                    self.puburi,
                    io_loop=self.io_loop
                )
                try:
                    self.io_loop.run_sync(
                        lambda: self.subscriber.connect(timeout=timeout))
                    self.cpub = True
                except Exception:
                    pass
        else:
            if self.subscriber is None:
                self.subscriber = salt.transport.ipc.IPCMessageSubscriber(
                self.puburi,
                io_loop=self.io_loop
            )

            # For the asynchronous case, the connect will be defered to when
            # set_event_handler() is invoked.
            self.cpub = True
        return self.cpub"
normalize_eols;"def normalize_eols(text, eol='\n'):
    """"""Use the same eol's in text""""""
    for eol_char, _ in EOL_CHARS:
        if eol_char != eol:
            text = text.replace(eol_char, eol)
    return text"
make_graph;"def make_graph(dists, scheme='default'):
    """"""Makes a dependency graph from the given distributions.

    :parameter dists: a list of distributions
    :type dists: list of :class:`distutils2.database.InstalledDistribution` and
                 :class:`distutils2.database.EggInfoDistribution` instances
    :rtype: a :class:`DependencyGraph` instance
    """"""
    scheme = get_scheme(scheme)
    graph = DependencyGraph()
    provided = {}  # maps names to lists of (version, dist) tuples

    # first, build the graph and find out what's provided
    for dist in dists:
        graph.add_distribution(dist)

        for p in dist.provides:
            name, version = parse_name_and_version(p)
            logger.debug('Add to provided: %s, %s, %s', name, version, dist)
            provided.setdefault(name, []).append((version, dist))

    # now make the edges
    for dist in dists:
        requires = (dist.run_requires | dist.meta_requires |
                    dist.build_requires | dist.dev_requires)
        for req in requires:
            try:
                matcher = scheme.matcher(req)
            except UnsupportedVersionError:
                # XXX compat-mode if cannot read the version
                logger.warning('could not read version %r - using name only',
                               req)
                name = req.split()[0]
                matcher = scheme.matcher(name)

            name = matcher.key   # case-insensitive

            matched = False
            if name in provided:
                for version, provider in provided[name]:
                    try:
                        match = matcher.match(version)
                    except UnsupportedVersionError:
                        match = False

                    if match:
                        graph.add_edge(dist, provider, req)
                        matched = True
                        break
            if not matched:
                graph.add_missing(dist, req)
    return graph"
reset_index;"def reset_index(self, **kwargs):
        """"""Removes all levels from index and sets a default level_0 index.

        Returns:
            A new QueryCompiler with updated data and reset index.
        """"""
        drop = kwargs.get(""drop"", False)
        new_index = pandas.RangeIndex(len(self.index))
        if not drop:
            if isinstance(self.index, pandas.MultiIndex):
                # TODO (devin-petersohn) ensure partitioning is properly aligned
                new_column_names = pandas.Index(self.index.names)
                new_columns = new_column_names.append(self.columns)
                index_data = pandas.DataFrame(list(zip(*self.index))).T
                result = self.data.from_pandas(index_data).concat(1, self.data)
                return self.__constructor__(result, new_index, new_columns)
            else:
                new_column_name = (
                    self.index.name
                    if self.index.name is not None
                    else ""index""
                    if ""index"" not in self.columns
                    else ""level_0""
                )
                new_columns = self.columns.insert(0, new_column_name)
                result = self.insert(0, new_column_name, self.index)
                return self.__constructor__(result.data, new_index, new_columns)
        else:
            # The copies here are to ensure that we do not give references to
            # this object for the purposes of updates.
            return self.__constructor__(
                self.data.copy(), new_index, self.columns.copy(), self._dtype_cache
            )"
invoke_common_options;"def invoke_common_options(f):
    """"""
    Common CLI options shared by ""local invoke"" and ""local start-api"" commands

    :param f: Callback passed by Click
    """"""

    invoke_options = [
        template_click_option(),

        click.option('--env-vars', '-n',
                     type=click.Path(exists=True),
                     help=""JSON file containing values for Lambda function's environment variables.""),

        parameter_override_click_option(),

        click.option('--debug-port', '-d',
                     help=""When specified, Lambda function container will start in debug mode and will expose this ""
                          ""port on localhost."",
                     envvar=""SAM_DEBUG_PORT""),

        click.option('--debugger-path',
                     help=""Host path to a debugger that will be mounted into the Lambda container.""),

        click.option('--debug-args',
                     help=""Additional arguments to be passed to the debugger."",
                     envvar=""DEBUGGER_ARGS""),

        click.option('--docker-volume-basedir', '-v',
                     envvar=""SAM_DOCKER_VOLUME_BASEDIR"",
                     help=""Specifies the location basedir where the SAM file exists. If the Docker is running on ""
                          ""a remote machine, you must mount the path where the SAM file exists on the docker machine ""
                          ""and modify this value to match the remote machine.""),

        click.option('--log-file', '-l',
                     help=""logfile to send runtime logs to.""),

        click.option('--layer-cache-basedir',
                     type=click.Path(exists=False, file_okay=False),
                     envvar=""SAM_LAYER_CACHE_BASEDIR"",
                     help=""Specifies the location basedir where the Layers your template uses will be downloaded to."",
                     default=get_default_layer_cache_dir()),

    ] + docker_click_options() + [

        click.option('--force-image-build',
                     is_flag=True,
                     help='Specify whether CLI should rebuild the image used for invoking functions with layers.',
                     envvar='SAM_FORCE_IMAGE_BUILD',
                     default=False),

    ]

    # Reverse the list to maintain ordering of options in help text printed with --help
    for option in reversed(invoke_options):
        option(f)

    return f"
add_color_scheme_stack;"def add_color_scheme_stack(self, scheme_name, custom=False):
        """"""Add a stack for a given scheme and connects the CONF values.""""""
        color_scheme_groups = [
            (_('Text'), [""normal"", ""comment"", ""string"", ""number"", ""keyword"",
                         ""builtin"", ""definition"", ""instance"", ]),
            (_('Highlight'), [""currentcell"", ""currentline"", ""occurrence"",
                              ""matched_p"", ""unmatched_p"", ""ctrlclick""]),
            (_('Background'), [""background"", ""sideareas""])
            ]

        parent = self.parent
        line_edit = parent.create_lineedit(_(""Scheme name:""),
                                           '{0}/name'.format(scheme_name))

        self.widgets[scheme_name] = {}

        # Widget setup
        line_edit.label.setAlignment(Qt.AlignRight | Qt.AlignVCenter)
        self.setWindowTitle(_('Color scheme editor'))

        # Layout
        name_layout = QHBoxLayout()
        name_layout.addWidget(line_edit.label)
        name_layout.addWidget(line_edit.textbox)
        self.scheme_name_textbox[scheme_name] = line_edit.textbox

        if not custom:
            line_edit.textbox.setDisabled(True)
        if not self.isVisible():
            line_edit.setVisible(False)

        cs_layout = QVBoxLayout()
        cs_layout.addLayout(name_layout)

        h_layout = QHBoxLayout()
        v_layout = QVBoxLayout()

        for index, item in enumerate(color_scheme_groups):
            group_name, keys = item
            group_layout = QGridLayout()

            for row, key in enumerate(keys):
                option = ""{0}/{1}"".format(scheme_name, key)
                value = self.parent.get_option(option)
                name = syntaxhighlighters.COLOR_SCHEME_KEYS[key]

                if is_text_string(value):
                    label, clayout = parent.create_coloredit(
                        name,
                        option,
                        without_layout=True,
                        )
                    label.setAlignment(Qt.AlignRight | Qt.AlignVCenter)
                    group_layout.addWidget(label, row+1, 0)
                    group_layout.addLayout(clayout, row+1, 1)

                    # Needed to update temp scheme to obtain instant preview
                    self.widgets[scheme_name][key] = [clayout]
                else:
                    label, clayout, cb_bold, cb_italic = parent.create_scedit(
                        name,
                        option,
                        without_layout=True,
                        )
                    label.setAlignment(Qt.AlignRight | Qt.AlignVCenter)
                    group_layout.addWidget(label, row+1, 0)
                    group_layout.addLayout(clayout, row+1, 1)
                    group_layout.addWidget(cb_bold, row+1, 2)
                    group_layout.addWidget(cb_italic, row+1, 3)

                    # Needed to update temp scheme to obtain instant preview
                    self.widgets[scheme_name][key] = [clayout, cb_bold,
                                                      cb_italic]

            group_box = QGroupBox(group_name)
            group_box.setLayout(group_layout)

            if index == 0:
                h_layout.addWidget(group_box)
            else:
                v_layout.addWidget(group_box)

        h_layout.addLayout(v_layout)
        cs_layout.addLayout(h_layout)

        stackitem = QWidget()
        stackitem.setLayout(cs_layout)
        self.stack.addWidget(stackitem)
        self.order.append(scheme_name)"
approximate_gradient;"def approximate_gradient(decision_function, sample, num_evals,
                         delta, constraint, shape, clip_min, clip_max):
  """""" Gradient direction estimation """"""
  # Generate random vectors.
  noise_shape = [num_evals] + list(shape)
  if constraint == 'l2':
    rv = np.random.randn(*noise_shape)
  elif constraint == 'linf':
    rv = np.random.uniform(low=-1, high=1, size=noise_shape)

  axis = tuple(range(1, 1 + len(shape)))
  rv = rv / np.sqrt(np.sum(rv ** 2, axis=axis, keepdims=True))
  perturbed = sample + delta * rv
  perturbed = clip_image(perturbed, clip_min, clip_max)
  rv = (perturbed - sample) / delta

  # query the model.
  decisions = decision_function(perturbed)
  decision_shape = [len(decisions)] + [1] * len(shape)
  fval = 2 * decisions.astype(np_dtype).reshape(decision_shape) - 1.0

  # Baseline subtraction (when fval differs)
  if np.mean(fval) == 1.0:  # label changes.
    gradf = np.mean(rv, axis=0)
  elif np.mean(fval) == -1.0:  # label not change.
    gradf = - np.mean(rv, axis=0)
  else:
    fval = fval - np.mean(fval)
    gradf = np.mean(fval * rv, axis=0)

  # Get the gradient direction.
  gradf = gradf / np.linalg.norm(gradf)

  return gradf"
get_hashed_rule_name;"def get_hashed_rule_name(event, function, lambda_name):
        """"""
        Returns an AWS-valid CloudWatch rule name using a digest of the event name, lambda name, and function.
        This allows support for rule names that may be longer than the 64 char limit.
        """"""
        event_name = event.get('name', function)
        name_hash = hashlib.sha1('{}-{}'.format(lambda_name, event_name).encode('UTF-8')).hexdigest()
        return Zappa.get_event_name(name_hash, function)"
parse_args;"def parse_args():
    """"""
    Parse arguments.
    """"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--gpu-id', type=int, default=0,
                        help='GPU id (-1 means CPU)')
    parser.add_argument('--train-file', default='snli_1.0/snli_1.0_train.txt',
                        help='training set file')
    parser.add_argument('--test-file', default='snli_1.0/snli_1.0_dev.txt',
                        help='validation set file')
    parser.add_argument('--max-num-examples', type=int, default=-1,
                        help='maximum number of examples to load (for debugging)')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='batch size')
    parser.add_argument('--print-interval', type=int, default=20,
                        help='the interval of two print')
    parser.add_argument('--mode', choices=['train', 'test'], default='train',
                        help='train or test')
    parser.add_argument('--lr', type=float, default=0.025,
                        help='learning rate')
    parser.add_argument('--epochs', type=int, default=300,
                        help='maximum number of epochs to train')
    parser.add_argument('--embedding', default='glove',
                        help='word embedding type')
    parser.add_argument('--embedding-source', default='glove.840B.300d',
                        help='embedding file source')
    parser.add_argument('--embedding-size', type=int, default=300,
                        help='size of pretrained word embedding')
    parser.add_argument('--hidden-size', type=int, default=200,
                        help='hidden layer size')
    parser.add_argument('--output-dir', default='./output',
                        help='directory for all experiment output')
    parser.add_argument('--model-dir', default='./output',
                        help='directory to load model')
    parser.add_argument('--seed', type=int, default=0,
                        help='random seed')
    parser.add_argument('--dropout', type=float, default=0.,
                        help='dropout rate')
    parser.add_argument('--weight-decay', type=float, default=0.,
                        help='l2 regularization weight')
    parser.add_argument('--intra-attention', action='store_true',
                        help='use intra-sentence attention')

    return parser.parse_args()"
new_log_files;"def new_log_files(self, name, redirect_output=True):
        """"""Generate partially randomized filenames for log files.

        Args:
            name (str): descriptive string for this log file.
            redirect_output (bool): True if files should be generated for
                logging stdout and stderr and false if stdout and stderr
                should not be redirected.
                If it is None, it will use the ""redirect_output"" Ray parameter.

        Returns:
            If redirect_output is true, this will return a tuple of two
                file handles. The first is for redirecting stdout and the
                second is for redirecting stderr.
                If redirect_output is false, this will return a tuple
                of two None objects.
        """"""
        if redirect_output is None:
            redirect_output = self._ray_params.redirect_output
        if not redirect_output:
            return None, None

        log_stdout = self._make_inc_temp(
            suffix="".out"", prefix=name, directory_name=self._logs_dir)
        log_stderr = self._make_inc_temp(
            suffix="".err"", prefix=name, directory_name=self._logs_dir)
        # Line-buffer the output (mode 1).
        log_stdout_file = open(log_stdout, ""a"", buffering=1)
        log_stderr_file = open(log_stderr, ""a"", buffering=1)
        return log_stdout_file, log_stderr_file"
set_harddisk_sleep;"def set_harddisk_sleep(minutes):
    '''
    Set the amount of idle time until the harddisk sleeps. Pass ""Never"" of ""Off""
    to never sleep.

    :param minutes: Can be an integer between 1 and 180 or ""Never"" or ""Off""
    :ptype: int, str

    :return: True if successful, False if not
    :rtype: bool

    CLI Example:

    .. code-block:: bash

        salt '*' power.set_harddisk_sleep 120
        salt '*' power.set_harddisk_sleep off
    '''
    value = _validate_sleep(minutes)
    cmd = 'systemsetup -setharddisksleep {0}'.format(value)
    salt.utils.mac_utils.execute_return_success(cmd)

    return salt.utils.mac_utils.confirm_updated(
        str(value),
        get_harddisk_sleep,
    )"
python_executable_changed;"def python_executable_changed(self, pyexec):
        """"""Custom Python executable value has been changed""""""
        if not self.cus_exec_radio.isChecked():
            return False
        def_pyexec = get_python_executable()
        if not is_text_string(pyexec):
            pyexec = to_text_string(pyexec.toUtf8(), 'utf-8')
        if pyexec == def_pyexec:
            return False
        if (not programs.is_python_interpreter(pyexec) or
                not self.warn_python_compatibility(pyexec)):
            QMessageBox.warning(self, _('Warning'),
                    _(""You selected an invalid Python interpreter for the ""
                      ""console so the previous interpreter will stay. Please ""
                      ""make sure to select a valid one.""), QMessageBox.Ok)
            self.def_exec_radio.setChecked(True)
            return False
        return True"
convert_permute;"def convert_permute(builder, layer, input_names, output_names, keras_layer):
    """"""Convert a softmax layer from keras to coreml.

    Parameters
    keras_layer: layer
    ----------
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    input_name, output_name = (input_names[0], output_names[0])

    keras_dims = keras_layer.dims
    # Keras permute layer index begins at 1
    if len(keras_dims) == 3:
        # Keras input tensor interpret as (H,W,C)
        x = list(np.array(keras_dims))
        i1, i2, i3 = x.index(1), x.index(2), x.index(3)
        x[i1], x[i2], x[i3] = 2, 3, 1
        # add a sequence axis
        x = [0] + x
        dim = tuple(x)
    elif len(keras_dims) == 4:
        # Here we use Keras converter as a place holder for inserting
        # permutations - the values here are not valid Keras dim parameters
        # but parameters we need to use to convert to CoreML model
        dim = keras_dims
    else:
        raise NotImplementedError('Supports only 3d permutation.')

    builder.add_permute(name = layer, dim=dim, input_name = input_name,
            output_name = output_name)"
copy_blob;"def copy_blob(
        self,
        blob,
        destination_bucket,
        new_name=None,
        client=None,
        preserve_acl=True,
        source_generation=None,
    ):
        """"""Copy the given blob to the given bucket, optionally with a new name.

        If :attr:`user_project` is set, bills the API request to that project.

        :type blob: :class:`google.cloud.storage.blob.Blob`
        :param blob: The blob to be copied.

        :type destination_bucket: :class:`google.cloud.storage.bucket.Bucket`
        :param destination_bucket: The bucket into which the blob should be
                                   copied.

        :type new_name: str
        :param new_name: (optional) the new name for the copied file.

        :type client: :class:`~google.cloud.storage.client.Client` or
                      ``NoneType``
        :param client: Optional. The client to use.  If not passed, falls back
                       to the ``client`` stored on the current bucket.

        :type preserve_acl: bool
        :param preserve_acl: Optional. Copies ACL from old blob to new blob.
                             Default: True.

        :type source_generation: long
        :param source_generation: Optional. The generation of the blob to be
                                  copied.

        :rtype: :class:`google.cloud.storage.blob.Blob`
        :returns: The new Blob.
        """"""
        client = self._require_client(client)
        query_params = {}

        if self.user_project is not None:
            query_params[""userProject""] = self.user_project

        if source_generation is not None:
            query_params[""sourceGeneration""] = source_generation

        if new_name is None:
            new_name = blob.name

        new_blob = Blob(bucket=destination_bucket, name=new_name)
        api_path = blob.path + ""/copyTo"" + new_blob.path
        copy_result = client._connection.api_request(
            method=""POST"",
            path=api_path,
            query_params=query_params,
            _target_object=new_blob,
        )

        if not preserve_acl:
            new_blob.acl.save(acl={}, client=client)

        new_blob._set_properties(copy_result)
        return new_blob"
get_meta_graph_copy;"def get_meta_graph_copy(self, tags=None):
    """"""Returns a copy of a MetaGraph with the identical set of tags.""""""
    meta_graph = self.get_meta_graph(tags)
    copy = tf_v1.MetaGraphDef()
    copy.CopyFrom(meta_graph)
    return copy"
del_password;"def del_password(name):
    '''
    Deletes the account password

    :param str name: The user name of the account

    :return: True if successful, otherwise False
    :rtype: bool

    :raises: CommandExecutionError on user not found or any other unknown error

    CLI Example:

    .. code-block:: bash

        salt '*' shadow.del_password username
    '''
    # This removes the password
    cmd = ""dscl . -passwd /Users/{0} ''"".format(name)
    try:
        salt.utils.mac_utils.execute_return_success(cmd)
    except CommandExecutionError as exc:
        if 'eDSUnknownNodeName' in exc.strerror:
            raise CommandExecutionError('User not found: {0}'.format(name))
        raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))

    # This is so it looks right in shadow.info
    cmd = ""dscl . -create /Users/{0} Password '*'"".format(name)
    salt.utils.mac_utils.execute_return_success(cmd)

    return info(name)['passwd'] == '*'"
get_port_def;"def get_port_def(port_num, proto='tcp'):
    '''
    Given a port number and protocol, returns the port definition expected by
    docker-py. For TCP ports this is simply an integer, for UDP ports this is
    (port_num, 'udp').

    port_num can also be a string in the format 'port_num/udp'. If so, the
    ""proto"" argument will be ignored. The reason we need to be able to pass in
    the protocol separately is because this function is sometimes invoked on
    data derived from a port range (e.g. '2222-2223/udp'). In these cases the
    protocol has already been stripped off and the port range resolved into the
    start and end of the range, and get_port_def() is invoked once for each
    port number in that range. So, rather than munge udp ports back into
    strings before passing them to this function, the function will see if it
    has a string and use the protocol from it if present.

    This function does not catch the TypeError or ValueError which would be
    raised if the port number is non-numeric. This function either needs to be
    run on known good input, or should be run within a try/except that catches
    these two exceptions.
    '''
    try:
        port_num, _, port_num_proto = port_num.partition('/')
    except AttributeError:
        pass
    else:
        if port_num_proto:
            proto = port_num_proto
    try:
        if proto.lower() == 'udp':
            return int(port_num), 'udp'
    except AttributeError:
        pass
    return int(port_num)"
linux_interfaces;"def linux_interfaces():
    '''
    Obtain interface information for *NIX/BSD variants
    '''
    ifaces = dict()
    ip_path = salt.utils.path.which('ip')
    ifconfig_path = None if ip_path else salt.utils.path.which('ifconfig')
    if ip_path:
        cmd1 = subprocess.Popen(
            '{0} link show'.format(ip_path),
            shell=True,
            close_fds=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT).communicate()[0]
        cmd2 = subprocess.Popen(
            '{0} addr show'.format(ip_path),
            shell=True,
            close_fds=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT).communicate()[0]
        ifaces = _interfaces_ip(""{0}\n{1}"".format(
            salt.utils.stringutils.to_str(cmd1),
            salt.utils.stringutils.to_str(cmd2)))
    elif ifconfig_path:
        cmd = subprocess.Popen(
            '{0} -a'.format(ifconfig_path),
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT).communicate()[0]
        ifaces = _interfaces_ifconfig(salt.utils.stringutils.to_str(cmd))
    return ifaces"
load_model;"def load_model(model_path):
    """"""Load a libsvm model from a path on disk.

    This currently supports:
      * C-SVC
      * NU-SVC
      * Epsilon-SVR
      * NU-SVR

    Parameters
    ----------
    model_path: str
        Path on disk where the libsvm model representation is.

    Returns
    -------
    model: libsvm_model
        A model of the libsvm format.
    """"""
    if not(HAS_LIBSVM):
        raise RuntimeError('libsvm not found. libsvm conversion API is disabled.')

    from svmutil import svm_load_model # From libsvm
    import os
    if (not os.path.exists(model_path)):
        raise IOError(""Expected a valid file path. %s does not exist"" % model_path)
    return svm_load_model(model_path)"
get_template_data;"def get_template_data(template_file):
    """"""
    Read the template file, parse it as JSON/YAML and return the template as a dictionary.

    Parameters
    ----------
    template_file : string
        Path to the template to read

    Returns
    -------
    Template data as a dictionary
    """"""

    if not pathlib.Path(template_file).exists():
        raise ValueError(""Template file not found at {}"".format(template_file))

    with open(template_file, 'r') as fp:
        try:
            return yaml_parse(fp.read())
        except (ValueError, yaml.YAMLError) as ex:
            raise ValueError(""Failed to parse template: {}"".format(str(ex)))"
process_buffers_for_display;"def process_buffers_for_display(s, limit=40):
  """"""Process a buffer for human-readable display.

  This function performs the following operation on each of the buffers in `s`.
    1. Truncate input buffer if the length of the buffer is greater than
       `limit`, to prevent large strings from overloading the frontend.
    2. Apply `binascii.b2a_qp` on the truncated buffer to make the buffer
       printable and convertible to JSON.
    3. If truncation happened (in step 1), append a string at the end
       describing the original length and the truncation.

  Args:
    s: The buffer to be processed, either a single buffer or a nested array of
      them.
    limit: Length limit for each buffer, beyond which truncation will occur.

  Return:
    A single processed buffer or a nested array of processed buffers.
  """"""
  if isinstance(s, (list, tuple)):
    return [process_buffers_for_display(elem, limit=limit) for elem in s]
  else:
    length = len(s)
    if length > limit:
      return (binascii.b2a_qp(s[:limit]) +
              b' (length-%d truncated at %d bytes)' % (length, limit))
    else:
      return binascii.b2a_qp(s)"
run_todo_finder;"def run_todo_finder(self):
        """"""Run TODO finder""""""
        if self.editor.is_python():
            self.threadmanager.add_thread(codeanalysis.find_tasks,
                                          self.todo_finished,
                                          self.get_source_code(), self)"
sed_contains;"def sed_contains(path,
                 text,
                 limit='',
                 flags='g'):
    '''
    .. deprecated:: 0.17.0
       Use :func:`search` instead.

    Return True if the file at ``path`` contains ``text``. Utilizes sed to
    perform the search (line-wise search).

    Note: the ``p`` flag will be added to any flags you pass in.

    CLI Example:

    .. code-block:: bash

        salt '*' file.contains /etc/crontab 'mymaintenance.sh'
    '''
    # Largely inspired by Fabric's contrib.files.contains()
    path = os.path.expanduser(path)

    if not os.path.exists(path):
        return False

    before = _sed_esc(six.text_type(text), False)
    limit = _sed_esc(six.text_type(limit), False)
    options = '-n -r -e'
    if sys.platform == 'darwin':
        options = options.replace('-r', '-E')

    cmd = ['sed']
    cmd.extend(salt.utils.args.shlex_split(options))
    cmd.append(
        r'{limit}s/{before}/$/{flags}'.format(
            limit='/{0}/ '.format(limit) if limit else '',
            before=before,
            flags='p{0}'.format(flags)
        )
    )
    cmd.append(path)

    result = __salt__['cmd.run'](cmd, python_shell=False)

    return bool(result)"
start_session;"def start_session(self):
        """"""
        Starts a Salesforce session and determines which SF instance to use for future requests.
        """"""
        if self.has_active_session():
            raise Exception(""Session already in progress."")

        response = requests.post(self._get_login_url(),
                                 headers=self._get_login_headers(),
                                 data=self._get_login_xml())
        response.raise_for_status()

        root = ET.fromstring(response.text)
        for e in root.iter(""%ssessionId"" % self.SOAP_NS):
            if self.session_id:
                raise Exception(""Invalid login attempt.  Multiple session ids found."")
            self.session_id = e.text

        for e in root.iter(""%sserverUrl"" % self.SOAP_NS):
            if self.server_url:
                raise Exception(""Invalid login attempt.  Multiple server urls found."")
            self.server_url = e.text

        if not self.has_active_session():
            raise Exception(""Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s]."" %
                            (self.session_id, self.server_url))
        self.hostname = urlsplit(self.server_url).hostname"
set_write_bit;"def set_write_bit(fn):
    # type: (str) -> None
    """"""
    Set read-write permissions for the current user on the target path.  Fail silently
    if the path doesn't exist.

    :param str fn: The target filename or path
    :return: None
    """"""

    fn = fs_encode(fn)
    if not os.path.exists(fn):
        return
    file_stat = os.stat(fn).st_mode
    os.chmod(fn, file_stat | stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)
    if not os.path.isdir(fn):
        for path in [fn, os.path.dirname(fn)]:
            try:
                os.chflags(path, 0)
            except AttributeError:
                pass
        return None
    for root, dirs, files in os.walk(fn, topdown=False):
        for dir_ in [os.path.join(root, d) for d in dirs]:
            set_write_bit(dir_)
        for file_ in [os.path.join(root, f) for f in files]:
            set_write_bit(file_)"
quick_layout_settings;"def quick_layout_settings(self):
        """"""Layout settings dialog""""""
        get = CONF.get
        set_ = CONF.set

        section = 'quick_layouts'

        names = get(section, 'names')
        order = get(section, 'order')
        active = get(section, 'active')

        dlg = self.dialog_layout_settings(self, names, order, active)
        if dlg.exec_():
            set_(section, 'names', dlg.names)
            set_(section, 'order', dlg.order)
            set_(section, 'active', dlg.active)
            self.quick_layout_set_menu()"
benchmark_command;"def benchmark_command(cmd, progress):
    """"""Benchmark one command execution""""""
    full_cmd = '/usr/bin/time --format=""%U %M"" {0}'.format(cmd)
    print '{0:6.2f}% Running {1}'.format(100.0 * progress, full_cmd)
    (_, err) = subprocess.Popen(
        ['/bin/sh', '-c', full_cmd],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    ).communicate('')

    values = err.strip().split(' ')
    if len(values) == 2:
        try:
            return (float(values[0]), float(values[1]))
        except:  # pylint:disable=I0011,W0702
            pass  # Handled by the code after the ""if""

    print err
    raise Exception('Error during benchmarking')"
run_query;"def run_query(method, params, **kwargs):
    '''
    Send Zabbix API call

    Args:
        method: actual operation to perform via the API
        params: parameters required for specific method

        optional kwargs:
                _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)
                _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)
                _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)

                all optional template.get parameters: keyword argument names depends on your zabbix version, see:

                https://www.zabbix.com/documentation/2.4/manual/api/reference/

    Returns:
        Response from Zabbix API

    CLI Example:
    .. code-block:: bash

        salt '*' zabbix.run_query proxy.create '{""host"": ""zabbixproxy.domain.com"", ""status"": ""5""}'
    '''
    conn_args = _login(**kwargs)
    ret = {}
    try:
        if conn_args:
            method = method
            params = params
            params = _params_extend(params, **kwargs)
            ret = _query(method, params, conn_args['url'], conn_args['auth'])
            if isinstance(ret['result'], bool):
                return ret['result']
            return ret['result'] if ret['result'] else False
        else:
            raise KeyError
    except KeyError:
        return ret"
get_device_opts;"def get_device_opts(opts, salt_obj=None):
    '''
    Returns the options of the napalm device.
    :pram: opts
    :return: the network device opts
    '''
    network_device = {}
    # by default, look in the proxy config details
    device_dict = opts.get('proxy', {}) if is_proxy(opts) else opts.get('napalm', {})
    if opts.get('proxy') or opts.get('napalm'):
        opts['multiprocessing'] = device_dict.get('multiprocessing', False)
        # Most NAPALM drivers are SSH-based, so multiprocessing should default to False.
        # But the user can be allows one to have a different value for the multiprocessing, which will
        #   override the opts.
    if not device_dict:
        # still not able to setup
        log.error('Incorrect minion config. Please specify at least the napalm driver name!')
    # either under the proxy hier, either under the napalm in the config file
    network_device['HOSTNAME'] = device_dict.get('host') or \
                                 device_dict.get('hostname') or \
                                 device_dict.get('fqdn') or \
                                 device_dict.get('ip')
    network_device['USERNAME'] = device_dict.get('username') or \
                                 device_dict.get('user')
    network_device['DRIVER_NAME'] = device_dict.get('driver') or \
                                    device_dict.get('os')
    network_device['PASSWORD'] = device_dict.get('passwd') or \
                                 device_dict.get('password') or \
                                 device_dict.get('pass') or \
                                 ''
    network_device['TIMEOUT'] = device_dict.get('timeout', 60)
    network_device['OPTIONAL_ARGS'] = device_dict.get('optional_args', {})
    network_device['ALWAYS_ALIVE'] = device_dict.get('always_alive', True)
    network_device['PROVIDER'] = device_dict.get('provider')
    network_device['UP'] = False
    # get driver object form NAPALM
    if 'config_lock' not in network_device['OPTIONAL_ARGS']:
        network_device['OPTIONAL_ARGS']['config_lock'] = False
    if network_device['ALWAYS_ALIVE'] and 'keepalive' not in network_device['OPTIONAL_ARGS']:
        network_device['OPTIONAL_ARGS']['keepalive'] = 5  # 5 seconds keepalive
    return network_device"
error_info;"def error_info():
    """"""Return information about failed tasks.""""""
    worker = global_worker
    worker.check_connected()
    return (global_state.error_messages(driver_id=worker.task_driver_id) +
            global_state.error_messages(driver_id=DriverID.nil()))"
construct_lanczos_params;"def construct_lanczos_params(self):
    """"""Computes matrices T and V using the Lanczos algorithm.

    Args:
      k: number of iterations and dimensionality of the tridiagonal matrix
    Returns:
      eig_vec: eigen vector corresponding to min eigenvalue
    """"""
    # Using autograph to automatically handle
    # the control flow of minimum_eigen_vector
    self.min_eigen_vec = autograph.to_graph(utils.tf_lanczos_smallest_eigval)

    def _m_vector_prod_fn(x):
      return self.get_psd_product(x, dtype=self.lanczos_dtype)
    def _h_vector_prod_fn(x):
      return self.get_h_product(x, dtype=self.lanczos_dtype)

    # Construct nodes for computing eigenvalue of M
    self.m_min_vec_estimate = np.zeros(shape=(self.matrix_m_dimension, 1), dtype=np.float64)
    zeros_m = tf.zeros(shape=(self.matrix_m_dimension, 1), dtype=tf.float64)
    self.m_min_vec_ph = tf.placeholder_with_default(input=zeros_m,
                                                    shape=(self.matrix_m_dimension, 1),
                                                    name='m_min_vec_ph')
    self.m_min_eig, self.m_min_vec = self.min_eigen_vec(_m_vector_prod_fn,
                                                        self.matrix_m_dimension,
                                                        self.m_min_vec_ph,
                                                        self.lzs_params['max_iter'],
                                                        dtype=self.lanczos_dtype)
    self.m_min_eig = tf.cast(self.m_min_eig, self.nn_dtype)
    self.m_min_vec = tf.cast(self.m_min_vec, self.nn_dtype)

    self.h_min_vec_estimate = np.zeros(shape=(self.matrix_m_dimension - 1, 1), dtype=np.float64)
    zeros_h = tf.zeros(shape=(self.matrix_m_dimension - 1, 1), dtype=tf.float64)
    self.h_min_vec_ph = tf.placeholder_with_default(input=zeros_h,
                                                    shape=(self.matrix_m_dimension - 1, 1),
                                                    name='h_min_vec_ph')
    self.h_min_eig, self.h_min_vec = self.min_eigen_vec(_h_vector_prod_fn,
                                                        self.matrix_m_dimension-1,
                                                        self.h_min_vec_ph,
                                                        self.lzs_params['max_iter'],
                                                        dtype=self.lanczos_dtype)
    self.h_min_eig = tf.cast(self.h_min_eig, self.nn_dtype)
    self.h_min_vec = tf.cast(self.h_min_vec, self.nn_dtype)"
list_sessions;"def list_sessions(logged_in_users_only=False):
    '''
    List information about the sessions.

    .. versionadded:: 2016.11.0

    :param logged_in_users_only: If True, only return sessions with users logged in.
    :return: A list containing dictionaries of session information.

    CLI Example:

    .. code-block:: bash

        salt '*' rdp.list_sessions
    '''
    ret = list()
    server = win32ts.WTS_CURRENT_SERVER_HANDLE
    protocols = {win32ts.WTS_PROTOCOL_TYPE_CONSOLE: 'console',
                 win32ts.WTS_PROTOCOL_TYPE_ICA: 'citrix',
                 win32ts.WTS_PROTOCOL_TYPE_RDP: 'rdp'}
    statuses = {win32ts.WTSActive: 'active', win32ts.WTSConnected: 'connected',
                win32ts.WTSConnectQuery: 'connect_query', win32ts.WTSShadow: 'shadow',
                win32ts.WTSDisconnected: 'disconnected', win32ts.WTSIdle: 'idle',
                win32ts.WTSListen: 'listen', win32ts.WTSReset: 'reset',
                win32ts.WTSDown: 'down', win32ts.WTSInit: 'init'}

    for session in win32ts.WTSEnumerateSessions(server):
        user = win32ts.WTSQuerySessionInformation(server, session['SessionId'],
                                                  win32ts.WTSUserName) or None
        protocol_id = win32ts.WTSQuerySessionInformation(server, session['SessionId'],
                                                         win32ts.WTSClientProtocolType)
        status_id = win32ts.WTSQuerySessionInformation(server, session['SessionId'],
                                                       win32ts.WTSConnectState)
        protocol = protocols.get(protocol_id, 'unknown')
        connection_status = statuses.get(status_id, 'unknown')
        station = session['WinStationName'] or 'Disconnected'
        connection_info = {'connection_status': connection_status, 'protocol': protocol,
                           'session_id': session['SessionId'], 'station': station,
                           'user': user}
        if logged_in_users_only:
            if user:
                ret.append(connection_info)
        else:
            ret.append(connection_info)

    if not ret:
        _LOG.warning('No sessions found.')
    return sorted(ret, key=lambda k: k['session_id'])"
list_tables;"def list_tables(self):
        """"""List the tables in this instance.

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_list_tables]
            :end-before: [END bigtable_list_tables]

        :rtype: list of :class:`Table <google.cloud.bigtable.table.Table>`
        :returns: The list of tables owned by the instance.
        :raises: :class:`ValueError <exceptions.ValueError>` if one of the
                 returned tables has a name that is not of the expected format.
        """"""
        table_list_pb = self._client.table_admin_client.list_tables(self.name)

        result = []
        for table_pb in table_list_pb:
            table_prefix = self.name + ""/tables/""
            if not table_pb.name.startswith(table_prefix):
                raise ValueError(
                    ""Table name {} not of expected format"".format(table_pb.name)
                )
            table_id = table_pb.name[len(table_prefix) :]
            result.append(self.table(table_id))

        return result"
under_name_scope;"def under_name_scope(name_scope=None):
    """"""
    Args:
        name_scope(str): the default scope to use. If None, will use the name of the function.

    Returns:
        A decorator which makes the function run under a name scope.
        The name scope is obtained by the following:
        1. The 'name_scope' keyword argument when the decorated function is called.
        2. The 'name_scope' argument of the decorator.
        3. (default) The name of the decorated function itself.

    Example:

    .. code-block:: python

        @under_name_scope()
        def rms(x):
            return tf.sqrt(
                tf.reduce_mean(tf.square(x)))

        rms(tensor)  # will be called under name scope 'rms'
        rms(tensor, name_scope='scope')  # will be called under name scope 'scope'


    Todo:
        Add a reuse option.
    """"""

    def _impl(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            scopename = kwargs.pop('name_scope', name_scope)
            if scopename is None:
                scopename = func.__name__

            with tf.name_scope(scopename):
                return func(*args, **kwargs)
        return wrapper
    return _impl"
get_distributions;"def get_distributions(self):
        """"""
        Provides an iterator that looks for distributions and returns
        :class:`InstalledDistribution` or
        :class:`EggInfoDistribution` instances for each one of them.

        :rtype: iterator of :class:`InstalledDistribution` and
                :class:`EggInfoDistribution` instances
        """"""
        if not self._cache_enabled:
            for dist in self._yield_distributions():
                yield dist
        else:
            self._generate_cache()

            for dist in self._cache.path.values():
                yield dist

            if self._include_egg:
                for dist in self._cache_egg.path.values():
                    yield dist"
pack_small_tensors;"def pack_small_tensors(tower_grads, max_bytes=0):
    """"""Concatenate gradients together more intelligently.

  Does binpacking
  Args:
    tower_grads: List of lists of (gradient, variable) tuples.
    max_bytes: Int giving max number of bytes in a tensor that
      may be considered small.
  """"""
    assert max_bytes >= 0
    orig_grads = [g for g, _ in tower_grads[0]]
    # Check to make sure sizes are accurate; not entirely important
    assert all(g.dtype == tf.float32 for g in orig_grads)
    sizes = [4 * g.shape.num_elements() for g in orig_grads]
    print_stats(sizes)
    small_ranges = []
    large_indices = []
    new_sizes = []

    def end_interval(indices, small_ranges, large_indices):
        if len(indices) > 1:
            small_ranges.insert(0, [indices[0], indices[-1]])
        else:
            large_indices.insert(0, indices[0])

    cur_range = []
    cur_size = 0
    for i, s in reversed(list(enumerate(sizes))):
        if cur_size > max_bytes:
            end_interval(cur_range, small_ranges, large_indices)
            new_sizes.insert(0, cur_size)
            cur_range = []
            cur_size = 0
        cur_range.insert(0, i)
        cur_size += s
    end_interval(cur_range, small_ranges, large_indices)
    new_sizes.insert(0, cur_size)

    print_stats(new_sizes)
    num_gv = len(orig_grads)
    packing = {}
    if len(small_ranges):
        new_tower_grads = []
        for dev_idx, gv_list in enumerate(tower_grads):
            assert len(gv_list) == num_gv, (
                ""Possible cause: ""
                ""Networks constructed on different workers ""
                ""don't have the same number of variables. ""
                ""If you use tf.GraphKeys or tf.global_variables() ""
                ""with multiple graphs per worker during network ""
                ""construction, you need to use ""
                ""appropriate scopes, see ""
                ""https://github.com/ray-project/ray/issues/3136"")
            new_gv_list = []
            for r in small_ranges:
                key = ""%d:%d"" % (dev_idx, len(new_gv_list))
                new_gv_list.append((pack_range(key, packing, gv_list, r),
                                    ""packing_var_placeholder""))
            for i in large_indices:
                new_gv_list.append(gv_list[i])
            new_tower_grads.append(new_gv_list)
        return new_tower_grads, packing
    else:
        return tower_grads, None"
hybrid_forward;"def hybrid_forward(self, F, words1, words2, weight):  # pylint: disable=arguments-differ
        """"""Predict the similarity of words1 and words2.

        Parameters
        ----------
        words1 : Symbol or NDArray
            The indices of the words the we wish to compare to the words in words2.
        words2 : Symbol or NDArray
            The indices of the words the we wish to compare to the words in words1.

        Returns
        -------
        similarity : Symbol or NDArray
            The similarity computed by WordEmbeddingSimilarity.similarity_function.
        """"""
        embeddings_words1 = F.Embedding(words1, weight,
                                        input_dim=self._vocab_size,
                                        output_dim=self._embed_size)
        embeddings_words2 = F.Embedding(words2, weight,
                                        input_dim=self._vocab_size,
                                        output_dim=self._embed_size)
        similarity = self.similarity(embeddings_words1, embeddings_words2)
        return similarity"
get_mean_and_std;"def get_mean_and_std(dataset):
    """"""Compute the mean and std value of dataset.""""""
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=1, shuffle=True, num_workers=2
    )
    mean = torch.zeros(3)
    std = torch.zeros(3)
    print(""==> Computing mean and std.."")
    for inputs, _ in dataloader:
        for i in range(3):
            mean[i] += inputs[:, i, :, :].mean()
            std[i] += inputs[:, i, :, :].std()
    mean.div_(len(dataset))
    std.div_(len(dataset))
    return mean, std"
get_current;"def get_current(self):
        """"""
        Get a Panel that is the current data in view. It is not safe to persist
        these objects because internal data might change
        """"""

        where = slice(self._oldest_frame_idx(), self._pos)
        major_axis = pd.DatetimeIndex(deepcopy(self.date_buf[where]), tz='utc')
        return pd.Panel(self.buffer.values[:, where, :], self.items,
                        major_axis, self.minor_axis, dtype=self.dtype)"
ext_pillar;"def ext_pillar(minion_id,
               pillar,  # pylint: disable=W0613
               conf):
    '''
    Check etcd for all data
    '''
    comps = conf.split()

    profile = None
    if comps[0]:
        profile = comps[0]
    client = salt.utils.etcd_util.get_conn(__opts__, profile)

    path = '/'
    if len(comps) > 1 and comps[1].startswith('root='):
        path = comps[1].replace('root=', '')

    # put the minion's ID in the path if necessary
    path %= {
        'minion_id': minion_id
    }

    try:
        pillar = salt.utils.etcd_util.tree(client, path)
    except KeyError:
        log.error('No such key in etcd profile %s: %s', profile, path)
        pillar = {}

    return pillar"
incomplete_dir;"def incomplete_dir(dirname):
  """"""Create temporary dir for dirname and rename on exit.""""""
  tmp_dir = get_incomplete_path(dirname)
  tf.io.gfile.makedirs(tmp_dir)
  try:
    yield tmp_dir
    tf.io.gfile.rename(tmp_dir, dirname)
  finally:
    if tf.io.gfile.exists(tmp_dir):
      tf.io.gfile.rmtree(tmp_dir)"
generate_project;"def generate_project(
        location=None, runtime=""nodejs"", dependency_manager=None,
        output_dir=""."", name='sam-sample-app', no_input=False):
    """"""Generates project using cookiecutter and options given

    Generate project scaffolds a project using default templates if user
    doesn't provide one via location parameter. Default templates are
    automatically chosen depending on runtime given by the user.

    Parameters
    ----------
    location: Path, optional
        Git, HTTP, Local path or Zip containing cookiecutter template
        (the default is None, which means no custom template)
    runtime: str, optional
        Lambda Runtime (the default is ""nodejs"", which creates a nodejs project)
    dependency_manager: str, optional
        Dependency Manager for the Lambda Runtime Project(the default is ""npm"" for a ""nodejs"" Lambda runtime)
    output_dir: str, optional
        Output directory where project should be generated
        (the default is ""."", which implies current folder)
    name: str, optional
        Name of the project
        (the default is ""sam-sample-app"", which implies a project named sam-sample-app will be created)
    no_input : bool, optional
        Whether to prompt for input or to accept default values
        (the default is False, which prompts the user for values it doesn't know for baking)

    Raises
    ------
    GenerateProjectFailedError
        If the process of baking a project fails
    """"""

    template = None

    for mapping in list(itertools.chain(*(RUNTIME_DEP_TEMPLATE_MAPPING.values()))):
        if runtime in mapping['runtimes'] or any([r.startswith(runtime) for r in mapping['runtimes']]):
            if not dependency_manager:
                template = mapping['init_location']
                break
            elif dependency_manager == mapping['dependency_manager']:
                template = mapping['init_location']

    if not template:
        msg = ""Lambda Runtime {} does not support dependency manager: {}"".format(runtime, dependency_manager)
        raise GenerateProjectFailedError(project=name, provider_error=msg)

    params = {
        ""template"": location if location else template,
        ""output_dir"": output_dir,
        ""no_input"": no_input
    }

    LOG.debug(""Parameters dict created with input given"")
    LOG.debug(""%s"", params)

    if not location and name is not None:
        params['extra_context'] = {'project_name': name, 'runtime': runtime}
        params['no_input'] = True
        LOG.debug(""Parameters dict updated with project name as extra_context"")
        LOG.debug(""%s"", params)

    try:
        LOG.debug(""Baking a new template with cookiecutter with all parameters"")
        cookiecutter(**params)
    except CookiecutterException as e:
        raise GenerateProjectFailedError(project=name, provider_error=e)"
replace_cluster_role_binding;"def replace_cluster_role_binding(self, name, body, **kwargs):
        """"""
        replace the specified ClusterRoleBinding
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_cluster_role_binding(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ClusterRoleBinding (required)
        :param V1ClusterRoleBinding body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1ClusterRoleBinding
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_cluster_role_binding_with_http_info(name, body, **kwargs)
        else:
            (data) = self.replace_cluster_role_binding_with_http_info(name, body, **kwargs)
            return data"
service_absent;"def service_absent(name, namespace='default', **kwargs):
    '''
    Ensures that the named service is absent from the given namespace.

    name
        The name of the service

    namespace
        The name of the namespace
    '''

    ret = {'name': name,
           'changes': {},
           'result': False,
           'comment': ''}

    service = __salt__['kubernetes.show_service'](name, namespace, **kwargs)

    if service is None:
        ret['result'] = True if not __opts__['test'] else None
        ret['comment'] = 'The service does not exist'
        return ret

    if __opts__['test']:
        ret['comment'] = 'The service is going to be deleted'
        ret['result'] = None
        return ret

    res = __salt__['kubernetes.delete_service'](name, namespace, **kwargs)
    if res['code'] == 200:
        ret['result'] = True
        ret['changes'] = {
            'kubernetes.service': {
                'new': 'absent', 'old': 'present'}}
        ret['comment'] = res['message']
    else:
        ret['comment'] = 'Something went wrong, response: {0}'.format(res)

    return ret"
check_format;"def check_format(self, full_check=True):
        """"""Check whether the NDArray format is valid.

        Parameters
        ----------
        full_check : bool, optional
            If `True`, rigorous check, O(N) operations. Otherwise
            basic check, O(1) operations (default True).
        """"""
        check_call(_LIB.MXNDArraySyncCheckFormat(self.handle, ctypes.c_bool(full_check)))"
create_toolbar;"def create_toolbar(self, title, object_name, iconsize=24):
        """"""Create and return toolbar with *title* and *object_name*""""""
        toolbar = self.addToolBar(title)
        toolbar.setObjectName(object_name)
        toolbar.setIconSize(QSize(iconsize, iconsize))
        self.toolbarslist.append(toolbar)
        return toolbar"
show_kernel_error;"def show_kernel_error(self, error):
        """"""Show kernel initialization errors in infowidget.""""""
        # Replace end of line chars with <br>
        eol = sourcecode.get_eol_chars(error)
        if eol:
            error = error.replace(eol, '<br>')

        # Don't break lines in hyphens
        # From https://stackoverflow.com/q/7691569/438386
        error = error.replace('-', '&#8209')

        # Create error page
        message = _(""An error ocurred while starting the kernel"")
        kernel_error_template = Template(KERNEL_ERROR)
        self.info_page = kernel_error_template.substitute(
            css_path=self.css_path,
            message=message,
            error=error)

        # Show error
        self.set_info_page()
        self.shellwidget.hide()
        self.infowidget.show()

        # Tell the client we're in error mode
        self.is_error_shown = True"
shift_dates;"def shift_dates(dates, start_date, end_date, shift):
    """"""
    Shift dates of a pipeline query back by `shift` days.

    load_adjusted_array is called with dates on which the user's algo
    will be shown data, which means we need to return the data that would
    be known at the start of each date.  This is often labeled with a
    previous date in the underlying data (e.g. at the start of today, we
    have the data as of yesterday). In this case, we can shift the query
    dates back to query the appropriate values.

    Parameters
    ----------
    dates : DatetimeIndex
        All known dates.
    start_date : pd.Timestamp
        Start date of the pipeline query.
    end_date : pd.Timestamp
        End date of the pipeline query.
    shift : int
        The number of days to shift back the query dates.
    """"""
    try:
        start = dates.get_loc(start_date)
    except KeyError:
        if start_date < dates[0]:
            raise NoFurtherDataError(
                msg=(
                    ""Pipeline Query requested data starting on {query_start}, ""
                    ""but first known date is {calendar_start}""
                ).format(
                    query_start=str(start_date),
                    calendar_start=str(dates[0]),
                )
            )
        else:
            raise ValueError(""Query start %s not in calendar"" % start_date)

    # Make sure that shifting doesn't push us out of the calendar.
    if start < shift:
        raise NoFurtherDataError(
            msg=(
                ""Pipeline Query requested data from {shift}""
                "" days before {query_start}, but first known date is only ""
                ""{start} days earlier.""
            ).format(shift=shift, query_start=start_date, start=start),
        )

    try:
        end = dates.get_loc(end_date)
    except KeyError:
        if end_date > dates[-1]:
            raise NoFurtherDataError(
                msg=(
                    ""Pipeline Query requesting data up to {query_end}, ""
                    ""but last known date is {calendar_end}""
                ).format(
                    query_end=end_date,
                    calendar_end=dates[-1],
                )
            )
        else:
            raise ValueError(""Query end %s not in calendar"" % end_date)
    return dates[start - shift], dates[end - shift]"
set_input;"def set_input(self, X_batch=None):
    """"""
    Preprocessing the inputs before calling session.run()

    :param X_batch: A dictionary of inputs to the first sub-graph
    :return: A tuple, `(fetches, fd)`, with `fetches` being a list of
             Tensors to be fetches and `fd` the feed dictionary.
    """"""
    inputs = self.inputs
    outputs = self.outputs

    # data for first gpu
    fd = {}
    if X_batch is not None:
      self.next_vals[0] = OrderedDict()
      for i, vname in enumerate(self.inputs[0]):
        if vname in X_batch:
          self.next_vals[0][vname] = X_batch[vname]
        else:
          self.next_vals[0][vname] = None
    else:
      self.next_vals[0] = None

    # Set `feed_dict` for each GPU. If there is something to run for that
    # GPU, collect outputs to be fetched.
    fetches = []
    self.active_gpus = []
    for i in range(len(outputs)):
      if self.next_vals[i] is None:
        self.active_gpus += [False]
        continue
      self.active_gpus += [True]
      for k in inputs[i]:
        if self.next_vals[i][k] is not None:
          fd[inputs[i][k]] = self.next_vals[i][k]
      for k, v in outputs[i].iteritems():
        fetches += [v]

    fd.update(self.feed_dict)

    return fetches, fd"
unload_extension;"def unload_extension(self, name):
        """"""Unloads an extension.

        When the extension is unloaded, all commands, listeners, and cogs are
        removed from the bot and the module is un-imported.

        The extension can provide an optional global function, ``teardown``,
        to do miscellaneous clean-up if necessary. This function takes a single
        parameter, the ``bot``, similar to ``setup`` from
        :meth:`~.Bot.load_extension`.

        Parameters
        ------------
        name: :class:`str`
            The extension name to unload. It must be dot separated like
            regular Python imports if accessing a sub-module. e.g.
            ``foo.test`` if you want to import ``foo/test.py``.

        Raises
        -------
        ExtensionNotLoaded
            The extension was not loaded.
        """"""

        lib = self.__extensions.get(name)
        if lib is None:
            raise errors.ExtensionNotLoaded(name)

        self._remove_module_references(lib.__name__)
        self._call_module_finalizers(lib, name)"
sell_avg_holding_price;"def sell_avg_holding_price(self):
        """"""
        [float] 卖方向持仓均价
        """"""
        return 0 if self.sell_quantity == 0 else self._sell_holding_cost / self.sell_quantity / self.contract_multiplier"
gain_focus;"def gain_focus(self):
        """"""Confirm if the tour regains focus and unhides the tips.""""""
        if (self.is_running and self.any_has_focus() and 
            not self.setting_data and self.hidden):
            self.unhide_tips()"
create_operation_job;"def create_operation_job(self, operation, obj, external_id_field_name=None, content_type=None):
        """"""
        Creates a new SF job that for doing any operation (insert, upsert, update, delete, query)

        :param operation: delete, insert, query, upsert, update, hardDelete. Must be lowercase.
        :param obj: Parent SF object
        :param external_id_field_name: Optional.
        """"""
        if not self.has_active_session():
            self.start_session()

        response = requests.post(self._get_create_job_url(),
                                 headers=self._get_create_job_headers(),
                                 data=self._get_create_job_xml(operation, obj, external_id_field_name, content_type))
        response.raise_for_status()

        root = ET.fromstring(response.text)
        job_id = root.find('%sid' % self.API_NS).text
        return job_id"
create_session;"def create_session(
        self,
        database,
        session=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates a new session. A session can be used to perform transactions
        that read and/or modify data in a Cloud Spanner database. Sessions are
        meant to be reused for many consecutive transactions.

        Sessions can only execute one transaction at a time. To execute multiple
        concurrent read-write/write-only transactions, create multiple sessions.
        Note that standalone reads and queries use a transaction internally, and
        count toward the one transaction limit.

        Cloud Spanner limits the number of sessions that can exist at any given
        time; thus, it is a good idea to delete idle and/or unneeded sessions.
        Aside from explicit deletes, Cloud Spanner can delete sessions for which
        no operations are sent for more than an hour. If a session is deleted,
        requests to it return ``NOT_FOUND``.

        Idle sessions can be kept alive by sending a trivial SQL query
        periodically, e.g., ``""SELECT 1""``.

        Example:
            >>> from google.cloud import spanner_v1
            >>>
            >>> client = spanner_v1.SpannerClient()
            >>>
            >>> database = client.database_path('[PROJECT]', '[INSTANCE]', '[DATABASE]')
            >>>
            >>> response = client.create_session(database)

        Args:
            database (str): Required. The database in which the new session is created.
            session (Union[dict, ~google.cloud.spanner_v1.types.Session]): The session to create.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.spanner_v1.types.Session`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.spanner_v1.types.Session` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_session"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_session""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_session,
                default_retry=self._method_configs[""CreateSession""].retry,
                default_timeout=self._method_configs[""CreateSession""].timeout,
                client_info=self._client_info,
            )

        request = spanner_pb2.CreateSessionRequest(database=database, session=session)
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""database"", database)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""create_session""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
custom_getter_scope;"def custom_getter_scope(custom_getter):
    """"""
    Args:
        custom_getter: the same as in :func:`tf.get_variable`

    Returns:
        The current variable scope with a custom_getter.
    """"""
    scope = tf.get_variable_scope()
    if get_tf_version_tuple() >= (1, 5):
        with tf.variable_scope(
                scope, custom_getter=custom_getter,
                auxiliary_name_scope=False):
            yield
    else:
        ns = tf.get_default_graph().get_name_scope()
        with tf.variable_scope(
                scope, custom_getter=custom_getter):
            with tf.name_scope(ns + '/' if ns else ''):
                yield"
execution_order;"def execution_order(self, refcounts):
        """"""
        Return a topologically-sorted iterator over the terms in ``self`` which
        need to be computed.
        """"""
        return iter(nx.topological_sort(
            self.graph.subgraph(
                {term for term, refcount in refcounts.items() if refcount > 0},
            ),
        ))"
highlight_block;"def highlight_block(self, text):
        """"""Implement highlight specific for CSS and HTML.""""""
        text = to_text_string(text)
        previous_state = tbh.get_state(self.currentBlock().previous())
        
        if previous_state == self.COMMENT:
            self.setFormat(0, len(text), self.formats[""comment""])
        else:
            previous_state = self.NORMAL
            self.setFormat(0, len(text), self.formats[""normal""])
        
        tbh.set_state(self.currentBlock(), previous_state)
        match = self.PROG.search(text)        

        match_count = 0
        n_characters = len(text)
        # There should never be more matches than characters in the text.
        while match and match_count < n_characters:
            match_dict = match.groupdict()
            for key, value in list(match_dict.items()):
                if value:
                    start, end = match.span(key)
                    if previous_state == self.COMMENT:
                        if key == ""multiline_comment_end"":
                            tbh.set_state(self.currentBlock(), self.NORMAL)
                            self.setFormat(end, len(text),
                                           self.formats[""normal""])
                        else:
                            tbh.set_state(self.currentBlock(), self.COMMENT)
                            self.setFormat(0, len(text),
                                           self.formats[""comment""])
                    else:
                        if key == ""multiline_comment_start"":
                            tbh.set_state(self.currentBlock(), self.COMMENT)
                            self.setFormat(start, len(text),
                                           self.formats[""comment""])
                        else:
                            tbh.set_state(self.currentBlock(), self.NORMAL)
                            try:
                                self.setFormat(start, end-start,
                                               self.formats[key])
                            except KeyError:
                                # happens with unmatched end-of-comment;
                                # see issue 1462
                                pass
            
            match = self.PROG.search(text, match.end())
            match_count += 1
        
        self.highlight_spaces(text)"
save_images;"def save_images(images, filenames, output_dir):
  """"""Saves images to the output directory.

  Args:
    images: array with minibatch of images
    filenames: list of filenames without path
      If number of file names in this list less than number of images in
      the minibatch then only first len(filenames) images will be saved.
    output_dir: directory where to save images
  """"""
  for i, filename in enumerate(filenames):
    # Images for inception classifier are normalized to be in [-1, 1] interval,
    # so rescale them back to [0, 1].
    with tf.gfile.Open(os.path.join(output_dir, filename), 'w') as f:
      img = (((images[i, :, :, :] + 1.0) * 0.5) * 255.0).astype(np.uint8)
      Image.fromarray(img).save(f, format='PNG')"
value_counts;"def value_counts(self):
        """"""
        Return an SFrame containing counts of unique values. The resulting
        SFrame will be sorted in descending frequency.

        Returns
        -------
        out : SFrame
            An SFrame containing 2 columns : 'value', and 'count'. The SFrame will
            be sorted in descending order by the column 'count'.

        See Also
        --------
        SFrame.summary

        Examples
        --------
        >>> sa = turicreate.SArray([1,1,2,2,2,2,3,3,3,3,3,3,3])
        >>> sa.value_counts()
            Columns:
                    value	int
                    count	int
            Rows: 3
            Data:
            +-------+-------+
            | value | count |
            +-------+-------+
            |   3   |   7   |
            |   2   |   4   |
            |   1   |   2   |
            +-------+-------+
            [3 rows x 2 columns]
        """"""
        from .sframe import SFrame as _SFrame
        return _SFrame({'value':self}).groupby('value', {'count':_aggregate.COUNT}).sort('count', ascending=False)"
api_request;"def api_request(
        self,
        method,
        path,
        query_params=None,
        data=None,
        content_type=None,
        headers=None,
        api_base_url=None,
        api_version=None,
        expect_json=True,
        _target_object=None,
    ):
        """"""Make a request over the HTTP transport to the API.

        You shouldn't need to use this method, but if you plan to
        interact with the API using these primitives, this is the
        correct one to use.

        :type method: str
        :param method: The HTTP method name (ie, ``GET``, ``POST``, etc).
                       Required.

        :type path: str
        :param path: The path to the resource (ie, ``'/b/bucket-name'``).
                     Required.

        :type query_params: dict or list
        :param query_params: A dictionary of keys and values (or list of
                             key-value pairs) to insert into the query
                             string of the URL.

        :type data: str
        :param data: The data to send as the body of the request. Default is
                     the empty string.

        :type content_type: str
        :param content_type: The proper MIME type of the data provided. Default
                             is None.

        :type headers: dict
        :param headers: extra HTTP headers to be sent with the request.

        :type api_base_url: str
        :param api_base_url: The base URL for the API endpoint.
                             Typically you won't have to provide this.
                             Default is the standard API base URL.

        :type api_version: str
        :param api_version: The version of the API to call.  Typically
                            you shouldn't provide this and instead use
                            the default for the library.  Default is the
                            latest API version supported by
                            google-cloud-python.

        :type expect_json: bool
        :param expect_json: If True, this method will try to parse the
                            response as JSON and raise an exception if
                            that cannot be done.  Default is True.

        :type _target_object: :class:`object`
        :param _target_object:
            (Optional) Protected argument to be used by library callers. This
            can allow custom behavior, for example, to defer an HTTP request
            and complete initialization of the object at a later time.

        :raises ~google.cloud.exceptions.GoogleCloudError: if the response code
            is not 200 OK.
        :raises ValueError: if the response content type is not JSON.
        :rtype: dict or str
        :returns: The API response payload, either as a raw string or
                  a dictionary if the response is valid JSON.
        """"""
        url = self.build_api_url(
            path=path,
            query_params=query_params,
            api_base_url=api_base_url,
            api_version=api_version,
        )

        # Making the executive decision that any dictionary
        # data will be sent properly as JSON.
        if data and isinstance(data, dict):
            data = json.dumps(data)
            content_type = ""application/json""

        response = self._make_request(
            method=method,
            url=url,
            data=data,
            content_type=content_type,
            headers=headers,
            target_object=_target_object,
        )

        if not 200 <= response.status_code < 300:
            raise exceptions.from_http_response(response)

        if expect_json and response.content:
            return response.json()
        else:
            return response.content"
add_timeout;"def add_timeout(
        self,
        deadline: Union[float, datetime.timedelta],
        callback: Callable[..., None],
        *args: Any,
        **kwargs: Any
    ) -> object:
        """"""Runs the ``callback`` at the time ``deadline`` from the I/O loop.

        Returns an opaque handle that may be passed to
        `remove_timeout` to cancel.

        ``deadline`` may be a number denoting a time (on the same
        scale as `IOLoop.time`, normally `time.time`), or a
        `datetime.timedelta` object for a deadline relative to the
        current time.  Since Tornado 4.0, `call_later` is a more
        convenient alternative for the relative case since it does not
        require a timedelta object.

        Note that it is not safe to call `add_timeout` from other threads.
        Instead, you must use `add_callback` to transfer control to the
        `IOLoop`'s thread, and then call `add_timeout` from there.

        Subclasses of IOLoop must implement either `add_timeout` or
        `call_at`; the default implementations of each will call
        the other.  `call_at` is usually easier to implement, but
        subclasses that wish to maintain compatibility with Tornado
        versions prior to 4.0 must use `add_timeout` instead.

        .. versionchanged:: 4.0
           Now passes through ``*args`` and ``**kwargs`` to the callback.
        """"""
        if isinstance(deadline, numbers.Real):
            return self.call_at(deadline, callback, *args, **kwargs)
        elif isinstance(deadline, datetime.timedelta):
            return self.call_at(
                self.time() + deadline.total_seconds(), callback, *args, **kwargs
            )
        else:
            raise TypeError(""Unsupported deadline %r"" % deadline)"
should_checkpoint;"def should_checkpoint(self):
        """"""Whether this trial is due for checkpointing.""""""
        result = self.last_result or {}

        if result.get(DONE) and self.checkpoint_at_end:
            return True

        if self.checkpoint_freq:
            return result.get(TRAINING_ITERATION,
                              0) % self.checkpoint_freq == 0
        else:
            return False"
ensure_ndarray;"def ensure_ndarray(ndarray_or_adjusted_array):
    """"""
    Return the input as a numpy ndarray.

    This is a no-op if the input is already an ndarray.  If the input is an
    adjusted_array, this extracts a read-only view of its internal data buffer.

    Parameters
    ----------
    ndarray_or_adjusted_array : numpy.ndarray | zipline.data.adjusted_array

    Returns
    -------
    out : The input, converted to an ndarray.
    """"""
    if isinstance(ndarray_or_adjusted_array, ndarray):
        return ndarray_or_adjusted_array
    elif isinstance(ndarray_or_adjusted_array, AdjustedArray):
        return ndarray_or_adjusted_array.data
    else:
        raise TypeError(
            ""Can't convert %s to ndarray"" %
            type(ndarray_or_adjusted_array).__name__
        )"
make_ndarray;"def make_ndarray(tensor):
    """"""Create a numpy ndarray from a tensor.

  Create a numpy ndarray with the same shape and data as the tensor.

  Args:
    tensor: A TensorProto.

  Returns:
    A numpy array with the tensor contents.

  Raises:
    TypeError: if tensor has unsupported type.

  """"""
    shape = [d.size for d in tensor.tensor_shape.dim]
    num_elements = np.prod(shape, dtype=np.int64)
    tensor_dtype = dtypes.as_dtype(tensor.dtype)
    dtype = tensor_dtype.as_numpy_dtype

    if tensor.tensor_content:
        return np.frombuffer(tensor.tensor_content, dtype=dtype).copy().reshape(shape)
    elif tensor_dtype == dtypes.float16 or tensor_dtype == dtypes.bfloat16:
        # the half_val field of the TensorProto stores the binary representation
        # of the fp16: we need to reinterpret this as a proper float16
        if len(tensor.half_val) == 1:
            tmp = np.array(tensor.half_val[0], dtype=np.uint16)
            tmp.dtype = tensor_dtype.as_numpy_dtype
            return np.repeat(tmp, num_elements).reshape(shape)
        else:
            tmp = np.fromiter(tensor.half_val, dtype=np.uint16)
            tmp.dtype = tensor_dtype.as_numpy_dtype
            return tmp.reshape(shape)
    elif tensor_dtype == dtypes.float32:
        if len(tensor.float_val) == 1:
            return np.repeat(
                np.array(tensor.float_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.fromiter(tensor.float_val, dtype=dtype).reshape(shape)
    elif tensor_dtype == dtypes.float64:
        if len(tensor.double_val) == 1:
            return np.repeat(
                np.array(tensor.double_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.fromiter(tensor.double_val, dtype=dtype).reshape(shape)
    elif tensor_dtype in [
        dtypes.int32,
        dtypes.uint8,
        dtypes.uint16,
        dtypes.int16,
        dtypes.int8,
        dtypes.qint32,
        dtypes.quint8,
        dtypes.qint8,
        dtypes.qint16,
        dtypes.quint16,
    ]:
        if len(tensor.int_val) == 1:
            return np.repeat(
                np.array(tensor.int_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.fromiter(tensor.int_val, dtype=dtype).reshape(shape)
    elif tensor_dtype == dtypes.int64:
        if len(tensor.int64_val) == 1:
            return np.repeat(
                np.array(tensor.int64_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.fromiter(tensor.int64_val, dtype=dtype).reshape(shape)
    elif tensor_dtype == dtypes.string:
        if len(tensor.string_val) == 1:
            return np.repeat(
                np.array(tensor.string_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.array([x for x in tensor.string_val], dtype=dtype).reshape(shape)
    elif tensor_dtype == dtypes.complex64:
        it = iter(tensor.scomplex_val)
        if len(tensor.scomplex_val) == 2:
            return np.repeat(
                np.array(
                    complex(tensor.scomplex_val[0], tensor.scomplex_val[1]), dtype=dtype
                ),
                num_elements,
            ).reshape(shape)
        else:
            return np.array(
                [complex(x[0], x[1]) for x in zip(it, it)], dtype=dtype
            ).reshape(shape)
    elif tensor_dtype == dtypes.complex128:
        it = iter(tensor.dcomplex_val)
        if len(tensor.dcomplex_val) == 2:
            return np.repeat(
                np.array(
                    complex(tensor.dcomplex_val[0], tensor.dcomplex_val[1]), dtype=dtype
                ),
                num_elements,
            ).reshape(shape)
        else:
            return np.array(
                [complex(x[0], x[1]) for x in zip(it, it)], dtype=dtype
            ).reshape(shape)
    elif tensor_dtype == dtypes.bool:
        if len(tensor.bool_val) == 1:
            return np.repeat(
                np.array(tensor.bool_val[0], dtype=dtype), num_elements
            ).reshape(shape)
        else:
            return np.fromiter(tensor.bool_val, dtype=dtype).reshape(shape)
    else:
        raise TypeError(""Unsupported tensor type: %s"" % tensor.dtype)"
parse_args;"def parse_args():
    '''Parse args
    '''
    parser = argparse.ArgumentParser(description='Train and Test an Adversarial Variatiional Encoder')

    parser.add_argument('--train', help='train the network', action='store_true')
    parser.add_argument('--test', help='test the network', action='store_true')
    parser.add_argument('--save_embedding', help='saves the shape embedding of each input image', action='store_true')
    parser.add_argument('--dataset', help='dataset name', default='caltech', type=str)
    parser.add_argument('--activation', help='activation i.e. sigmoid or tanh', default='sigmoid', type=str)
    parser.add_argument('--training_data_path', help='training data path', default='datasets/caltech101/data/images32x32', type=str)
    parser.add_argument('--testing_data_path', help='testing data path', default='datasets/caltech101/test_data', type=str)
    parser.add_argument('--pretrained_encoder_path', help='pretrained encoder model path', default='checkpoints32x32_sigmoid/caltech_E-0045.params', type=str)
    parser.add_argument('--pretrained_generator_path', help='pretrained generator model path', default='checkpoints32x32_sigmoid/caltech_G-0045.params', type=str)
    parser.add_argument('--output_path', help='output path for the generated images', default='outputs32x32_sigmoid', type=str)
    parser.add_argument('--embedding_path', help='output path for the generated embeddings', default='outputs32x32_sigmoid', type=str)
    parser.add_argument('--checkpoint_path', help='checkpoint saving path ', default='checkpoints32x32_sigmoid', type=str)
    parser.add_argument('--nef', help='encoder filter count in the first layer', default=64, type=int)
    parser.add_argument('--ndf', help='discriminator filter count in the first layer', default=64, type=int)
    parser.add_argument('--ngf', help='generator filter count in the second last layer', default=64, type=int)
    parser.add_argument('--nc', help='generator filter count in the last layer i.e. 1 for grayscale image, 3 for RGB image', default=1, type=int)
    parser.add_argument('--batch_size', help='batch size, keep it 1 during testing', default=64, type=int)
    parser.add_argument('--Z', help='embedding size', default=100, type=int)
    parser.add_argument('--lr', help='learning rate', default=0.0002, type=float)
    parser.add_argument('--beta1', help='beta1 for adam optimizer', default=0.5, type=float)
    parser.add_argument('--epsilon', help='epsilon for adam optimizer', default=1e-5, type=float)
    parser.add_argument('--g_dl_weight', help='discriminator layer loss weight', default=1e-1, type=float)
    parser.add_argument('--gpu', help='gpu index', default=0, type=int)
    parser.add_argument('--use_cpu', help='use cpu', action='store_true')
    parser.add_argument('--num_epoch', help='number of maximum epochs ', default=45, type=int)
    parser.add_argument('--save_after_every', help='save checkpoint after every this number of epochs ', default=5, type=int)
    parser.add_argument('--visualize_after_every', help='save output images after every this number of epochs', default=5, type=int)
    parser.add_argument('--show_after_every', help='show metrics after this number of iterations', default=10, type=int)

    args = parser.parse_args()
    return args"
load_corpus;"def load_corpus(*data_file_paths):
    """"""
    Return the data contained within a specified corpus.
    """"""
    for file_path in data_file_paths:
        corpus = []
        corpus_data = read_corpus(file_path)

        conversations = corpus_data.get('conversations', [])
        corpus.extend(conversations)

        categories = corpus_data.get('categories', [])

        yield corpus, categories, file_path"
hclust_ordering;"def hclust_ordering(X, metric=""sqeuclidean""):
    """""" A leaf ordering is under-defined, this picks the ordering that keeps nearby samples similar.
    """"""
    
    # compute a hierarchical clustering
    D = sp.spatial.distance.pdist(X, metric)
    cluster_matrix = sp.cluster.hierarchy.complete(D)
    
    # merge clusters, rotating them to make the end points match as best we can
    sets = [[i] for i in range(X.shape[0])]
    for i in range(cluster_matrix.shape[0]):
        s1 = sets[int(cluster_matrix[i,0])]
        s2 = sets[int(cluster_matrix[i,1])]
        
        # compute distances between the end points of the lists
        d_s1_s2 = pdist(np.vstack([X[s1[-1],:], X[s2[0],:]]), metric)[0]
        d_s2_s1 = pdist(np.vstack([X[s1[0],:], X[s2[-1],:]]), metric)[0]
        d_s1r_s2 = pdist(np.vstack([X[s1[0],:], X[s2[0],:]]), metric)[0]
        d_s1_s2r = pdist(np.vstack([X[s1[-1],:], X[s2[-1],:]]), metric)[0]

        # concatenete the lists in the way the minimizes the difference between
        # the samples at the junction
        best = min(d_s1_s2, d_s2_s1, d_s1r_s2, d_s1_s2r)
        if best == d_s1_s2:
            sets.append(s1 + s2)
        elif best == d_s2_s1:
            sets.append(s2 + s1)
        elif best == d_s1r_s2:
            sets.append(list(reversed(s1)) + s2)
        else:
            sets.append(s1 + list(reversed(s2)))
    
    return sets[-1]"
get_paths;"def get_paths(scheme=_get_default_scheme(), vars=None, expand=True):
    """"""Return a mapping containing an install scheme.

    ``scheme`` is the install scheme name. If not provided, it will
    return the default scheme for the current platform.
    """"""
    _ensure_cfg_read()
    if expand:
        return _expand_vars(scheme, vars)
    else:
        return dict(_SCHEMES.items(scheme))"
get_query_info;"def get_query_info(sql, con, partition_column):
    """""" Return a columns name list and the query string

    Args:
        sql: SQL query or table name
        con: database connection or url string
        partition_column: column used to share the data between the workers

    Returns:
        Columns name list and query string
    """"""
    engine = create_engine(con)
    if is_table(engine, sql):
        table_metadata = get_table_metadata(engine, sql)
        query = build_query_from_table(sql)
        cols = get_table_columns(table_metadata)
    else:
        check_query(sql)
        query = sql.replace("";"", """")
        cols = get_query_columns(engine, query)
    # TODO allow validation that takes into account edge cases of pandas e.g. ""[index]""
    # check_partition_column(partition_column, cols)
    cols_names = list(cols.keys())
    return cols_names, query"
drop_pathlist;"def drop_pathlist(self, pathlist):
        """"""Drop path list""""""
        if pathlist:
            files = [""r'%s'"" % path for path in pathlist]
            if len(files) == 1:
                text = files[0]
            else:
                text = ""["" + "", "".join(files) + ""]""
            if self.new_input_line:
                self.on_new_line()
            self.insert_text(text)
            self.setFocus()"
setup_context_menu;"def setup_context_menu(self):
        """"""Setup shell context menu""""""
        self.menu = QMenu(self)
        self.cut_action = create_action(self, _(""Cut""),
                                        shortcut=keybinding('Cut'),
                                        icon=ima.icon('editcut'),
                                        triggered=self.cut)
        self.copy_action = create_action(self, _(""Copy""),
                                         shortcut=keybinding('Copy'),
                                         icon=ima.icon('editcopy'),
                                         triggered=self.copy)
        paste_action = create_action(self, _(""Paste""),
                                     shortcut=keybinding('Paste'),
                                     icon=ima.icon('editpaste'),
                                     triggered=self.paste)
        save_action = create_action(self, _(""Save history log...""),
                                    icon=ima.icon('filesave'),
                                    tip=_(""Save current history log (i.e. all ""
                                          ""inputs and outputs) in a text file""),
                                    triggered=self.save_historylog)
        self.delete_action = create_action(self, _(""Delete""),
                                    shortcut=keybinding('Delete'),
                                    icon=ima.icon('editdelete'),
                                    triggered=self.delete)
        selectall_action = create_action(self, _(""Select All""),
                                    shortcut=keybinding('SelectAll'),
                                    icon=ima.icon('selectall'),
                                    triggered=self.selectAll)
        add_actions(self.menu, (self.cut_action, self.copy_action,
                                paste_action, self.delete_action, None,
                                selectall_action, None, save_action) )"
list_cron_job_for_all_namespaces;"def list_cron_job_for_all_namespaces(self, **kwargs):
        """"""
        list or watch objects of kind CronJob
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_cron_job_for_all_namespaces(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str pretty: If 'true', then the output is pretty printed.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V2alpha1CronJobList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_cron_job_for_all_namespaces_with_http_info(**kwargs)
        else:
            (data) = self.list_cron_job_for_all_namespaces_with_http_info(**kwargs)
            return data"
calc_expectation;"def calc_expectation(grad_dict, num_batches):
    """"""Calculates the expectation of the gradients per epoch for each parameter w.r.t number of batches

    Parameters
    ----------
    grad_dict: dict
        dictionary that maps parameter name to gradients in the mod executor group
    num_batches: int
        number of batches

    Returns
    ----------
    grad_dict: dict
        dictionary with new keys mapping to gradients expectations

    """"""
    for key in grad_dict.keys():
        grad_dict[str.format(key+""_expectation"")] = mx.ndarray.sum(grad_dict[key], axis=0) / num_batches

    return grad_dict"
exists_type;"def exists_type(self, index, doc_type, params=None):
        """"""
        Check if a type/types exists in an index/indices.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-types-exists.html>`_

        :arg index: A comma-separated list of index names; use `_all` to check
            the types across all indices
        :arg doc_type: A comma-separated list of document types to check
        :arg allow_no_indices: Whether to ignore if a wildcard indices
            expression resolves into no concrete indices. (This includes `_all`
            string or when no indices have been specified)
        :arg expand_wildcards: Whether to expand wildcard expression to concrete
            indices that are open, closed or both., default 'open', valid
            choices are: 'open', 'closed', 'none', 'all'
        :arg ignore_unavailable: Whether specified concrete indices should be
            ignored when unavailable (missing or closed)
        :arg local: Return local information, do not retrieve the state from
            master node (default: false)
        """"""
        for param in (index, doc_type):
            if param in SKIP_IN_PATH:
                raise ValueError(""Empty value passed for a required argument."")
        return self.transport.perform_request(
            ""HEAD"", _make_path(index, ""_mapping"", doc_type), params=params
        )"
parse_voc_rec;"def parse_voc_rec(filename):
    """"""
    parse pascal voc record into a dictionary
    :param filename: xml file path
    :return: list of dict
    """"""
    import xml.etree.ElementTree as ET
    tree = ET.parse(filename)
    objects = []
    for obj in tree.findall('object'):
        obj_dict = dict()
        obj_dict['name'] = obj.find('name').text
        obj_dict['difficult'] = int(obj.find('difficult').text)
        bbox = obj.find('bndbox')
        obj_dict['bbox'] = [int(bbox.find('xmin').text),
                            int(bbox.find('ymin').text),
                            int(bbox.find('xmax').text),
                            int(bbox.find('ymax').text)]
        objects.append(obj_dict)
    return objects"
send_audio_packet;"def send_audio_packet(self, data, *, encode=True):
        """"""Sends an audio packet composed of the data.

        You must be connected to play audio.

        Parameters
        ----------
        data: bytes
            The :term:`py:bytes-like object` denoting PCM or Opus voice data.
        encode: bool
            Indicates if ``data`` should be encoded into Opus.

        Raises
        -------
        ClientException
            You are not connected.
        OpusError
            Encoding the data failed.
        """"""

        self.checked_add('sequence', 1, 65535)
        if encode:
            encoded_data = self.encoder.encode(data, self.encoder.SAMPLES_PER_FRAME)
        else:
            encoded_data = data
        packet = self._get_voice_packet(encoded_data)
        try:
            self.socket.sendto(packet, (self.endpoint_ip, self.voice_port))
        except BlockingIOError:
            log.warning('A packet has been dropped (seq: %s, timestamp: %s)', self.sequence, self.timestamp)

        self.checked_add('timestamp', self.encoder.SAMPLES_PER_FRAME, 4294967295)"
restore_directory_state;"def restore_directory_state(self, fname):
        """"""Restore directory expanded state""""""
        root = osp.normpath(to_text_string(fname))
        if not osp.exists(root):
            # Directory has been (re)moved outside Spyder
            return
        for basename in os.listdir(root):
            path = osp.normpath(osp.join(root, basename))
            if osp.isdir(path) and path in self.__expanded_state:
                self.__expanded_state.pop(self.__expanded_state.index(path))
                if self._to_be_loaded is None:
                    self._to_be_loaded = []
                self._to_be_loaded.append(path)
                self.setExpanded(self.get_index(path), True)
        if not self.__expanded_state:
            self.fsmodel.directoryLoaded.disconnect(self.restore_directory_state)"
set_flat;"def set_flat(self, new_weights):
        """"""Sets the weights to new_weights, converting from a flat array.

        Note:
            You can only set all weights in the network using this function,
            i.e., the length of the array must match get_flat_size.

        Args:
            new_weights (np.ndarray): Flat array containing weights.
        """"""
        self._check_sess()
        shapes = [v.get_shape().as_list() for v in self.variables.values()]
        arrays = unflatten(new_weights, shapes)
        placeholders = [
            self.placeholders[k] for k, v in self.variables.items()
        ]
        self.sess.run(
            list(self.assignment_nodes.values()),
            feed_dict=dict(zip(placeholders, arrays)))"
load_term_config;"def load_term_config(filter_name,
                     term_name,
                     filter_options=None,
                     pillar_key='acl',
                     pillarenv=None,
                     saltenv=None,
                     merge_pillar=True,
                     revision_id=None,
                     revision_no=None,
                     revision_date=True,
                     revision_date_format='%Y/%m/%d',
                     test=False,
                     commit=True,
                     debug=False,
                     source_service=None,
                     destination_service=None,
                     **term_fields):
    '''
    Generate and load the configuration of a policy term.

    filter_name
        The name of the policy filter.

    term_name
        The name of the term.

    filter_options
        Additional filter options. These options are platform-specific.
        See the complete list of options_.

        .. _options: https://github.com/google/capirca/wiki/Policy-format#header-section

    pillar_key: ``acl``
        The key in the pillar containing the default attributes values. Default: ``acl``.
        If the pillar contains the following structure:

        .. code-block:: yaml

            firewall:
              - my-filter:
                  terms:
                    - my-term:
                        source_port: 1234
                        source_address:
                            - 1.2.3.4/32
                            - 5.6.7.8/32

        The ``pillar_key`` field would be specified as ``firewall``.

    pillarenv
        Query the master to generate fresh pillar data on the fly,
        specifically from the requested pillar environment.

    saltenv
        Included only for compatibility with
        :conf_minion:`pillarenv_from_saltenv`, and is otherwise ignored.

    merge_pillar: ``True``
        Merge the CLI variables with the pillar. Default: ``True``.

        The properties specified through the CLI have higher priority than the pillar.

    revision_id
        Add a comment in the term config having the description for the changes applied.

    revision_no
        The revision count.

    revision_date: ``True``
        Boolean flag: display the date when the term configuration was generated. Default: ``True``.

    revision_date_format: ``%Y/%m/%d``
        The date format to be used when generating the perforce data. Default: ``%Y/%m/%d`` (<year>/<month>/<day>).

    test: ``False``
        Dry run? If set as ``True``, will apply the config, discard and return the changes.
        Default: ``False`` and will commit the changes on the device.

    commit: ``True``
        Commit? Default: ``True``.

    debug: ``False``
        Debug mode. Will insert a new key under the output dictionary,
        as ``loaded_config`` containing the raw configuration loaded on the device.

    source_service
        A special service to choose from. This is a helper so the user is able to
        select a source just using the name, instead of specifying a source_port and protocol.

        As this module is available on Unix platforms only,
        it reads the IANA_ port assignment from /etc/services.

        If the user requires additional shortcuts to be referenced, they can add entries under /etc/services,
        which can be managed using the :mod:`file state <salt.states.file>`.

        .. _IANA: http://www.iana.org/assignments/port-numbers

    destination_service
        A special service to choose from. This is a helper so the user is able to
        select a source just using the name, instead of specifying a destination_port and protocol.
        Allows the same options as ``source_service``.

    term_fields
        Term attributes. To see what fields are supported, please consult the
        list of supported keywords_. Some platforms have a few other optional_
        keywords.

        .. _keywords: https://github.com/google/capirca/wiki/Policy-format#keywords
        .. _optional: https://github.com/google/capirca/wiki/Policy-format#optionally-supported-keywords

    .. note::
        The following fields are accepted (some being platform-specific):

        - action
        - address
        - address_exclude
        - comment
        - counter
        - expiration
        - destination_address
        - destination_address_exclude
        - destination_port
        - destination_prefix
        - forwarding_class
        - forwarding_class_except
        - logging
        - log_name
        - loss_priority
        - option
        - policer
        - port
        - precedence
        - principals
        - protocol
        - protocol_except
        - qos
        - pan_application
        - routing_instance
        - source_address
        - source_address_exclude
        - source_port
        - source_prefix
        - verbatim
        - packet_length
        - fragment_offset
        - hop_limit
        - icmp_type
        - ether_type
        - traffic_class_count
        - traffic_type
        - translated
        - dscp_set
        - dscp_match
        - dscp_except
        - next_ip
        - flexible_match_range
        - source_prefix_except
        - destination_prefix_except
        - vpn
        - source_tag
        - destination_tag
        - source_interface
        - destination_interface
        - flattened
        - flattened_addr
        - flattened_saddr
        - flattened_daddr
        - priority

    .. note::
        The following fields can be also a single value and a list of values:

        - action
        - address
        - address_exclude
        - comment
        - destination_address
        - destination_address_exclude
        - destination_port
        - destination_prefix
        - forwarding_class
        - forwarding_class_except
        - logging
        - option
        - port
        - precedence
        - principals
        - protocol
        - protocol_except
        - pan_application
        - source_address
        - source_address_exclude
        - source_port
        - source_prefix
        - verbatim
        - icmp_type
        - ether_type
        - traffic_type
        - dscp_match
        - dscp_except
        - flexible_match_range
        - source_prefix_except
        - destination_prefix_except
        - source_tag
        - destination_tag
        - source_service
        - destination_service

        Example: ``destination_address`` can be either defined as:

        .. code-block:: yaml

            destination_address: 172.17.17.1/24

        or as a list of destination IP addresses:

        .. code-block:: yaml

            destination_address:
                - 172.17.17.1/24
                - 172.17.19.1/24

        or a list of services to be matched:

        .. code-block:: yaml

            source_service:
                - ntp
                - snmp
                - ldap
                - bgpd

    .. note::
        The port fields ``source_port`` and ``destination_port`` can be used as above to select either
        a single value, either a list of values, but also they can select port ranges. Example:

        .. code-block:: yaml

            source_port:
                - - 1000
                  - 2000
                - - 3000
                  - 4000

        With the configuration above, the user is able to select the 1000-2000 and 3000-4000 source port ranges.

    The output is a dictionary having the same form as :mod:`net.load_config <salt.modules.napalm_network.load_config>`.

    CLI Example:

    .. code-block:: bash

        salt 'edge01.bjm01' netacl.load_term_config filter-name term-name source_address=1.2.3.4 destination_address=5.6.7.8 action=accept test=True debug=True

    Output Example:

    .. code-block:: jinja

        edge01.bjm01:
            ----------
            already_configured:
                False
            comment:
                Configuration discarded.
            diff:
                [edit firewall]
                +    family inet {
                +        /*
                +         ** $Date: 2017/03/22 $
                +         **
                +         */
                +        filter filter-name {
                +            interface-specific;
                +            term term-name {
                +                from {
                +                    source-address {
                +                        1.2.3.4/32;
                +                    }
                +                    destination-address {
                +                        5.6.7.8/32;
                +                    }
                +                }
                +                then accept;
                +            }
                +        }
                +    }
            loaded_config:
                firewall {
                    family inet {
                        replace:
                        /*
                        ** $Date: 2017/03/22 $
                        **
                        */
                        filter filter-name {
                            interface-specific;
                            term term-name {
                                from {
                                    source-address {
                                        1.2.3.4/32;
                                    }
                                    destination-address {
                                        5.6.7.8/32;
                                    }
                                }
                                then accept;
                            }
                        }
                    }
                }
            result:
                True
    '''
    if not filter_options:
        filter_options = []
    platform = _get_capirca_platform()
    term_config = __salt__['capirca.get_term_config'](platform,
                                                      filter_name,
                                                      term_name,
                                                      filter_options=filter_options,
                                                      pillar_key=pillar_key,
                                                      pillarenv=pillarenv,
                                                      saltenv=saltenv,
                                                      merge_pillar=merge_pillar,
                                                      revision_id=revision_id,
                                                      revision_no=revision_no,
                                                      revision_date=revision_date,
                                                      revision_date_format=revision_date_format,
                                                      source_service=source_service,
                                                      destination_service=destination_service,
                                                      **term_fields)
    return __salt__['net.load_config'](text=term_config,
                                       test=test,
                                       commit=commit,
                                       debug=debug,
                                       inherit_napalm_device=napalm_device)"
add_debugged_source_file;"def add_debugged_source_file(self, debugged_source_file):
    """"""Add a DebuggedSourceFile proto.""""""
    # TODO(cais): Should the key include a host name, for certain distributed
    #   cases?
    key = debugged_source_file.file_path
    self._source_file_host[key] = debugged_source_file.host
    self._source_file_last_modified[key] = debugged_source_file.last_modified
    self._source_file_bytes[key] = debugged_source_file.bytes
    self._source_file_content[key] = debugged_source_file.lines"
send_ready;"def send_ready(self):
        """"""
        Returns true if data can be written to this channel without blocking.
        This means the channel is either closed (so any write attempt would
        return immediately) or there is at least one byte of space in the
        outbound buffer. If there is at least one byte of space in the
        outbound buffer, a `send` call will succeed immediately and return
        the number of bytes actually written.

        :return:
            ``True`` if a `send` call on this channel would immediately succeed
            or fail
        """"""
        self.lock.acquire()
        try:
            if self.closed or self.eof_sent:
                return True
            return self.out_window_size > 0
        finally:
            self.lock.release()"
activation_atlas;"def activation_atlas(
    model,
    layer,
    grid_size=10,
    icon_size=96,
    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,
    icon_batch_size=32,
    verbose=False,
):
    """"""Renders an Activation Atlas of the given model's layer.""""""

    activations = layer.activations[:number_activations, ...]
    layout, = aligned_umap(activations, verbose=verbose)
    directions, coordinates, _ = bin_laid_out_activations(
        layout, activations, grid_size
    )
    icons = []
    for directions_batch in chunked(directions, icon_batch_size):
        icon_batch, losses = render_icons(
            directions_batch, model, layer=layer.name, size=icon_size, num_attempts=1
        )
        icons += icon_batch
    canvas = make_canvas(icons, coordinates, grid_size)

    return canvas"
load_environment_vars;"def load_environment_vars(self, prefix=SANIC_PREFIX):
        """"""
        Looks for prefixed environment variables and applies
        them to the configuration if present.
        """"""
        for k, v in os.environ.items():
            if k.startswith(prefix):
                _, config_key = k.split(prefix, 1)
                try:
                    self[config_key] = int(v)
                except ValueError:
                    try:
                        self[config_key] = float(v)
                    except ValueError:
                        try:
                            self[config_key] = strtobool(v)
                        except ValueError:
                            self[config_key] = v"
log_deprecated;"def log_deprecated(name="""", text="""", eos=""""):
    """"""
    Log deprecation warning.

    Args:
        name (str): name of the deprecated item.
        text (str, optional): information about the deprecation.
        eos (str, optional): end of service date such as ""YYYY-MM-DD"".
    """"""
    assert name or text
    if eos:
        eos = ""after "" + datetime(*map(int, eos.split(""-""))).strftime(""%d %b"")
    if name:
        if eos:
            warn_msg = ""%s will be deprecated %s. %s"" % (name, eos, text)
        else:
            warn_msg = ""%s was deprecated. %s"" % (name, text)
    else:
        warn_msg = text
        if eos:
            warn_msg += "" Legacy period ends %s"" % eos
    logger.warn(""[Deprecated] "" + warn_msg)"
notify_owner;"def notify_owner(func):
    ''' A decorator for mutating methods of property container classes
    that notifies owners of the property container about mutating changes.

    Args:
        func (callable) : the container method to wrap in a notification

    Returns:
        wrapped method

    Examples:

        A ``__setitem__`` could be wrapped like this:

        .. code-block:: python

            # x[i] = y
            @notify_owner
            def __setitem__(self, i, y):
                return super(PropertyValueDict, self).__setitem__(i, y)

    The returned wrapped method will have a docstring indicating what
    original method it is wrapping.

    '''
    def wrapper(self, *args, **kwargs):
        old = self._saved_copy()
        result = func(self, *args, **kwargs)
        self._notify_owners(old)
        return result
    wrapper.__doc__ = ""Container method ``%s`` instrumented to notify property owners"" % func.__name__
    return wrapper"
get_zone;"def get_zone(zone_id, profile):
    '''
    Get zone information for the given zone_id on the given profile

    :param zone_id: Zone to export.
    :type  zone_id: ``str``

    :param profile: The profile key
    :type  profile: ``str``

    CLI Example:

    .. code-block:: bash

        salt myminion libcloud_dns.get_zone google.com profile1
    '''
    conn = _get_driver(profile=profile)
    return _simple_zone(conn.get_zone(zone_id))"
write_csvs;"def write_csvs(self,
                   asset_map,
                   show_progress=False,
                   invalid_data_behavior='warn'):
        """"""Read CSVs as DataFrames from our asset map.

        Parameters
        ----------
        asset_map : dict[int -> str]
            A mapping from asset id to file path with the CSV data for that
            asset
        show_progress : bool
            Whether or not to show a progress bar while writing.
        invalid_data_behavior : {'warn', 'raise', 'ignore'}
            What to do when data is encountered that is outside the range of
            a uint32.
        """"""
        read = partial(
            read_csv,
            parse_dates=['day'],
            index_col='day',
            dtype=self._csv_dtypes,
        )
        return self.write(
            ((asset, read(path)) for asset, path in iteritems(asset_map)),
            assets=viewkeys(asset_map),
            show_progress=show_progress,
            invalid_data_behavior=invalid_data_behavior,
        )"
allocate_ids;"def allocate_ids(self, incomplete_key, num_ids):
        """"""Allocate a list of IDs from a partial key.

        :type incomplete_key: :class:`google.cloud.datastore.key.Key`
        :param incomplete_key: Partial key to use as base for allocated IDs.

        :type num_ids: int
        :param num_ids: The number of IDs to allocate.

        :rtype: list of :class:`google.cloud.datastore.key.Key`
        :returns: The (complete) keys allocated with ``incomplete_key`` as
                  root.
        :raises: :class:`ValueError` if ``incomplete_key`` is not a
                 partial key.
        """"""
        if not incomplete_key.is_partial:
            raise ValueError((""Key is not partial."", incomplete_key))

        incomplete_key_pb = incomplete_key.to_protobuf()
        incomplete_key_pbs = [incomplete_key_pb] * num_ids

        response_pb = self._datastore_api.allocate_ids(
            incomplete_key.project, incomplete_key_pbs
        )
        allocated_ids = [
            allocated_key_pb.path[-1].id for allocated_key_pb in response_pb.keys
        ]
        return [
            incomplete_key.completed_key(allocated_id) for allocated_id in allocated_ids
        ]"
make_parser;"def make_parser(self, ctx):
        """"""Creates the underlying option parser for this command.""""""
        parser = OptionParser(ctx)
        for param in self.get_params(ctx):
            param.add_to_parser(parser, ctx)
        return parser"
ssh_configured;"def ssh_configured(name,
                   service_running,
                   ssh_key=None,
                   ssh_key_file=None,
                   service_policy=None,
                   service_restart=False,
                   certificate_verify=False):
    '''
    Manage the SSH configuration for a host including whether or not SSH is running or
    the presence of a given SSH key. Note: Only one ssh key can be uploaded for root.
    Uploading a second key will replace any existing key.

    name
        Name of the state.

    service_running
        Ensures whether or not the SSH service should be running on a host. Represented
        as a boolean value where ``True`` indicates that SSH should be running and
        ``False`` indicates that SSH should stopped.

        In order to update SSH keys, the SSH service must be running.

    ssh_key
        Public SSH key to added to the authorized_keys file on the ESXi host. You can
        use ``ssh_key`` or ``ssh_key_file``, but not both.

    ssh_key_file
        File containing the public SSH key to be added to the authorized_keys file on
        the ESXi host. You can use ``ssh_key_file`` or ``ssh_key``, but not both.

    service_policy
        The policy to set for the NTP service.

        .. note::

            When setting the service policy to ``off`` or ``on``, you *must* quote the
            setting. If you don't, the yaml parser will set the string to a boolean,
            which will cause trouble checking for stateful changes and will error when
            trying to set the policy on the ESXi host.

    service_restart
        If set to ``True``, the SSH service will be restarted, regardless of its
        previous running state. Default is ``False``.

    certificate_verify
        If set to ``True``, the SSL connection must present a valid certificate.
        Default is ``False``.

    Example:

    .. code-block:: yaml

        configure-host-ssh:
          esxi.ssh_configured:
            - service_running: True
            - ssh_key_file: /etc/salt/ssh_keys/my_key.pub
            - service_policy: 'on'
            - service_restart: True
            - certificate_verify: True

    '''
    ret = {'name': name,
           'result': False,
           'changes': {},
           'comment': ''}
    esxi_cmd = 'esxi.cmd'
    host = __pillar__['proxy']['host']
    ssh = 'ssh'

    ssh_running = __salt__[esxi_cmd]('get_service_running',
                                     service_name=ssh).get(host)
    error = ssh_running.get('Error')
    if error:
        ret['comment'] = 'Error: {0}'.format(error)
        return ret
    ssh_running = ssh_running.get(ssh)

    # Configure SSH service_running state, if changed.
    if service_running != ssh_running:
        # Only actually run the command if not using test=True
        if not __opts__['test']:
            # Start SSH if service_running=True
            if service_running is True:
                enable = __salt__[esxi_cmd]('service_start',
                                            service_name=ssh).get(host)
                error = enable.get('Error')
                if error:
                    ret['comment'] = 'Error: {0}'.format(error)
                    return ret
            # Disable SSH if service_running=False
            else:
                disable = __salt__[esxi_cmd]('service_stop',
                                             service_name=ssh).get(host)
                error = disable.get('Error')
                if error:
                    ret['comment'] = 'Error: {0}'.format(error)
                    return ret

        ret['changes'].update({'service_running':
                              {'old': ssh_running,
                               'new': service_running}})

    # If uploading an SSH key or SSH key file, see if there's a current
    # SSH key and compare the current key to the key set in the state.
    current_ssh_key, ssh_key_changed = None, False
    if ssh_key or ssh_key_file:
        current_ssh_key = __salt__[esxi_cmd]('get_ssh_key',
                                             certificate_verify=certificate_verify)
        error = current_ssh_key.get('Error')
        if error:
            ret['comment'] = 'Error: {0}'.format(error)
            return ret
        current_ssh_key = current_ssh_key.get('key')
        if current_ssh_key:
            clean_current_key = _strip_key(current_ssh_key).split(' ')
            if not ssh_key:
                ssh_key = ''
                # Open ssh key file and read in contents to create one key string
                with salt.utils.files.fopen(ssh_key_file, 'r') as key_file:
                    for line in key_file:
                        if line.startswith('#'):
                            # Commented line
                            continue
                        ssh_key = ssh_key + line

            clean_ssh_key = _strip_key(ssh_key).split(' ')
            # Check that the first two list items of clean key lists are equal.
            if clean_current_key[0] != clean_ssh_key[0] or clean_current_key[1] != clean_ssh_key[1]:
                ssh_key_changed = True
        else:
            # If current_ssh_key is None, but we're setting a new key with
            # either ssh_key or ssh_key_file, then we need to flag the change.
            ssh_key_changed = True

    # Upload SSH key, if changed.
    if ssh_key_changed:
        if not __opts__['test']:
            # Upload key
            response = __salt__[esxi_cmd]('upload_ssh_key',
                                          ssh_key=ssh_key,
                                          ssh_key_file=ssh_key_file,
                                          certificate_verify=certificate_verify)
            error = response.get('Error')
            if error:
                ret['comment'] = 'Error: {0}'.format(error)
                return ret
        ret['changes'].update({'SSH Key':
                              {'old': current_ssh_key,
                               'new': ssh_key if ssh_key else ssh_key_file}})

    # Configure service_policy
    if service_policy:
        current_service_policy = __salt__[esxi_cmd]('get_service_policy',
                                                    service_name=ssh).get(host)
        error = current_service_policy.get('Error')
        if error:
            ret['comment'] = 'Error: {0}'.format(error)
            return ret
        current_service_policy = current_service_policy.get(ssh)

        if service_policy != current_service_policy:
            # Only run the command if not using test=True
            if not __opts__['test']:
                response = __salt__[esxi_cmd]('set_service_policy',
                                              service_name=ssh,
                                              service_policy=service_policy).get(host)
                error = response.get('Error')
                if error:
                    ret['comment'] = 'Error: {0}'.format(error)
                    return ret
            ret['changes'].update({'service_policy':
                                  {'old': current_service_policy,
                                   'new': service_policy}})

    # Restart ssh_service if service_restart=True
    if service_restart:
        # Only run the command if not using test=True
        if not __opts__['test']:
            response = __salt__[esxi_cmd]('service_restart',
                                          service_name=ssh).get(host)
            error = response.get('Error')
            if error:
                ret['comment'] = 'Error: {0}'.format(error)
                return ret
        ret['changes'].update({'service_restart':
                              {'old': '',
                               'new': 'SSH service restarted.'}})

    ret['result'] = True
    if ret['changes'] == {}:
        ret['comment'] = 'SSH service is already in the desired state.'
        return ret

    if __opts__['test']:
        ret['result'] = None
        ret['comment'] = 'SSH service state will change.'

    return ret"
delete_forecast;"def delete_forecast(self, job_id, forecast_id=None, params=None):
        """"""
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_

        :arg job_id: The ID of the job from which to delete forecasts
        :arg forecast_id: The ID of the forecast to delete, can be comma
            delimited list. Leaving blank implies `_all`
        :arg allow_no_forecasts: Whether to ignore if `_all` matches no
            forecasts
        :arg timeout: Controls the time to wait until the forecast(s) are
            deleted. Default to 30 seconds
        """"""
        if job_id in SKIP_IN_PATH:
            raise ValueError(""Empty value passed for a required argument 'job_id'."")
        return self.transport.perform_request(
            ""DELETE"",
            _make_path(""_ml"", ""anomaly_detectors"", job_id, ""_forecast"", forecast_id),
            params=params,
        )"
artifact_path;"def artifact_path(cls, project, incident, artifact):
        """"""Return a fully-qualified artifact string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/incidents/{incident}/artifacts/{artifact}"",
            project=project,
            incident=incident,
            artifact=artifact,
        )"
select_header_accept;"def select_header_accept(self, accepts):
        """"""
        Returns `Accept` based on an array of accepts provided.

        :param accepts: List of headers.
        :return: Accept (e.g. application/json).
        """"""
        if not accepts:
            return

        accepts = [x.lower() for x in accepts]

        if 'application/json' in accepts:
            return 'application/json'
        else:
            return ', '.join(accepts)"
collect_split_adjustments;"def collect_split_adjustments(self,
                                  adjustments_for_sid,
                                  requested_qtr_data,
                                  dates,
                                  sid,
                                  sid_idx,
                                  sid_estimates,
                                  split_adjusted_asof_idx,
                                  pre_adjustments,
                                  post_adjustments,
                                  requested_split_adjusted_columns):
        """"""
        Collect split adjustments for previous quarters and apply them to the
        given dictionary of splits for the given sid. Since overwrites just
        replace all estimates before the new quarter with NaN, we don't need to
        worry about re-applying split adjustments.

        Parameters
        ----------
        adjustments_for_sid : dict[str -> dict[int -> list]]
            The dictionary of adjustments to which splits need to be added.
            Initially it contains only overwrites.
        requested_qtr_data : pd.DataFrame
            The requested quarter data for each calendar date per sid.
        dates : pd.DatetimeIndex
            The calendar dates for which estimates data is requested.
        sid : int
            The sid for which adjustments need to be collected.
        sid_idx : int
            The index of `sid` in the adjusted array.
        sid_estimates : pd.DataFrame
            The raw estimates data for the given sid.
        split_adjusted_asof_idx : int
            The index in `dates` as-of which the data is split adjusted.
        pre_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)
            The adjustment values and indexes in `dates` for
            adjustments that happened before the split-asof-date.
        post_adjustments : tuple(list(float), list(int), pd.DatetimeIndex)
            The adjustment values, indexes in `dates`, and timestamps for
            adjustments that happened after the split-asof-date.
        requested_split_adjusted_columns : list of str
            List of requested split adjusted column names.
        """"""
        (pre_adjustments_dict,
         post_adjustments_dict) = self._collect_adjustments(
            requested_qtr_data,
            sid,
            sid_idx,
            sid_estimates,
            split_adjusted_asof_idx,
            pre_adjustments,
            post_adjustments,
            requested_split_adjusted_columns
        )
        self.merge_split_adjustments_with_overwrites(
            pre_adjustments_dict,
            post_adjustments_dict,
            adjustments_for_sid,
            requested_split_adjusted_columns
        )"
load_market_data;"def load_market_data(trading_day=None, trading_days=None, bm_symbol='SPY',
                     environ=None):
    """"""
    Load benchmark returns and treasury yield curves for the given calendar and
    benchmark symbol.

    Benchmarks are downloaded as a Series from IEX Trading.  Treasury curves
    are US Treasury Bond rates and are downloaded from 'www.federalreserve.gov'
    by default.  For Canadian exchanges, a loader for Canadian bonds from the
    Bank of Canada is also available.

    Results downloaded from the internet are cached in
    ~/.zipline/data. Subsequent loads will attempt to read from the cached
    files before falling back to redownload.

    Parameters
    ----------
    trading_day : pandas.CustomBusinessDay, optional
        A trading_day used to determine the latest day for which we
        expect to have data.  Defaults to an NYSE trading day.
    trading_days : pd.DatetimeIndex, optional
        A calendar of trading days.  Also used for determining what cached
        dates we should expect to have cached. Defaults to the NYSE calendar.
    bm_symbol : str, optional
        Symbol for the benchmark index to load. Defaults to 'SPY', the ticker
        for the S&P 500, provided by IEX Trading.

    Returns
    -------
    (benchmark_returns, treasury_curves) : (pd.Series, pd.DataFrame)

    Notes
    -----

    Both return values are DatetimeIndexed with values dated to midnight in UTC
    of each stored date.  The columns of `treasury_curves` are:

    '1month', '3month', '6month',
    '1year','2year','3year','5year','7year','10year','20year','30year'
    """"""
    if trading_day is None:
        trading_day = get_calendar('XNYS').day
    if trading_days is None:
        trading_days = get_calendar('XNYS').all_sessions

    first_date = trading_days[0]
    now = pd.Timestamp.utcnow()

    # we will fill missing benchmark data through latest trading date
    last_date = trading_days[trading_days.get_loc(now, method='ffill')]

    br = ensure_benchmark_data(
        bm_symbol,
        first_date,
        last_date,
        now,
        # We need the trading_day to figure out the close prior to the first
        # date so that we can compute returns for the first date.
        trading_day,
        environ,
    )
    tc = ensure_treasury_data(
        bm_symbol,
        first_date,
        last_date,
        now,
        environ,
    )

    # combine dt indices and reindex using ffill then bfill
    all_dt = br.index.union(tc.index)
    br = br.reindex(all_dt, method='ffill').fillna(method='bfill')
    tc = tc.reindex(all_dt, method='ffill').fillna(method='bfill')

    benchmark_returns = br[br.index.slice_indexer(first_date, last_date)]
    treasury_curves = tc[tc.index.slice_indexer(first_date, last_date)]
    return benchmark_returns, treasury_curves"
create_product_set;"def create_product_set(
        self,
        parent,
        product_set,
        product_set_id,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates and returns a new ProductSet resource.

        Possible errors:

        -  Returns INVALID\_ARGUMENT if display\_name is missing, or is longer
           than 4096 characters.

        Example:
            >>> from google.cloud import vision_v1p3beta1
            >>>
            >>> client = vision_v1p3beta1.ProductSearchClient()
            >>>
            >>> parent = client.location_path('[PROJECT]', '[LOCATION]')
            >>>
            >>> # TODO: Initialize `product_set`:
            >>> product_set = {}
            >>>
            >>> # TODO: Initialize `product_set_id`:
            >>> product_set_id = ''
            >>>
            >>> response = client.create_product_set(parent, product_set, product_set_id)

        Args:
            parent (str): The project in which the ProductSet should be created.

                Format is ``projects/PROJECT_ID/locations/LOC_ID``.
            product_set (Union[dict, ~google.cloud.vision_v1p3beta1.types.ProductSet]): The ProductSet to create.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.vision_v1p3beta1.types.ProductSet`
            product_set_id (str): A user-supplied resource id for this ProductSet. If set, the server will
                attempt to use this value as the resource id. If it is already in use,
                an error is returned with code ALREADY\_EXISTS. Must be at most 128
                characters long. It cannot contain the character ``/``.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.vision_v1p3beta1.types.ProductSet` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_product_set"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_product_set""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_product_set,
                default_retry=self._method_configs[""CreateProductSet""].retry,
                default_timeout=self._method_configs[""CreateProductSet""].timeout,
                client_info=self._client_info,
            )

        request = product_search_service_pb2.CreateProductSetRequest(
            parent=parent, product_set=product_set, product_set_id=product_set_id
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""create_product_set""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
distributions_route;"def distributions_route(self, request):
    """"""Given a tag and single run, return an array of compressed histograms.""""""
    tag = request.args.get('tag')
    run = request.args.get('run')
    try:
      (body, mime_type) = self.distributions_impl(tag, run)
      code = 200
    except ValueError as e:
      (body, mime_type) = (str(e), 'text/plain')
      code = 400
    return http_util.Respond(request, body, mime_type, code=code)"
collapse_addresses;"def collapse_addresses(addresses):
    """"""Collapse a list of IP objects.

    Example:
        collapse_addresses([IPv4Network('192.0.2.0/25'),
                            IPv4Network('192.0.2.128/25')]) ->
                           [IPv4Network('192.0.2.0/24')]

    Args:
        addresses: An iterator of IPv4Network or IPv6Network objects.

    Returns:
        An iterator of the collapsed IPv(4|6)Network objects.

    Raises:
        TypeError: If passed a list of mixed version objects.

    """"""
    addrs = []
    ips = []
    nets = []

    # split IP addresses and networks
    for ip in addresses:
        if isinstance(ip, _BaseAddress):
            if ips and ips[-1]._version != ip._version:
                raise TypeError(""%s and %s are not of the same version"" % (
                                ip, ips[-1]))
            ips.append(ip)
        elif ip._prefixlen == ip._max_prefixlen:
            if ips and ips[-1]._version != ip._version:
                raise TypeError(""%s and %s are not of the same version"" % (
                                ip, ips[-1]))
            try:
                ips.append(ip.ip)
            except AttributeError:
                ips.append(ip.network_address)
        else:
            if nets and nets[-1]._version != ip._version:
                raise TypeError(""%s and %s are not of the same version"" % (
                                ip, nets[-1]))
            nets.append(ip)

    # sort and dedup
    ips = sorted(set(ips))

    # find consecutive address ranges in the sorted sequence and summarize them
    if ips:
        for first, last in _find_address_range(ips):
            addrs.extend(summarize_address_range(first, last))

    return _collapse_addresses_internal(addrs + nets)"
disassociate_network_acl;"def disassociate_network_acl(subnet_id=None, vpc_id=None, subnet_name=None, vpc_name=None,
                             region=None, key=None, keyid=None, profile=None):
    '''
    Given a subnet ID, disassociates a network acl.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_vpc.disassociate_network_acl 'subnet-6a1fe403'

    '''

    if not _exactly_one((subnet_name, subnet_id)):
        raise SaltInvocationError('One (but not both) of subnet_id or subnet_name '
                                  'must be provided.')

    if all((vpc_name, vpc_id)):
        raise SaltInvocationError('Only one of vpc_id or vpc_name '
                                  'may be provided.')
    try:
        if subnet_name:
            subnet_id = _get_resource_id('subnet', subnet_name,
                                         region=region, key=key,
                                         keyid=keyid, profile=profile)
            if not subnet_id:
                return {'disassociated': False,
                        'error': {'message': 'Subnet {0} does not exist.'.format(subnet_name)}}

        if vpc_name or vpc_id:
            vpc_id = check_vpc(vpc_id, vpc_name, region, key, keyid, profile)

        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        association_id = conn.disassociate_network_acl(subnet_id, vpc_id=vpc_id)
        return {'disassociated': True, 'association_id': association_id}
    except BotoServerError as e:
        return {'disassociated': False, 'error': __utils__['boto.get_error'](e)}"
needs_renewal;"def needs_renewal(name, window=None):
    '''
    Check if a certificate needs renewal

    :param name: CommonName of cert
    :param window: Window in days to renew earlier or True/force to just return True

    Code example:

    .. code-block:: python

        if __salt__['acme.needs_renewal']('dev.example.com'):
            __salt__['acme.cert']('dev.example.com', **kwargs)
        else:
            log.info('Your certificate is still good')
    '''
    if window is not None and window in ('force', 'Force', True):
        return True

    return _renew_by(name, window) <= datetime.datetime.today()"
generate_ticket;"def generate_ticket(name, output=None, grain=None, key=None, overwrite=True):
    '''
    Generate an icinga2 ticket on the master.

    name
        The domain name for which this ticket will be generated

    output
        grain: output in a grain
        other: the file to store results
        None:  output to the result comment (default)

    grain:
        grain to store the output (need output=grain)

    key:
        the specified grain will be treated as a dictionary, the result
        of this state will be stored under the specified key.

    overwrite:
        The file or grain will be overwritten if it already exists (default)
    '''
    ret = {'name': name,
           'changes': {},
           'result': True,
           'comment': ''}

    # Checking if execution is needed.
    if output == 'grain':
        if grain and not key:
            if not overwrite and grain in __salt__['grains.ls']():
                ret['comment'] = 'No execution needed. Grain {0} already set'.format(grain)
                return ret
            elif __opts__['test']:
                ret['result'] = None
                ret['comment'] = 'Ticket generation would be executed, storing result in grain: {0}'.format(grain)
                return ret
        elif grain:
            if grain in __salt__['grains.ls']():
                grain_value = __salt__['grains.get'](grain)
            else:
                grain_value = {}
            if not overwrite and key in grain_value:
                ret['comment'] = 'No execution needed. Grain {0}:{1} already set'.format(grain, key)
                return ret
            elif __opts__['test']:
                ret['result'] = None
                ret['comment'] = 'Ticket generation would be executed, storing result in grain: {0}:{1}'.format(grain, key)
                return ret
        else:
            ret['result'] = False
            ret['comment'] = ""Error: output type 'grain' needs the grain parameter\n""
            return ret
    elif output:
        if not overwrite and os.path.isfile(output):
            ret['comment'] = 'No execution needed. File {0} already set'.format(output)
            return ret
        elif __opts__['test']:
            ret['result'] = None
            ret['comment'] = 'Ticket generation would be executed, storing result in file: {0}'.format(output)
            return ret
    elif __opts__['test']:
        ret['result'] = None
        ret['comment'] = 'Ticket generation would be executed, not storing result'
        return ret

    # Executing the command.
    ticket_res = __salt__['icinga2.generate_ticket'](name)
    ticket = ticket_res['stdout']
    if not ticket_res['retcode']:
        ret['comment'] = six.text_type(ticket)

    if output == 'grain':
        if grain and not key:
            __salt__['grains.setval'](grain, ticket)
            ret['changes']['ticket'] = ""Executed. Output into grain: {0}"".format(grain)
        elif grain:
            if grain in __salt__['grains.ls']():
                grain_value = __salt__['grains.get'](grain)
            else:
                grain_value = {}
            grain_value[key] = ticket
            __salt__['grains.setval'](grain, grain_value)
            ret['changes']['ticket'] = ""Executed. Output into grain: {0}:{1}"".format(grain, key)
    elif output:
        ret['changes']['ticket'] = ""Executed. Output into {0}"".format(output)
        with salt.utils.files.fopen(output, 'w') as output_file:
            output_file.write(salt.utils.stringutils.to_str(ticket))
    else:
        ret['changes']['ticket'] = ""Executed""

    return ret"
pnl_lifo;"def pnl_lifo(self):
        """"""
        使用后进先出法配对成交记录
        """"""
        X = dict(
            zip(
                self.target.code,
                [LifoQueue() for i in range(len(self.target.code))]
            )
        )
        pair_table = []
        for _, data in self.target.history_table_min.iterrows():
            while True:
                if X[data.code].qsize() == 0:
                    X[data.code].put((data.datetime, data.amount, data.price))
                    break
                else:
                    l = X[data.code].get()
                    if (l[1] * data.amount) < 0:
                        # 原有多仓/ 平仓 或者原有空仓/平仓

                        if abs(l[1]) > abs(data.amount):
                            temp = (l[0], l[1] + data.amount, l[2])
                            X[data.code].put_nowait(temp)
                            if data.amount < 0:
                                pair_table.append(
                                    [
                                        data.code,
                                        data.datetime,
                                        l[0],
                                        abs(data.amount),
                                        data.price,
                                        l[2]
                                    ]
                                )
                                break
                            else:
                                pair_table.append(
                                    [
                                        data.code,
                                        l[0],
                                        data.datetime,
                                        abs(data.amount),
                                        l[2],
                                        data.price

                                    ]
                                )
                                break

                        elif abs(l[1]) < abs(data.amount):
                            data.amount = data.amount + l[1]

                            if data.amount < 0:
                                pair_table.append(
                                    [
                                        data.code,
                                        data.datetime,
                                        l[0],
                                        l[1],
                                        data.price,
                                        l[2]
                                    ]
                                )
                            else:
                                pair_table.append(
                                    [
                                        data.code,
                                        l[0],
                                        data.datetime,
                                        l[1],
                                        l[2],
                                        data.price
                                    ]
                                )
                        else:
                            if data.amount < 0:
                                pair_table.append(
                                    [
                                        data.code,
                                        data.datetime,
                                        l[0],
                                        abs(data.amount),
                                        data.price,
                                        l[2]
                                    ]
                                )
                                break
                            else:
                                pair_table.append(
                                    [
                                        data.code,
                                        l[0],
                                        data.datetime,
                                        abs(data.amount),
                                        l[2],
                                        data.price
                                    ]
                                )
                                break

                    else:
                        X[data.code].put_nowait(l)
                        X[data.code].put_nowait(
                            (data.datetime,
                             data.amount,
                             data.price)
                        )
                        break

        pair_title = [
            'code',
            'sell_date',
            'buy_date',
            'amount',
            'sell_price',
            'buy_price'
        ]
        pnl = pd.DataFrame(pair_table, columns=pair_title).set_index('code')
        pnl = pnl.assign(
            unit=pnl.code.apply(lambda x: self.market_preset.get_unit(x)),
            pnl_ratio=(pnl.sell_price / pnl.buy_price) - 1,
            sell_date=pd.to_datetime(pnl.sell_date),
            buy_date=pd.to_datetime(pnl.buy_date)
        )
        pnl = pnl.assign(
            pnl_money=(pnl.sell_price - pnl.buy_price) * pnl.amount * pnl.unit,
            hold_gap=abs(pnl.sell_date - pnl.buy_date),
            if_buyopen=(pnl.sell_date -
                        pnl.buy_date) > datetime.timedelta(days=0)
        )
        pnl = pnl.assign(
            openprice=pnl.if_buyopen.apply(
                lambda pnl: 1 if pnl else 0) * pnl.buy_price + pnl.if_buyopen.apply(lambda pnl: 0 if pnl else 1) * pnl.sell_price,
            opendate=pnl.if_buyopen.apply(
                lambda pnl: 1 if pnl else 0) * pnl.buy_date.map(str) + pnl.if_buyopen.apply(lambda pnl: 0 if pnl else 1) * pnl.sell_date.map(str),
            closeprice=pnl.if_buyopen.apply(
                lambda pnl: 0 if pnl else 1) * pnl.buy_price + pnl.if_buyopen.apply(lambda pnl: 1 if pnl else 0) * pnl.sell_price,
            closedate=pnl.if_buyopen.apply(
                lambda pnl: 0 if pnl else 1) * pnl.buy_date.map(str) + pnl.if_buyopen.apply(lambda pnl: 1 if pnl else 0) * pnl.sell_date.map(str))
        return pnl.set_index('code')"
date_range;"def date_range(cls,start_time,end_time,freq):
        '''
        Returns a new SArray that represents a fixed frequency datetime index.

        Parameters
        ----------
        start_time : datetime.datetime
          Left bound for generating dates.

        end_time : datetime.datetime
          Right bound for generating dates.

        freq : datetime.timedelta
          Fixed frequency between two consecutive data points.

        Returns
        -------
        out : SArray

        Examples
        --------
        >>> import datetime as dt
        >>> start = dt.datetime(2013, 5, 7, 10, 4, 10)
        >>> end = dt.datetime(2013, 5, 10, 10, 4, 10)
        >>> sa = tc.SArray.date_range(start,end,dt.timedelta(1))
        >>> print sa
        dtype: datetime
        Rows: 4
        [datetime.datetime(2013, 5, 7, 10, 4, 10),
         datetime.datetime(2013, 5, 8, 10, 4, 10),
         datetime.datetime(2013, 5, 9, 10, 4, 10),
         datetime.datetime(2013, 5, 10, 10, 4, 10)]
       '''

        if not isinstance(start_time,datetime.datetime):
            raise TypeError(""The ``start_time`` argument must be from type datetime.datetime."")

        if not isinstance(end_time,datetime.datetime):
            raise TypeError(""The ``end_time`` argument must be from type datetime.datetime."")

        if not isinstance(freq,datetime.timedelta):
            raise TypeError(""The ``freq`` argument must be from type datetime.timedelta."")

        from .. import extensions
        return extensions.date_range(start_time,end_time,freq.total_seconds())"
get_unit;"def get_unit(self, ureg, unit_variations):
        """"""
        Get the first match unit metric object supported by pint library
        given a variation of unit metric names (Ex:['HOUR', 'hour']).

        :param ureg: unit registry which units are defined and handled
        :type ureg: pint.registry.UnitRegistry object

        :param unit_variations: A list of strings with names of units
        :type unit_variations: str
        """"""
        for unit in unit_variations:
            try:
                return getattr(ureg, unit)
            except Exception:
                continue
        return None"
keypoint_scale;"def keypoint_scale(keypoint, scale_x, scale_y, **params):
    """"""Scales a keypoint by scale_x and scale_y.""""""
    x, y, a, s = keypoint
    return [x * scale_x, y * scale_y, a, s * max(scale_x, scale_y)]"
get_jid;"def get_jid(jid):
    '''
    Return the information returned when the specified job id was executed
    '''
    with _get_serv(ret=None, commit=True) as cur:

        sql = '''SELECT id, full_ret FROM `salt_returns`
                WHERE `jid` = %s'''

        cur.execute(sql, (jid,))
        data = cur.fetchall()
        ret = {}
        if data:
            for minion, full_ret in data:
                ret[minion] = salt.utils.json.loads(full_ret)
        return ret"
get_data;"def get_data(img_path):
    """"""get the (1, 3, h, w) np.array data for the supplied image
                Args:
                    img_path (string): the input image path

                Returns:
                    np.array: image data in a (1, 3, h, w) shape

    """"""
    mean = np.array([123.68, 116.779, 103.939])  # (R,G,B)
    img = Image.open(img_path)
    img = np.array(img, dtype=np.float32)
    reshaped_mean = mean.reshape(1, 1, 3)
    img = img - reshaped_mean
    img = np.swapaxes(img, 0, 2)
    img = np.swapaxes(img, 1, 2)
    img = np.expand_dims(img, axis=0)
    return img"
get_submodules;"def get_submodules(mod):
    """"""Get all submodules of a given module""""""
    def catch_exceptions(module):
        pass
    try:
        m = __import__(mod)
        submodules = [mod]
        submods = pkgutil.walk_packages(m.__path__, m.__name__ + '.',
                                        catch_exceptions)
        for sm in submods:
            sm_name = sm[1]
            submodules.append(sm_name)
    except ImportError:
        return []
    except:
        return [mod]
    
    return submodules"
request_offline_members;"async def request_offline_members(self, *guilds):
        r""""""|coro|

        Requests previously offline members from the guild to be filled up
        into the :attr:`Guild.members` cache. This function is usually not
        called. It should only be used if you have the ``fetch_offline_members``
        parameter set to ``False``.

        When the client logs on and connects to the websocket, Discord does
        not provide the library with offline members if the number of members
        in the guild is larger than 250. You can check if a guild is large
        if :attr:`Guild.large` is ``True``.

        Parameters
        -----------
        \*guilds: :class:`Guild`
            An argument list of guilds to request offline members for.

        Raises
        -------
        InvalidArgument
            If any guild is unavailable or not large in the collection.
        """"""
        if any(not g.large or g.unavailable for g in guilds):
            raise InvalidArgument('An unavailable or non-large guild was passed.')

        _guilds = sorted(guilds, key=lambda g: g.shard_id)
        for shard_id, sub_guilds in itertools.groupby(_guilds, key=lambda g: g.shard_id):
            sub_guilds = list(sub_guilds)
            await self._connection.request_offline_members(sub_guilds, shard_id=shard_id)"
set_max_position_size;"def set_max_position_size(self,
                              asset=None,
                              max_shares=None,
                              max_notional=None,
                              on_error='fail'):
        """"""Set a limit on the number of shares and/or dollar value held for the
        given sid. Limits are treated as absolute values and are enforced at
        the time that the algo attempts to place an order for sid. This means
        that it's possible to end up with more than the max number of shares
        due to splits/dividends, and more than the max notional due to price
        improvement.

        If an algorithm attempts to place an order that would result in
        increasing the absolute value of shares/dollar value exceeding one of
        these limits, raise a TradingControlException.

        Parameters
        ----------
        asset : Asset, optional
            If provided, this sets the guard only on positions in the given
            asset.
        max_shares : int, optional
            The maximum number of shares to hold for an asset.
        max_notional : float, optional
            The maximum value to hold for an asset.
        """"""
        control = MaxPositionSize(asset=asset,
                                  max_shares=max_shares,
                                  max_notional=max_notional,
                                  on_error=on_error)
        self.register_trading_control(control)"
cached_request;"def cached_request(self, request):
        """"""
        Return a cached response if it exists in the cache, otherwise
        return False.
        """"""
        cache_url = self.cache_url(request.url)
        logger.debug('Looking up ""%s"" in the cache', cache_url)
        cc = self.parse_cache_control(request.headers)

        # Bail out if the request insists on fresh data
        if ""no-cache"" in cc:
            logger.debug('Request header has ""no-cache"", cache bypassed')
            return False

        if ""max-age"" in cc and cc[""max-age""] == 0:
            logger.debug('Request header has ""max_age"" as 0, cache bypassed')
            return False

        # Request allows serving from the cache, let's see if we find something
        cache_data = self.cache.get(cache_url)
        if cache_data is None:
            logger.debug(""No cache entry available"")
            return False

        # Check whether it can be deserialized
        resp = self.serializer.loads(request, cache_data)
        if not resp:
            logger.warning(""Cache entry deserialization failed, entry ignored"")
            return False

        # If we have a cached 301, return it immediately. We don't
        # need to test our response for other headers b/c it is
        # intrinsically ""cacheable"" as it is Permanent.
        # See:
        #   https://tools.ietf.org/html/rfc7231#section-6.4.2
        #
        # Client can try to refresh the value by repeating the request
        # with cache busting headers as usual (ie no-cache).
        if resp.status == 301:
            msg = (
                'Returning cached ""301 Moved Permanently"" response '
                ""(ignoring date and etag information)""
            )
            logger.debug(msg)
            return resp

        headers = CaseInsensitiveDict(resp.headers)
        if not headers or ""date"" not in headers:
            if ""etag"" not in headers:
                # Without date or etag, the cached response can never be used
                # and should be deleted.
                logger.debug(""Purging cached response: no date or etag"")
                self.cache.delete(cache_url)
            logger.debug(""Ignoring cached response: no date"")
            return False

        now = time.time()
        date = calendar.timegm(parsedate_tz(headers[""date""]))
        current_age = max(0, now - date)
        logger.debug(""Current age based on date: %i"", current_age)

        # TODO: There is an assumption that the result will be a
        #       urllib3 response object. This may not be best since we
        #       could probably avoid instantiating or constructing the
        #       response until we know we need it.
        resp_cc = self.parse_cache_control(headers)

        # determine freshness
        freshness_lifetime = 0

        # Check the max-age pragma in the cache control header
        if ""max-age"" in resp_cc:
            freshness_lifetime = resp_cc[""max-age""]
            logger.debug(""Freshness lifetime from max-age: %i"", freshness_lifetime)

        # If there isn't a max-age, check for an expires header
        elif ""expires"" in headers:
            expires = parsedate_tz(headers[""expires""])
            if expires is not None:
                expire_time = calendar.timegm(expires) - date
                freshness_lifetime = max(0, expire_time)
                logger.debug(""Freshness lifetime from expires: %i"", freshness_lifetime)

        # Determine if we are setting freshness limit in the
        # request. Note, this overrides what was in the response.
        if ""max-age"" in cc:
            freshness_lifetime = cc[""max-age""]
            logger.debug(
                ""Freshness lifetime from request max-age: %i"", freshness_lifetime
            )

        if ""min-fresh"" in cc:
            min_fresh = cc[""min-fresh""]
            # adjust our current age by our min fresh
            current_age += min_fresh
            logger.debug(""Adjusted current age from min-fresh: %i"", current_age)

        # Return entry if it is fresh enough
        if freshness_lifetime > current_age:
            logger.debug('The response is ""fresh"", returning cached response')
            logger.debug(""%i > %i"", freshness_lifetime, current_age)
            return resp

        # we're not fresh. If we don't have an Etag, clear it out
        if ""etag"" not in headers:
            logger.debug('The cached response is ""stale"" with no etag, purging')
            self.cache.delete(cache_url)

        # return the original handler
        return False"
path_constant;"def path_constant(self, name, value):
        """"""Declare and set a project global constant, whose value is a path. The
        path is adjusted to be relative to the invocation directory. The given
        value path is taken to be either absolute, or relative to this project
        root.""""""
        assert is_iterable_typed(name, basestring)
        assert is_iterable_typed(value, basestring)
        if len(value) > 1:
            self.registry.manager.errors()(""path constant should have one element"")
        self.registry.current().add_constant(name[0], value, path=1)"
get_input_grads;"def get_input_grads(self, merge_multi_context=True):
        """"""Get the gradients with respect to the inputs of the module.

        Parameters
        ----------
        merge_multi_context : bool
            Defaults to ``True``. In the case when data-parallelism is used, the outputs
            will be collected from multiple devices. A `True` value indicate that we
            should merge the collected results so that they look like from a single
            executor.

        Returns
        -------
        If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output
        elements are `NDArray`.
        """"""
        assert self.inputs_need_grad
        if merge_multi_context:
            return _merge_multi_context(self.input_grad_arrays, self.data_layouts)
        return self.input_grad_arrays"
add_event;"def add_event(self, rule, callback):
        """"""Adds an event to the algorithm's EventManager.

        Parameters
        ----------
        rule : EventRule
            The rule for when the callback should be triggered.
        callback : callable[(context, data) -> None]
            The function to execute when the rule is triggered.
        """"""
        self.event_manager.add_event(
            zipline.utils.events.Event(rule, callback),
        )"
create_event_permission;"def create_event_permission(self, lambda_name, principal, source_arn):
        """"""
        Create permissions to link to an event.

        Related: http://docs.aws.amazon.com/lambda/latest/dg/with-s3-example-configure-event-source.html
        """"""
        logger.debug('Adding new permission to invoke Lambda function: {}'.format(lambda_name))
        permission_response = self.lambda_client.add_permission(
            FunctionName=lambda_name,
            StatementId=''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)),
            Action='lambda:InvokeFunction',
            Principal=principal,
            SourceArn=source_arn,
        )

        if permission_response['ResponseMetadata']['HTTPStatusCode'] != 201:
            print('Problem creating permission to invoke Lambda function')
            return None  # XXX: Raise?

        return permission_response"
cell_values;"def cell_values(self, column_family_id, column, max_count=None):
        """"""Get a time series of cells stored on this instance.

        For example:

        .. literalinclude:: snippets_table.py
            :start-after: [START bigtable_row_cell_values]
            :end-before: [END bigtable_row_cell_values]

        Args:
            column_family_id (str): The ID of the column family. Must be of the
                form ``[_a-zA-Z0-9][-_.a-zA-Z0-9]*``.
            column (bytes): The column within the column family where the cells
                are located.
            max_count (int): The maximum number of cells to use.

        Returns:
            A generator which provides: cell.value, cell.timestamp_micros
                for each cell in the list of cells

        Raises:
            KeyError: If ``column_family_id`` is not among the cells stored
                in this row.
            KeyError: If ``column`` is not among the cells stored in this row
                for the given ``column_family_id``.
        """"""
        cells = self.find_cells(column_family_id, column)
        if max_count is None:
            max_count = len(cells)

        for index, cell in enumerate(cells):
            if index == max_count:
                break

            yield cell.value, cell.timestamp_micros"
get_nni_installation_path;"def get_nni_installation_path():
    ''' Find nni lib from the following locations in order
    Return nni root directory if it exists
    '''
    def try_installation_path_sequentially(*sitepackages):
        '''Try different installation path sequentially util nni is found.
        Return None if nothing is found
        '''
        def _generate_installation_path(sitepackages_path):
            python_dir = get_python_dir(sitepackages_path)
            entry_file = os.path.join(python_dir, 'nni', 'main.js')
            if os.path.isfile(entry_file):
                return python_dir
            return None

        for sitepackage in sitepackages:
            python_dir = _generate_installation_path(sitepackage)
            if python_dir:
                return python_dir
        return None

    if os.getenv('VIRTUAL_ENV'):
        # if 'virtualenv' package is used, `site` has not attr getsitepackages, so we will instead use VIRTUAL_ENV
        # Note that conda venv will not have VIRTUAL_ENV
        python_dir = os.getenv('VIRTUAL_ENV')
    else:
        python_sitepackage = site.getsitepackages()[0]
        # If system-wide python is used, we will give priority to using `local sitepackage`--""usersitepackages()"" given that nni exists there
        if python_sitepackage.startswith('/usr') or python_sitepackage.startswith('/Library'):
            python_dir = try_installation_path_sequentially(site.getusersitepackages(), site.getsitepackages()[0])
        else:
            python_dir = try_installation_path_sequentially(site.getsitepackages()[0], site.getusersitepackages())

    if python_dir:
        entry_file = os.path.join(python_dir, 'nni', 'main.js')
        if os.path.isfile(entry_file):
            return os.path.join(python_dir, 'nni')
    print_error('Fail to find nni under python library')
    exit(1)"
list_namespaced_lease;"def list_namespaced_lease(self, namespace, **kwargs):
        """"""
        list or watch objects of kind Lease
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_namespaced_lease(namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1LeaseList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_namespaced_lease_with_http_info(namespace, **kwargs)
        else:
            (data) = self.list_namespaced_lease_with_http_info(namespace, **kwargs)
            return data"
begin_state;"def begin_state(self, batch_size=0, func=ndarray.zeros, **kwargs):
        """"""Initial state for this cell.

        Parameters
        ----------
        func : callable, default symbol.zeros
            Function for creating initial state.

            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,
            `symbol.var etc`. Use `symbol.var` if you want to directly
            feed input as states.

            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.
        batch_size: int, default 0
            Only required for NDArray API. Size of the batch ('N' in layout)
            dimension of input.

        **kwargs :
            Additional keyword arguments passed to func. For example
            `mean`, `std`, `dtype`, etc.

        Returns
        -------
        states : nested list of Symbol
            Starting states for the first RNN step.
        """"""
        assert not self._modified, \
            ""After applying modifier cells (e.g. ZoneoutCell) the base "" \
            ""cell cannot be called directly. Call the modifier cell instead.""
        states = []
        for info in self.state_info(batch_size):
            self._init_counter += 1
            if info is not None:
                info.update(kwargs)
            else:
                info = kwargs
            state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),
                         **info)
            states.append(state)
        return states"
count_characters;"def count_characters(root, out):
    """"""Count the occurrances of the different characters in the files""""""
    if os.path.isfile(root):
        with open(root, 'rb') as in_f:
            for line in in_f:
                for char in line:
                    if char not in out:
                        out[char] = 0
                    out[char] = out[char] + 1
    elif os.path.isdir(root):
        for filename in os.listdir(root):
            count_characters(os.path.join(root, filename), out)"
most_specific_convertible_shape;"def most_specific_convertible_shape(self, other):
        """"""Returns the most specific TensorShape convertible with `self` and `other`.

        * TensorShape([None, 1]) is the most specific TensorShape convertible with
          both TensorShape([2, 1]) and TensorShape([5, 1]). Note that
          TensorShape(None) is also convertible with above mentioned TensorShapes.

        * TensorShape([1, 2, 3]) is the most specific TensorShape convertible with
          both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more
          less specific TensorShapes convertible with above mentioned TensorShapes,
          e.g. TensorShape([1, 2, None]), TensorShape(None).

        Args:
          other: Another `TensorShape`.

        Returns:
          A `TensorShape` which is the most specific convertible shape of `self`
          and `other`.
        """"""

        other = as_shape(other)
        if self._dims is None or other.dims is None or self.ndims != other.ndims:
            return unknown_shape()

        dims = [(Dimension(None))] * self.ndims
        for i, (d1, d2) in enumerate(zip(self._dims, other.dims)):
            if d1 is not None and d2 is not None and d1 == d2:
                dims[i] = d1
        return TensorShape(dims)"
write_ping;"def write_ping(self, data: bytes) -> None:
        """"""Send ping frame.""""""
        assert isinstance(data, bytes)
        self._write_frame(True, 0x9, data)"
search_keys;"def search_keys(text, keyserver=None, user=None):
    '''
    Search keys from keyserver

    text
        Text to search the keyserver for, e.g. email address, keyID or fingerprint.

    keyserver
        Keyserver to use for searching for GPG keys, defaults to pgp.mit.edu.

    user
        Which user's keychain to access, defaults to user Salt is running as.
        Passing the user as ``salt`` will set the GnuPG home directory to the
        ``/etc/salt/gpgkeys``.

    CLI Example:

    .. code-block:: bash

        salt '*' gpg.search_keys user@example.com

        salt '*' gpg.search_keys user@example.com keyserver=keyserver.ubuntu.com

        salt '*' gpg.search_keys user@example.com keyserver=keyserver.ubuntu.com user=username

    '''
    if GPG_1_3_1:
        raise SaltInvocationError('The search_keys function is not support with this version of python-gnupg.')
    else:
        if not keyserver:
            keyserver = 'pgp.mit.edu'

        _keys = []
        for _key in _search_keys(text, keyserver, user):
            tmp = {'keyid': _key['keyid'],
                   'uids': _key['uids']}

            expires = _key.get('expires', None)
            date = _key.get('date', None)
            length = _key.get('length', None)

            if expires:
                tmp['expires'] = time.strftime('%Y-%m-%d',
                                               time.localtime(float(_key['expires'])))
            if date:
                tmp['created'] = time.strftime('%Y-%m-%d',
                                               time.localtime(float(_key['date'])))
            if length:
                tmp['keyLength'] = _key['length']
            _keys.append(tmp)
        return _keys"
log_group_name;"def log_group_name(self):
        """"""
        Name of the AWS CloudWatch Log Group that we will be querying. It generates the name based on the
        Lambda Function name and stack name provided.

        Returns
        -------
        str
            Name of the CloudWatch Log Group
        """"""

        function_id = self._function_name
        if self._stack_name:
            function_id = self._get_resource_id_from_stack(self._cfn_client, self._stack_name, self._function_name)
            LOG.debug(""Function with LogicalId '%s' in stack '%s' resolves to actual physical ID '%s'"",
                      self._function_name, self._stack_name, function_id)

        return LogGroupProvider.for_lambda_function(function_id)"
get_loginclass;"def get_loginclass(name):
    '''
    Get the login class of the user

    .. versionadded:: 2016.3.0

    CLI Example:

    .. code-block:: bash

        salt '*' user.get_loginclass foo

    '''

    userinfo = __salt__['cmd.run_stdout'](['pw', 'usershow', '-n', name])
    userinfo = userinfo.split(':')

    return userinfo[4] if len(userinfo) == 10 else ''"
srv_name;"def srv_name(svc, proto='tcp', domain=None):
    '''
    Generate SRV record name
    :param svc: ldap, 389 etc
    :param proto: tcp, udp, sctp etc.
    :param domain: name to append
    :return:
    '''
    proto = RFC.validate(proto, RFC.SRV_PROTO)
    if isinstance(svc, int) or svc.isdigit():
        svc = _to_port(svc)

    if domain:
        domain = '.' + domain
    return '_{0}._{1}{2}'.format(svc, proto, domain)"
list_clusters;"def list_clusters(
        self,
        project_id,
        zone,
        parent=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Lists all clusters owned by a project in either the specified zone or all
        zones.

        Example:
            >>> from google.cloud import container_v1
            >>>
            >>> client = container_v1.ClusterManagerClient()
            >>>
            >>> # TODO: Initialize `project_id`:
            >>> project_id = ''
            >>>
            >>> # TODO: Initialize `zone`:
            >>> zone = ''
            >>>
            >>> response = client.list_clusters(project_id, zone)

        Args:
            project_id (str): Deprecated. The Google Developers Console `project ID or project
                number <https://support.google.com/cloud/answer/6158840>`__. This field
                has been deprecated and replaced by the parent field.
            zone (str): Deprecated. The name of the Google Compute Engine
                `zone <https://cloud.google.com/compute/docs/zones#available>`__ in
                which the cluster resides, or ""-"" for all zones. This field has been
                deprecated and replaced by the parent field.
            parent (str): The parent (project and location) where the clusters will be listed.
                Specified in the format 'projects/*/locations/*'. Location ""-"" matches
                all zones and all regions.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.container_v1.types.ListClustersResponse` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""list_clusters"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""list_clusters""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.list_clusters,
                default_retry=self._method_configs[""ListClusters""].retry,
                default_timeout=self._method_configs[""ListClusters""].timeout,
                client_info=self._client_info,
            )

        request = cluster_service_pb2.ListClustersRequest(
            project_id=project_id, zone=zone, parent=parent
        )
        return self._inner_api_calls[""list_clusters""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
main_loop;"def main_loop(args):
    '''main loop logic for trial keeper'''
    
    if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
    
    stdout_file = open(STDOUT_FULL_PATH, 'a+')
    stderr_file = open(STDERR_FULL_PATH, 'a+')
    trial_keeper_syslogger = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, 'trial_keeper', StdOutputType.Stdout, args.log_collection)
    # redirect trial keeper's stdout and stderr to syslog
    trial_syslogger_stdout = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, 'trial', StdOutputType.Stdout, args.log_collection)
    sys.stdout = sys.stderr = trial_keeper_syslogger
    # backward compatibility
    hdfs_host = None
    hdfs_output_dir = None
    if args.hdfs_host:
        hdfs_host = args.hdfs_host
    elif args.pai_hdfs_host:
        hdfs_host = args.pai_hdfs_host
    if args.hdfs_output_dir:
        hdfs_output_dir = args.hdfs_output_dir
    elif args.pai_hdfs_output_dir:
        hdfs_output_dir = args.pai_hdfs_output_dir

    if hdfs_host is not None and args.nni_hdfs_exp_dir is not None:
        try:
            if args.webhdfs_path:
                hdfs_client = HdfsClient(hosts='{0}:80'.format(hdfs_host), user_name=args.pai_user_name, webhdfs_path=args.webhdfs_path, timeout=5)
            else:
                # backward compatibility
                hdfs_client = HdfsClient(hosts='{0}:{1}'.format(hdfs_host, '50070'), user_name=args.pai_user_name, timeout=5)
        except Exception as e:
            nni_log(LogType.Error, 'Create HDFS client error: ' + str(e))
            raise e
        copyHdfsDirectoryToLocal(args.nni_hdfs_exp_dir, os.getcwd(), hdfs_client)

    # Notice: We don't appoint env, which means subprocess wil inherit current environment and that is expected behavior
    log_pipe_stdout = trial_syslogger_stdout.get_pipelog_reader()
    process = Popen(args.trial_command, shell = True, stdout = log_pipe_stdout, stderr = log_pipe_stdout)
    nni_log(LogType.Info, 'Trial keeper spawns a subprocess (pid {0}) to run command: {1}'.format(process.pid, shlex.split(args.trial_command)))

    while True:
        retCode = process.poll()
        # child worker process exits and all stdout data is read
        if retCode is not None and log_pipe_stdout.set_process_exit() and log_pipe_stdout.is_read_completed == True:
            nni_log(LogType.Info, 'subprocess terminated. Exit code is {}. Quit'.format(retCode))
            if hdfs_output_dir is not None:
                # Copy local directory to hdfs for OpenPAI
                nni_local_output_dir = os.environ['NNI_OUTPUT_DIR']
                try:
                    if copyDirectoryToHdfs(nni_local_output_dir, hdfs_output_dir, hdfs_client):
                        nni_log(LogType.Info, 'copy directory from {0} to {1} success!'.format(nni_local_output_dir, hdfs_output_dir))
                    else:
                        nni_log(LogType.Info, 'copy directory from {0} to {1} failed!'.format(nni_local_output_dir, hdfs_output_dir))
                except Exception as e:
                    nni_log(LogType.Error, 'HDFS copy directory got exception: ' + str(e))
                    raise e

            ## Exit as the retCode of subprocess(trial)
            exit(retCode)
            break

        time.sleep(2)"
init_state;"def init_state(self, x):
    """"""
    Initialize t, m, and u
    """"""
    optim_state = {}
    optim_state[""t""] = 0.
    optim_state[""m""] = [tf.zeros_like(v) for v in x]
    optim_state[""u""] = [tf.zeros_like(v) for v in x]
    return optim_state"
add_option;"def add_option(self, opts, dest, action=None, nargs=1, const=None,
                   obj=None):
        """"""Adds a new option named `dest` to the parser.  The destination
        is not inferred (unlike with optparse) and needs to be explicitly
        provided.  Action can be any of ``store``, ``store_const``,
        ``append``, ``appnd_const`` or ``count``.

        The `obj` can be used to identify the option in the order list
        that is returned from the parser.
        """"""
        if obj is None:
            obj = dest
        opts = [normalize_opt(opt, self.ctx) for opt in opts]
        option = Option(opts, dest, action=action, nargs=nargs,
                        const=const, obj=obj)
        self._opt_prefixes.update(option.prefixes)
        for opt in option._short_opts:
            self._short_opt[opt] = option
        for opt in option._long_opts:
            self._long_opt[opt] = option"
closed_issue;"def closed_issue(issue, after=None):
    """"""Returns True iff this issue was closed after given date. If after not given, only checks if issue is closed.""""""
    if issue['state'] == 'closed':
        if after is None or parse_timestamp(issue['closed_at']) > after:
            return True
    return False"
get_conn;"def get_conn():
    '''
    Return a conn object for the passed VM data
    '''
    if __active_provider_name__ in __context__:
        return __context__[__active_provider_name__]
    vm_ = get_configured_provider()
    profile = vm_.pop('profile', None)
    if profile is not None:
        vm_ = __utils__['dictupdate.update'](os_client_config.vendors.get_profile(profile), vm_)
    conn = shade.openstackcloud.OpenStackCloud(cloud_config=None, **vm_)
    if __active_provider_name__ is not None:
        __context__[__active_provider_name__] = conn
    return conn"
update_filter;"def update_filter(self, filter_id, body, params=None):
        """"""
        `<>`_

        :arg filter_id: The ID of the filter to update
        :arg body: The filter update
        """"""
        for param in (filter_id, body):
            if param in SKIP_IN_PATH:
                raise ValueError(""Empty value passed for a required argument."")
        return self.transport.perform_request(
            ""POST"",
            _make_path(""_ml"", ""filters"", filter_id, ""_update""),
            params=params,
            body=body,
        )"
import_key;"def import_key(text=None,
               filename=None,
               user=None,
               gnupghome=None):
    r'''
    Import a key from text or file

    text
        The text containing to import.

    filename
        The filename containing the key to import.

    user
        Which user's keychain to access, defaults to user Salt is running as.
        Passing the user as ``salt`` will set the GnuPG home directory to the
        ``/etc/salt/gpgkeys``.

    gnupghome
        Specify the location where GPG keyring and related files are stored.

    CLI Example:

    .. code-block:: bash

        salt '*' gpg.import_key text='-----BEGIN PGP PUBLIC KEY BLOCK-----\n ... -----END PGP PUBLIC KEY BLOCK-----'
        salt '*' gpg.import_key filename='/path/to/public-key-file'

    '''
    ret = {
        'res': True,
        'message': ''
        }

    gpg = _create_gpg(user, gnupghome)

    if not text and not filename:
        raise SaltInvocationError('filename or text must be passed.')

    if filename:
        try:
            with salt.utils.files.flopen(filename, 'rb') as _fp:
                lines = _fp.readlines()
                text = ''.join(lines)
        except IOError:
            raise SaltInvocationError('filename does not exist.')

    imported_data = gpg.import_keys(text)

    if GPG_1_3_1:
        counts = imported_data.counts
        if counts.get('imported') or counts.get('imported_rsa'):
            ret['message'] = 'Successfully imported key(s).'
        elif counts.get('unchanged'):
            ret['message'] = 'Key(s) already exist in keychain.'
        elif counts.get('not_imported'):
            ret['res'] = False
            ret['message'] = 'Unable to import key.'
        elif not counts.get('count'):
            ret['res'] = False
            ret['message'] = 'Unable to import key.'
    else:
        if imported_data.imported or imported_data.imported_rsa:
            ret['message'] = 'Successfully imported key(s).'
        elif imported_data.unchanged:
            ret['message'] = 'Key(s) already exist in keychain.'
        elif imported_data.not_imported:
            ret['res'] = False
            ret['message'] = 'Unable to import key.'
        elif not imported_data.count:
            ret['res'] = False
            ret['message'] = 'Unable to import key.'
    return ret"
usermacro_get;"def usermacro_get(macro=None, hostids=None, templateids=None, hostmacroids=None,
                  globalmacroids=None, globalmacro=False, **kwargs):
    '''
    Retrieve user macros according to the given parameters.

    Args:
        macro:          name of the usermacro
        hostids:        Return macros for the given hostids
        templateids:    Return macros for the given templateids
        hostmacroids:   Return macros with the given hostmacroids
        globalmacroids: Return macros with the given globalmacroids (implies globalmacro=True)
        globalmacro:    if True, returns only global macros


        optional kwargs:
                _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)
                _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)
                _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)

    Returns:
        Array with usermacro details, False if no usermacro found or on failure.

    CLI Example:
    .. code-block:: bash

        salt '*' zabbix.usermacro_get macro='{$SNMP_COMMUNITY}'
    '''
    conn_args = _login(**kwargs)
    ret = {}
    try:
        if conn_args:
            method = 'usermacro.get'
            params = {""output"": ""extend"", ""filter"": {}}
            if macro:
                # Python mistakenly interprets macro names starting and ending with '{' and '}' as a dict
                if isinstance(macro, dict):
                    macro = ""{"" + six.text_type(macro.keys()[0]) +""}""
                if not macro.startswith('{') and not macro.endswith('}'):
                    macro = ""{"" + macro + ""}""
                params['filter'].setdefault('macro', macro)
            if hostids:
                params.setdefault('hostids', hostids)
            elif templateids:
                params.setdefault('templateids', hostids)
            if hostmacroids:
                params.setdefault('hostmacroids', hostmacroids)
            elif globalmacroids:
                globalmacro = True
                params.setdefault('globalmacroids', globalmacroids)
            if globalmacro:
                params = _params_extend(params, globalmacro=True)
            params = _params_extend(params, **kwargs)
            ret = _query(method, params, conn_args['url'], conn_args['auth'])
            return ret['result'] if ret['result'] else False
        else:
            raise KeyError
    except KeyError:
        return ret"
extract_scalar_reward;"def extract_scalar_reward(value, scalar_key='default'):
    """"""
    Extract scalar reward from trial result.

    Raises
    ------
    RuntimeError
        Incorrect final result: the final result should be float/int,
        or a dict which has a key named ""default"" whose value is float/int.
    """"""
    if isinstance(value, float) or isinstance(value, int):
        reward = value
    elif isinstance(value, dict) and scalar_key in value and isinstance(value[scalar_key], (float, int)):
        reward = value[scalar_key]
    else:
        raise RuntimeError('Incorrect final result: the final result should be float/int, or a dict which has a key named ""default"" whose value is float/int.')
    return reward"
prepare_post_parameters;"def prepare_post_parameters(self, post_params=None, files=None):
        """"""Builds form parameters.

        :param post_params: Normal form parameters.
        :param files: File parameters.
        :return: Form parameters with files.
        """"""
        params = []

        if post_params:
            params = post_params

        if files:
            for k, v in six.iteritems(files):
                if not v:
                    continue
                file_names = v if type(v) is list else [v]
                for n in file_names:
                    with open(n, 'rb') as f:
                        filename = os.path.basename(f.name)
                        filedata = f.read()
                        mimetype = (mimetypes.guess_type(filename)[0] or
                                    'application/octet-stream')
                        params.append(
                            tuple([k, tuple([filename, filedata, mimetype])]))

        return params"
reidentify_content;"def reidentify_content(
        self,
        parent,
        reidentify_config=None,
        inspect_config=None,
        item=None,
        inspect_template_name=None,
        reidentify_template_name=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Re-identifies content that has been de-identified. See
        https://cloud.google.com/dlp/docs/pseudonymization#re-identification\_in\_free\_text\_code\_example
        to learn more.

        Example:
            >>> from google.cloud import dlp_v2
            >>>
            >>> client = dlp_v2.DlpServiceClient()
            >>>
            >>> parent = client.project_path('[PROJECT]')
            >>>
            >>> response = client.reidentify_content(parent)

        Args:
            parent (str): The parent resource name.
            reidentify_config (Union[dict, ~google.cloud.dlp_v2.types.DeidentifyConfig]): Configuration for the re-identification of the content item. This field
                shares the same proto message type that is used for de-identification,
                however its usage here is for the reversal of the previous
                de-identification. Re-identification is performed by examining the
                transformations used to de-identify the items and executing the reverse.
                This requires that only reversible transformations be provided here. The
                reversible transformations are:

                -  ``CryptoReplaceFfxFpeConfig``

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.dlp_v2.types.DeidentifyConfig`
            inspect_config (Union[dict, ~google.cloud.dlp_v2.types.InspectConfig]): Configuration for the inspector.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.dlp_v2.types.InspectConfig`
            item (Union[dict, ~google.cloud.dlp_v2.types.ContentItem]): The item to re-identify. Will be treated as text.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.dlp_v2.types.ContentItem`
            inspect_template_name (str): Optional template to use. Any configuration directly specified in
                ``inspect_config`` will override those set in the template. Singular
                fields that are set in this request will replace their corresponding
                fields in the template. Repeated fields are appended. Singular
                sub-messages and groups are recursively merged.
            reidentify_template_name (str): Optional template to use. References an instance of
                ``DeidentifyTemplate``. Any configuration directly specified in
                ``reidentify_config`` or ``inspect_config`` will override those set in
                the template. Singular fields that are set in this request will replace
                their corresponding fields in the template. Repeated fields are
                appended. Singular sub-messages and groups are recursively merged.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.dlp_v2.types.ReidentifyContentResponse` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""reidentify_content"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""reidentify_content""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.reidentify_content,
                default_retry=self._method_configs[""ReidentifyContent""].retry,
                default_timeout=self._method_configs[""ReidentifyContent""].timeout,
                client_info=self._client_info,
            )

        request = dlp_pb2.ReidentifyContentRequest(
            parent=parent,
            reidentify_config=reidentify_config,
            inspect_config=inspect_config,
            item=item,
            inspect_template_name=inspect_template_name,
            reidentify_template_name=reidentify_template_name,
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""reidentify_content""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
file_hash;"def file_hash(load, fnd):
    '''
    Return a file hash based on the hash type set in the master config
    '''
    if not all(x in load for x in ('path', 'saltenv')):
        return '', None
    ret = {'hash_type': __opts__['hash_type']}
    relpath = fnd['rel']
    path = fnd['path']
    hash_cachedir = os.path.join(__opts__['cachedir'], 'azurefs', 'hashes')
    hashdest = salt.utils.path.join(hash_cachedir,
                                    load['saltenv'],
                                    '{0}.hash.{1}'.format(relpath,
                                                          __opts__['hash_type']))
    if not os.path.isfile(hashdest):
        if not os.path.exists(os.path.dirname(hashdest)):
            os.makedirs(os.path.dirname(hashdest))
        ret['hsum'] = salt.utils.hashutils.get_hash(path, __opts__['hash_type'])
        with salt.utils.files.fopen(hashdest, 'w+') as fp_:
            fp_.write(salt.utils.stringutils.to_str(ret['hsum']))
        return ret
    else:
        with salt.utils.files.fopen(hashdest, 'rb') as fp_:
            ret['hsum'] = salt.utils.stringutils.to_unicode(fp_.read())
        return ret"
update_image_size_range;"def update_image_size_range(spec, feature_name, size_range):
    """"""
    Annotate an input or output Image feature in a Neural Network spec to
    to accommodate a range of image sizes

    :param spec: MLModel
        The MLModel spec containing the feature

    :param feature_name: str
        The name of the Image feature for which to add shape information.
        If the feature is not found in the input or output descriptions then
        an exception is thrown

    :param size_range: NeuralNetworkImageSizeRange
        A NeuralNetworkImageSizeRange object with the populated image size
        range information.

    Examples
    --------
    .. sourcecode:: python

        >>> import coremltools
        >>> from coremltools.models.neural_network import flexible_shape_utils
        >>> spec = coremltools.utils.load_spec('mymodel.mlmodel')
        >>> img_size_ranges = flexible_shape_utils.NeuralNetworkImageSizeRange()
        >>> img_size_ranges.add_height_range(64, 128)
        >>> img_size_ranges.add_width_range(128, -1)
        >>> flexible_shape_utils.update_image_size_range(spec, feature_name='my_multiarray_featurename', size_range=img_size_ranges)

    :return:
        None. The spec object is updated
    """"""
    if not isinstance(size_range, NeuralNetworkImageSizeRange):
        raise Exception(
            'Shape ranges should be of type NeuralNetworkImageSizeRange')

    feature = _get_feature(spec, feature_name)
    if feature.type.WhichOneof('Type') != 'imageType':
        raise Exception('Trying to add size ranges for '
                        'a non-image feature type')

    feature.type.imageType.ClearField('SizeFlexibility')
    feature.type.imageType.imageSizeRange.heightRange.lowerBound = size_range.get_height_range().lowerBound
    feature.type.imageType.imageSizeRange.heightRange.upperBound = size_range.get_height_range().upperBound

    feature.type.imageType.imageSizeRange.widthRange.lowerBound = size_range.get_width_range().lowerBound
    feature.type.imageType.imageSizeRange.widthRange.upperBound = size_range.get_width_range().upperBound

    # Bump up specification version
    spec.specificationVersion = max(_MINIMUM_FLEXIBLE_SHAPES_SPEC_VERSION,
                                    spec.specificationVersion)"
word_under_mouse_cursor;"def word_under_mouse_cursor(self):
        """"""
        Selects the word under the **mouse** cursor.

        :return: A QTextCursor with the word under mouse cursor selected.
        """"""
        editor = self._editor
        text_cursor = editor.cursorForPosition(editor._last_mouse_pos)
        text_cursor = self.word_under_cursor(True, text_cursor)
        return text_cursor"
normalize_path;"def normalize_path(path, resolve_symlinks=True):
    # type: (str, bool) -> str
    """"""
    Convert a path to its canonical, case-normalized, absolute version.

    """"""
    path = expanduser(path)
    if resolve_symlinks:
        path = os.path.realpath(path)
    else:
        path = os.path.abspath(path)
    return os.path.normcase(path)"
get_query_arguments;"def get_query_arguments(self, name: str, strip: bool = True) -> List[str]:
        """"""Returns a list of the query arguments with the given name.

        If the argument is not present, returns an empty list.

        .. versionadded:: 3.2
        """"""
        return self._get_arguments(name, self.request.query_arguments, strip)"
list_nodes_min;"def list_nodes_min(call=None):
    '''
    Return a list of the VMs that are on the provider. Only a list of VM names and
    their state is returned. This is the minimum amount of information needed to
    check for existing VMs.

    .. versionadded:: 2015.8.0

    CLI Example:

    .. code-block:: bash

        salt-cloud -f list_nodes_min my-linode-config
        salt-cloud --function list_nodes_min my-linode-config
    '''
    if call == 'action':
        raise SaltCloudSystemExit(
            'The list_nodes_min function must be called with -f or --function.'
        )

    ret = {}
    nodes = _query('linode', 'list')['DATA']

    for node in nodes:
        name = node['LABEL']
        this_node = {
            'id': six.text_type(node['LINODEID']),
            'state': _get_status_descr_by_id(int(node['STATUS']))
        }

        ret[name] = this_node

    return ret"
get_meta_graph;"def get_meta_graph(self, tags=None):
    """"""Returns the matching MetaGraphDef or raises KeyError.""""""
    matches = [meta_graph
               for meta_graph in self.meta_graphs
               if set(meta_graph.meta_info_def.tags) == set(tags or [])]
    if not matches:
      raise KeyError(""SavedModelHandler has no graph with tags: %r"" % tags)
    if len(matches) != 1:
      raise KeyError(
          ""SavedModelHandler has multiple graphs with tags %r"" % tags)
    return matches[0]"
experiment_list;"def experiment_list(args):
    '''get the information of all experiments'''
    experiment_config = Experiments()
    experiment_dict = experiment_config.get_all_experiments()
    if not experiment_dict:
        print('There is no experiment running...')
        exit(1)
    update_experiment()
    experiment_id_list = []
    if args.all and args.all == 'all':
        for key in experiment_dict.keys():
            experiment_id_list.append(key)
    else:
        for key in experiment_dict.keys():
            if experiment_dict[key]['status'] != 'STOPPED':
                experiment_id_list.append(key)
        if not experiment_id_list:
            print_warning('There is no experiment running...\nYou can use \'nnictl experiment list all\' to list all stopped experiments!')
    experiment_information = """"
    for key in experiment_id_list:
        
        experiment_information += (EXPERIMENT_DETAIL_FORMAT % (key, experiment_dict[key]['status'], experiment_dict[key]['port'],\
        experiment_dict[key].get('platform'), experiment_dict[key]['startTime'], experiment_dict[key]['endTime']))
    print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)"
read_dataset_metadata;"def read_dataset_metadata(self):
    """"""Read `dataset_meta` field from bucket""""""
    if self.dataset_meta:
      return
    shell_call(['gsutil', 'cp',
                'gs://' + self.storage_client.bucket_name + '/'
                + 'dataset/' + self.dataset_name + '_dataset.csv',
                LOCAL_DATASET_METADATA_FILE])
    with open(LOCAL_DATASET_METADATA_FILE, 'r') as f:
      self.dataset_meta = eval_lib.DatasetMetadata(f)"
evaluate_similarity;"def evaluate_similarity(args, token_embedding, ctx, logfile=None,
                        global_step=0):
    """"""Evaluate on specified similarity datasets.""""""

    results = []
    for similarity_function in args.similarity_functions:
        evaluator = nlp.embedding.evaluation.WordEmbeddingSimilarity(
            idx_to_vec=token_embedding.idx_to_vec,
            similarity_function=similarity_function)
        evaluator.initialize(ctx=ctx)
        if not args.no_hybridize:
            evaluator.hybridize()

        # Evaluate all datasets
        for (dataset_name, dataset_kwargs,
             dataset) in iterate_similarity_datasets(args):
            initial_length = len(dataset)
            dataset_coded = [[
                token_embedding.token_to_idx[d[0]],
                token_embedding.token_to_idx[d[1]], d[2]
            ] for d in dataset if d[0] in token_embedding.token_to_idx
                             and d[1] in token_embedding.token_to_idx]
            num_dropped = initial_length - len(dataset_coded)

            # All words are unknown
            if not len(dataset_coded):
                correlation = 0
            else:
                words1, words2, scores = zip(*dataset_coded)
                pred_similarity = evaluator(
                    mx.nd.array(words1, ctx=ctx), mx.nd.array(words2, ctx=ctx))
                sr = stats.spearmanr(pred_similarity.asnumpy(),
                                     np.array(scores))
                correlation = sr.correlation

            logging.info(
                'Spearman rank correlation on %s (%s pairs) %s with %s:\t%s',
                dataset.__class__.__name__, len(dataset_coded),
                str(dataset_kwargs), similarity_function, correlation)

            result = dict(
                task='similarity',
                dataset_name=dataset_name,
                dataset_kwargs=dataset_kwargs,
                similarity_function=similarity_function,
                spearmanr=correlation,
                num_dropped=num_dropped,
                global_step=global_step,
            )
            log_similarity_result(logfile, result)
            results.append(result)

    return results"
process_notifications;"def process_notifications(self, messages):
        """"""Process notifications.""""""
        for object_id, object_size, metadata_size in messages:
            if object_size > 0 and object_id in self._waiting_dict:
                linked_list = self._waiting_dict[object_id]
                self._complete_future(linked_list)"
update_total;"def update_total(self, n=1):
    """"""Increment total pbar value.""""""
    with self._lock:
      self._pbar.total += n
      self.refresh()"
get_minions;"def get_minions():
    '''
    Return a list of minions
    '''
    query = '''SELECT DISTINCT minion_id
               FROM {keyspace}.minions;'''.format(keyspace=_get_keyspace())

    ret = []

    # cassandra_cql.cql_query may raise a CommandExecutionError
    try:
        data = __salt__['cassandra_cql.cql_query'](query)
        if data:
            for row in data:
                minion = row.get('minion_id')
                if minion:
                    ret.append(minion)
    except CommandExecutionError:
        log.critical('Could not get the list of minions.')
        raise
    except Exception as e:
        log.critical(
            'Unexpected error while getting list of minions: %s', e)
        raise

    return ret"
copy_value;"def copy_value(self, orig_key, new_key):
        """"""Copy value""""""
        data = self.model.get_data()
        if isinstance(data, list):
            data.append(data[orig_key])
        if isinstance(data, set):
            data.add(data[orig_key])
        else:
            data[new_key] = data[orig_key]
        self.set_data(data)"
report_open_file;"def report_open_file(self, options):
        """"""Request to start a LSP server to attend a language.""""""
        filename = options['filename']
        logger.debug('Call LSP for %s' % filename)
        language = options['language']
        callback = options['codeeditor']
        stat = self.main.lspmanager.start_client(language.lower())
        self.main.lspmanager.register_file(
            language.lower(), filename, callback)
        if stat:
            if language.lower() in self.lsp_editor_settings:
                self.lsp_server_ready(
                    language.lower(), self.lsp_editor_settings[
                        language.lower()])
            else:
                editor = self.get_current_editor()
                editor.lsp_ready = False"
set_params;"def set_params(self, arg_params, aux_params):
        """"""Set parameter and aux values.

        Parameters
        ----------
        arg_params : list of NDArray
            Source parameter arrays
        aux_params : list of NDArray
            Source aux arrays.
        """"""

        for texec in self.execgrp.train_execs:
            texec.copy_params_from(arg_params, aux_params)"
close_window;"def close_window(self):
        """"""Close QMainWindow instance that contains this plugin.""""""
        if self.undocked_window is not None:
            self.undocked_window.close()
            self.undocked_window = None

            # Oddly, these actions can appear disabled after the Dock
            # action is pressed
            self.undock_action.setDisabled(False)
            self.close_plugin_action.setDisabled(False)"
get_valid_units;"def get_valid_units(self, ureg, from_unit, target_unit):
        """"""
        Returns the firt match `pint.unit.Unit` object for from_unit and
        target_unit strings from a possible variation of metric unit names
        supported by pint library.

        :param ureg: unit registry which units are defined and handled
        :type ureg: `pint.registry.UnitRegistry`

        :param from_unit: source metric unit
        :type from_unit: str

        :param from_unit: target metric unit
        :type from_unit: str
        """"""
        from_unit_variations = [from_unit.lower(), from_unit.upper()]
        target_unit_variations = [target_unit.lower(), target_unit.upper()]
        from_unit = self.get_unit(ureg, from_unit_variations)
        target_unit = self.get_unit(ureg, target_unit_variations)
        return from_unit, target_unit"
verify_challenge;"def verify_challenge(uri):
    """"""
    Loop until our challenge is verified, else fail.
    """"""
    while True:
        try:
            resp = urlopen(uri)
            challenge_status = json.loads(resp.read().decode('utf8'))
        except IOError as e:
            raise ValueError(""Error checking challenge: {0} {1}"".format(
                e.code, json.loads(e.read().decode('utf8'))))
        if challenge_status['status'] == ""pending"":
            time.sleep(2)
        elif challenge_status['status'] == ""valid"":
            LOGGER.info(""Domain verified!"")
            break
        else:
            raise ValueError(""Domain challenge did not pass: {0}"".format(
                challenge_status))"
miscs_update_idxs_vals;"def miscs_update_idxs_vals(self, miscs, idxs, vals,
                               assert_all_vals_used=True,
                               idxs_map=None):
        """"""
        Unpack the idxs-vals format into the list of dictionaries that is
        `misc`.

        Parameters
        ----------
        idxs_map : dict
            idxs_map is a dictionary of id->id mappings so that the misc['idxs'] can
        contain different numbers than the idxs argument.
        """"""
        if idxs_map is None:
            idxs_map = {}

        assert set(idxs.keys()) == set(vals.keys())

        misc_by_id = {m['tid']: m for m in miscs}
        for m in miscs:
            m['idxs'] = dict([(key, []) for key in idxs])
            m['vals'] = dict([(key, []) for key in idxs])

        for key in idxs:
            assert len(idxs[key]) == len(vals[key])
            for tid, val in zip(idxs[key], vals[key]):
                tid = idxs_map.get(tid, tid)
                if assert_all_vals_used or tid in misc_by_id:
                    misc_by_id[tid]['idxs'][key] = [tid]
                    misc_by_id[tid]['vals'][key] = [val]"
unset_role;"def unset_role(username, role, **kwargs):
    '''
    Remove role from username.

    username
        Username for role removal

    role
        Role to remove

    no_save_config
        If True, don't save configuration commands to startup configuration.
        If False, save configuration to startup configuration.
        Default: False

    .. code-block:: bash

        salt '*' nxos.cmd unset_role username=daniel role=vdc-admin
    '''
    role_line = 'no username {0} role {1}'.format(username, role)
    return config(role_line, **kwargs)"
remove_all_thumbnails;"def remove_all_thumbnails(self):
        """"""Remove all thumbnails.""""""
        for thumbnail in self._thumbnails:
            self.layout().removeWidget(thumbnail)
            thumbnail.sig_canvas_clicked.disconnect()
            thumbnail.sig_remove_figure.disconnect()
            thumbnail.sig_save_figure.disconnect()
            thumbnail.deleteLater()
        self._thumbnails = []
        self.current_thumbnail = None
        self.figure_viewer.figcanvas.clear_canvas()"
image_list;"def image_list(self, lookup='all'):
        '''
        Return a mapping of all image data for available providers
        '''
        data = {}

        lookups = self.lookup_providers(lookup)
        if not lookups:
            return data

        for alias, driver in lookups:
            fun = '{0}.avail_images'.format(driver)
            if fun not in self.clouds:
                # The capability to gather images is not supported by this
                # cloud module
                log.debug(
                    'The \'%s\' cloud driver defined under \'%s\' provider '
                    'alias is unable to get the images information',
                    driver, alias
                )
                continue

            if alias not in data:
                data[alias] = {}

            try:
                with salt.utils.context.func_globals_inject(
                    self.clouds[fun],
                    __active_provider_name__=':'.join([alias, driver])
                ):
                    data[alias][driver] = self.clouds[fun]()
            except Exception as err:
                log.error(
                    'Failed to get the output of \'%s()\': %s',
                    fun, err, exc_info_on_loglevel=logging.DEBUG
                )
        return data"
get_value;"def get_value(self, sid, dt, field):
        """"""
        Retrieve the value at the given coordinates.

        Parameters
        ----------
        sid : int
            The asset identifier.
        dt : pd.Timestamp
            The timestamp for the desired data point.
        field : string
            The OHLVC name for the desired data point.

        Returns
        -------
        value : float|int
            The value at the given coordinates, ``float`` for OHLC, ``int``
            for 'volume'.

        Raises
        ------
        NoDataOnDate
            If the given dt is not a valid market minute (in minute mode) or
            session (in daily mode) according to this reader's tradingcalendar.
        NoDataForSid
            If the given sid is not valid.
        """"""
        try:
            country_code = self._country_code_for_assets([sid])
        except ValueError as exc:
            raise_from(
                NoDataForSid(
                    'Asset not contained in daily pricing file: {}'.format(sid)
                ),
                exc
            )
        return self._readers[country_code].get_value(sid, dt, field)"
special_type;"def special_type(self):
        """"""
        [str] 特别处理状态。’Normal’ - 正常上市, ‘ST’ - ST处理, ‘StarST’ - *ST代表该股票正在接受退市警告,
        ‘PT’ - 代表该股票连续3年收入为负，将被暂停交易, ‘Other’ - 其他（股票专用）
        """"""
        try:
            return self.__dict__[""special_type""]
        except (KeyError, ValueError):
            raise AttributeError(
                ""Instrument(order_book_id={}) has no attribute 'special_type' "".format(self.order_book_id)
            )"
get_subnet_association;"def get_subnet_association(subnets, region=None, key=None, keyid=None,
                           profile=None):
    '''
    Given a subnet (aka: a vpc zone identifier) or list of subnets, returns
    vpc association.

    Returns a VPC ID if the given subnets are associated with the same VPC ID.
    Returns False on an error or if the given subnets are associated with
    different VPC IDs.

    CLI Examples:

    .. code-block:: bash

        salt myminion boto_vpc.get_subnet_association subnet-61b47516

    .. code-block:: bash

        salt myminion boto_vpc.get_subnet_association ['subnet-61b47516','subnet-2cb9785b']

    '''
    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)

        # subnet_ids=subnets can accept either a string or a list
        subnets = conn.get_all_subnets(subnet_ids=subnets)
    except BotoServerError as e:
        return {'error': __utils__['boto.get_error'](e)}

    # using a set to store vpc_ids - the use of set prevents duplicate
    # vpc_id values
    vpc_ids = set()
    for subnet in subnets:
        log.debug('examining subnet id: %s for vpc_id', subnet.id)
        if subnet in subnets:
            log.debug('subnet id: %s is associated with vpc id: %s',
                      subnet.id, subnet.vpc_id)
            vpc_ids.add(subnet.vpc_id)
    if not vpc_ids:
        return {'vpc_id': None}
    elif len(vpc_ids) == 1:
        return {'vpc_id': vpc_ids.pop()}
    else:
        return {'vpc_ids': list(vpc_ids)}"
get_plugin_actions;"def get_plugin_actions(self):
        """"""Return a list of actions related to plugin.""""""
        create_client_action = create_action(
                                   self,
                                   _(""New console (default settings)""),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_new_client,
                                   context=Qt.WidgetWithChildrenShortcut)
        self.register_shortcut(create_client_action, context=""ipython_console"",
                               name=""New tab"")

        create_pylab_action = create_action(
                                   self,
                                   _(""New Pylab console (data plotting)""),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_pylab_client,
                                   context=Qt.WidgetWithChildrenShortcut)

        create_sympy_action = create_action(
                                   self,
                                   _(""New SymPy console (symbolic math)""),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_sympy_client,
                                   context=Qt.WidgetWithChildrenShortcut)

        create_cython_action = create_action(
                                   self,
                                   _(""New Cython console (Python with ""
                                     ""C extensions)""),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_cython_client,
                                   context=Qt.WidgetWithChildrenShortcut)
        special_console_action_group = QActionGroup(self)
        special_console_actions = (create_pylab_action, create_sympy_action,
                                   create_cython_action)
        add_actions(special_console_action_group, special_console_actions)
        special_console_menu = QMenu(_(""New special console""), self)
        add_actions(special_console_menu, special_console_actions)

        restart_action = create_action(self, _(""Restart kernel""),
                                       icon=ima.icon('restart'),
                                       triggered=self.restart_kernel,
                                       context=Qt.WidgetWithChildrenShortcut)

        reset_action = create_action(self, _(""Remove all variables""),
                                     icon=ima.icon('editdelete'),
                                     triggered=self.reset_kernel,
                                     context=Qt.WidgetWithChildrenShortcut)

        if self.interrupt_action is None:
            self.interrupt_action = create_action(
                self, _(""Interrupt kernel""),
                icon=ima.icon('stop'),
                triggered=self.interrupt_kernel,
                context=Qt.WidgetWithChildrenShortcut)

        self.register_shortcut(restart_action, context=""ipython_console"",
                               name=""Restart kernel"")

        connect_to_kernel_action = create_action(self,
               _(""Connect to an existing kernel""), None, None,
               _(""Open a new IPython console connected to an existing kernel""),
               triggered=self.create_client_for_kernel)
        
        rename_tab_action = create_action(self, _(""Rename tab""),
                                       icon=ima.icon('rename'),
                                       triggered=self.tab_name_editor)
        
        # Add the action to the 'Consoles' menu on the main window
        main_consoles_menu = self.main.consoles_menu_actions
        main_consoles_menu.insert(0, create_client_action)
        main_consoles_menu += [special_console_menu, connect_to_kernel_action,
                               MENU_SEPARATOR,
                               self.interrupt_action, restart_action,
                               reset_action]

        # Plugin actions
        self.menu_actions = [create_client_action, special_console_menu,
                             connect_to_kernel_action,
                             MENU_SEPARATOR,
                             self.interrupt_action,
                             restart_action, reset_action, rename_tab_action]

        self.update_execution_state_kernel()

        # Check for a current client. Since it manages more actions.
        client = self.get_current_client()
        if client:
            return client.get_options_menu()
        return self.menu_actions"
get_most_frequent_response;"def get_most_frequent_response(input_statement, response_list, storage=None):
    """"""
    :param input_statement: A statement, that closely matches an input to the chat bot.
    :type input_statement: Statement

    :param response_list: A list of statement options to choose a response from.
    :type response_list: list

    :param storage: An instance of a storage adapter to allow the response selection
                    method to access other statements if needed.
    :type storage: StorageAdapter

    :return: The response statement with the greatest number of occurrences.
    :rtype: Statement
    """"""
    matching_response = None
    occurrence_count = -1

    logger = logging.getLogger(__name__)
    logger.info('Selecting response with greatest number of occurrences.')

    for statement in response_list:
        count = len(list(storage.filter(
            text=statement.text,
            in_response_to=input_statement.text)
        ))

        # Keep the more common statement
        if count >= occurrence_count:
            matching_response = statement
            occurrence_count = count

    # Choose the most commonly occuring matching response
    return matching_response"
list_namespace;"def list_namespace(self, **kwargs):
        """"""
        list or watch objects of kind Namespace
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_namespace(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1NamespaceList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_namespace_with_http_info(**kwargs)
        else:
            (data) = self.list_namespace_with_http_info(**kwargs)
            return data"
auth_password;"def auth_password(self, username, password, event=None, fallback=True):
        """"""
        Authenticate to the server using a password.  The username and password
        are sent over an encrypted link.

        If an ``event`` is passed in, this method will return immediately, and
        the event will be triggered once authentication succeeds or fails.  On
        success, `is_authenticated` will return ``True``.  On failure, you may
        use `get_exception` to get more detailed error information.

        Since 1.1, if no event is passed, this method will block until the
        authentication succeeds or fails.  On failure, an exception is raised.
        Otherwise, the method simply returns.

        Since 1.5, if no event is passed and ``fallback`` is ``True`` (the
        default), if the server doesn't support plain password authentication
        but does support so-called ""keyboard-interactive"" mode, an attempt
        will be made to authenticate using this interactive mode.  If it fails,
        the normal exception will be thrown as if the attempt had never been
        made.  This is useful for some recent Gentoo and Debian distributions,
        which turn off plain password authentication in a misguided belief
        that interactive authentication is ""more secure"".  (It's not.)

        If the server requires multi-step authentication (which is very rare),
        this method will return a list of auth types permissible for the next
        step.  Otherwise, in the normal case, an empty list is returned.

        :param str username: the username to authenticate as
        :param basestring password: the password to authenticate with
        :param .threading.Event event:
            an event to trigger when the authentication attempt is complete
            (whether it was successful or not)
        :param bool fallback:
            ``True`` if an attempt at an automated ""interactive"" password auth
            should be made if the server doesn't support normal password auth
        :return:
            list of auth types permissible for the next stage of
            authentication (normally empty)

        :raises:
            `.BadAuthenticationType` -- if password authentication isn't
            allowed by the server for this user (and no event was passed in)
        :raises:
            `.AuthenticationException` -- if the authentication failed (and no
            event was passed in)
        :raises: `.SSHException` -- if there was a network error
        """"""
        if (not self.active) or (not self.initial_kex_done):
            # we should never try to send the password unless we're on a secure
            # link
            raise SSHException(""No existing session"")
        if event is None:
            my_event = threading.Event()
        else:
            my_event = event
        self.auth_handler = AuthHandler(self)
        self.auth_handler.auth_password(username, password, my_event)
        if event is not None:
            # caller wants to wait for event themselves
            return []
        try:
            return self.auth_handler.wait_for_response(my_event)
        except BadAuthenticationType as e:
            # if password auth isn't allowed, but keyboard-interactive *is*,
            # try to fudge it
            if not fallback or (""keyboard-interactive"" not in e.allowed_types):
                raise
            try:

                def handler(title, instructions, fields):
                    if len(fields) > 1:
                        raise SSHException(""Fallback authentication failed."")
                    if len(fields) == 0:
                        # for some reason, at least on os x, a 2nd request will
                        # be made with zero fields requested.  maybe it's just
                        # to try to fake out automated scripting of the exact
                        # type we're doing here.  *shrug* :)
                        return []
                    return [password]

                return self.auth_interactive(username, handler)
            except SSHException:
                # attempt failed; just raise the original exception
                raise e"
put_multipart;"def put_multipart(self, local_path, destination_s3_path, part_size=DEFAULT_PART_SIZE, **kwargs):
        """"""
        Put an object stored locally to an S3 path
        using S3 multi-part upload (for files > 8Mb).
        :param local_path: Path to source local file
        :param destination_s3_path: URL for target S3 location
        :param part_size: Part size in bytes. Default: 8388608 (8MB)
        :param kwargs: Keyword arguments are passed to the boto function `upload_fileobj` as ExtraArgs
        """"""
        self._check_deprecated_argument(**kwargs)

        from boto3.s3.transfer import TransferConfig
        # default part size for boto3 is 8Mb, changing it to fit part_size
        # provided as a parameter
        transfer_config = TransferConfig(multipart_chunksize=part_size)

        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)

        self.s3.meta.client.upload_fileobj(
            Fileobj=open(local_path, 'rb'), Bucket=bucket, Key=key, Config=transfer_config, ExtraArgs=kwargs)"
repeat_last_axis;"def repeat_last_axis(array, count):
    """"""
    Restride `array` to repeat `count` times along the last axis.

    Parameters
    ----------
    array : np.array
        The array to restride.
    count : int
        Number of times to repeat `array`.

    Returns
    -------
    result : array
        Array of shape array.shape + (count,) composed of `array` repeated
        `count` times along the last axis.

    Example
    -------
    >>> from numpy import arange
    >>> a = arange(3); a
    array([0, 1, 2])
    >>> repeat_last_axis(a, 2)
    array([[0, 0],
           [1, 1],
           [2, 2]])
    >>> repeat_last_axis(a, 4)
    array([[0, 0, 0, 0],
           [1, 1, 1, 1],
           [2, 2, 2, 2]])

    Notes
    ----
    The resulting array will share memory with `array`.  If you need to assign
    to the input or output, you should probably make a copy first.

    See Also
    --------
    repeat_last_axis
    """"""
    return as_strided(array, array.shape + (count,), array.strides + (0,))"
get_widget;"def get_widget(self, index=None, path=None, tabs=None):
        """"""Get widget by index.

        If no tabs and index specified the current active widget is returned.
        """"""
        if (index and tabs) or (path and tabs):
            return tabs.widget(index)
        elif self.plugin:
            return self.get_plugin_tabwidget(self.plugin).currentWidget()
        else:
            return self.plugins_tabs[0][0].currentWidget()"
find_top_level_bracket_locations;"def find_top_level_bracket_locations(string_toparse):
        """"""Get the locations of top-level brackets in a string.""""""
        bracket_stack = []
        replace_args_list = []
        bracket_type = None
        literal_type = ''
        brackets = {'(': ')', '[': ']', '{': '}'}
        for idx, character in enumerate(string_toparse):
            if (not bracket_stack and character in brackets.keys()
                    or character == bracket_type):
                bracket_stack.append(idx)
                bracket_type = character
            elif bracket_type and character == brackets[bracket_type]:
                begin_idx = bracket_stack.pop()
                if not bracket_stack:
                    if not literal_type:
                        if bracket_type == '(':
                            literal_type = '(None)'
                        elif bracket_type == '[':
                            literal_type = '[list]'
                        elif bracket_type == '{':
                            if idx - begin_idx <= 1:
                                literal_type = '{dict}'
                            else:
                                literal_type = '{set}'
                    replace_args_list.append(
                        (string_toparse[begin_idx:idx + 1],
                         literal_type, 1))
                    bracket_type = None
                    literal_type = ''
            elif len(bracket_stack) == 1:
                if bracket_type == '(' and character == ',':
                    literal_type = '(tuple)'
                elif bracket_type == '{' and character == ':':
                    literal_type = '{dict}'
                elif bracket_type == '(' and character == ':':
                    literal_type = '[slice]'

        if bracket_stack:
            raise IndexError('Bracket mismatch')
        for replace_args in replace_args_list:
            string_toparse = string_toparse.replace(*replace_args)
        return string_toparse"
dataset_view;"def dataset_view(self, dataset):
        """""" view metadata for a dataset.

            Parameters
            ==========
            dataset: the string identified of the dataset
                     should be in format [owner]/[dataset-name]
        """"""
        if '/' in dataset:
            self.validate_dataset_string(dataset)
            dataset_urls = dataset.split('/')
            owner_slug = dataset_urls[0]
            dataset_slug = dataset_urls[1]
        else:
            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)
            dataset_slug = dataset

        result = self.process_response(
            self.datasets_view_with_http_info(owner_slug, dataset_slug))
        return Dataset(result)"
fix_indentation;"def fix_indentation(self, index=None):
        """"""Replace tab characters by spaces""""""
        if index is None:
            index = self.get_stack_index()
        finfo = self.data[index]
        finfo.editor.fix_indentation()"
run_task;"def run_task(task_name, args=None, kwargs=None, broker=None, backend=None, wait_for_result=False, timeout=None,
             propagate=True, interval=0.5, no_ack=True, raise_timeout=True, config=None):
    '''
    Execute celery tasks. For celery specific parameters see celery documentation.


    CLI Example:

    .. code-block:: bash

        salt '*' celery.run_task tasks.sleep args=[4] broker=redis://localhost \\
        backend=redis://localhost wait_for_result=true

    task_name
        The task name, e.g. tasks.sleep

    args
        Task arguments as a list

    kwargs
        Task keyword arguments

    broker
        Broker for celeryapp, see celery documentation

    backend
        Result backend for celeryapp, see celery documentation

    wait_for_result
        Wait until task result is read from result backend and return result, Default: False

    timeout
        Timeout waiting for result from celery, see celery AsyncResult.get documentation

    propagate
        Propagate exceptions from celery task, see celery AsyncResult.get documentation, Default: True

    interval
        Interval to check for task result, see celery AsyncResult.get documentation, Default: 0.5

    no_ack
        see celery AsyncResult.get documentation. Default: True

    raise_timeout
        Raise timeout exception if waiting for task result times out. Default: False

    config
        Config dict for celery app, See celery documentation

    '''
    if not broker:
        raise SaltInvocationError('broker parameter is required')

    with Celery(broker=broker, backend=backend, set_as_current=False) as app:
        if config:
            app.conf.update(config)

        with app.connection():
            args = args or []
            kwargs = kwargs or {}
            async_result = app.send_task(task_name, args=args, kwargs=kwargs)

            if wait_for_result:
                try:
                    return async_result.get(timeout=timeout, propagate=propagate,
                                            interval=interval, no_ack=no_ack)
                except TimeoutError as ex:
                    log.error('Waiting for the result of a celery task execution timed out.')
                    if raise_timeout:
                        raise ex
                    return False"
download_dataset;"def download_dataset(storage_client, image_batches, target_dir,
                     local_dataset_copy=None):
  """"""Downloads dataset, organize it by batches and rename images.

  Args:
    storage_client: instance of the CompetitionStorageClient
    image_batches: subclass of ImageBatchesBase with data about images
    target_dir: target directory, should exist and be empty
    local_dataset_copy: directory with local dataset copy, if local copy is
      available then images will be takes from there instead of Cloud Storage

  Data in the target directory will be organized into subdirectories by batches,
  thus path to each image will be ""target_dir/BATCH_ID/IMAGE_ID.png""
  where BATCH_ID - ID of the batch (key of image_batches.data),
  IMAGE_ID - ID of the image (key of image_batches.data[batch_id]['images'])
  """"""
  for batch_id, batch_value in iteritems(image_batches.data):
    batch_dir = os.path.join(target_dir, batch_id)
    os.mkdir(batch_dir)
    for image_id, image_val in iteritems(batch_value['images']):
      dst_filename = os.path.join(batch_dir, image_id + '.png')
      # try to use local copy first
      if local_dataset_copy:
        local_filename = os.path.join(local_dataset_copy,
                                      os.path.basename(image_val['image_path']))
        if os.path.exists(local_filename):
          shutil.copyfile(local_filename, dst_filename)
          continue
      # download image from cloud
      cloud_path = ('gs://' + storage_client.bucket_name
                    + '/' + image_val['image_path'])
      if not os.path.exists(dst_filename):
        subprocess.call(['gsutil', 'cp', cloud_path, dst_filename])"
log_trial;"def log_trial(args):
    ''''get trial log path'''
    trial_id_path_dict = {}
    nni_config = Config(get_config_filename(args))
    rest_port = nni_config.get_config('restServerPort')
    rest_pid = nni_config.get_config('restServerPid')
    if not detect_process(rest_pid):
        print_error('Experiment is not running...')
        return
    running, response = check_rest_server_quick(rest_port)
    if running:
        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)
        if response and check_response(response):
            content = json.loads(response.text)
            for trial in content:
                trial_id_path_dict[trial['id']] = trial['logPath']
    else:
        print_error('Restful server is not running...')
        exit(1)
    if args.id:
        if args.trial_id:
            if trial_id_path_dict.get(args.trial_id):
                print_normal('id:' + args.trial_id + ' path:' + trial_id_path_dict[args.trial_id])
            else:
                print_error('trial id is not valid!')
                exit(1)
        else:
            print_error('please specific the trial id!')
            exit(1)
    else:
        for key in trial_id_path_dict:
            print('id:' + key + ' path:' + trial_id_path_dict[key])"
forward_backward;"def forward_backward(self, x):
        """"""Perform forward and backward computation for a batch of src seq and dst seq""""""
        (src_seq, tgt_seq, src_valid_length, tgt_valid_length), batch_size = x
        with mx.autograd.record():
            out, _ = self._model(src_seq, tgt_seq[:, :-1],
                                 src_valid_length, tgt_valid_length - 1)
            smoothed_label = self._label_smoothing(tgt_seq[:, 1:])
            ls = self._loss(out, smoothed_label, tgt_valid_length - 1).sum()
            ls = (ls * (tgt_seq.shape[1] - 1)) / batch_size / self._rescale_loss
        ls.backward()
        return ls"
get_authenticated_user;"async def get_authenticated_user(
        self,
        redirect_uri: str,
        client_id: str,
        client_secret: str,
        code: str,
        extra_fields: Dict[str, Any] = None,
    ) -> Optional[Dict[str, Any]]:
        """"""Handles the login for the Facebook user, returning a user object.

        Example usage:

        .. testcode::

            class FacebookGraphLoginHandler(tornado.web.RequestHandler,
                                            tornado.auth.FacebookGraphMixin):
              async def get(self):
                  if self.get_argument(""code"", False):
                      user = await self.get_authenticated_user(
                          redirect_uri='/auth/facebookgraph/',
                          client_id=self.settings[""facebook_api_key""],
                          client_secret=self.settings[""facebook_secret""],
                          code=self.get_argument(""code""))
                      # Save the user with e.g. set_secure_cookie
                  else:
                      await self.authorize_redirect(
                          redirect_uri='/auth/facebookgraph/',
                          client_id=self.settings[""facebook_api_key""],
                          extra_params={""scope"": ""read_stream,offline_access""})

        .. testoutput::
           :hide:

        This method returns a dictionary which may contain the following fields:

        * ``access_token``, a string which may be passed to `facebook_request`
        * ``session_expires``, an integer encoded as a string representing
          the time until the access token expires in seconds. This field should
          be used like ``int(user['session_expires'])``; in a future version of
          Tornado it will change from a string to an integer.
        * ``id``, ``name``, ``first_name``, ``last_name``, ``locale``, ``picture``,
          ``link``, plus any fields named in the ``extra_fields`` argument. These
          fields are copied from the Facebook graph API
          `user object <https://developers.facebook.com/docs/graph-api/reference/user>`_

        .. versionchanged:: 4.5
           The ``session_expires`` field was updated to support changes made to the
           Facebook API in March 2017.

        .. versionchanged:: 6.0

           The ``callback`` argument was removed. Use the returned awaitable object instead.
        """"""
        http = self.get_auth_http_client()
        args = {
            ""redirect_uri"": redirect_uri,
            ""code"": code,
            ""client_id"": client_id,
            ""client_secret"": client_secret,
        }

        fields = set(
            [""id"", ""name"", ""first_name"", ""last_name"", ""locale"", ""picture"", ""link""]
        )
        if extra_fields:
            fields.update(extra_fields)

        response = await http.fetch(
            self._oauth_request_token_url(**args)  # type: ignore
        )
        args = escape.json_decode(response.body)
        session = {
            ""access_token"": args.get(""access_token""),
            ""expires_in"": args.get(""expires_in""),
        }
        assert session[""access_token""] is not None

        user = await self.facebook_request(
            path=""/me"",
            access_token=session[""access_token""],
            appsecret_proof=hmac.new(
                key=client_secret.encode(""utf8""),
                msg=session[""access_token""].encode(""utf8""),
                digestmod=hashlib.sha256,
            ).hexdigest(),
            fields="","".join(fields),
        )

        if user is None:
            return None

        fieldmap = {}
        for field in fields:
            fieldmap[field] = user.get(field)

        # session_expires is converted to str for compatibility with
        # older versions in which the server used url-encoding and
        # this code simply returned the string verbatim.
        # This should change in Tornado 5.0.
        fieldmap.update(
            {
                ""access_token"": session[""access_token""],
                ""session_expires"": str(session.get(""expires_in"")),
            }
        )
        return fieldmap"
zipline_magic;"def zipline_magic(line, cell=None):
    """"""The zipline IPython cell magic.
    """"""
    load_extensions(
        default=True,
        extensions=[],
        strict=True,
        environ=os.environ,
    )
    try:
        return run.main(
            # put our overrides at the start of the parameter list so that
            # users may pass values with higher precedence
            [
                '--algotext', cell,
                '--output', os.devnull,  # don't write the results by default
            ] + ([
                # these options are set when running in line magic mode
                # set a non None algo text to use the ipython user_ns
                '--algotext', '',
                '--local-namespace',
            ] if cell is None else []) + line.split(),
            '%s%%zipline' % ((cell or '') and '%'),
            # don't use system exit and propogate errors to the caller
            standalone_mode=False,
        )
    except SystemExit as e:
        # https://github.com/mitsuhiko/click/pull/533
        # even in standalone_mode=False `--help` really wants to kill us ;_;
        if e.code:
            raise ValueError('main returned non-zero status code: %d' % e.code)"
set_subsystem_handler;"def set_subsystem_handler(self, name, handler, *larg, **kwarg):
        """"""
        Set the handler class for a subsystem in server mode.  If a request
        for this subsystem is made on an open ssh channel later, this handler
        will be constructed and called -- see `.SubsystemHandler` for more
        detailed documentation.

        Any extra parameters (including keyword arguments) are saved and
        passed to the `.SubsystemHandler` constructor later.

        :param str name: name of the subsystem.
        :param handler:
            subclass of `.SubsystemHandler` that handles this subsystem.
        """"""
        try:
            self.lock.acquire()
            self.subsystem_table[name] = (handler, larg, kwarg)
        finally:
            self.lock.release()"
get_predictor;"def get_predictor(self, input_names, output_names, device=0):
        """"""
        This method will build the trainer's tower function under ``TowerContext(is_training=False)``,
        and returns a callable predictor with input placeholders & output tensors in this tower.

        This method handles the common case of inference with the same tower function.
        If you want to do inference with a different tower function, you can always build the tower by yourself,
        under a ""reuse"" variable scope and a `TowerContext(is_training=False)`.

        Args:
            input_names (list): list of input names, matching the inputs declared for the trainer.
            output_names(list): list of tensor names without the tower prefix.
            device (int): build the predictor on device '/gpu:{device}' or use -1 for '/cpu:0'.

        Returns:
            an :class:`OnlinePredictor`.

        Example:

        .. code-block:: none

            # in the graph:
            interesting_tensor = tf.identity(x, name='fun')
            # in _setup_graph callback method:
            self._predictor = self.trainer.get_predictor(['input1', 'input2'], ['fun'])
            # After session is initialized (see Tutorials - Write a Callback), can use it by:
            outputs = self._predictor(input1, input2)

        The CycleGAN example and DQN example have more concrete use of this method.
        """"""
        assert self.tower_func is not None, ""Must set tower_func on the trainer to use get_predictor()!""
        tower_name = 'tower-pred-{}'.format(device) if device >= 0 else 'tower-pred-cpu'
        device_id = device
        device = '/gpu:{}'.format(device_id) if device_id >= 0 else '/cpu:0'

        try:
            tower = self.tower_func.towers[tower_name]
            assert tower is not None, ""This is a bug!""
        except KeyError:
            tower = None

        if tower is None:
            input = PlaceholderInput()
            input.setup(self.input_signature)

            vs_name = self._vs_name_for_predictor(device_id)
            with tfv1.variable_scope(tfv1.get_variable_scope(), reuse=True), \
                    tf.device(device), PredictTowerContext(
                        tower_name, vs_name=vs_name):
                logger.info(""Building graph for predict tower '{}' on device {} {}..."".format(
                    tower_name, device,
                    ""with variable scope '{}'"".format(vs_name) if vs_name else ''))
                self.tower_func(*input.get_input_tensors())
            tower = self.tower_func.towers[tower_name]
        input_tensors = tower.get_tensors(input_names)
        output_tensors = tower.get_tensors(output_names)
        predictor = OnlinePredictor(input_tensors, output_tensors)
        self._predictors.append(predictor)
        return predictor"
get_blob;"def get_blob(
        self, blob_name, client=None, encryption_key=None, generation=None, **kwargs
    ):
        """"""Get a blob object by name.

        This will return None if the blob doesn't exist:

        .. literalinclude:: snippets.py
          :start-after: [START get_blob]
          :end-before: [END get_blob]

        If :attr:`user_project` is set, bills the API request to that project.

        :type blob_name: str
        :param blob_name: The name of the blob to retrieve.

        :type client: :class:`~google.cloud.storage.client.Client` or
                      ``NoneType``
        :param client: Optional. The client to use.  If not passed, falls back
                       to the ``client`` stored on the current bucket.

        :type encryption_key: bytes
        :param encryption_key:
            Optional 32 byte encryption key for customer-supplied encryption.
            See
            https://cloud.google.com/storage/docs/encryption#customer-supplied.

        :type generation: long
        :param generation: Optional. If present, selects a specific revision of
                           this object.

        :type kwargs: dict
        :param kwargs: Keyword arguments to pass to the
                       :class:`~google.cloud.storage.blob.Blob` constructor.

        :rtype: :class:`google.cloud.storage.blob.Blob` or None
        :returns: The blob object if it exists, otherwise None.
        """"""
        blob = Blob(
            bucket=self,
            name=blob_name,
            encryption_key=encryption_key,
            generation=generation,
            **kwargs
        )
        try:
            # NOTE: This will not fail immediately in a batch. However, when
            #       Batch.finish() is called, the resulting `NotFound` will be
            #       raised.
            blob.reload(client=client)
        except NotFound:
            return None
        else:
            return blob"
michalewicz_function;"def michalewicz_function(config, reporter):
    """"""f(x) = -sum{sin(xi) * [sin(i*xi^2 / pi)]^(2m)}""""""
    import numpy as np
    x = np.array(
        [config[""x1""], config[""x2""], config[""x3""], config[""x4""], config[""x5""]])
    sin_x = np.sin(x)
    z = (np.arange(1, 6) / np.pi * (x * x))
    sin_z = np.power(np.sin(z), 20)  # let m = 20
    y = np.dot(sin_x, sin_z)

    # Negate y since we want to minimize y value
    reporter(timesteps_total=1, neg_mean_loss=-y)"
server_enable;"def server_enable(s_name, **connection_args):
    '''
    Enables a server globally

    CLI Example:

    .. code-block:: bash

        salt '*' netscaler.server_enable 'serverName'
    '''
    ret = True
    server = _server_get(s_name, **connection_args)
    if server is None:
        return False
    if server.get_state() == 'ENABLED':
        return True
    nitro = _connect(**connection_args)
    if nitro is None:
        return False
    try:
        NSServer.enable(nitro, server)
    except NSNitroError as error:
        log.debug('netscaler module error - NSServer.enable() failed: %s', error)
        ret = False
    _disconnect(nitro)
    return ret"
update_domain_name;"def update_domain_name(self,
                           domain_name,
                           certificate_name=None,
                           certificate_body=None,
                           certificate_private_key=None,
                           certificate_chain=None,
                           certificate_arn=None,
                           lambda_name=None,
                           stage=None,
                           route53=True,
                           base_path=None):
        """"""
        This updates your certificate information for an existing domain,
        with similar arguments to boto's update_domain_name API Gateway api.

        It returns the resulting new domain information including the new certificate's ARN
        if created during this process.

        Previously, this method involved downtime that could take up to 40 minutes
        because the API Gateway api only allowed this by deleting, and then creating it.

        Related issues:     https://github.com/Miserlou/Zappa/issues/590
                            https://github.com/Miserlou/Zappa/issues/588
                            https://github.com/Miserlou/Zappa/pull/458
                            https://github.com/Miserlou/Zappa/issues/882
                            https://github.com/Miserlou/Zappa/pull/883
        """"""

        print(""Updating domain name!"")

        certificate_name = certificate_name + str(time.time())

        api_gateway_domain = self.apigateway_client.get_domain_name(domainName=domain_name)
        if not certificate_arn\
           and certificate_body and certificate_private_key and certificate_chain:
            acm_certificate = self.acm_client.import_certificate(Certificate=certificate_body,
                                                                 PrivateKey=certificate_private_key,
                                                                 CertificateChain=certificate_chain)
            certificate_arn = acm_certificate['CertificateArn']

        self.update_domain_base_path_mapping(domain_name, lambda_name, stage, base_path)

        return self.apigateway_client.update_domain_name(domainName=domain_name,
                                                         patchOperations=[
                                                             {""op"" : ""replace"",
                                                              ""path"" : ""/certificateName"",
                                                              ""value"" : certificate_name},
                                                             {""op"" : ""replace"",
                                                              ""path"" : ""/certificateArn"",
                                                              ""value"" : certificate_arn}
                                                         ])"
recv_exit_status;"def recv_exit_status(self):
        """"""
        Return the exit status from the process on the server.  This is
        mostly useful for retrieving the results of an `exec_command`.
        If the command hasn't finished yet, this method will wait until
        it does, or until the channel is closed.  If no exit status is
        provided by the server, -1 is returned.

        .. warning::
            In some situations, receiving remote output larger than the current
            `.Transport` or session's ``window_size`` (e.g. that set by the
            ``default_window_size`` kwarg for `.Transport.__init__`) will cause
            `.recv_exit_status` to hang indefinitely if it is called prior to a
            sufficiently large `.Channel.recv` (or if there are no threads
            calling `.Channel.recv` in the background).

            In these cases, ensuring that `.recv_exit_status` is called *after*
            `.Channel.recv` (or, again, using threads) can avoid the hang.

        :return: the exit code (as an `int`) of the process on the server.

        .. versionadded:: 1.2
        """"""
        self.status_event.wait()
        assert self.status_event.is_set()
        return self.exit_status"
import_keypair;"def import_keypair(kwargs=None, call=None):
    '''
    Upload public key to cloud provider.
    Similar to EC2 import_keypair.

    .. versionadded:: 2016.11.0

    kwargs
        file(mandatory): public key file-name
        keyname(mandatory): public key name in the provider
    '''
    with salt.utils.files.fopen(kwargs['file'], 'r') as public_key_filename:
        public_key_content = salt.utils.stringutils.to_unicode(public_key_filename.read())

    digitalocean_kwargs = {
        'name': kwargs['keyname'],
        'public_key': public_key_content
    }

    created_result = create_key(digitalocean_kwargs, call=call)
    return created_result"
swap_columns;"def swap_columns(self, column_name_1, column_name_2, inplace=False):
        """"""
        Returns an SFrame with two column positions swapped.

        If inplace == False (default) this operation does not modify the
        current SFrame, returning a new SFrame.

        If inplace == True, this operation modifies the current
        SFrame, returning self.

        Parameters
        ----------
        column_name_1 : string
            Name of column to swap

        column_name_2 : string
            Name of other column to swap

        inplace : bool, optional. Defaults to False.
            Whether the SFrame is modified in place.

        Returns
        -------
        out : SFrame
            The SFrame with swapped columns.

        Examples
        --------
        >>> sf = turicreate.SFrame({'id': [1, 2, 3], 'val': ['A', 'B', 'C']})
        >>> res = sf.swap_columns('id', 'val')
        >>> res
        +-----+-----+
        | val | id  |
        +-----+-----+
        |  A  |  1  |
        |  B  |  2  |
        |  C  |  3  |
        +----+-----+
        [3 rows x 2 columns]
        """"""
        colnames = self.column_names()
        colid_1 = colnames.index(column_name_1)
        colid_2 = colnames.index(column_name_2)

        if inplace:
            ret = self
        else:
            ret = self.copy()

        with cython_context():
            ret.__proxy__.swap_columns(colid_1, colid_2)

        ret._cache = None
        return ret"
query_all_issues;"def query_all_issues(after):
    """"""Hits the github API for all closed issues after the given date, returns the data.""""""
    page = count(1)
    data = []
    while True:
        page_data = query_issues(next(page), after)
        if not page_data:
            break
        data.extend(page_data)
    return data"
run_script;"def run_script(self, requires, script_name):
        """"""Locate distribution for `requires` and run `script_name` script""""""
        ns = sys._getframe(1).f_globals
        name = ns['__name__']
        ns.clear()
        ns['__name__'] = name
        self.require(requires)[0].run_script(script_name, ns)"
dump_graphviz;"def dump_graphviz(tree, output_format='dot'):
    """"""Output dependency graph as one of the supported GraphViz output formats.

    :param dict tree: dependency graph
    :param string output_format: output format
    :returns: representation of tree in the specified output format
    :rtype: str or binary representation depending on the output format

    """"""
    try:
        from graphviz import backend, Digraph
    except ImportError:
        print('graphviz is not available, but necessary for the output '
              'option. Please install it.', file=sys.stderr)
        sys.exit(1)

    if output_format not in backend.FORMATS:
        print('{0} is not a supported output format.'.format(output_format),
              file=sys.stderr)
        print('Supported formats are: {0}'.format(
            ', '.join(sorted(backend.FORMATS))), file=sys.stderr)
        sys.exit(1)

    graph = Digraph(format=output_format)
    for package, deps in tree.items():
        project_name = package.project_name
        label = '{0}\n{1}'.format(project_name, package.version)
        graph.node(project_name, label=label)
        for dep in deps:
            label = dep.version_spec
            if not label:
                label = 'any'
            graph.edge(project_name, dep.project_name, label=label)

    # Allow output of dot format, even if GraphViz isn't installed.
    if output_format == 'dot':
        return graph.source

    # As it's unknown if the selected output format is binary or not, try to
    # decode it as UTF8 and only print it out in binary if that's not possible.
    try:
        return graph.pipe().decode('utf-8')
    except UnicodeDecodeError:
        return graph.pipe()"
fingerprint_path;"def fingerprint_path(cls, user, fingerprint):
        """"""Return a fully-qualified fingerprint string.""""""
        return google.api_core.path_template.expand(
            ""users/{user}/sshPublicKeys/{fingerprint}"",
            user=user,
            fingerprint=fingerprint,
        )"
add_current_text;"def add_current_text(self):
        """"""
        Add current text to combo box history (convenient method).
        If path ends in os separator (""\"" windows, ""/"" unix) remove it.
        """"""
        text = self.currentText()
        if osp.isdir(text) and text:
            if text[-1] == os.sep:
                text = text[:-1]
        self.add_text(text)"
args_wrapper;"def args_wrapper(*args):
    """"""Generates callback arguments for model.fit()
    for a set of callback objects.
    Callback objects like PandasLogger(), LiveLearningCurve()
    get passed in.  This assembles all their callback arguments.
    """"""
    out = defaultdict(list)
    for callback in args:
        callback_args = callback.callback_args()
        for k, v in callback_args.items():
            out[k].append(v)
    return dict(out)"
delete_zone;"def delete_zone(zone, restart=True):
    '''
    Delete an existing zone

    CLI Example:

    .. code-block:: bash

        salt '*' firewalld.delete_zone my_zone

    By default firewalld will be reloaded. However, to avoid reloading
    you need to specify the restart as False

    .. code-block:: bash

        salt '*' firewalld.delete_zone my_zone False
    '''
    out = __mgmt(zone, 'zone', 'delete')

    if restart:

        if out == 'success':
            return __firewall_cmd('--reload')

    return out"
get_image_and_mask;"def get_image_and_mask(self, label, positive_only=True, hide_rest=False,
                           num_features=5, min_weight=0.):
        """"""Init function.

        Args:
            label: label to explain
            positive_only: if True, only take superpixels that contribute to
                the prediction of the label. Otherwise, use the top
                num_features superpixels, which can be positive or negative
                towards the label
            hide_rest: if True, make the non-explanation part of the return
                image gray
            num_features: number of superpixels to include in explanation
            min_weight: TODO

        Returns:
            (image, mask), where image is a 3d numpy array and mask is a 2d
            numpy array that can be used with
            skimage.segmentation.mark_boundaries
        """"""
        if label not in self.local_exp:
            raise KeyError('Label not in explanation')
        segments = self.segments
        image = self.image
        exp = self.local_exp[label]
        mask = np.zeros(segments.shape, segments.dtype)
        if hide_rest:
            temp = np.zeros(self.image.shape)
        else:
            temp = self.image.copy()
        if positive_only:
            fs = [x[0] for x in exp
                  if x[1] > 0 and x[1] > min_weight][:num_features]
            for f in fs:
                temp[segments == f] = image[segments == f].copy()
                mask[segments == f] = 1
            return temp, mask
        else:
            for f, w in exp[:num_features]:
                if np.abs(w) < min_weight:
                    continue
                c = 0 if w < 0 else 1
                mask[segments == f] = 1 if w < 0 else 2
                temp[segments == f] = image[segments == f].copy()
                temp[segments == f, c] = np.max(image)
                for cp in [0, 1, 2]:
                    if c == cp:
                        continue
                    # temp[segments == f, cp] *= 0.5
            return temp, mask"
set_maxdays;"def set_maxdays(name, days):
    '''
    Set the maximum age of the password in days

    :param str name: The username of the account

    :param int days: The maximum age of the account in days

    :return: True if successful, False if not
    :rtype: bool

    :raises: CommandExecutionError on user not found or any other unknown error

    CLI Example:

    .. code-block:: bash

        salt '*' shadow.set_maxdays admin 90
    '''
    minutes = days * 24 * 60

    _set_account_policy(
        name, 'maxMinutesUntilChangePassword={0}'.format(minutes))

    return get_maxdays(name) == days"
get_plugin_actions;"def get_plugin_actions(self):
        """"""Return a list of actions related to plugin""""""
        self.history_action = create_action(self, _(""History...""),
                                       None, ima.icon('history'),
                                       _(""Set history maximum entries""),
                                       triggered=self.change_history_depth)
        self.wrap_action = create_action(self, _(""Wrap lines""),
                                    toggled=self.toggle_wrap_mode)
        self.wrap_action.setChecked( self.get_option('wrap') )
        self.linenumbers_action = create_action(
                self, _(""Show line numbers""), toggled=self.toggle_line_numbers)
        self.linenumbers_action.setChecked(self.get_option('line_numbers'))

        menu_actions = [self.history_action, self.wrap_action,
                        self.linenumbers_action]
        return menu_actions"
terminate_all;"def terminate_all(self):
        """"""Terminate all worker processes.""""""
        for worker in self._workers:
            worker.terminate()

        # for thread in self._threads:
        #     try:
        #         thread.terminate()
        #         thread.wait()
        #     except Exception:
        #         pass
        self._queue_workers = deque()"
get_data;"def get_data(self, columns, type='ndarray', with_index=False):
        """"""获取不同格式的数据

        Arguments:
            columns {[type]} -- [description]

        Keyword Arguments:
            type {str} -- [description] (default: {'ndarray'})
            with_index {bool} -- [description] (default: {False})

        Returns:
            [type] -- [description]
        """"""

        res = self.select_columns(columns)
        if type == 'ndarray':
            if with_index:
                return res.reset_index().values
            else:
                return res.values
        elif type == 'list':
            if with_index:
                return res.reset_index().values.tolist()
            else:
                return res.values.tolist()
        elif type == 'dataframe':
            if with_index:
                return res.reset_index()
            else:
                return res"
numeric_function_clean_dataframe;"def numeric_function_clean_dataframe(self, axis):
        """"""Preprocesses numeric functions to clean dataframe and pick numeric indices.

        Args:
            axis: '0' if columns and '1' if rows.

        Returns:
            Tuple with return value(if any), indices to apply func to & cleaned Manager.
        """"""
        result = None
        query_compiler = self
        # If no numeric columns and over columns, then return empty Series
        if not axis and len(self.index) == 0:
            result = pandas.Series(dtype=np.int64)

        nonnumeric = [
            col
            for col, dtype in zip(self.columns, self.dtypes)
            if not is_numeric_dtype(dtype)
        ]
        if len(nonnumeric) == len(self.columns):
            # If over rows and no numeric columns, return this
            if axis:
                result = pandas.Series([np.nan for _ in self.index])
            else:
                result = pandas.Series([0 for _ in self.index])
        else:
            query_compiler = self.drop(columns=nonnumeric)
        return result, query_compiler"
prepend_path_variable_command;"def prepend_path_variable_command(variable, paths):
    """"""
        Returns a command that prepends the given paths to the named path variable on
        the current platform.
    """"""
    assert isinstance(variable, basestring)
    assert is_iterable_typed(paths, basestring)
    return path_variable_setting_command(
        variable, paths + [expand_variable(variable)])"
list_traces;"def list_traces(
        self,
        project_id,
        view=None,
        page_size=None,
        start_time=None,
        end_time=None,
        filter_=None,
        order_by=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Returns of a list of traces that match the specified filter conditions.

        Example:
            >>> from google.cloud import trace_v1
            >>>
            >>> client = trace_v1.TraceServiceClient()
            >>>
            >>> # TODO: Initialize `project_id`:
            >>> project_id = ''
            >>>
            >>> # Iterate over all results
            >>> for element in client.list_traces(project_id):
            ...     # process element
            ...     pass
            >>>
            >>>
            >>> # Alternatively:
            >>>
            >>> # Iterate over results one page at a time
            >>> for page in client.list_traces(project_id).pages:
            ...     for element in page:
            ...         # process element
            ...         pass

        Args:
            project_id (str): ID of the Cloud project where the trace data is stored.
            view (~google.cloud.trace_v1.types.ViewType): Type of data returned for traces in the list. Optional. Default is
                ``MINIMAL``.
            page_size (int): The maximum number of resources contained in the
                underlying API response. If page streaming is performed per-
                resource, this parameter does not affect the return value. If page
                streaming is performed per-page, this determines the maximum number
                of resources in a page.
            start_time (Union[dict, ~google.cloud.trace_v1.types.Timestamp]): Start of the time interval (inclusive) during which the trace data was
                collected from the application.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.trace_v1.types.Timestamp`
            end_time (Union[dict, ~google.cloud.trace_v1.types.Timestamp]): End of the time interval (inclusive) during which the trace data was
                collected from the application.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.trace_v1.types.Timestamp`
            filter_ (str): An optional filter against labels for the request.

                By default, searches use prefix matching. To specify exact match,
                prepend a plus symbol (``+``) to the search term. Multiple terms are
                ANDed. Syntax:

                -  ``root:NAME_PREFIX`` or ``NAME_PREFIX``: Return traces where any root
                   span starts with ``NAME_PREFIX``.
                -  ``+root:NAME`` or ``+NAME``: Return traces where any root span's name
                   is exactly ``NAME``.
                -  ``span:NAME_PREFIX``: Return traces where any span starts with
                   ``NAME_PREFIX``.
                -  ``+span:NAME``: Return traces where any span's name is exactly
                   ``NAME``.
                -  ``latency:DURATION``: Return traces whose overall latency is greater
                   or equal to than ``DURATION``. Accepted units are nanoseconds
                   (``ns``), milliseconds (``ms``), and seconds (``s``). Default is
                   ``ms``. For example, ``latency:24ms`` returns traces whose overall
                   latency is greater than or equal to 24 milliseconds.
                -  ``label:LABEL_KEY``: Return all traces containing the specified label
                   key (exact match, case-sensitive) regardless of the key:value pair's
                   value (including empty values).
                -  ``LABEL_KEY:VALUE_PREFIX``: Return all traces containing the
                   specified label key (exact match, case-sensitive) whose value starts
                   with ``VALUE_PREFIX``. Both a key and a value must be specified.
                -  ``+LABEL_KEY:VALUE``: Return all traces containing a key:value pair
                   exactly matching the specified text. Both a key and a value must be
                   specified.
                -  ``method:VALUE``: Equivalent to ``/http/method:VALUE``.
                -  ``url:VALUE``: Equivalent to ``/http/url:VALUE``.
            order_by (str): Field used to sort the returned traces. Optional. Can be one of the
                following:

                -  ``trace_id``
                -  ``name`` (``name`` field of root span in the trace)
                -  ``duration`` (difference between ``end_time`` and ``start_time``
                   fields of the root span)
                -  ``start`` (``start_time`` field of the root span)

                Descending order can be specified by appending ``desc`` to the sort
                field (for example, ``name desc``).

                Only one sort field is permitted.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.gax.PageIterator` instance. By default, this
            is an iterable of :class:`~google.cloud.trace_v1.types.Trace` instances.
            This object can also be configured to iterate over the pages
            of the response through the `options` parameter.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""list_traces"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""list_traces""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.list_traces,
                default_retry=self._method_configs[""ListTraces""].retry,
                default_timeout=self._method_configs[""ListTraces""].timeout,
                client_info=self._client_info,
            )

        request = trace_pb2.ListTracesRequest(
            project_id=project_id,
            view=view,
            page_size=page_size,
            start_time=start_time,
            end_time=end_time,
            filter=filter_,
            order_by=order_by,
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""project_id"", project_id)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        iterator = google.api_core.page_iterator.GRPCIterator(
            client=None,
            method=functools.partial(
                self._inner_api_calls[""list_traces""],
                retry=retry,
                timeout=timeout,
                metadata=metadata,
            ),
            request=request,
            items_field=""traces"",
            request_token_field=""page_token"",
            response_token_field=""next_page_token"",
        )
        return iterator"
setup_and_check;"def setup_and_check(self, data, title='', readonly=False,
                        xlabels=None, ylabels=None):
        """"""
        Setup ArrayEditor:
        return False if data is not supported, True otherwise
        """"""
        self.data = data
        readonly = readonly or not self.data.flags.writeable
        is_record_array = data.dtype.names is not None
        is_masked_array = isinstance(data, np.ma.MaskedArray)

        if data.ndim > 3:
            self.error(_(""Arrays with more than 3 dimensions are not ""
                         ""supported""))
            return False
        if xlabels is not None and len(xlabels) != self.data.shape[1]:
            self.error(_(""The 'xlabels' argument length do no match array ""
                         ""column number""))
            return False
        if ylabels is not None and len(ylabels) != self.data.shape[0]:
            self.error(_(""The 'ylabels' argument length do no match array row ""
                         ""number""))
            return False
        if not is_record_array:
            dtn = data.dtype.name
            if dtn not in SUPPORTED_FORMATS and not dtn.startswith('str') \
               and not dtn.startswith('unicode'):
                arr = _(""%s arrays"") % data.dtype.name
                self.error(_(""%s are currently not supported"") % arr)
                return False
        
        self.layout = QGridLayout()
        self.setLayout(self.layout)
        self.setWindowIcon(ima.icon('arredit'))
        if title:
            title = to_text_string(title) + "" - "" + _(""NumPy array"")
        else:
            title = _(""Array editor"")
        if readonly:
            title += ' (' + _('read only') + ')'
        self.setWindowTitle(title)
        self.resize(600, 500)
        
        # Stack widget
        self.stack = QStackedWidget(self)
        if is_record_array:
            for name in data.dtype.names:
                self.stack.addWidget(ArrayEditorWidget(self, data[name],
                                                   readonly, xlabels, ylabels))
        elif is_masked_array:
            self.stack.addWidget(ArrayEditorWidget(self, data, readonly,
                                                   xlabels, ylabels))
            self.stack.addWidget(ArrayEditorWidget(self, data.data, readonly,
                                                   xlabels, ylabels))
            self.stack.addWidget(ArrayEditorWidget(self, data.mask, readonly,
                                                   xlabels, ylabels))
        elif data.ndim == 3:
            pass
        else:
            self.stack.addWidget(ArrayEditorWidget(self, data, readonly,
                                                   xlabels, ylabels))
        self.arraywidget = self.stack.currentWidget()
        if self.arraywidget:
            self.arraywidget.model.dataChanged.connect(
                                                    self.save_and_close_enable)
        self.stack.currentChanged.connect(self.current_widget_changed)
        self.layout.addWidget(self.stack, 1, 0)

        # Buttons configuration
        btn_layout = QHBoxLayout()
        if is_record_array or is_masked_array or data.ndim == 3:
            if is_record_array:
                btn_layout.addWidget(QLabel(_(""Record array fields:"")))
                names = []
                for name in data.dtype.names:
                    field = data.dtype.fields[name]
                    text = name
                    if len(field) >= 3:
                        title = field[2]
                        if not is_text_string(title):
                            title = repr(title)
                        text += ' - '+title
                    names.append(text)
            else:
                names = [_('Masked data'), _('Data'), _('Mask')]
            if data.ndim == 3:
                # QSpinBox
                self.index_spin = QSpinBox(self, keyboardTracking=False)
                self.index_spin.valueChanged.connect(self.change_active_widget)
                # QComboBox
                names = [str(i) for i in range(3)]
                ra_combo = QComboBox(self)
                ra_combo.addItems(names)
                ra_combo.currentIndexChanged.connect(self.current_dim_changed)    
                # Adding the widgets to layout
                label = QLabel(_(""Axis:""))
                btn_layout.addWidget(label)
                btn_layout.addWidget(ra_combo)
                self.shape_label = QLabel()
                btn_layout.addWidget(self.shape_label)
                label = QLabel(_(""Index:""))
                btn_layout.addWidget(label)
                btn_layout.addWidget(self.index_spin)
                self.slicing_label = QLabel()
                btn_layout.addWidget(self.slicing_label)
                # set the widget to display when launched
                self.current_dim_changed(self.last_dim)
            else:
                ra_combo = QComboBox(self)
                ra_combo.currentIndexChanged.connect(self.stack.setCurrentIndex)
                ra_combo.addItems(names)
                btn_layout.addWidget(ra_combo)
            if is_masked_array:
                label = QLabel(_(""<u>Warning</u>: changes are applied separately""))
                label.setToolTip(_(""For performance reasons, changes applied ""\
                                   ""to masked array won't be reflected in ""\
                                   ""array's data (and vice-versa).""))
                btn_layout.addWidget(label)

        btn_layout.addStretch()

        if not readonly:
            self.btn_save_and_close = QPushButton(_('Save and Close'))
            self.btn_save_and_close.setDisabled(True)
            self.btn_save_and_close.clicked.connect(self.accept)
            btn_layout.addWidget(self.btn_save_and_close)

        self.btn_close = QPushButton(_('Close'))
        self.btn_close.setAutoDefault(True)
        self.btn_close.setDefault(True)
        self.btn_close.clicked.connect(self.reject)
        btn_layout.addWidget(self.btn_close)
        self.layout.addLayout(btn_layout, 2, 0)

        self.setMinimumSize(400, 300)
        
        # Make the dialog act as a window
        self.setWindowFlags(Qt.Window)
        
        return True"
get_eligible_features;"def get_eligible_features(examples, num_mutants):
  """"""Returns a list of JSON objects for each feature in the examples.

    This list is used to drive partial dependence plots in the plugin.

    Args:
      examples: Examples to examine to determine the eligible features.
      num_mutants: The number of mutations to make over each feature.

    Returns:
      A list with a JSON object for each feature.
      Numeric features are represented as {name: observedMin: observedMax:}.
      Categorical features are repesented as {name: samples:[]}.
    """"""
  features_dict = (
      get_numeric_features_to_observed_range(
          examples))

  features_dict.update(
      get_categorical_features_to_sampling(
          examples, num_mutants))

  # Massage the features_dict into a sorted list before returning because
  # Polymer dom-repeat needs a list.
  features_list = []
  for k, v in sorted(features_dict.items()):
    v['name'] = k
    features_list.append(v)
  return features_list"
setup_docstring_style_convention;"def setup_docstring_style_convention(self, text):
        """"""Handle convention changes.""""""
        if text == 'Custom':
            self.docstring_style_select.label.setText(
                _(""Show the following errors:""))
            self.docstring_style_ignore.label.setText(
                _(""Ignore the following errors:""))
        else:
            self.docstring_style_select.label.setText(
                _(""Show the following errors in addition ""
                  ""to the specified convention:""))
            self.docstring_style_ignore.label.setText(
                _(""Ignore the following errors in addition ""
                  ""to the specified convention:""))"
gen_send_stdout_url;"def gen_send_stdout_url(ip, port):
    '''Generate send stdout url'''
    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, STDOUT_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)"
find_template;"def find_template(repo_dir):
    """"""Determine which child directory of `repo_dir` is the project template.

    :param repo_dir: Local directory of newly cloned repo.
    :returns project_template: Relative path to project template.
    """"""
    logger.debug('Searching {} for the project template.'.format(repo_dir))

    repo_dir_contents = os.listdir(repo_dir)

    project_template = None
    for item in repo_dir_contents:
        if 'cookiecutter' in item and '{{' in item and '}}' in item:
            project_template = item
            break

    if project_template:
        project_template = os.path.join(repo_dir, project_template)
        logger.debug(
            'The project template appears to be {}'.format(project_template)
        )
        return project_template
    else:
        raise NonTemplatedInputDirException"
get_operation;"def get_operation(
        self, name, retry=gapic_v1.method.DEFAULT, timeout=gapic_v1.method.DEFAULT
    ):
        """"""Gets the latest state of a long-running operation.

        Clients can use this method to poll the operation result at intervals
        as recommended by the API service.

        Example:
            >>> from google.api_core import operations_v1
            >>> api = operations_v1.OperationsClient()
            >>> name = ''
            >>> response = api.get_operation(name)

        Args:
            name (str): The name of the operation resource.
            retry (google.api_core.retry.Retry): The retry strategy to use
                when invoking the RPC. If unspecified, the default retry from
                the client configuration will be used. If ``None``, then this
                method will not retry the RPC at all.
            timeout (float): The amount of time in seconds to wait for the RPC
                to complete. Note that if ``retry`` is used, this timeout
                applies to each individual attempt and the overall time it
                takes for this method to complete may be longer. If
                unspecified, the the default timeout in the client
                configuration is used. If ``None``, then the RPC method will
                not time out.

        Returns:
            google.longrunning.operations_pb2.Operation: The state of the
                operation.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If an error occurred
                while invoking the RPC, the appropriate ``GoogleAPICallError``
                subclass will be raised.
        """"""
        request = operations_pb2.GetOperationRequest(name=name)
        return self._get_operation(request, retry=retry, timeout=timeout)"
hold_available;"def hold_available(self):
        """"""可用持仓
        """"""
        return self.history_table.groupby('code').amount.sum().replace(
            0,
            np.nan
        ).dropna().sort_index()"
read_bytes;"def read_bytes(self, num_bytes: int, partial: bool = False) -> Awaitable[bytes]:
        """"""Asynchronously read a number of bytes.

        If ``partial`` is true, data is returned as soon as we have
        any bytes to return (but never more than ``num_bytes``)

        .. versionchanged:: 4.0
            Added the ``partial`` argument.  The callback argument is now
            optional and a `.Future` will be returned if it is omitted.

        .. versionchanged:: 6.0

           The ``callback`` and ``streaming_callback`` arguments have
           been removed. Use the returned `.Future` (and
           ``partial=True`` for ``streaming_callback``) instead.

        """"""
        future = self._start_read()
        assert isinstance(num_bytes, numbers.Integral)
        self._read_bytes = num_bytes
        self._read_partial = partial
        try:
            self._try_inline_read()
        except:
            future.add_done_callback(lambda f: f.exception())
            raise
        return future"
start_server;"def start_server(self):
        """"""Start pydoc server""""""
        if self.server is None:
            self.port = select_port(default_port=self.DEFAULT_PORT)
            self.set_home_url('http://localhost:%d/' % self.port)
        elif self.server.isRunning():
            self.server.server_started.disconnect(self.initialize_continued)
            self.server.quit()
        self.server = PydocServer(port=self.port)
        self.server.server_started.connect(self.initialize_continued)
        self.server.start()"
intersecting_ranges;"def intersecting_ranges(ranges):
    """"""Return any ranges that intersect.

    Parameters
    ----------
    ranges : iterable[ranges]
        A sequence of ranges to check for intersections.

    Returns
    -------
    intersections : iterable[ranges]
        A sequence of all of the ranges that intersected in ``ranges``.

    Examples
    --------
    >>> ranges = [range(0, 1), range(2, 5), range(4, 7)]
    >>> list(intersecting_ranges(ranges))
    [range(2, 5), range(4, 7)]

    >>> ranges = [range(0, 1), range(2, 3)]
    >>> list(intersecting_ranges(ranges))
    []

    >>> ranges = [range(0, 1), range(1, 2)]
    >>> list(intersecting_ranges(ranges))
    [range(0, 1), range(1, 2)]
    """"""
    ranges = sorted(ranges, key=op.attrgetter('start'))
    return sorted_diff(ranges, group_ranges(ranges))"
get_datafeeds;"def get_datafeeds(self, datafeed_id=None, params=None):
        """"""
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html>`_

        :arg datafeed_id: The ID of the datafeeds to fetch
        :arg allow_no_datafeeds: Whether to ignore if a wildcard expression
            matches no datafeeds. (This includes `_all` string or when no
            datafeeds have been specified)
        """"""
        return self.transport.perform_request(
            ""GET"", _make_path(""_ml"", ""datafeeds"", datafeed_id), params=params
        )"
copy_figure;"def copy_figure(self):
        """"""Copy figure from figviewer to clipboard.""""""
        if self.figviewer and self.figviewer.figcanvas.fig:
            self.figviewer.figcanvas.copy_figure()"
put_settings;"def put_settings(self, body=None, params=None):
        """"""
        Update cluster wide specific settings.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html>`_

        :arg body: The settings to be updated. Can be either `transient` or
            `persistent` (survives cluster restart).
        :arg flat_settings: Return settings in flat format (default: false)
        :arg master_timeout: Explicit operation timeout for connection to master
            node
        :arg timeout: Explicit operation timeout
        """"""
        return self.transport.perform_request('PUT', '/_cluster/settings',
            params=params, body=body)"
task_family;"def task_family(cls):
        """"""
        Internal note: This function will be deleted soon.
        """"""
        if not cls.get_task_namespace():
            return cls.__name__
        else:
            return ""{}.{}"".format(cls.get_task_namespace(), cls.__name__)"
grid_visual;"def grid_visual(data):
  """"""
  This function displays a grid of images to show full misclassification
  :param data: grid data of the form;
      [nb_classes : nb_classes : img_rows : img_cols : nb_channels]
  :return: if necessary, the matplot figure to reuse
  """"""
  import matplotlib.pyplot as plt

  # Ensure interactive mode is disabled and initialize our graph
  plt.ioff()
  figure = plt.figure()
  figure.canvas.set_window_title('Cleverhans: Grid Visualization')

  # Add the images to the plot
  num_cols = data.shape[0]
  num_rows = data.shape[1]
  num_channels = data.shape[4]
  for y in range(num_rows):
    for x in range(num_cols):
      figure.add_subplot(num_rows, num_cols, (x + 1) + (y * num_cols))
      plt.axis('off')

      if num_channels == 1:
        plt.imshow(data[x, y, :, :, 0], cmap='gray')
      else:
        plt.imshow(data[x, y, :, :, :])

  # Draw the plot and return
  plt.show()
  return figure"
iter_find_files;"def iter_find_files(directory, patterns, ignored=None):
    """"""Returns a generator that yields file paths under a *directory*,
    matching *patterns* using `glob`_ syntax (e.g., ``*.txt``). Also
    supports *ignored* patterns.

    Args:
        directory (str): Path that serves as the root of the
            search. Yielded paths will include this as a prefix.
        patterns (str or list): A single pattern or list of
            glob-formatted patterns to find under *directory*.
        ignored (str or list): A single pattern or list of
            glob-formatted patterns to ignore.

    For example, finding Python files in the directory of this module:

    >>> files = set(iter_find_files(os.path.dirname(__file__), '*.py'))

    Or, Python files while ignoring emacs lockfiles:

    >>> filenames = iter_find_files('.', '*.py', ignored='.#*')

    .. _glob: https://en.wikipedia.org/wiki/Glob_%28programming%29

    """"""
    if isinstance(patterns, basestring):
        patterns = [patterns]
    pats_re = re.compile('|'.join([fnmatch.translate(p) for p in patterns]))

    if not ignored:
        ignored = []
    elif isinstance(ignored, basestring):
        ignored = [ignored]
    ign_re = re.compile('|'.join([fnmatch.translate(p) for p in ignored]))
    for root, dirs, files in os.walk(directory):
        for basename in files:
            if pats_re.match(basename):
                if ignored and ign_re.match(basename):
                    continue
                filename = os.path.join(root, basename)
                yield filename
    return"
set_font;"def set_font(self, font):
        """"""Set shell styles font""""""
        self.setFont(font)
        self.set_pythonshell_font(font)
        cursor = self.textCursor()
        cursor.select(QTextCursor.Document)
        charformat = QTextCharFormat()
        charformat.setFontFamily(font.family())
        charformat.setFontPointSize(font.pointSize())
        cursor.mergeCharFormat(charformat)"
wait_for_compute_global_operation;"def wait_for_compute_global_operation(project_name, operation):
    """"""Poll for global compute operation until finished.""""""
    logger.info(""wait_for_compute_global_operation: ""
                ""Waiting for operation {} to finish..."".format(
                    operation[""name""]))

    for _ in range(MAX_POLLS):
        result = compute.globalOperations().get(
            project=project_name,
            operation=operation[""name""],
        ).execute()
        if ""error"" in result:
            raise Exception(result[""error""])

        if result[""status""] == ""DONE"":
            logger.info(""wait_for_compute_global_operation: ""
                        ""Operation done."")
            break

        time.sleep(POLL_INTERVAL)

    return result"
get_dummies;"def get_dummies(self, columns, **kwargs):
        """"""Convert categorical variables to dummy variables for certain columns.

        Args:
            columns: The columns to convert.

        Returns:
            A new QueryCompiler.
        """"""
        cls = type(self)
        # `columns` as None does not mean all columns, by default it means only
        # non-numeric columns.
        if columns is None:
            columns = [c for c in self.columns if not is_numeric_dtype(self.dtypes[c])]
            # If we aren't computing any dummies, there is no need for any
            # remote compute.
            if len(columns) == 0:
                return self.copy()
        elif not is_list_like(columns):
            columns = [columns]

        # We have to do one of two things in order to ensure the final columns
        # are correct. Our first option is to map over the data and assign the
        # columns in a separate pass. That is what we have chosen to do here.
        # This is not as efficient, but it requires less information from the
        # lower layers and does not break any of our internal requirements. The
        # second option is that we assign the columns as a part of the
        # `get_dummies` call. This requires knowledge of the length of each
        # partition, and breaks some of our assumptions and separation of
        # concerns.
        def set_columns(df, columns):
            df.columns = columns
            return df

        set_cols = self.columns
        columns_applied = self._map_across_full_axis(
            1, lambda df: set_columns(df, set_cols)
        )
        # In some cases, we are mapping across all of the data. It is more
        # efficient if we are mapping over all of the data to do it this way
        # than it would be to reuse the code for specific columns.
        if len(columns) == len(self.columns):

            def get_dummies_builder(df):
                if df is not None:
                    if not df.empty:
                        return pandas.get_dummies(df, **kwargs)
                    else:
                        return pandas.DataFrame([])

            func = self._prepare_method(lambda df: get_dummies_builder(df))
            new_data = columns_applied.map_across_full_axis(0, func)
            untouched_data = None
        else:

            def get_dummies_builder(df, internal_indices=[]):
                return pandas.get_dummies(
                    df.iloc[:, internal_indices], columns=None, **kwargs
                )

            numeric_indices = list(self.columns.get_indexer_for(columns))
            new_data = columns_applied.apply_func_to_select_indices_along_full_axis(
                0, get_dummies_builder, numeric_indices, keep_remaining=False
            )
            untouched_data = self.drop(columns=columns)
        # Since we set the columns in the beginning, we can just extract them
        # here. There is fortunately no required extra steps for a correct
        # column index.
        final_columns = self.compute_index(1, new_data, False)
        # If we mapped over all the data we are done. If not, we need to
        # prepend the `new_data` with the raw data from the columns that were
        # not selected.
        if len(columns) != len(self.columns):
            new_data = untouched_data.data.concat(1, new_data)
            final_columns = untouched_data.columns.append(pandas.Index(final_columns))
        return cls(new_data, self.index, final_columns)"
set_current_filename;"def set_current_filename(self, filename, focus=True):
        """"""Set current filename and return the associated editor instance.""""""
        index = self.has_filename(filename)
        if index is not None:
            if focus:
                self.set_stack_index(index)
            editor = self.data[index].editor
            if focus:
                editor.setFocus()
            else:
                self.stack_history.remove_and_append(index)

            return editor"
download_data_impl;"def download_data_impl(self, run, tag, response_format):
    """"""Provides a response for downloading scalars data for a data series.

    Args:
      run: The run.
      tag: The specific tag.
      response_format: A string. One of the values of the OutputFormat enum of
        the scalar plugin.

    Raises:
      ValueError: If the scalars plugin is not registered.

    Returns:
      2 entities:
        - A JSON object response body.
        - A mime type (string) for the response.
    """"""
    scalars_plugin_instance = self._get_scalars_plugin()
    if not scalars_plugin_instance:
      raise ValueError(('Failed to respond to request for /download_data. '
                        'The scalars plugin is oddly not registered.'))

    body, mime_type = scalars_plugin_instance.scalars_impl(
        tag, run, None, response_format)
    return body, mime_type"
delete_collection_namespaced_secret;"def delete_collection_namespaced_secret(self, namespace, **kwargs):
        """"""
        delete collection of Secret
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_collection_namespaced_secret(namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_collection_namespaced_secret_with_http_info(namespace, **kwargs)
        else:
            (data) = self.delete_collection_namespaced_secret_with_http_info(namespace, **kwargs)
            return data"
delete_cluster_role;"def delete_cluster_role(self, name, **kwargs):
        """"""
        delete a ClusterRole
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_cluster_role(name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ClusterRole (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param V1DeleteOptions body:
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \""orphan\"" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.
        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_cluster_role_with_http_info(name, **kwargs)
        else:
            (data) = self.delete_cluster_role_with_http_info(name, **kwargs)
            return data"
build_graph;"def build_graph(self):
        """"""Build a whole graph for the model.""""""
        self.global_step = tf.Variable(0, trainable=False)
        self._build_model()
        if self.mode == ""train"":
            self._build_train_op()
        else:
            # Additional initialization for the test network.
            self.variables = ray.experimental.tf_utils.TensorFlowVariables(
                self.cost)
            self.summaries = tf.summary.merge_all()"
register_blueprint;"def register_blueprint(self, *args, **kwargs):
        """"""
        Proxy method provided for invoking the :func:`blueprint` method

        .. note::
            To be deprecated in 1.0. Use :func:`blueprint` instead.

        :param args: Blueprint object or (list, tuple) thereof
        :param kwargs: option dictionary with blueprint defaults
        :return: None
        """"""

        if self.debug:
            warnings.simplefilter(""default"")
        warnings.warn(
            ""Use of register_blueprint will be deprecated in ""
            ""version 1.0.  Please use the blueprint method""
            "" instead"",
            DeprecationWarning,
        )
        return self.blueprint(*args, **kwargs)"
get_host;"def get_host(url):
    """"""
    Deprecated. Use :func:`parse_url` instead.
    """"""
    p = parse_url(url)
    return p.scheme or 'http', p.hostname, p.port"
organization_inspect_template_path;"def organization_inspect_template_path(cls, organization, inspect_template):
        """"""Return a fully-qualified organization_inspect_template string.""""""
        return google.api_core.path_template.expand(
            ""organizations/{organization}/inspectTemplates/{inspect_template}"",
            organization=organization,
            inspect_template=inspect_template,
        )"
get_spec;"def get_spec(self):
        """"""
        Return the Core ML spec
        """"""
        if _mac_ver() >= (10, 14):
            return self.vggish_model.get_spec()
        else:
            vggish_model_file = VGGish()
            coreml_model_path = vggish_model_file.get_model_path(format='coreml')
            return MLModel(coreml_model_path).get_spec()"
num_discarded;"def num_discarded(self):
    """"""Get the number of values discarded due to exceeding both limits.""""""
    if not self._data:
      return 0
    n = 0
    while n < len(self._data):
      if not isinstance(self._data[n], _TensorValueDiscarded):
        break
      n += 1
    return n"
set_computer_sleep;"def set_computer_sleep(minutes):
    '''
    Set the amount of idle time until the computer sleeps. Pass ""Never"" of ""Off""
    to never sleep.

    :param minutes: Can be an integer between 1 and 180 or ""Never"" or ""Off""
    :ptype: int, str

    :return: True if successful, False if not
    :rtype: bool

    CLI Example:

    .. code-block:: bash

        salt '*' power.set_computer_sleep 120
        salt '*' power.set_computer_sleep off
    '''
    value = _validate_sleep(minutes)
    cmd = 'systemsetup -setcomputersleep {0}'.format(value)
    salt.utils.mac_utils.execute_return_success(cmd)

    return salt.utils.mac_utils.confirm_updated(
        str(value),
        get_computer_sleep,
    )"
run_command;"def run_command(self, cmd, new_prompt=True):
        """"""Run command in interpreter""""""
        if cmd == 'exit()':
            self.exit_flag = True
            self.write('\n')
            return
        # -- Special commands type I
        #    (transformed into commands executed in the interpreter)
        # ? command
        special_pattern = r""^%s (?:r\')?(?:u\')?\""?\'?([a-zA-Z0-9_\.]+)""
        run_match = re.match(special_pattern % 'run', cmd)
        help_match = re.match(r'^([a-zA-Z0-9_\.]+)\?$', cmd)
        cd_match = re.match(r""^\!cd \""?\'?([a-zA-Z0-9_ \.]+)"", cmd)
        if help_match:
            cmd = 'help(%s)' % help_match.group(1)
        # run command
        elif run_match:
            filename = guess_filename(run_match.groups()[0])
            cmd = ""runfile('%s', args=None)"" % remove_backslashes(filename)
        # !cd system command
        elif cd_match:
            cmd = 'import os; os.chdir(r""%s"")' % cd_match.groups()[0].strip()
        # -- End of Special commands type I
            
        # -- Special commands type II
        #    (don't need code execution in interpreter)
        xedit_match = re.match(special_pattern % 'xedit', cmd)
        edit_match = re.match(special_pattern % 'edit', cmd)
        clear_match = re.match(r""^clear ([a-zA-Z0-9_, ]+)"", cmd)
        # (external) edit command
        if xedit_match:
            filename = guess_filename(xedit_match.groups()[0])
            self.widget_proxy.edit(filename, external_editor=True)
        # local edit command
        elif edit_match:
            filename = guess_filename(edit_match.groups()[0])
            if osp.isfile(filename):
                self.widget_proxy.edit(filename)
            else:
                self.stderr_write.write(
                                ""No such file or directory: %s\n"" % filename)
        # remove reference (equivalent to MATLAB's clear command)
        elif clear_match:
            varnames = clear_match.groups()[0].replace(' ', '').split(',')
            for varname in varnames:
                try:
                    self.namespace.pop(varname)
                except KeyError:
                    pass
        # Execute command
        elif cmd.startswith('!'):
            # System ! command
            pipe = programs.run_shell_command(cmd[1:])
            txt_out = encoding.transcode( pipe.stdout.read().decode() )
            txt_err = encoding.transcode( pipe.stderr.read().decode().rstrip() )
            if txt_err:
                self.stderr_write.write(txt_err)
            if txt_out:
                self.stdout_write.write(txt_out)
            self.stdout_write.write('\n')
            self.more = False
        # -- End of Special commands type II
        else:
            # Command executed in the interpreter
#            self.widget_proxy.set_readonly(True)
            self.more = self.push(cmd)
#            self.widget_proxy.set_readonly(False)
        
        if new_prompt:
            self.widget_proxy.new_prompt(self.p2 if self.more else self.p1)
        if not self.more:
            self.resetbuffer()"
backbone_scope;"def backbone_scope(freeze):
    """"""
    Args:
        freeze (bool): whether to freeze all the variables under the scope
    """"""
    def nonlin(x):
        x = get_norm()(x)
        return tf.nn.relu(x)

    with argscope([Conv2D, MaxPooling, BatchNorm], data_format='channels_first'), \
            argscope(Conv2D, use_bias=False, activation=nonlin,
                     kernel_initializer=tf.variance_scaling_initializer(
                         scale=2.0, mode='fan_out')), \
            ExitStack() as stack:
        if cfg.BACKBONE.NORM in ['FreezeBN', 'SyncBN']:
            if freeze or cfg.BACKBONE.NORM == 'FreezeBN':
                stack.enter_context(argscope(BatchNorm, training=False))
            else:
                stack.enter_context(argscope(
                    BatchNorm, sync_statistics='nccl' if cfg.TRAINER == 'replicated' else 'horovod'))

        if freeze:
            stack.enter_context(freeze_variables(stop_gradient=False, skip_collection=True))
        else:
            # the layers are not completely freezed, but we may want to only freeze the affine
            if cfg.BACKBONE.FREEZE_AFFINE:
                stack.enter_context(custom_getter_scope(freeze_affine_getter))
        yield"
extract_ranges;"def extract_ranges(index_list, range_size_limit=32):
    """"""Extract consecutive ranges and singles from index_list.

  Args:
    index_list: List of monotone increasing non-negative integers.
    range_size_limit: Largest size range to return.  If a larger
      consecutive range exists it will be returned as multiple
      ranges.

  Returns:
   ranges, singles where ranges is a list of [first, last] pairs of
     consecutive elements in index_list, and singles is all of the
     other elements, in original order.
  """"""
    if not index_list:
        return [], []
    first = index_list[0]
    last = first
    ranges = []
    singles = []
    for i in index_list[1:]:
        if i == last + 1 and (last - first) <= range_size_limit:
            last = i
        else:
            if last > first:
                ranges.append([first, last])
            else:
                singles.append(first)
            first = i
            last = i
    if last > first:
        ranges.append([first, last])
    else:
        singles.append(first)
    return ranges, singles"
visualize_instance_html;"def visualize_instance_html(self, exp, label, div_name, exp_object_name,
                                text=True, opacity=True):
        """"""Adds text with highlighted words to visualization.

        Args:
             exp: list of tuples [(id, weight), (id,weight)]
             label: label id (integer)
             div_name: name of div object to be used for rendering(in js)
             exp_object_name: name of js explanation object
             text: if False, return empty
             opacity: if True, fade colors according to weight
        """"""
        if not text:
            return u''
        text = (self.indexed_string.raw_string()
                .encode('utf-8', 'xmlcharrefreplace').decode('utf-8'))
        text = re.sub(r'[<>&]', '|', text)
        exp = [(self.indexed_string.word(x[0]),
                self.indexed_string.string_position(x[0]),
                x[1]) for x in exp]
        all_occurrences = list(itertools.chain.from_iterable(
            [itertools.product([x[0]], x[1], [x[2]]) for x in exp]))
        all_occurrences = [(x[0], int(x[1]), x[2]) for x in all_occurrences]
        ret = '''
            %s.show_raw_text(%s, %d, %s, %s, %s);
            ''' % (exp_object_name, json.dumps(all_occurrences), label,
                   json.dumps(text), div_name, json.dumps(opacity))
        return ret"
start_connect;"def start_connect(self):
        """""" Start the timeout clock, used during a connect() attempt

        :raises urllib3.exceptions.TimeoutStateError: if you attempt
            to start a timer that has been started already.
        """"""
        if self._start_connect is not None:
            raise TimeoutStateError(""Timeout timer has already been started."")
        self._start_connect = current_time()
        return self._start_connect"
monitor_experiment;"def monitor_experiment(args):
    '''monitor the experiment'''
    if args.time <= 0:
        print_error('please input a positive integer as time interval, the unit is second.')
        exit(1)
    while True:
        try:
            os.system('clear')
            update_experiment()
            show_experiment_info()
            time.sleep(args.time)
        except KeyboardInterrupt:
            exit(0)
        except Exception as exception:
            print_error(exception)
            exit(1)"
join_locale;"def join_locale(comps):
    '''
    Join a locale specifier split in the format returned by split_locale.
    '''
    loc = comps['language']
    if comps.get('territory'):
        loc += '_' + comps['territory']
    if comps.get('codeset'):
        loc += '.' + comps['codeset']
    if comps.get('modifier'):
        loc += '@' + comps['modifier']
    if comps.get('charmap'):
        loc += ' ' + comps['charmap']
    return loc"
create_nat_gateway;"def create_nat_gateway(subnet_id=None,
                       subnet_name=None, allocation_id=None,
                       region=None, key=None, keyid=None, profile=None):
    '''
    Create a NAT Gateway within an existing subnet. If allocation_id is
    specified, the elastic IP address it references is associated with the
    gateway. Otherwise, a new allocation_id is created and used.

    This function requires boto3 to be installed.

    Returns the nat gateway id if the nat gateway was created and
    returns False if the nat gateway was not created.

    .. versionadded:: 2016.11.0

    CLI Example:

    .. code-block:: bash

        salt myminion boto_vpc.create_nat_gateway subnet_name=mysubnet

    '''

    try:
        if all((subnet_id, subnet_name)):
            raise SaltInvocationError('Only one of subnet_name or subnet_id may be '
                                  'provided.')
        if subnet_name:
            subnet_id = _get_resource_id('subnet', subnet_name,
                                     region=region, key=key,
                                     keyid=keyid, profile=profile)
            if not subnet_id:
                return {'created': False,
                        'error': {'message': 'Subnet {0} does not exist.'.format(subnet_name)}}
        else:
            if not _get_resource('subnet', resource_id=subnet_id,
                                 region=region, key=key, keyid=keyid, profile=profile):
                return {'created': False,
                        'error': {'message': 'Subnet {0} does not exist.'.format(subnet_id)}}

        conn3 = _get_conn3(region=region, key=key, keyid=keyid, profile=profile)

        if not allocation_id:
            address = conn3.allocate_address(Domain='vpc')
            allocation_id = address.get('AllocationId')

        # Have to go to boto3 to create NAT gateway
        r = conn3.create_nat_gateway(SubnetId=subnet_id, AllocationId=allocation_id)
        return {'created': True, 'id': r.get('NatGateway', {}).get('NatGatewayId')}
    except BotoServerError as e:
        return {'created': False, 'error': __utils__['boto.get_error'](e)}"
get_platform;"def get_platform():
    # type: () -> str
    """"""Return our platform name 'win32', 'linux_x86_64'""""""
    if sys.platform == 'darwin':
        # distutils.util.get_platform() returns the release based on the value
        # of MACOSX_DEPLOYMENT_TARGET on which Python was built, which may
        # be significantly older than the user's current machine.
        release, _, machine = platform.mac_ver()
        split_ver = release.split('.')

        if machine == ""x86_64"" and _is_running_32bit():
            machine = ""i386""
        elif machine == ""ppc64"" and _is_running_32bit():
            machine = ""ppc""

        return 'macosx_{}_{}_{}'.format(split_ver[0], split_ver[1], machine)

    # XXX remove distutils dependency
    result = distutils.util.get_platform().replace('.', '_').replace('-', '_')
    if result == ""linux_x86_64"" and _is_running_32bit():
        # 32 bit Python program (running on a 64 bit Linux): pip should only
        # install and run 32 bit compiled extensions in that case.
        result = ""linux_i686""

    return result"
get_namespaces;"def get_namespaces(namespace="""", apiserver_url=None):
    '''
    .. versionadded:: 2016.3.0

    Get one or all kubernetes namespaces.

    If namespace parameter is omitted, all namespaces will be returned back to user, similar to following kubectl example:

    .. code-block:: bash

        kubectl get namespaces -o json

    In case namespace is set by user, the output will be similar to the one from kubectl:

    .. code-block:: bash

        kubectl get namespaces namespace_name -o json


    CLI Example:

    .. code-block:: bash

        salt '*' k8s.get_namespaces
        salt '*' k8s.get_namespaces namespace_name http://kube-master.cluster.local

    '''
    # Try to get kubernetes master
    apiserver_url = _guess_apiserver(apiserver_url)
    if apiserver_url is None:
        return False

    # Get data
    ret = _get_namespaces(apiserver_url, namespace)
    return ret"
raise_option_error;"def raise_option_error(parser, option, msg):
    """"""
    Raise an option parsing error using parser.error().

    Args:
      parser: an OptionParser instance.
      option: an Option instance.
      msg: the error text.
    """"""
    msg = '{} error: {}'.format(option, msg)
    msg = textwrap.fill(' '.join(msg.split()))
    parser.error(msg)"
read_into;"def read_into(self, buf: bytearray, partial: bool = False) -> Awaitable[int]:
        """"""Asynchronously read a number of bytes.

        ``buf`` must be a writable buffer into which data will be read.

        If ``partial`` is true, the callback is run as soon as any bytes
        have been read.  Otherwise, it is run when the ``buf`` has been
        entirely filled with read data.

        .. versionadded:: 5.0

        .. versionchanged:: 6.0

           The ``callback`` argument was removed. Use the returned
           `.Future` instead.

        """"""
        future = self._start_read()

        # First copy data already in read buffer
        available_bytes = self._read_buffer_size
        n = len(buf)
        if available_bytes >= n:
            end = self._read_buffer_pos + n
            buf[:] = memoryview(self._read_buffer)[self._read_buffer_pos : end]
            del self._read_buffer[:end]
            self._after_user_read_buffer = self._read_buffer
        elif available_bytes > 0:
            buf[:available_bytes] = memoryview(self._read_buffer)[
                self._read_buffer_pos :
            ]

        # Set up the supplied buffer as our temporary read buffer.
        # The original (if it had any data remaining) has been
        # saved for later.
        self._user_read_buffer = True
        self._read_buffer = buf
        self._read_buffer_pos = 0
        self._read_buffer_size = available_bytes
        self._read_bytes = n
        self._read_partial = partial

        try:
            self._try_inline_read()
        except:
            future.add_done_callback(lambda f: f.exception())
            raise
        return future"
refresh_grains;"def refresh_grains(**kwargs):
    '''
    .. versionadded:: 2016.3.6,2016.11.4,2017.7.0

    Refresh the minion's grains without syncing custom grains modules from
    ``salt://_grains``.

    .. note::
        The available execution modules will be reloaded as part of this
        proceess, as grains can affect which modules are available.

    refresh_pillar : True
        Set to ``False`` to keep pillar data from being refreshed.

    CLI Examples:

    .. code-block:: bash

        salt '*' saltutil.refresh_grains
    '''
    kwargs = salt.utils.args.clean_kwargs(**kwargs)
    _refresh_pillar = kwargs.pop('refresh_pillar', True)
    if kwargs:
        salt.utils.args.invalid_kwargs(kwargs)
    # Modules and pillar need to be refreshed in case grains changes affected
    # them, and the module refresh process reloads the grains and assigns the
    # newly-reloaded grains to each execution module's __grains__ dunder.
    refresh_modules()
    if _refresh_pillar:
        refresh_pillar()
    return True"
change_password;"def change_password(self, body, username=None, params=None):
        """"""
        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html>`_

        :arg body: the new password for the user
        :arg username: The username of the user to change the password for
        :arg refresh: If `true` (the default) then refresh the affected shards
            to make this operation visible to search, if `wait_for` then wait
            for a refresh to make this operation visible to search, if `false`
            then do nothing with refreshes., valid choices are: 'true', 'false',
            'wait_for'
        """"""
        if body in SKIP_IN_PATH:
            raise ValueError(""Empty value passed for a required argument 'body'."")
        return self.transport.perform_request(
            ""PUT"",
            _make_path(""_security"", ""user"", username, ""_password""),
            params=params,
            body=body,
        )"
zone_present;"def zone_present(domain, type, profile):
    '''
    Ensures a record is present.

    :param domain: Zone name, i.e. the domain name
    :type  domain: ``str``

    :param type: Zone type (master / slave), defaults to master
    :type  type: ``str``

    :param profile: The profile key
    :type  profile: ``str``
    '''
    zones = __salt__['libcloud_dns.list_zones'](profile)
    if not type:
        type = 'master'
    matching_zone = [z for z in zones if z['domain'] == domain]
    if matching_zone:
        return state_result(True, 'Zone already exists', domain)
    else:
        result = __salt__['libcloud_dns.create_zone'](domain, profile, type)
        return state_result(True, 'Created new zone', domain, result)"
convert_tree_ensemble;"def convert_tree_ensemble(model, feature_names, target, force_32bit_float):
    """"""Convert a generic tree model to the protobuf spec.

    This currently supports:
      * Decision tree regression

    Parameters
    ----------
    model: str | Booster
        Path on disk where the XGboost JSON representation of the model is or
        a handle to the XGboost model.

    feature_names : list of strings or None
        Names of each of the features. When set to None, the feature names are
        extracted from the model.

    target: str,
        Name of the output column.

    force_32bit_float: bool
        If True, then the resulting CoreML model will use 32 bit floats internally.

    Returns
    -------
    model_spec: An object of type Model_pb.
        Protobuf representation of the model
    """"""
    if not(_HAS_XGBOOST):
        raise RuntimeError('xgboost not found. xgboost conversion API is disabled.')

    import json
    import os
    feature_map = None
    if isinstance(model,  (_xgboost.core.Booster, _xgboost.XGBRegressor)):

        # Testing a few corner cases that we don't support
        if isinstance(model, _xgboost.XGBRegressor):
            try:
                objective = model.get_xgb_params()[""objective""]
            except:
                objective = None
            if objective in [""reg:gamma"", ""reg:tweedie""]:
                raise ValueError(""Regression objective '%s' not supported for export."" % objective)

        # Now use the booster API.
        if isinstance(model, _xgboost.XGBRegressor):
            # Name change in 0.7
            if hasattr(model, 'get_booster'):
                model = model.get_booster()
            else:
                model = model.booster()

        # Xgboost sometimes has feature names in there. Sometimes does not.
        if (feature_names is None) and (model.feature_names is None):
            raise ValueError(""Feature names not present in the model. Must be provided during conversion."")
            feature_names = model.feature_names
        if feature_names is None:
            feature_names = model.feature_names

        xgb_model_str = model.get_dump(with_stats=True, dump_format = 'json')

        if model.feature_names:
            feature_map = {f:i for i,f in enumerate(model.feature_names)}

    # Path on the file system where the XGboost model exists.
    elif isinstance(model, str):
        if not os.path.exists(model):
            raise TypeError(""Invalid path %s."" % model)
        with open(model) as f:
            xgb_model_str = json.load(f)
        feature_map = {f:i for i,f in enumerate(feature_names)}
    else:
        raise TypeError(""Unexpected type. Expecting XGBoost model."")

    mlkit_tree = _TreeEnsembleRegressor(feature_names, target)
    mlkit_tree.set_default_prediction_value(0.5)
    for xgb_tree_id, xgb_tree_str in enumerate(xgb_model_str):
        xgb_tree_json = json.loads(xgb_tree_str)
        recurse_json(mlkit_tree, xgb_tree_json, xgb_tree_id, node_id = 0,
                feature_map = feature_map, force_32bit_float = force_32bit_float)

    return mlkit_tree.spec"
read_configuration;"def read_configuration(self):
        """"""
        Read the PyPI access configuration as supported by distutils, getting
        PyPI to do the actual work. This populates ``username``, ``password``,
        ``realm`` and ``url`` attributes from the configuration.
        """"""
        # get distutils to do the work
        c = self._get_pypirc_command()
        c.repository = self.url
        cfg = c._read_pypirc()
        self.username = cfg.get('username')
        self.password = cfg.get('password')
        self.realm = cfg.get('realm', 'pypi')
        self.url = cfg.get('repository', self.url)"
rpn_losses;"def rpn_losses(anchor_labels, anchor_boxes, label_logits, box_logits):
    """"""
    Args:
        anchor_labels: fHxfWxNA
        anchor_boxes: fHxfWxNAx4, encoded
        label_logits:  fHxfWxNA
        box_logits: fHxfWxNAx4

    Returns:
        label_loss, box_loss
    """"""
    with tf.device('/cpu:0'):
        valid_mask = tf.stop_gradient(tf.not_equal(anchor_labels, -1))
        pos_mask = tf.stop_gradient(tf.equal(anchor_labels, 1))
        nr_valid = tf.stop_gradient(tf.count_nonzero(valid_mask, dtype=tf.int32), name='num_valid_anchor')
        nr_pos = tf.identity(tf.count_nonzero(pos_mask, dtype=tf.int32), name='num_pos_anchor')
        # nr_pos is guaranteed >0 in C4. But in FPN. even nr_valid could be 0.

        valid_anchor_labels = tf.boolean_mask(anchor_labels, valid_mask)
    valid_label_logits = tf.boolean_mask(label_logits, valid_mask)

    with tf.name_scope('label_metrics'):
        valid_label_prob = tf.nn.sigmoid(valid_label_logits)
        summaries = []
        with tf.device('/cpu:0'):
            for th in [0.5, 0.2, 0.1]:
                valid_prediction = tf.cast(valid_label_prob > th, tf.int32)
                nr_pos_prediction = tf.reduce_sum(valid_prediction, name='num_pos_prediction')
                pos_prediction_corr = tf.count_nonzero(
                    tf.logical_and(
                        valid_label_prob > th,
                        tf.equal(valid_prediction, valid_anchor_labels)),
                    dtype=tf.int32)
                placeholder = 0.5   # A small value will make summaries appear lower.
                recall = tf.cast(tf.truediv(pos_prediction_corr, nr_pos), tf.float32)
                recall = tf.where(tf.equal(nr_pos, 0), placeholder, recall, name='recall_th{}'.format(th))
                precision = tf.cast(tf.truediv(pos_prediction_corr, nr_pos_prediction), tf.float32)
                precision = tf.where(tf.equal(nr_pos_prediction, 0),
                                     placeholder, precision, name='precision_th{}'.format(th))
                summaries.extend([precision, recall])
        add_moving_summary(*summaries)

    # Per-level loss summaries in FPN may appear lower due to the use of a small placeholder.
    # But the total RPN loss will be fine.  TODO make the summary op smarter
    placeholder = 0.
    label_loss = tf.nn.sigmoid_cross_entropy_with_logits(
        labels=tf.cast(valid_anchor_labels, tf.float32), logits=valid_label_logits)
    label_loss = tf.reduce_sum(label_loss) * (1. / cfg.RPN.BATCH_PER_IM)
    label_loss = tf.where(tf.equal(nr_valid, 0), placeholder, label_loss, name='label_loss')

    pos_anchor_boxes = tf.boolean_mask(anchor_boxes, pos_mask)
    pos_box_logits = tf.boolean_mask(box_logits, pos_mask)
    delta = 1.0 / 9
    box_loss = tf.losses.huber_loss(
        pos_anchor_boxes, pos_box_logits, delta=delta,
        reduction=tf.losses.Reduction.SUM) / delta
    box_loss = box_loss * (1. / cfg.RPN.BATCH_PER_IM)
    box_loss = tf.where(tf.equal(nr_pos, 0), placeholder, box_loss, name='box_loss')

    add_moving_summary(label_loss, box_loss, nr_valid, nr_pos)
    return [label_loss, box_loss]"
decode_example;"def decode_example(self, example):
    """"""Reconstruct the image from the tf example.""""""
    img = tf.image.decode_image(
        example, channels=self._shape[-1], dtype=tf.uint8)
    img.set_shape(self._shape)
    return img"
reset_stats;"def reset_stats(self):
        """"""
        Returns:
            mean, max: two stats of the runners, to be added to backend
        """"""
        scores = list(itertools.chain.from_iterable([v.total_scores for v in self._runners]))
        for v in self._runners:
            v.total_scores.clear()

        try:
            return np.mean(scores), np.max(scores)
        except Exception:
            logger.exception(""Cannot compute total scores in EnvRunner."")
            return None, None"
find_requirement;"def find_requirement(self, req, upgrade, ignore_compatibility=False):
        # type: (InstallRequirement, bool, bool) -> Optional[Link]
        """"""Try to find a Link matching req

        Expects req, an InstallRequirement and upgrade, a boolean
        Returns a Link if found,
        Raises DistributionNotFound or BestVersionAlreadyInstalled otherwise
        """"""
        all_candidates = self.find_all_candidates(req.name)

        # Filter out anything which doesn't match our specifier
        compatible_versions = set(
            req.specifier.filter(
                # We turn the version object into a str here because otherwise
                # when we're debundled but setuptools isn't, Python will see
                # packaging.version.Version and
                # pkg_resources._vendor.packaging.version.Version as different
                # types. This way we'll use a str as a common data interchange
                # format. If we stop using the pkg_resources provided specifier
                # and start using our own, we can drop the cast to str().
                [str(c.version) for c in all_candidates],
                prereleases=(
                    self.allow_all_prereleases
                    if self.allow_all_prereleases else None
                ),
            )
        )
        applicable_candidates = [
            # Again, converting to str to deal with debundling.
            c for c in all_candidates if str(c.version) in compatible_versions
        ]

        if applicable_candidates:
            best_candidate = max(applicable_candidates,
                                 key=self._candidate_sort_key)
        else:
            best_candidate = None

        if req.satisfied_by is not None:
            installed_version = parse_version(req.satisfied_by.version)
        else:
            installed_version = None

        if installed_version is None and best_candidate is None:
            logger.critical(
                'Could not find a version that satisfies the requirement %s '
                '(from versions: %s)',
                req,
                ', '.join(
                    sorted(
                        {str(c.version) for c in all_candidates},
                        key=parse_version,
                    )
                )
            )

            raise DistributionNotFound(
                'No matching distribution found for %s' % req
            )

        best_installed = False
        if installed_version and (
                best_candidate is None or
                best_candidate.version <= installed_version):
            best_installed = True

        if not upgrade and installed_version is not None:
            if best_installed:
                logger.debug(
                    'Existing installed version (%s) is most up-to-date and '
                    'satisfies requirement',
                    installed_version,
                )
            else:
                logger.debug(
                    'Existing installed version (%s) satisfies requirement '
                    '(most up-to-date version is %s)',
                    installed_version,
                    best_candidate.version,
                )
            return None

        if best_installed:
            # We have an existing version, and its the best version
            logger.debug(
                'Installed version (%s) is most up-to-date (past versions: '
                '%s)',
                installed_version,
                ', '.join(sorted(compatible_versions, key=parse_version)) or
                ""none"",
            )
            raise BestVersionAlreadyInstalled

        logger.debug(
            'Using version %s (newest of versions: %s)',
            best_candidate.version,
            ', '.join(sorted(compatible_versions, key=parse_version))
        )
        return best_candidate.location"
permissions_for;"def permissions_for(self, member):
        """"""Handles permission resolution for the current :class:`Member`.

        This function takes into consideration the following cases:

        - Guild owner
        - Guild roles
        - Channel overrides
        - Member overrides

        Parameters
        ----------
        member: :class:`Member`
            The member to resolve permissions for.

        Returns
        -------
        :class:`Permissions`
            The resolved permissions for the member.
        """"""

        # The current cases can be explained as:
        # Guild owner get all permissions -- no questions asked. Otherwise...
        # The @everyone role gets the first application.
        # After that, the applied roles that the user has in the channel
        # (or otherwise) are then OR'd together.
        # After the role permissions are resolved, the member permissions
        # have to take into effect.
        # After all that is done.. you have to do the following:

        # If manage permissions is True, then all permissions are set to True.

        # The operation first takes into consideration the denied
        # and then the allowed.

        o = self.guild.owner
        if o is not None and member.id == o.id:
            return Permissions.all()

        default = self.guild.default_role
        base = Permissions(default.permissions.value)
        roles = member.roles

        # Apply guild roles that the member has.
        for role in roles:
            base.value |= role.permissions.value

        # Guild-wide Administrator -> True for everything
        # Bypass all channel-specific overrides
        if base.administrator:
            return Permissions.all()

        # Apply @everyone allow/deny first since it's special
        try:
            maybe_everyone = self._overwrites[0]
            if maybe_everyone.id == self.guild.id:
                base.handle_overwrite(allow=maybe_everyone.allow, deny=maybe_everyone.deny)
                remaining_overwrites = self._overwrites[1:]
            else:
                remaining_overwrites = self._overwrites
        except IndexError:
            remaining_overwrites = self._overwrites

        # not sure if doing member._roles.get(...) is better than the
        # set approach. While this is O(N) to re-create into a set for O(1)
        # the direct approach would just be O(log n) for searching with no
        # extra memory overhead. For now, I'll keep the set cast
        # Note that the member.roles accessor up top also creates a
        # temporary list
        member_role_ids = {r.id for r in roles}
        denies = 0
        allows = 0

        # Apply channel specific role permission overwrites
        for overwrite in remaining_overwrites:
            if overwrite.type == 'role' and overwrite.id in member_role_ids:
                denies |= overwrite.deny
                allows |= overwrite.allow

        base.handle_overwrite(allow=allows, deny=denies)

        # Apply member specific permission overwrites
        for overwrite in remaining_overwrites:
            if overwrite.type == 'member' and overwrite.id == member.id:
                base.handle_overwrite(allow=overwrite.allow, deny=overwrite.deny)
                break

        # if you can't send a message in a channel then you can't have certain
        # permissions as well
        if not base.send_messages:
            base.send_tts_messages = False
            base.mention_everyone = False
            base.embed_links = False
            base.attach_files = False

        # if you can't read a channel then you have no permissions there
        if not base.read_messages:
            denied = Permissions.all_channel()
            base.value &= ~denied.value

        return base"
datasets_list_files;"def datasets_list_files(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501
        """"""List dataset files  # noqa: E501

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.datasets_list_files(owner_slug, dataset_slug, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str owner_slug: Dataset owner (required)
        :param str dataset_slug: Dataset name (required)
        :return: Result
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501
        else:
            (data) = self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501
            return data"
data_mnist;"def data_mnist(datadir=tempfile.gettempdir(), train_start=0,
               train_end=60000, test_start=0, test_end=10000):
  """"""
  Load and preprocess MNIST dataset
  :param datadir: path to folder where data should be stored
  :param train_start: index of first training set example
  :param train_end: index of last training set example
  :param test_start: index of first test set example
  :param test_end: index of last test set example
  :return: tuple of four arrays containing training data, training labels,
           testing data and testing labels.
  """"""
  assert isinstance(train_start, int)
  assert isinstance(train_end, int)
  assert isinstance(test_start, int)
  assert isinstance(test_end, int)

  X_train = download_and_parse_mnist_file(
      'train-images-idx3-ubyte.gz', datadir=datadir) / 255.
  Y_train = download_and_parse_mnist_file(
      'train-labels-idx1-ubyte.gz', datadir=datadir)
  X_test = download_and_parse_mnist_file(
      't10k-images-idx3-ubyte.gz', datadir=datadir) / 255.
  Y_test = download_and_parse_mnist_file(
      't10k-labels-idx1-ubyte.gz', datadir=datadir)

  X_train = np.expand_dims(X_train, -1)
  X_test = np.expand_dims(X_test, -1)

  X_train = X_train[train_start:train_end]
  Y_train = Y_train[train_start:train_end]
  X_test = X_test[test_start:test_end]
  Y_test = Y_test[test_start:test_end]

  Y_train = utils.to_categorical(Y_train, nb_classes=10)
  Y_test = utils.to_categorical(Y_test, nb_classes=10)
  return X_train, Y_train, X_test, Y_test"
embedding_plot;"def embedding_plot(ind, shap_values, feature_names=None, method=""pca"", alpha=1.0, show=True):
    """""" Use the SHAP values as an embedding which we project to 2D for visualization.

    Parameters
    ----------
    ind : int or string
        If this is an int it is the index of the feature to use to color the embedding.
        If this is a string it is either the name of the feature, or it can have the
        form ""rank(int)"" to specify the feature with that rank (ordered by mean absolute
        SHAP value over all the samples), or ""sum()"" to mean the sum of all the SHAP values,
        which is the model's output (minus it's expected value).

    shap_values : numpy.array
        Matrix of SHAP values (# samples x # features).

    feature_names : None or list
        The names of the features in the shap_values array.

    method : ""pca"" or numpy.array
        How to reduce the dimensions of the shap_values to 2D. If ""pca"" then the 2D
        PCA projection of shap_values is used. If a numpy array then is should be
        (# samples x 2) and represent the embedding of that values. 

    alpha : float
        The transparency of the data points (between 0 and 1). This can be useful to the
        show density of the data points when using a large dataset.
    """"""
    
    if feature_names is None:
        feature_names = [labels['FEATURE'] % str(i) for i in range(shap_values.shape[1])]
    
    ind = convert_name(ind, shap_values, feature_names)
    if ind == ""sum()"":
        cvals = shap_values.sum(1)
        fname = ""sum(SHAP values)""
    else:
        cvals = shap_values[:,ind]
        fname = feature_names[ind]
    
    # see if we need to compute the embedding
    if type(method) == str and method == ""pca"":
        pca = sklearn.decomposition.PCA(2)
        embedding_values = pca.fit_transform(shap_values)
    elif hasattr(method, ""shape"") and method.shape[1] == 2:
        embedding_values = method
    else:
        print(""Unsupported embedding method:"", method)

    pl.scatter(
        embedding_values[:,0], embedding_values[:,1], c=cvals,
        cmap=colors.red_blue, alpha=alpha, linewidth=0
    )
    pl.axis(""off"")
    #pl.title(feature_names[ind])
    cb = pl.colorbar()
    cb.set_label(""SHAP value for\n""+fname, size=13)
    cb.outline.set_visible(False)
    
    
    pl.gcf().set_size_inches(7.5, 5)
    bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())
    cb.ax.set_aspect((bbox.height - 0.7) * 10)
    cb.set_alpha(1)
    if show:
        pl.show()"
normalize_weight;"def normalize_weight(self, samples):
        """"""normalize weight
        
        Parameters
        ----------
        samples: list
            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,
            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}

        Returns
        -------
        list
            samples after normalize weight
        """"""
        for i in range(NUM_OF_INSTANCE):
            total = 0
            for j in range(self.effective_model_num):
                total += samples[i][j]
            for j in range(self.effective_model_num):
                samples[i][j] /= total
        return samples"
uninstall_ruby;"def uninstall_ruby(ruby, runas=None):
    '''
    Uninstall a ruby implementation.

    ruby
        The version of ruby to uninstall. Should match one of the versions
        listed by :py:func:`rbenv.versions <salt.modules.rbenv.versions>`.

    runas
        The user under which to run rbenv. If not specified, then rbenv will be
        run as the user under which Salt is running.

    CLI Example:

    .. code-block:: bash

        salt '*' rbenv.uninstall_ruby 2.0.0-p0
    '''
    ruby = re.sub(r'^ruby-', '', ruby)
    _rbenv_exec(['uninstall', '--force', ruby], runas=runas)
    return True"
load_pyproject_toml;"def load_pyproject_toml(self):
        # type: () -> None
        """"""Load the pyproject.toml file.

        After calling this routine, all of the attributes related to PEP 517
        processing for this requirement have been set. In particular, the
        use_pep517 attribute can be used to determine whether we should
        follow the PEP 517 or legacy (setup.py) code path.
        """"""
        pep517_data = load_pyproject_toml(
            self.use_pep517,
            self.pyproject_toml,
            self.setup_py,
            str(self)
        )

        if pep517_data is None:
            self.use_pep517 = False
        else:
            self.use_pep517 = True
            requires, backend, check = pep517_data
            self.requirements_to_check = check
            self.pyproject_requires = requires
            self.pep517_backend = Pep517HookCaller(self.setup_py_dir, backend)

            # Use a custom function to call subprocesses
            self.spin_message = """"

            def runner(cmd, cwd=None, extra_environ=None):
                with open_spinner(self.spin_message) as spinner:
                    call_subprocess(
                        cmd,
                        cwd=cwd,
                        extra_environ=extra_environ,
                        show_stdout=False,
                        spinner=spinner
                    )
                self.spin_message = """"

            self.pep517_backend._subprocess_runner = runner"
import_product_sets;"def import_product_sets(
        self,
        parent,
        input_config,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Asynchronous API that imports a list of reference images to specified
        product sets based on a list of image information.

        The ``google.longrunning.Operation`` API can be used to keep track of
        the progress and results of the request. ``Operation.metadata`` contains
        ``BatchOperationMetadata``. (progress) ``Operation.response`` contains
        ``ImportProductSetsResponse``. (results)

        The input source of this method is a csv file on Google Cloud Storage.
        For the format of the csv file please see
        ``ImportProductSetsGcsSource.csv_file_uri``.

        Example:
            >>> from google.cloud import vision_v1p3beta1
            >>>
            >>> client = vision_v1p3beta1.ProductSearchClient()
            >>>
            >>> parent = client.location_path('[PROJECT]', '[LOCATION]')
            >>>
            >>> # TODO: Initialize `input_config`:
            >>> input_config = {}
            >>>
            >>> response = client.import_product_sets(parent, input_config)
            >>>
            >>> def callback(operation_future):
            ...     # Handle result.
            ...     result = operation_future.result()
            >>>
            >>> response.add_done_callback(callback)
            >>>
            >>> # Handle metadata.
            >>> metadata = response.metadata()

        Args:
            parent (str): The project in which the ProductSets should be imported.

                Format is ``projects/PROJECT_ID/locations/LOC_ID``.
            input_config (Union[dict, ~google.cloud.vision_v1p3beta1.types.ImportProductSetsInputConfig]): The input content for the list of requests.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.vision_v1p3beta1.types.ImportProductSetsInputConfig`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.vision_v1p3beta1.types._OperationFuture` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""import_product_sets"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""import_product_sets""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.import_product_sets,
                default_retry=self._method_configs[""ImportProductSets""].retry,
                default_timeout=self._method_configs[""ImportProductSets""].timeout,
                client_info=self._client_info,
            )

        request = product_search_service_pb2.ImportProductSetsRequest(
            parent=parent, input_config=input_config
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        operation = self._inner_api_calls[""import_product_sets""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )
        return google.api_core.operation.from_gapic(
            operation,
            self.transport._operations_client,
            product_search_service_pb2.ImportProductSetsResponse,
            metadata_type=product_search_service_pb2.BatchOperationMetadata,
        )"
get_roles;"def get_roles(username):
    '''
    Get roles that the username is assigned from switch

    .. code-block: bash

        salt '*' onyx.cmd get_roles username=admin
    '''
    info = sendline('show user-account {0}'.format(username))
    roles = re.search(r'^\s*roles:(.*)$', info, re.MULTILINE)
    if roles:
        roles = roles.group(1).strip().split(' ')
    else:
        roles = []
    return roles"
get_body_arguments;"def get_body_arguments(self, name: str, strip: bool = True) -> List[str]:
        """"""Returns a list of the body arguments with the given name.

        If the argument is not present, returns an empty list.

        .. versionadded:: 3.2
        """"""
        return self._get_arguments(name, self.request.body_arguments, strip)"
flatten_binary_scores;"def flatten_binary_scores(scores, labels, ignore=None):
    """"""
    Flattens predictions in the batch (binary case)
    Remove labels equal to 'ignore'
    """"""
    scores = scores.view(-1)
    labels = labels.view(-1)
    if ignore is None:
        return scores, labels
    valid = (labels != ignore)
    vscores = scores[valid]
    vlabels = labels[valid]
    return vscores, vlabels"
create_tenant;"def create_tenant(
        self,
        parent,
        tenant,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates a new tenant entity.

        Example:
            >>> from google.cloud import talent_v4beta1
            >>>
            >>> client = talent_v4beta1.TenantServiceClient()
            >>>
            >>> parent = client.project_path('[PROJECT]')
            >>>
            >>> # TODO: Initialize `tenant`:
            >>> tenant = {}
            >>>
            >>> response = client.create_tenant(parent, tenant)

        Args:
            parent (str): Required.

                Resource name of the project under which the tenant is created.

                The format is ""projects/{project\_id}"", for example,
                ""projects/api-test-project"".
            tenant (Union[dict, ~google.cloud.talent_v4beta1.types.Tenant]): Required.

                The tenant to be created.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.talent_v4beta1.types.Tenant`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.talent_v4beta1.types.Tenant` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_tenant"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_tenant""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_tenant,
                default_retry=self._method_configs[""CreateTenant""].retry,
                default_timeout=self._method_configs[""CreateTenant""].timeout,
                client_info=self._client_info,
            )

        request = tenant_service_pb2.CreateTenantRequest(parent=parent, tenant=tenant)
        return self._inner_api_calls[""create_tenant""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
openssh_tunnel;"def openssh_tunnel(self, lport, rport, server, remoteip='127.0.0.1',
                   keyfile=None, password=None, timeout=0.4):
    """"""
    We decided to replace pyzmq's openssh_tunnel method to work around
    issue https://github.com/zeromq/pyzmq/issues/589 which was solved
    in pyzmq https://github.com/zeromq/pyzmq/pull/615
    """"""
    ssh = ""ssh ""
    if keyfile:
        ssh += ""-i "" + keyfile

    if ':' in server:
        server, port = server.split(':')
        ssh += "" -p %s"" % port

    cmd = ""%s -O check %s"" % (ssh, server)
    (output, exitstatus) = pexpect.run(cmd, withexitstatus=True)
    if not exitstatus:
        pid = int(output[output.find(""(pid="")+5:output.find("")"")])
        cmd = ""%s -O forward -L 127.0.0.1:%i:%s:%i %s"" % (
            ssh, lport, remoteip, rport, server)
        (output, exitstatus) = pexpect.run(cmd, withexitstatus=True)
        if not exitstatus:
            atexit.register(_stop_tunnel, cmd.replace(""-O forward"",
                                                      ""-O cancel"",
                                                      1))
            return pid
    cmd = ""%s -f -S none -L 127.0.0.1:%i:%s:%i %s sleep %i"" % (
                                  ssh, lport, remoteip, rport, server, timeout)

    # pop SSH_ASKPASS from env
    env = os.environ.copy()
    env.pop('SSH_ASKPASS', None)

    ssh_newkey = 'Are you sure you want to continue connecting'
    tunnel = pexpect.spawn(cmd, env=env)
    failed = False
    while True:
        try:
            i = tunnel.expect([ssh_newkey, '[Pp]assword:'], timeout=.1)
            if i == 0:
                host = server.split('@')[-1]
                question = _(""The authenticity of host <b>%s</b> can't be ""
                             ""established. Are you sure you want to continue ""
                             ""connecting?"") % host
                reply = QMessageBox.question(self, _('Warning'), question,
                                             QMessageBox.Yes | QMessageBox.No,
                                             QMessageBox.No)
                if reply == QMessageBox.Yes:
                    tunnel.sendline('yes')
                    continue
                else:
                    tunnel.sendline('no')
                    raise RuntimeError(
                       _(""The authenticity of the host can't be established""))
            if i == 1 and password is not None:
                tunnel.sendline(password)
        except pexpect.TIMEOUT:
            continue
        except pexpect.EOF:
            if tunnel.exitstatus:
                raise RuntimeError(_(""Tunnel '%s' failed to start"") % cmd)
            else:
                return tunnel.pid
        else:
            if failed or password is None:
                raise RuntimeError(_(""Could not connect to remote host""))
                # TODO: Use this block when pyzmq bug #620 is fixed
                # # Prompt a passphrase dialog to the user for a second attempt
                # password, ok = QInputDialog.getText(self, _('Password'),
                #             _('Enter password for: ') + server,
                #             echo=QLineEdit.Password)
                # if ok is False:
                #      raise RuntimeError('Could not connect to remote host.')
            tunnel.sendline(password)
            failed = True"
delete_repository;"def delete_repository(self, repository, params=None):
        """"""
        Removes a shared file system repository.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_

        :arg repository: A comma-separated list of repository names
        :arg master_timeout: Explicit operation timeout for connection to master
            node
        :arg timeout: Explicit operation timeout
        """"""
        if repository in SKIP_IN_PATH:
            raise ValueError(""Empty value passed for a required argument 'repository'."")
        return self.transport.perform_request('DELETE',
            _make_path('_snapshot', repository), params=params)"
run_example;"def run_example(example_name, environ):
    """"""
    Run an example module from zipline.examples.
    """"""
    mod = EXAMPLE_MODULES[example_name]

    register_calendar(""YAHOO"", get_calendar(""NYSE""), force=True)

    return run_algorithm(
        initialize=getattr(mod, 'initialize', None),
        handle_data=getattr(mod, 'handle_data', None),
        before_trading_start=getattr(mod, 'before_trading_start', None),
        analyze=getattr(mod, 'analyze', None),
        bundle='test',
        environ=environ,
        # Provide a default capital base, but allow the test to override.
        **merge({'capital_base': 1e7}, mod._test_args())
    )"
put_script;"def put_script(self, id, body, context=None, params=None):
        """"""
        Create a script in given language with specified ID.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting.html>`_

        :arg id: Script ID
        :arg body: The document
        """"""
        for param in (id, body):
            if param in SKIP_IN_PATH:
                raise ValueError(""Empty value passed for a required argument."")
        return self.transport.perform_request(
            ""PUT"", _make_path(""_scripts"", id, context), params=params, body=body
        )"
wait_for_handshake;"def wait_for_handshake(self) -> ""Future[SSLIOStream]"":
        """"""Wait for the initial SSL handshake to complete.

        If a ``callback`` is given, it will be called with no
        arguments once the handshake is complete; otherwise this
        method returns a `.Future` which will resolve to the
        stream itself after the handshake is complete.

        Once the handshake is complete, information such as
        the peer's certificate and NPN/ALPN selections may be
        accessed on ``self.socket``.

        This method is intended for use on server-side streams
        or after using `IOStream.start_tls`; it should not be used
        with `IOStream.connect` (which already waits for the
        handshake to complete). It may only be called once per stream.

        .. versionadded:: 4.2

        .. versionchanged:: 6.0

           The ``callback`` argument was removed. Use the returned
           `.Future` instead.

        """"""
        if self._ssl_connect_future is not None:
            raise RuntimeError(""Already waiting"")
        future = self._ssl_connect_future = Future()
        if not self._ssl_accepting:
            self._finish_ssl_connect()
        return future"
image_data;"def image_data(verbose=False):
  """"""Get the raw encoded image data, downloading it if necessary.""""""
  # This is a principled use of the `global` statement; don't lint me.
  global _IMAGE_DATA  # pylint: disable=global-statement
  if _IMAGE_DATA is None:
    if verbose:
      logger.info(""--- Downloading image."")
    with contextlib.closing(urllib.request.urlopen(IMAGE_URL)) as infile:
      _IMAGE_DATA = infile.read()
  return _IMAGE_DATA"
show_keypair;"def show_keypair(kwargs=None, call=None):
    '''
    Show the details of an SSH keypair
    '''
    if call != 'function':
        log.error(
            'The show_keypair function must be called with -f or --function.'
        )
        return False

    if not kwargs:
        kwargs = {}

    if 'keyname' not in kwargs:
        log.error('A keyname is required.')
        return False

    keypairs = list_keypairs(call='function')
    keyid = keypairs[kwargs['keyname']]['id']
    log.debug('Key ID is %s', keyid)

    details = query(method='account/keys', command=keyid)

    return details"
schedule_transfer_runs;"def schedule_transfer_runs(
        self,
        parent,
        start_time,
        end_time,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates transfer runs for a time range [start\_time, end\_time]. For
        each date - or whatever granularity the data source supports - in the
        range, one transfer run is created. Note that runs are created per UTC
        time in the time range.

        Example:
            >>> from google.cloud import bigquery_datatransfer_v1
            >>>
            >>> client = bigquery_datatransfer_v1.DataTransferServiceClient()
            >>>
            >>> parent = client.project_transfer_config_path('[PROJECT]', '[TRANSFER_CONFIG]')
            >>>
            >>> # TODO: Initialize `start_time`:
            >>> start_time = {}
            >>>
            >>> # TODO: Initialize `end_time`:
            >>> end_time = {}
            >>>
            >>> response = client.schedule_transfer_runs(parent, start_time, end_time)

        Args:
            parent (str): Transfer configuration name in the form:
                ``projects/{project_id}/transferConfigs/{config_id}``.
            start_time (Union[dict, ~google.cloud.bigquery_datatransfer_v1.types.Timestamp]): Start time of the range of transfer runs. For example,
                ``""2017-05-25T00:00:00+00:00""``.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.bigquery_datatransfer_v1.types.Timestamp`
            end_time (Union[dict, ~google.cloud.bigquery_datatransfer_v1.types.Timestamp]): End time of the range of transfer runs. For example,
                ``""2017-05-30T00:00:00+00:00""``.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.bigquery_datatransfer_v1.types.Timestamp`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.bigquery_datatransfer_v1.types.ScheduleTransferRunsResponse` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""schedule_transfer_runs"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""schedule_transfer_runs""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.schedule_transfer_runs,
                default_retry=self._method_configs[""ScheduleTransferRuns""].retry,
                default_timeout=self._method_configs[""ScheduleTransferRuns""].timeout,
                client_info=self._client_info,
            )

        request = datatransfer_pb2.ScheduleTransferRunsRequest(
            parent=parent, start_time=start_time, end_time=end_time
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""schedule_transfer_runs""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
find_include_path;"def find_include_path():
    """"""Find MXNet included header files.

    Returns
    -------
    incl_path : string
        Path to the header files.
    """"""
    incl_from_env = os.environ.get('MXNET_INCLUDE_PATH')
    if incl_from_env:
        if os.path.isdir(incl_from_env):
            if not os.path.isabs(incl_from_env):
                logging.warning(""MXNET_INCLUDE_PATH should be an absolute path, instead of: %s"",
                                incl_from_env)
            else:
                return incl_from_env
        else:
            logging.warning(""MXNET_INCLUDE_PATH '%s' doesn't exist"", incl_from_env)

    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
    # include path in pip package
    pip_incl_path = os.path.join(curr_path, 'include/')
    if os.path.isdir(pip_incl_path):
        return pip_incl_path
    else:
        # include path if build from source
        src_incl_path = os.path.join(curr_path, '../../include/')
        if os.path.isdir(src_incl_path):
            return src_incl_path
        else:
            raise RuntimeError('Cannot find the MXNet include path in either ' + pip_incl_path +
                               ' or ' + src_incl_path + '\n')"
get_template;"def get_template(template_dict, parameter_overrides=None):
        """"""
        Given a SAM template dictionary, return a cleaned copy of the template where SAM plugins have been run
        and parameter values have been substituted.

        Parameters
        ----------
        template_dict : dict
            unprocessed SAM template dictionary

        parameter_overrides: dict
            Optional dictionary of values for template parameters

        Returns
        -------
        dict
            Processed SAM template
        """"""

        template_dict = template_dict or {}
        if template_dict:
            template_dict = SamTranslatorWrapper(template_dict).run_plugins()

        template_dict = SamBaseProvider._resolve_parameters(template_dict, parameter_overrides)
        ResourceMetadataNormalizer.normalize(template_dict)
        return template_dict"
check_xsrf_cookie;"def check_xsrf_cookie(self) -> None:
        """"""Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.

        To prevent cross-site request forgery, we set an ``_xsrf``
        cookie and include the same value as a non-cookie
        field with all ``POST`` requests. If the two do not match, we
        reject the form submission as a potential forgery.

        The ``_xsrf`` value may be set as either a form field named ``_xsrf``
        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``
        (the latter is accepted for compatibility with Django).

        See http://en.wikipedia.org/wiki/Cross-site_request_forgery

        .. versionchanged:: 3.2.2
           Added support for cookie version 2.  Both versions 1 and 2 are
           supported.
        """"""
        # Prior to release 1.1.1, this check was ignored if the HTTP header
        # ``X-Requested-With: XMLHTTPRequest`` was present.  This exception
        # has been shown to be insecure and has been removed.  For more
        # information please see
        # http://www.djangoproject.com/weblog/2011/feb/08/security/
        # http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails
        token = (
            self.get_argument(""_xsrf"", None)
            or self.request.headers.get(""X-Xsrftoken"")
            or self.request.headers.get(""X-Csrftoken"")
        )
        if not token:
            raise HTTPError(403, ""'_xsrf' argument missing from POST"")
        _, token, _ = self._decode_xsrf_token(token)
        _, expected_token, _ = self._get_raw_xsrf_token()
        if not token:
            raise HTTPError(403, ""'_xsrf' argument has invalid format"")
        if not hmac.compare_digest(utf8(token), utf8(expected_token)):
            raise HTTPError(403, ""XSRF cookie does not match POST argument"")"
map_vals;"def map_vals(val, *names, **extra_opts):
    '''
    Many arguments come in as a list of VAL1:VAL2 pairs, but map to a list
    of dicts in the format {NAME1: VAL1, NAME2: VAL2}. This function
    provides common code to handle these instances.
    '''
    fill = extra_opts.pop('fill', NOTSET)
    expected_num_elements = len(names)
    val = translate_stringlist(val)
    for idx, item in enumerate(val):
        if not isinstance(item, dict):
            elements = [x.strip() for x in item.split(':')]
            num_elements = len(elements)
            if num_elements < expected_num_elements:
                if fill is NOTSET:
                    raise SaltInvocationError(
                        '\'{0}\' contains {1} value(s) (expected {2})'.format(
                            item, num_elements, expected_num_elements
                        )
                    )
                elements.extend([fill] * (expected_num_elements - num_elements))
            elif num_elements > expected_num_elements:
                raise SaltInvocationError(
                    '\'{0}\' contains {1} value(s) (expected {2})'.format(
                        item,
                        num_elements,
                        expected_num_elements if fill is NOTSET
                            else 'up to {0}'.format(expected_num_elements)
                    )
                )
            val[idx] = dict(zip(names, elements))
    return val"
add_note;"def add_note(path, filename=""note.txt""):
    """"""Opens a txt file at the given path where user can add and save notes.

    Args:
        path (str): Directory where note will be saved.
        filename (str): Name of note. Defaults to ""note.txt""
    """"""
    path = os.path.expanduser(path)
    assert os.path.isdir(path), ""{} is not a valid directory."".format(path)

    filepath = os.path.join(path, filename)
    exists = os.path.isfile(filepath)

    try:
        subprocess.call([EDITOR, filepath])
    except Exception as exc:
        logger.error(""Editing note failed!"")
        raise exc
    if exists:
        print(""Note updated at:"", filepath)
    else:
        print(""Note created at:"", filepath)"
infer_domain;"def infer_domain(terms):
    """"""
    Infer the domain from a collection of terms.

    The algorithm for inferring domains is as follows:

    - If all input terms have a domain of GENERIC, the result is GENERIC.

    - If there is exactly one non-generic domain in the input terms, the result
      is that domain.

    - Otherwise, an AmbiguousDomain error is raised.

    Parameters
    ----------
    terms : iterable[zipline.pipeline.term.Term]

    Returns
    -------
    inferred : Domain or NotSpecified

    Raises
    ------
    AmbiguousDomain
        Raised if more than one concrete domain is present in the input terms.
    """"""
    domains = {t.domain for t in terms}
    num_domains = len(domains)

    if num_domains == 0:
        return GENERIC
    elif num_domains == 1:
        return domains.pop()
    elif num_domains == 2 and GENERIC in domains:
        domains.remove(GENERIC)
        return domains.pop()
    else:
        # Remove GENERIC if it's present before raising. Showing it to the user
        # is confusing because it doesn't contribute to the error.
        domains.discard(GENERIC)
        raise AmbiguousDomain(sorted(domains, key=repr))"
put_notification_configuration;"def put_notification_configuration(Bucket,
           TopicConfigurations=None, QueueConfigurations=None,
           LambdaFunctionConfigurations=None,
           region=None, key=None, keyid=None, profile=None):
    '''
    Given a valid config, update the notification parameters for a bucket.

    Returns {updated: true} if parameters were updated and returns
    {updated: False} if parameters were not updated.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_s3_bucket.put_notification_configuration my_bucket
                [{...}] \\
                [{...}] \\
                [{...}]

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        if TopicConfigurations is None:
            TopicConfigurations = []
        elif isinstance(TopicConfigurations, six.string_types):
            TopicConfigurations = salt.utils.json.loads(TopicConfigurations)
        if QueueConfigurations is None:
            QueueConfigurations = []
        elif isinstance(QueueConfigurations, six.string_types):
            QueueConfigurations = salt.utils.json.loads(QueueConfigurations)
        if LambdaFunctionConfigurations is None:
            LambdaFunctionConfigurations = []
        elif isinstance(LambdaFunctionConfigurations, six.string_types):
            LambdaFunctionConfigurations = salt.utils.json.loads(LambdaFunctionConfigurations)
        # TODO allow the user to use simple names & substitute ARNs for those names
        conn.put_bucket_notification_configuration(Bucket=Bucket, NotificationConfiguration={
                'TopicConfigurations': TopicConfigurations,
                'QueueConfigurations': QueueConfigurations,
                'LambdaFunctionConfigurations': LambdaFunctionConfigurations,
        })
        return {'updated': True, 'name': Bucket}
    except ClientError as e:
        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}"
top_level;"def top_level(url, fix_protocol=True):
    """"""Extract the top level domain from an URL.""""""
    ext = tld.get_tld(url, fix_protocol=fix_protocol)
    toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split(
        ext)[0] + ext
    return toplevel"
add_entity;"def add_entity(self, entity):
        """"""Add an entity to the ACL.

        :type entity: :class:`_ACLEntity`
        :param entity: The entity to add to this ACL.
        """"""
        self._ensure_loaded()
        self.entities[str(entity)] = entity"
parse_parameter_string;"def parse_parameter_string(cls, parameter_string):
        """"""Parse a parameter string into its constituent name, type, and
        pattern

        For example::

            parse_parameter_string('<param_one:[A-z]>')` ->
                ('param_one', str, '[A-z]')

        :param parameter_string: String to parse
        :return: tuple containing
            (parameter_name, parameter_type, parameter_pattern)
        """"""
        # We could receive NAME or NAME:PATTERN
        name = parameter_string
        pattern = ""string""
        if "":"" in parameter_string:
            name, pattern = parameter_string.split("":"", 1)
            if not name:
                raise ValueError(
                    ""Invalid parameter syntax: {}"".format(parameter_string)
                )

        default = (str, pattern)
        # Pull from pre-configured types
        _type, pattern = REGEX_TYPES.get(pattern, default)

        return name, _type, pattern"
update_document;"def update_document(
        self,
        document,
        update_mask,
        mask=None,
        current_document=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Updates or inserts a document.

        Example:
            >>> from google.cloud import firestore_v1beta1
            >>>
            >>> client = firestore_v1beta1.FirestoreClient()
            >>>
            >>> # TODO: Initialize `document`:
            >>> document = {}
            >>>
            >>> # TODO: Initialize `update_mask`:
            >>> update_mask = {}
            >>>
            >>> response = client.update_document(document, update_mask)

        Args:
            document (Union[dict, ~google.cloud.firestore_v1beta1.types.Document]): The updated document.
                Creates the document if it does not already exist.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.firestore_v1beta1.types.Document`
            update_mask (Union[dict, ~google.cloud.firestore_v1beta1.types.DocumentMask]): The fields to update.
                None of the field paths in the mask may contain a reserved name.

                If the document exists on the server and has fields not referenced in the
                mask, they are left unchanged.
                Fields referenced in the mask, but not present in the input document, are
                deleted from the document on the server.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.firestore_v1beta1.types.DocumentMask`
            mask (Union[dict, ~google.cloud.firestore_v1beta1.types.DocumentMask]): The fields to return. If not set, returns all fields.

                If the document has a field that is not present in this mask, that field
                will not be returned in the response.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.firestore_v1beta1.types.DocumentMask`
            current_document (Union[dict, ~google.cloud.firestore_v1beta1.types.Precondition]): An optional precondition on the document.
                The request will fail if this is set and not met by the target document.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.firestore_v1beta1.types.Precondition`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.firestore_v1beta1.types.Document` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""update_document"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""update_document""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.update_document,
                default_retry=self._method_configs[""UpdateDocument""].retry,
                default_timeout=self._method_configs[""UpdateDocument""].timeout,
                client_info=self._client_info,
            )

        request = firestore_pb2.UpdateDocumentRequest(
            document=document,
            update_mask=update_mask,
            mask=mask,
            current_document=current_document,
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""document.name"", document.name)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""update_document""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
add_signature;"def add_signature(name=None, inputs=None, outputs=None):
  """"""Adds a signature to the module definition.

  NOTE: This must be called within a `module_fn` that is defining a Module.

  Args:
    name: Signature name as a string. If omitted, it is interpreted as 'default'
      and is the signature used when `Module.__call__` `signature` is not
      specified.
    inputs: A dict from input name to Tensor or SparseTensor to feed when
      applying the signature. If a single tensor is passed, it is interpreted
      as a dict with a single 'default' entry.
    outputs: A dict from output name to Tensor or SparseTensor to return from
      applying the signature. If a single tensor is passed, it is interpreted
      as a dict with a single 'default' entry.

  Raises:
    ValueError: if the arguments are invalid.
  """"""
  if not name:
    name = ""default""
  if inputs is None:
    inputs = {}
  if outputs is None:
    outputs = {}
  if not isinstance(inputs, dict):
    inputs = {""default"": inputs}
  if not isinstance(outputs, dict):
    outputs = {""default"": outputs}
  message = find_signature_inputs_from_multivalued_ops(inputs)
  if message: logging.error(message)
  message = find_signature_input_colocation_error(name, inputs)
  if message: raise ValueError(message)
  saved_model_lib.add_signature(name, inputs, outputs)"
remove_vdir;"def remove_vdir(name, site, app='/'):
    '''
    Remove an IIS virtual directory.

    :param str name: The virtual directory name.
    :param str site: The IIS site name.
    :param str app: The IIS application.

    Example of usage with only the required arguments:

    .. code-block:: yaml

        site0-foo-vdir-remove:
            win_iis.remove_vdir:
                - name: foo
                - site: site0

    Example of usage specifying all available arguments:

    .. code-block:: yaml

        site0-foo-vdir-remove:
            win_iis.remove_vdir:
                - name: foo
                - site: site0
                - app: v1
    '''
    ret = {'name': name,
           'changes': {},
           'comment': str(),
           'result': None}

    current_vdirs = __salt__['win_iis.list_vdirs'](site, app)

    if name not in current_vdirs:
        ret['comment'] = 'Virtual directory has already been removed: {0}'.format(name)
        ret['result'] = True
    elif __opts__['test']:
        ret['comment'] = 'Virtual directory will be removed: {0}'.format(name)
        ret['changes'] = {'old': name,
                          'new': None}
    else:
        ret['comment'] = 'Removed virtual directory: {0}'.format(name)
        ret['changes'] = {'old': name,
                          'new': None}
        ret['result'] = __salt__['win_iis.remove_vdir'](name, site, app)

    return ret"
delete_lifecycle_configuration;"def delete_lifecycle_configuration(Bucket,
           region=None, key=None, keyid=None, profile=None):
    '''
    Delete the lifecycle configuration for the given bucket

    Returns {deleted: true} if Lifecycle was deleted and returns
    {deleted: False} if Lifecycle was not deleted.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_s3_bucket.delete_lifecycle_configuration my_bucket

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        conn.delete_bucket_lifecycle(Bucket=Bucket)
        return {'deleted': True, 'name': Bucket}
    except ClientError as e:
        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"
rolling_count;"def rolling_count(self, window_start, window_end):
        """"""
        Count the number of non-NULL values of different subsets over this
        SArray.

        The subset that the count is executed on is defined as an inclusive
        range relative to the position to each value in the SArray, using
        `window_start` and `window_end`. For a better understanding of this,
        see the examples below.

        Parameters
        ----------
        window_start : int
            The start of the subset to count relative to the current value.

        window_end : int
            The end of the subset to count relative to the current value. Must
            be greater than `window_start`.

        Returns
        -------
        out : SArray

        Examples
        --------
        >>> import pandas
        >>> sa = SArray([1,2,3,None,5])
        >>> series = pandas.Series([1,2,3,None,5])

        A rolling count with a window including the previous 2 entries including
        the current:
        >>> sa.rolling_count(-2,0)
        dtype: int
        Rows: 5
        [1, 2, 3, 2, 2]

        Pandas equivalent:
        >>> pandas.rolling_count(series, 3)
        0     1
        1     2
        2     3
        3     2
        4     2
        dtype: float64

        A rolling count with a size of 3, centered around the current:
        >>> sa.rolling_count(-1,1)
        dtype: int
        Rows: 5
        [2, 3, 2, 2, 1]

        Pandas equivalent:
        >>> pandas.rolling_count(series, 3, center=True)
        0    2
        1    3
        2    2
        3    2
        4    1
        dtype: float64

        A rolling count with a window including the current and the 2 entries
        following:
        >>> sa.rolling_count(0,2)
        dtype: int
        Rows: 5
        [3, 2, 2, 1, 1]

        A rolling count with a window including the previous 2 entries NOT
        including the current:
        >>> sa.rolling_count(-2,-1)
        dtype: int
        Rows: 5
        [0, 1, 2, 2, 1]
        """"""
        agg_op = '__builtin__nonnull__count__'
        return SArray(_proxy=self.__proxy__.builtin_rolling_apply(agg_op, window_start, window_end, 0))"
get_rolls;"def get_rolls(self, root_symbol, start, end, offset):
        """"""
        Get the rolls, i.e. the session at which to hop from contract to
        contract in the chain.

        Parameters
        ----------
        root_symbol : str
            The root symbol for which to calculate rolls.
        start : Timestamp
            Start of the date range.
        end : Timestamp
            End of the date range.
        offset : int
            Offset from the primary.

        Returns
        -------
        rolls - list[tuple(sid, roll_date)]
            A list of rolls, where first value is the first active `sid`,
        and the `roll_date` on which to hop to the next contract.
            The last pair in the chain has a value of `None` since the roll
            is after the range.
        """"""
        oc = self.asset_finder.get_ordered_contracts(root_symbol)
        front = self._get_active_contract_at_offset(root_symbol, end, 0)
        back = oc.contract_at_offset(front, 1, end.value)
        if back is not None:
            end_session = self.trading_calendar.minute_to_session_label(end)
            first = self._active_contract(oc, front, back, end_session)
        else:
            first = front
        first_contract = oc.sid_to_contract[first]
        rolls = [((first_contract >> offset).contract.sid, None)]
        tc = self.trading_calendar
        sessions = tc.sessions_in_range(tc.minute_to_session_label(start),
                                        tc.minute_to_session_label(end))
        freq = sessions.freq
        if first == front:
            # This is a bit tricky to grasp. Once we have the active contract
            # on the given end date, we want to start walking backwards towards
            # the start date and checking for rolls. For this, we treat the
            # previous month's contract as the 'first' contract, and the
            # contract we just found to be active as the 'back'. As we walk
            # towards the start date, if the 'back' is no longer active, we add
            # that date as a roll.
            curr = first_contract << 1
        else:
            curr = first_contract << 2
        session = sessions[-1]

        while session > start and curr is not None:
            front = curr.contract.sid
            back = rolls[0][0]
            prev_c = curr.prev
            while session > start:
                prev = session - freq
                if prev_c is not None:
                    if prev < prev_c.contract.auto_close_date:
                        break
                if back != self._active_contract(oc, front, back, prev):
                    # TODO: Instead of listing each contract with its roll date
                    # as tuples, create a series which maps every day to the
                    # active contract on that day.
                    rolls.insert(0, ((curr >> offset).contract.sid, session))
                    break
                session = prev
            curr = curr.prev
            if curr is not None:
                session = min(session, curr.contract.auto_close_date + freq)

        return rolls"
convert_custom;"def convert_custom(net, node, module, builder):
    """"""Convert highly specific ops""""""
    input_name, output_name = _get_input_output_name(net, node)
    name = node['name']
    param = _get_attr(node)
    if param['op_type'] == 'special-darknet-maxpool':
        _add_pooling.add_pooling_with_padding_types(
            builder=builder,
            name=name,
            height=2,
            width=2,
            stride_height=1,
            stride_width=1,
            layer_type='MAX',
            padding_type='SAME',
            is_global=False,
            same_padding_asymmetry_mode='BOTTOM_RIGHT_HEAVY',
            input_name=input_name,
            output_name=output_name
        )
    else:
        raise TypeError(""MXNet layer of type Custom is not supported."")"
request_encode_url;"def request_encode_url(self, method, url, fields=None, headers=None,
                           **urlopen_kw):
        """"""
        Make a request using :meth:`urlopen` with the ``fields`` encoded in
        the url. This is useful for request methods like GET, HEAD, DELETE, etc.
        """"""
        if headers is None:
            headers = self.headers

        extra_kw = {'headers': headers}
        extra_kw.update(urlopen_kw)

        if fields:
            url += '?' + urlencode(fields)

        return self.urlopen(method, url, **extra_kw)"
similarity_graph;"def similarity_graph(self, k=5, radius=None, include_self_edges=False,
                         output_type='SGraph', verbose=True):
        """"""
        Construct the similarity graph on the reference dataset, which is
        already stored in the model. This is conceptually very similar to
        running `query` with the reference set, but this method is optimized
        for the purpose, syntactically simpler, and automatically removes
        self-edges.

        Parameters
        ----------
        k : int, optional
            Maximum number of neighbors to return for each point in the
            dataset. Setting this to ``None`` deactivates the constraint, so
            that all neighbors are returned within ``radius`` of a given point.

        radius : float, optional
            For a given point, only neighbors within this distance are
            returned. The default is ``None``, in which case the ``k`` nearest
            neighbors are returned for each query point, regardless of
            distance.

        include_self_edges : bool, optional
            For most distance functions, each point in the model's reference
            dataset is its own nearest neighbor. If this parameter is set to
            False, this result is ignored, and the nearest neighbors are
            returned *excluding* the point itself.

        output_type : {'SGraph', 'SFrame'}, optional
            By default, the results are returned in the form of an SGraph,
            where each point in the reference dataset is a vertex and an edge A
            -> B indicates that vertex B is a nearest neighbor of vertex A. If
            'output_type' is set to 'SFrame', the output is in the same form as
            the results of the 'query' method: an SFrame with columns
            indicating the query label (in this case the query data is the same
            as the reference data), reference label, distance between the two
            points, and the rank of the neighbor.

        verbose : bool, optional
            If True, print progress updates and model details.

        Returns
        -------
        out : SFrame or SGraph
            The type of the output object depends on the 'output_type'
            parameter. See the parameter description for more detail.

        Notes
        -----
        - If both ``k`` and ``radius`` are set to ``None``, each data point is
          matched to the entire dataset. If the reference dataset has
          :math:`n` rows, the output is an SFrame with :math:`n^2` rows (or an
          SGraph with :math:`n^2` edges).

        - For models created with the 'lsh' method, the output similarity graph
          may have fewer vertices than there are data points in the original
          reference set. Because LSH is an approximate method, a query point
          may have fewer than 'k' neighbors. If LSH returns no neighbors at all
          for a query and self-edges are excluded, the query point is omitted
          from the results.

        Examples
        --------
        First construct an SFrame and create a nearest neighbors model:

        >>> sf = turicreate.SFrame({'x1': [0.98, 0.62, 0.11],
        ...                       'x2': [0.69, 0.58, 0.36]})
        ...
        >>> model = turicreate.nearest_neighbors.create(sf, distance='euclidean')

        Unlike the ``query`` method, there is no need for a second dataset with
        ``similarity_graph``.

        >>> g = model.similarity_graph(k=1)  # an SGraph
        >>> g.edges
        +----------+----------+----------------+------+
        | __src_id | __dst_id |    distance    | rank |
        +----------+----------+----------------+------+
        |    0     |    1     | 0.376430604494 |  1   |
        |    2     |    1     | 0.55542776308  |  1   |
        |    1     |    0     | 0.376430604494 |  1   |
        +----------+----------+----------------+------+
        """"""
        ## Validate inputs.
        if k is not None:
            if not isinstance(k, int):
                raise ValueError(""Input 'k' must be an integer."")

            if k <= 0:
                raise ValueError(""Input 'k' must be larger than 0."")

        if radius is not None:
            if not isinstance(radius, (int, float)):
                raise ValueError(""Input 'radius' must be an integer or float."")

            if radius < 0:
                raise ValueError(""Input 'radius' must be non-negative."")


        ## Set k and radius to special values to indicate 'None'
        if k is None:
            k = -1

        if radius is None:
            radius = -1.0

        opts = {'model': self.__proxy__,
                'model_name': self.__name__,
                'k': k,
                'radius': radius,
                'include_self_edges': include_self_edges}

        with QuietProgress(verbose):
            result = _turicreate.extensions._nearest_neighbors.similarity_graph(opts)

        knn = result['neighbors']

        if output_type == ""SFrame"":
            return knn

        else:
            sg = _SGraph(edges=knn, src_field='query_label',
                         dst_field='reference_label')
            return sg"
configure_cmake;"def configure_cmake(self):
        """"""Create CMake instance and execute configure step
        """"""
        cmake = CMake(self)
        cmake.definitions[""FLATBUFFERS_BUILD_TESTS""] = False
        cmake.definitions[""FLATBUFFERS_BUILD_SHAREDLIB""] = self.options.shared
        cmake.definitions[""FLATBUFFERS_BUILD_FLATLIB""] = not self.options.shared
        cmake.configure()
        return cmake"
save_metadata;"def save_metadata(self, data_dir, feature_name=None):
    """"""See base class for details.""""""
    # Recursively save all child features
    for feature_key, feature in six.iteritems(self._feature_dict):
      if feature_name:
        feature_key = '-'.join((feature_name, feature_key))
      feature.save_metadata(data_dir, feature_name=feature_key)"
update_content_encoding;"def update_content_encoding(self, data: Any) -> None:
        """"""Set request content encoding.""""""
        if not data:
            return

        enc = self.headers.get(hdrs.CONTENT_ENCODING, '').lower()
        if enc:
            if self.compress:
                raise ValueError(
                    'compress can not be set '
                    'if Content-Encoding header is set')
        elif self.compress:
            if not isinstance(self.compress, str):
                self.compress = 'deflate'
            self.headers[hdrs.CONTENT_ENCODING] = self.compress
            self.chunked = True"
dir_list;"def dir_list(saltenv='base', backend=None):
    '''
    Return a list of directories in the given environment

    saltenv : base
        The salt fileserver environment to be listed

    backend
        Narrow fileserver backends to a subset of the enabled ones. If all
        passed backends start with a minus sign (``-``), then these backends
        will be excluded from the enabled backends. However, if there is a mix
        of backends with and without a minus sign (ex:
        ``backend=-roots,git``) then the ones starting with a minus sign will
        be disregarded.

        .. versionadded:: 2015.5.0

    .. note:
        Keep in mind that executing this function spawns a new process,
        separate from the master. This means that if the fileserver
        configuration has been changed in some way since the master has been
        restarted (e.g. if :conf_master:`fileserver_backend`,
        :conf_master:`gitfs_remotes`, :conf_master:`hgfs_remotes`, etc. have
        been updated), then the results of this runner will not accurately
        reflect what dirs are available to minions.

        When in doubt, use :py:func:`cp.list_master_dirs
        <salt.modules.cp.list_master_dirs>` to see what dirs the minion can see,
        and always remember to restart the salt-master daemon when updating
        the fileserver configuration.

    CLI Example:

    .. code-block:: bash

        salt-run fileserver.dir_list
        salt-run fileserver.dir_list saltenv=prod
        salt-run fileserver.dir_list saltenv=dev backend=git
        salt-run fileserver.dir_list base hg,roots
        salt-run fileserver.dir_list -git
    '''
    fileserver = salt.fileserver.Fileserver(__opts__)
    load = {'saltenv': saltenv, 'fsbackend': backend}
    return fileserver.dir_list(load=load)"
get_prob;"def get_prob(self, src, tgt, mask, pre_compute, return_logits=False):
        '''
        :param s: [src_sequence_length, batch_size, src_dim]
        :param h: [batch_size, tgt_dim] or [tgt_sequence_length, batch_size, tgt_dim]
        :param mask: [src_sequence_length, batch_size]\
             or [tgt_sequence_length, src_sequence_length, batch_sizse]
        :param pre_compute: [src_sequence_length, batch_size, hidden_dim]
        :return: [src_sequence_length, batch_size]\
             or [tgt_sequence_length, src_sequence_length, batch_size]
        '''
        s_shape = src.get_shape().as_list()
        h_shape = tgt.get_shape().as_list()
        src_dim = s_shape[-1]
        tgt_dim = h_shape[-1]
        assert src_dim is not None, 'src dimension must be defined'
        assert tgt_dim is not None, 'tgt dimension must be defined'

        self._define_params(src_dim, tgt_dim)

        if len(h_shape) == 2:
            tgt = tf.expand_dims(tgt, 0)
        if pre_compute is None:
            pre_compute = self.get_pre_compute(src)

        buf0 = pre_compute
        buf1 = tf.tensordot(tgt, self.var['U'], axes=[[2], [0]])
        buf2 = tf.tanh(tf.expand_dims(buf0, 0) + tf.expand_dims(buf1, 1))

        if not self.is_vanilla:
            xh1 = tgt
            xh2 = tgt
            s1 = src
            if self.need_padding:
                xh1 = tf.tensordot(xh1, self.var['V_t'], 1)
                xh2 = tf.tensordot(xh2, self.var['S_t'], 1)
                s1 = tf.tensordot(s1, self.var['V_s'], 1)
            if not self.is_identity_transform:
                xh1 = tf.tensordot(xh1, self.var['T'], 1)
                xh2 = tf.tensordot(xh2, self.var['T'], 1)
            buf3 = tf.expand_dims(s1, 0) * tf.expand_dims(xh1, 1)
            buf3 = tf.tanh(tf.tensordot(buf3, self.var['V'], axes=[[3], [0]]))
            buf = tf.reshape(tf.tanh(buf2 + buf3), shape=tf.shape(buf3))
        else:
            buf = buf2
        v = self.var['v']
        e = tf.tensordot(buf, v, [[3], [0]])
        e = tf.squeeze(e, axis=[3])
        tmp = tf.reshape(e + (mask - 1) * 10000.0, shape=tf.shape(e))
        prob = tf.nn.softmax(tmp, 1)
        if len(h_shape) == 2:
            prob = tf.squeeze(prob, axis=[0])
            tmp = tf.squeeze(tmp, axis=[0])
        if return_logits:
            return prob, tmp
        return prob"
hybrid_forward;"def hybrid_forward(self, F, candidates_like, prob, alias):
        # pylint: disable=unused-argument
        """"""Draw samples from uniform distribution and return sampled candidates.

        Parameters
        ----------
        candidates_like: mxnet.nd.NDArray or mxnet.sym.Symbol
            This input specifies the shape of the to be sampled candidates. #
            TODO shape selection is not yet supported. Shape must be specified
            in the constructor.

        Returns
        -------
        samples: mxnet.nd.NDArray or mxnet.sym.Symbol
            The sampled candidates of shape candidates_like.shape. Candidates
            are sampled based on the weights specified on creation of the
            UnigramCandidateSampler.
        """"""
        flat_shape = functools.reduce(operator.mul, self._shape)
        idx = F.random.uniform(low=0, high=self.N, shape=flat_shape,
                               dtype='float64').floor()
        prob = F.gather_nd(prob, idx.reshape((1, -1)))
        alias = F.gather_nd(alias, idx.reshape((1, -1)))
        where = F.random.uniform(shape=flat_shape,
                                 dtype='float64') < prob
        hit = idx * where
        alt = alias * (1 - where)
        candidates = (hit + alt).reshape(self._shape)

        return candidates.astype(self._dtype)"
setup_environment;"def setup_environment():
  """"""Makes recommended modifications to the environment.

  This functions changes global state in the Python process. Calling
  this function is a good idea, but it can't appropriately be called
  from library routines.
  """"""
  absl.logging.set_verbosity(absl.logging.WARNING)

  # The default is HTTP/1.0 for some strange reason. If we don't use
  # HTTP/1.1 then a new TCP socket and Python thread is created for
  # each HTTP request. The tradeoff is we must always specify the
  # Content-Length header, or do chunked encoding for streaming.
  serving.WSGIRequestHandler.protocol_version = 'HTTP/1.1'"
evaluate_analogy;"def evaluate_analogy(args, token_embedding, ctx, logfile=None, global_step=0):
    """"""Evaluate on specified analogy datasets.

    The analogy task is an open vocabulary task, make sure to pass a
    token_embedding with a sufficiently large number of supported tokens.

    """"""
    results = []
    exclude_question_words = not args.analogy_dont_exclude_question_words
    for analogy_function in args.analogy_functions:
        evaluator = nlp.embedding.evaluation.WordEmbeddingAnalogy(
            idx_to_vec=token_embedding.idx_to_vec,
            exclude_question_words=exclude_question_words,
            analogy_function=analogy_function)
        evaluator.initialize(ctx=ctx)
        if not args.no_hybridize:
            evaluator.hybridize()

        for (dataset_name, dataset_kwargs,
             dataset) in iterate_analogy_datasets(args):
            initial_length = len(dataset)
            dataset_coded = [[
                token_embedding.token_to_idx[d[0]],
                token_embedding.token_to_idx[d[1]],
                token_embedding.token_to_idx[d[2]],
                token_embedding.token_to_idx[d[3]]
            ] for d in dataset if d[0] in token_embedding.token_to_idx
                             and d[1] in token_embedding.token_to_idx
                             and d[2] in token_embedding.token_to_idx
                             and d[3] in token_embedding.token_to_idx]
            num_dropped = initial_length - len(dataset_coded)

            dataset_coded_batched = mx.gluon.data.DataLoader(
                dataset_coded, batch_size=args.eval_batch_size)

            acc = mx.metric.Accuracy()
            for batch in dataset_coded_batched:
                batch = batch.as_in_context(ctx)
                words1, words2, words3, words4 = (batch[:, 0], batch[:, 1],
                                                  batch[:, 2], batch[:, 3])
                pred_idxs = evaluator(words1, words2, words3)
                acc.update(pred_idxs[:, 0], words4.astype(np.float32))

            logging.info('Accuracy on %s (%s quadruples) %s with %s:\t%s',
                         dataset.__class__.__name__, len(dataset_coded),
                         str(dataset_kwargs), analogy_function,
                         acc.get()[1])

            result = dict(
                task='analogy',
                dataset_name=dataset_name,
                dataset_kwargs=dataset_kwargs,
                analogy_function=analogy_function,
                accuracy=acc.get()[1],
                num_dropped=num_dropped,
                global_step=global_step,
            )
            log_analogy_result(logfile, result)
            results.append(result)
    return results"
previous_row;"def previous_row(self):
        """"""Move to previous row from currently selected row.""""""
        row = self.currentIndex().row()
        rows = self.proxy_model.rowCount()
        if row == 0:
            row = rows
        self.selectRow(row - 1)"
list_records;"def list_records(zone_id, profile, type=None):
    '''
    List records for the given zone_id on the given profile

    :param zone_id: Zone to export.
    :type  zone_id: ``str``

    :param profile: The profile key
    :type  profile: ``str``

    :param type: The record type, e.g. A, NS
    :type  type: ``str``

    CLI Example:

    .. code-block:: bash

        salt myminion libcloud_dns.list_records google.com profile1
    '''
    conn = _get_driver(profile=profile)
    zone = conn.get_zone(zone_id)
    if type is not None:
        return [_simple_record(record) for record in conn.list_records(zone) if record.type == type]
    else:
        return [_simple_record(record) for record in conn.list_records(zone)]"
format_help;"def format_help(help):
    """"""Formats the help string.""""""
    help = help.replace(""Options:"", str(crayons.normal(""Options:"", bold=True)))
    help = help.replace(
        ""Usage: pipenv"", str(""Usage: {0}"".format(crayons.normal(""pipenv"", bold=True)))
    )
    help = help.replace(""  check"", str(crayons.red(""  check"", bold=True)))
    help = help.replace(""  clean"", str(crayons.red(""  clean"", bold=True)))
    help = help.replace(""  graph"", str(crayons.red(""  graph"", bold=True)))
    help = help.replace(""  install"", str(crayons.magenta(""  install"", bold=True)))
    help = help.replace(""  lock"", str(crayons.green(""  lock"", bold=True)))
    help = help.replace(""  open"", str(crayons.red(""  open"", bold=True)))
    help = help.replace(""  run"", str(crayons.yellow(""  run"", bold=True)))
    help = help.replace(""  shell"", str(crayons.yellow(""  shell"", bold=True)))
    help = help.replace(""  sync"", str(crayons.green(""  sync"", bold=True)))
    help = help.replace(""  uninstall"", str(crayons.magenta(""  uninstall"", bold=True)))
    help = help.replace(""  update"", str(crayons.green(""  update"", bold=True)))
    additional_help = """"""
Usage Examples:
   Create a new project using Python 3.7, specifically:
   $ {1}

   Remove project virtualenv (inferred from current directory):
   $ {9}

   Install all dependencies for a project (including dev):
   $ {2}

   Create a lockfile containing pre-releases:
   $ {6}

   Show a graph of your installed dependencies:
   $ {4}

   Check your installed dependencies for security vulnerabilities:
   $ {7}

   Install a local setup.py into your virtual environment/Pipfile:
   $ {5}

   Use a lower-level pip command:
   $ {8}

Commands:"""""".format(
        crayons.red(""pipenv --three""),
        crayons.red(""pipenv --python 3.7""),
        crayons.red(""pipenv install --dev""),
        crayons.red(""pipenv lock""),
        crayons.red(""pipenv graph""),
        crayons.red(""pipenv install -e .""),
        crayons.red(""pipenv lock --pre""),
        crayons.red(""pipenv check""),
        crayons.red(""pipenv run pip freeze""),
        crayons.red(""pipenv --rm""),
    )
    help = help.replace(""Commands:"", additional_help)
    return help"
viable_source_types;"def viable_source_types (target_type):
    """""" Helper rule, caches the result of '__viable_source_types_real'.
    """"""
    assert isinstance(target_type, basestring)
    if target_type not in __viable_source_types_cache:
        __vst_cached_types.append(target_type)
        __viable_source_types_cache [target_type] = __viable_source_types_real (target_type)
    return __viable_source_types_cache [target_type]"
setup_logging;"def setup_logging(
        self, log_level=logging.INFO, excluded_loggers=EXCLUDED_LOGGER_DEFAULTS, **kw
    ):
        """"""Attach default Stackdriver logging handler to the root logger.

        This method uses the default log handler, obtained by
        :meth:`~get_default_handler`, and attaches it to the root Python
        logger, so that a call such as ``logging.warn``, as well as all child
        loggers, will report to Stackdriver logging.

        :type log_level: int
        :param log_level: (Optional) Python logging log level. Defaults to
                          :const:`logging.INFO`.

        :type excluded_loggers: tuple
        :param excluded_loggers: (Optional) The loggers to not attach the
                                 handler to. This will always include the
                                 loggers in the path of the logging client
                                 itself.

        :type kw: dict
        :param kw: keyword args passed to handler constructor
        """"""
        handler = self.get_default_handler(**kw)
        setup_logging(handler, log_level=log_level, excluded_loggers=excluded_loggers)"
instance_default;"def instance_default(self, obj):
        ''' Get the default value that will be used for a specific instance.

        Args:
            obj (HasProps) : The instance to get the default value for.

        Returns:
            object

        '''
        return self.property.themed_default(obj.__class__, self.name, obj.themed_values())"
make_secure_channel;"def make_secure_channel(credentials, user_agent, host, extra_options=()):
    """"""Makes a secure channel for an RPC service.

    Uses / depends on gRPC.

    :type credentials: :class:`google.auth.credentials.Credentials`
    :param credentials: The OAuth2 Credentials to use for creating
                        access tokens.

    :type user_agent: str
    :param user_agent: The user agent to be used with API requests.

    :type host: str
    :param host: The host for the service.

    :type extra_options: tuple
    :param extra_options: (Optional) Extra gRPC options used when creating the
                          channel.

    :rtype: :class:`grpc._channel.Channel`
    :returns: gRPC secure channel with credentials attached.
    """"""
    target = ""%s:%d"" % (host, http_client.HTTPS_PORT)
    http_request = google.auth.transport.requests.Request()

    user_agent_option = (""grpc.primary_user_agent"", user_agent)
    options = (user_agent_option,) + extra_options
    return google.auth.transport.grpc.secure_authorized_channel(
        credentials, http_request, target, options=options
    )"
get_command_signature;"def get_command_signature(self, command):
        """"""Retrieves the signature portion of the help page.

        Parameters
        ------------
        command: :class:`Command`
            The command to get the signature of.

        Returns
        --------
        :class:`str`
            The signature for the command.
        """"""

        parent = command.full_parent_name
        if len(command.aliases) > 0:
            aliases = '|'.join(command.aliases)
            fmt = '[%s|%s]' % (command.name, aliases)
            if parent:
                fmt = parent + ' ' + fmt
            alias = fmt
        else:
            alias = command.name if not parent else parent + ' ' + command.name

        return '%s%s %s' % (self.clean_prefix, alias, command.signature)"
transform_data_fasttext;"def transform_data_fasttext(data, vocab, idx_to_counts, cbow, ngram_buckets,
                            ngrams, batch_size, window_size,
                            frequent_token_subsampling=1E-4, dtype='float32',
                            index_dtype='int64'):
    """"""Transform a DataStream of coded DataSets to a DataStream of batches.

    Parameters
    ----------
    data : gluonnlp.data.DataStream
        DataStream where each sample is a valid input to
        gluonnlp.data.EmbeddingCenterContextBatchify.
    vocab : gluonnlp.Vocab
        Vocabulary containing all tokens whose indices occur in data. For each
        token, it's associated subwords will be computed and used for
        constructing the batches. No subwords are used if ngram_buckets is 0.
    idx_to_counts : list of int
        List of integers such that idx_to_counts[idx] represents the count of
        vocab.idx_to_token[idx] in the underlying dataset. The count
        information is used to subsample frequent words in the dataset.
        Each token is independently dropped with probability 1 - sqrt(t /
        (count / sum_counts)) where t is the hyperparameter
        frequent_token_subsampling.
    cbow : boolean
        If True, batches for CBOW are returned.
    ngram_buckets : int
        Number of hash buckets to consider for the fastText
        nlp.vocab.NGramHashes subword function.
    ngrams : list of int
        For each integer n in the list, all ngrams of length n will be
        considered by the nlp.vocab.NGramHashes subword function.
    batch_size : int
        The returned data stream iterates over batches of batch_size.
    window_size : int
        The context window size for
        gluonnlp.data.EmbeddingCenterContextBatchify.
    frequent_token_subsampling : float
        Hyperparameter for subsampling. See idx_to_counts above for more
        information.
    dtype : str or np.dtype, default 'float32'
        Data type of data array.
    index_dtype : str or np.dtype, default 'int64'
        Data type of index arrays.

    Returns
    -------
    gluonnlp.data.DataStream
        Stream over batches. Each returned element is a list corresponding to
        the arguments for the forward pass of model.SG or model.CBOW
        respectively based on if cbow is False or True. If ngarm_buckets > 0,
        the returned sample will contain ngrams. Both model.SG or model.CBOW
        will handle them correctly as long as they are initialized with the
        subword_function returned as second argument by this function (see
        below).
    gluonnlp.vocab.NGramHashes
        The subword_function used for obtaining the subwords in the returned
        batches.

    """"""
    if ngram_buckets <= 0:
        raise ValueError('Invalid ngram_buckets. Use Word2Vec training '
                         'pipeline if not interested in ngrams.')

    sum_counts = float(sum(idx_to_counts))
    idx_to_pdiscard = [
        1 - math.sqrt(frequent_token_subsampling / (count / sum_counts))
        for count in idx_to_counts]

    def subsample(shard):
        return [[
            t for t, r in zip(sentence,
                              np.random.uniform(0, 1, size=len(sentence)))
            if r > idx_to_pdiscard[t]] for sentence in shard]

    data = data.transform(subsample)

    batchify = nlp.data.batchify.EmbeddingCenterContextBatchify(
        batch_size=batch_size, window_size=window_size, cbow=cbow,
        weight_dtype=dtype, index_dtype=index_dtype)
    data = data.transform(batchify)

    with print_time('prepare subwords'):
        subword_function = nlp.vocab.create_subword_function(
            'NGramHashes', ngrams=ngrams, num_subwords=ngram_buckets)

        # Store subword indices for all words in vocabulary
        idx_to_subwordidxs = list(subword_function(vocab.idx_to_token))
        subwordidxs = np.concatenate(idx_to_subwordidxs)
        subwordidxsptr = np.cumsum([
            len(subwordidxs) for subwordidxs in idx_to_subwordidxs])
        subwordidxsptr = np.concatenate([
            np.zeros(1, dtype=np.int64), subwordidxsptr])
        if cbow:
            subword_lookup = functools.partial(
                cbow_lookup, subwordidxs=subwordidxs,
                subwordidxsptr=subwordidxsptr, offset=len(vocab))
        else:
            subword_lookup = functools.partial(
                skipgram_lookup, subwordidxs=subwordidxs,
                subwordidxsptr=subwordidxsptr, offset=len(vocab))
        max_subwordidxs_len = max(len(s) for s in idx_to_subwordidxs)
        if max_subwordidxs_len > 500:
            warnings.warn(
                'The word with largest number of subwords '
                'has {} subwords, suggesting there are '
                'some noisy words in your vocabulary. '
                'You should filter out very long words '
                'to avoid memory issues.'.format(max_subwordidxs_len))

    data = UnchainStream(data)

    if cbow:
        batchify_fn = cbow_fasttext_batch
    else:
        batchify_fn = skipgram_fasttext_batch
    batchify_fn = functools.partial(
        batchify_fn, num_tokens=len(vocab) + len(subword_function),
        subword_lookup=subword_lookup, dtype=dtype, index_dtype=index_dtype)

    return data, batchify_fn, subword_function"
get_jids;"def get_jids():
    '''
    Return a list of all job ids
    '''
    serv = _get_serv(ret=None)
    sql = ""select distinct(jid) from jids group by load""

    # [{u'points': [[0, jid, load],
    #               [0, jid, load]],
    #   u'name': u'jids',
    #   u'columns': [u'time', u'distinct', u'load']}]
    data = serv.query(sql)
    ret = {}
    if data:
        for _, jid, load in data[0]['points']:
            ret[jid] = salt.utils.jid.format_jid_instance(jid, salt.utils.json.loads(load))
    return ret"
field_mask;"def field_mask(original, modified):
    """"""Create a field mask by comparing two messages.

    Args:
        original (~google.protobuf.message.Message): the original message.
            If set to None, this field will be interpretted as an empty
            message.
        modified (~google.protobuf.message.Message): the modified message.
            If set to None, this field will be interpretted as an empty
            message.

    Returns:
        google.protobuf.field_mask_pb2.FieldMask: field mask that contains
        the list of field names that have different values between the two
        messages. If the messages are equivalent, then the field mask is empty.

    Raises:
        ValueError: If the ``original`` or ``modified`` are not the same type.
    """"""
    if original is None and modified is None:
        return field_mask_pb2.FieldMask()

    if original is None and modified is not None:
        original = copy.deepcopy(modified)
        original.Clear()

    if modified is None and original is not None:
        modified = copy.deepcopy(original)
        modified.Clear()

    if type(original) != type(modified):
        raise ValueError(
            ""expected that both original and modified should be of the ""
            'same type, received ""{!r}"" and ""{!r}"".'.format(
                type(original), type(modified)
            )
        )

    return field_mask_pb2.FieldMask(paths=_field_mask_helper(original, modified))"
create_shortcuts;"def create_shortcuts(self):
        """"""Create shortcuts for this file explorer.""""""
        # Configurable
        copy_clipboard_file = config_shortcut(self.copy_file_clipboard,
                                              context='explorer',
                                              name='copy file', parent=self)
        paste_clipboard_file = config_shortcut(self.save_file_clipboard,
                                               context='explorer',
                                               name='paste file', parent=self)
        copy_absolute_path = config_shortcut(self.copy_absolute_path,
                                             context='explorer',
                                             name='copy absolute path',
                                             parent=self)
        copy_relative_path = config_shortcut(self.copy_relative_path,
                                             context='explorer',
                                             name='copy relative path',
                                             parent=self)
        return [copy_clipboard_file, paste_clipboard_file, copy_absolute_path,
                copy_relative_path]"
prune_feed_map;"def prune_feed_map(meta_graph, feed_map):
  """"""Function to prune the feedmap of nodes which no longer exist.""""""
  node_names = [x.name + "":0"" for x in meta_graph.graph_def.node]
  keys_to_delete = []
  for k, _ in feed_map.items():
    if k not in node_names:
      keys_to_delete.append(k)
  for k in keys_to_delete:
    del feed_map[k]"
service_create;"def service_create(name, service_type, description=None, profile=None,
                   **connection_args):
    '''
    Add service to Keystone service catalog

    CLI Examples:

    .. code-block:: bash

        salt '*' keystone.service_create nova compute \
'OpenStack Compute Service'
    '''
    kstone = auth(profile, **connection_args)
    service = kstone.services.create(name, service_type, description=description)
    return service_get(service.id, profile=profile, **connection_args)"
render_sphinx_doc;"def render_sphinx_doc(self, doc, context=None, css_path=CSS_PATH):
        """"""Transform doc string dictionary to HTML and show it""""""
        # Math rendering option could have changed
        if self.main.editor is not None:
            fname = self.main.editor.get_current_filename()
            dname = osp.dirname(fname)
        else:
            dname = ''
        self._sphinx_thread.render(doc, context, self.get_option('math'),
                                   dname, css_path=self.css_path)"
document_single_builder;"def document_single_builder(builder):
  """"""Doc string for a single builder, with or without configs.""""""
  mod_name = builder.__class__.__module__
  cls_name = builder.__class__.__name__
  mod_file = sys.modules[mod_name].__file__
  if mod_file.endswith(""pyc""):
    mod_file = mod_file[:-1]

  description_prefix = """"


  if builder.builder_configs:
    # Dataset with configs; document each one
    config_docs = []
    for config in builder.BUILDER_CONFIGS:
      builder = tfds.builder(builder.name, config=config)
      info = builder.info
      # TODO(rsepassi): document the actual config object
      config_doc = SINGLE_CONFIG_ENTRY.format(
          builder_name=builder.name,
          config_name=config.name,
          description=config.description,
          version=config.version,
          feature_information=make_feature_information(info),
          size=tfds.units.size_str(info.size_in_bytes),
      )
      config_docs.append(config_doc)
    out_str = DATASET_WITH_CONFIGS_ENTRY.format(
        snakecase_name=builder.name,
        module_and_class=""%s.%s"" % (tfds_mod_name(mod_name), cls_name),
        cls_url=cls_url(mod_name),
        config_names=""\n"".join([
            CONFIG_BULLET.format(name=config.name,
                                 description=config.description,
                                 version=config.version,
                                 size=tfds.units.size_str(tfds.builder(
                                     builder.name, config=config)
                                                          .info.size_in_bytes))
            for config in builder.BUILDER_CONFIGS]),
        config_cls=""%s.%s"" % (tfds_mod_name(mod_name),
                              type(builder.builder_config).__name__),
        configs=""\n"".join(config_docs),
        urls=format_urls(info.urls),
        url=url_from_info(info),
        supervised_keys=str(info.supervised_keys),
        citation=make_citation(info.citation),
        statistics_information=make_statistics_information(info),
        description=builder.info.description,
        description_prefix=description_prefix,
    )
  else:
    info = builder.info
    out_str = DATASET_ENTRY.format(
        snakecase_name=builder.name,
        module_and_class=""%s.%s"" % (tfds_mod_name(mod_name), cls_name),
        cls_url=cls_url(mod_name),
        description=info.description,
        description_prefix=description_prefix,
        version=info.version,
        feature_information=make_feature_information(info),
        statistics_information=make_statistics_information(info),
        urls=format_urls(info.urls),
        url=url_from_info(info),
        supervised_keys=str(info.supervised_keys),
        citation=make_citation(info.citation),
        size=tfds.units.size_str(info.size_in_bytes),
    )

  out_str = schema_org(builder) + ""\n"" + out_str
  return out_str"
kill_process_children;"def kill_process_children(pid):
    """"""Find and kill child processes of a process.

    :param pid: PID of parent process (process ID)
    :return: Nothing
    """"""
    if sys.platform == ""darwin"":
        kill_process_children_osx(pid)
    elif sys.platform == ""linux"":
        kill_process_children_unix(pid)
    else:
        pass"
unpack_url;"def unpack_url(
    link,  # type: Optional[Link]
    location,  # type: Optional[str]
    download_dir=None,  # type: Optional[str]
    only_download=False,  # type: bool
    session=None,  # type: Optional[PipSession]
    hashes=None,  # type: Optional[Hashes]
    progress_bar=""on""  # type: str
):
    # type: (...) -> None
    """"""Unpack link.
       If link is a VCS link:
         if only_download, export into download_dir and ignore location
          else unpack into location
       for other types of link:
         - unpack into location
         - if download_dir, copy the file into download_dir
         - if only_download, mark location for deletion

    :param hashes: A Hashes object, one of whose embedded hashes must match,
        or HashMismatch will be raised. If the Hashes is empty, no matches are
        required, and unhashable types of requirements (like VCS ones, which
        would ordinarily raise HashUnsupported) are allowed.
    """"""
    # non-editable vcs urls
    if is_vcs_url(link):
        unpack_vcs_link(link, location)

    # file urls
    elif is_file_url(link):
        unpack_file_url(link, location, download_dir, hashes=hashes)

    # http urls
    else:
        if session is None:
            session = PipSession()

        unpack_http_url(
            link,
            location,
            download_dir,
            session,
            hashes=hashes,
            progress_bar=progress_bar
        )
    if only_download:
        write_delete_marker_file(location)"
stop_data_frame_transform;"def stop_data_frame_transform(self, transform_id, params=None):
        """"""
        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_

        :arg transform_id: The id of the transform to stop
        :arg timeout: Controls the time to wait until the transform has stopped.
            Default to 30 seconds
        :arg wait_for_completion: Whether to wait for the transform to fully
            stop before returning or not. Default to false
        """"""
        if transform_id in SKIP_IN_PATH:
            raise ValueError(
                ""Empty value passed for a required argument 'transform_id'.""
            )
        return self.transport.perform_request(
            ""POST"",
            _make_path(""_data_frame"", ""transforms"", transform_id, ""_stop""),
            params=params,
        )"
shuffle_step;"def shuffle_step(entries, step):
    '''
    Shuffle the step
    '''
    answer = []
    for i in range(0, len(entries), step):
        sub = entries[i:i+step]
        shuffle(sub)
        answer += sub
    return answer"
closing_plugin;"def closing_plugin(self, cancelable=False):
        """"""Perform actions before parent main window is closed""""""
        self.findinfiles.closing_widget()  # stop search thread and clean-up
        options = self.findinfiles.find_options.get_options(to_save=True)
        if options is not None:
            (search_text, text_re,
             exclude, exclude_idx, exclude_re,
             more_options, case_sensitive,
             path_history) = options
            hist_limit = 15
            search_text = search_text[:hist_limit]
            exclude = exclude[:hist_limit]
            path_history = path_history[-hist_limit:]
            self.set_option('search_text', search_text)
            self.set_option('search_text_regexp', text_re)
            self.set_option('exclude', exclude)
            self.set_option('exclude_idx', exclude_idx)
            self.set_option('exclude_regexp', exclude_re)
            self.set_option('more_options', more_options)
            self.set_option('case_sensitive', case_sensitive)
            self.set_option('path_history', path_history)
        return True"
merge_all;"def merge_all(lst, strategy='smart', renderer='yaml', merge_lists=False):
    '''
    .. versionadded:: 2019.2.0

    Merge a list of objects into each other in order

    :type lst: Iterable
    :param lst: List of objects to be merged.

    :type strategy: String
    :param strategy: Merge strategy. See utils.dictupdate.

    :type renderer: String
    :param renderer:
        Renderer type. Used to determine strategy when strategy is 'smart'.

    :type merge_lists: Bool
    :param merge_lists: Defines whether to merge embedded object lists.

    CLI Example:

    .. code-block:: shell

        $ salt-call --output=txt slsutil.merge_all '[{foo: Foo}, {foo: Bar}]'
        local: {u'foo': u'Bar'}
    '''

    ret = {}
    for obj in lst:
        ret = salt.utils.dictupdate.merge(
            ret, obj, strategy, renderer, merge_lists
        )

    return ret"
rebuild_proxies;"def rebuild_proxies(self, prepared_request, proxies):
        """"""This method re-evaluates the proxy configuration by considering the
        environment variables. If we are redirected to a URL covered by
        NO_PROXY, we strip the proxy configuration. Otherwise, we set missing
        proxy keys for this URL (in case they were stripped by a previous
        redirect).

        This method also replaces the Proxy-Authorization header where
        necessary.

        :rtype: dict
        """"""
        proxies = proxies if proxies is not None else {}
        headers = prepared_request.headers
        url = prepared_request.url
        scheme = urlparse(url).scheme
        new_proxies = proxies.copy()
        no_proxy = proxies.get('no_proxy')

        bypass_proxy = should_bypass_proxies(url, no_proxy=no_proxy)
        if self.trust_env and not bypass_proxy:
            environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)

            proxy = environ_proxies.get(scheme, environ_proxies.get('all'))

            if proxy:
                new_proxies.setdefault(scheme, proxy)

        if 'Proxy-Authorization' in headers:
            del headers['Proxy-Authorization']

        try:
            username, password = get_auth_from_url(new_proxies[scheme])
        except KeyError:
            username, password = None, None

        if username and password:
            headers['Proxy-Authorization'] = _basic_auth_str(username, password)

        return new_proxies"
template_update;"def template_update(call=None, kwargs=None):
    '''
    Replaces the template contents.

    .. versionadded:: 2016.3.0

    template_id
        The ID of the template to update. Can be used instead of ``template_name``.

    template_name
        The name of the template to update. Can be used instead of ``template_id``.

    path
        The path to a file containing the elements of the template to be updated.
        Syntax within the file can be the usual attribute=value or XML. Can be
        used instead of ``data``.

    data
        Contains the elements of the template to be updated. Syntax can be the
        usual attribute=value or XML. Can be used instead of ``path``.

    update_type
        There are two ways to update a template: ``replace`` the whole template
        or ``merge`` the new template with the existing one.

    CLI Example:

    .. code-block:: bash

        salt-cloud --function template_update opennebula template_id=1 update_type=replace \\
            path=/path/to/template_update_file.txt
        salt-cloud -f template_update opennebula template_name=my-template update_type=merge \\
            data='CPU=""1.0"" DISK=[IMAGE=""Ubuntu-14.04""] GRAPHICS=[LISTEN=""0.0.0.0"",TYPE=""vnc""] \\
            MEMORY=""1024"" NETWORK=""yes"" NIC=[NETWORK=""192net"",NETWORK_UNAME=""oneadmin""] \\
            OS=[ARCH=""x86_64""] SUNSTONE_CAPACITY_SELECT=""YES"" SUNSTONE_NETWORK_SELECT=""YES"" \\
            VCPU=""1""'
    '''
    if call != 'function':
        raise SaltCloudSystemExit(
            'The template_update function must be called with -f or --function.'
        )

    if kwargs is None:
        kwargs = {}

    template_id = kwargs.get('template_id', None)
    template_name = kwargs.get('template_name', None)
    path = kwargs.get('path', None)
    data = kwargs.get('data', None)
    update_type = kwargs.get('update_type', None)
    update_args = ['replace', 'merge']

    if update_type is None:
        raise SaltCloudSystemExit(
            'The template_update function requires an \'update_type\' to be provided.'
        )

    if update_type == update_args[0]:
        update_number = 0
    elif update_type == update_args[1]:
        update_number = 1
    else:
        raise SaltCloudSystemExit(
            'The update_type argument must be either {0} or {1}.'.format(
                update_args[0],
                update_args[1]
            )
        )

    if template_id:
        if template_name:
            log.warning(
                'Both the \'template_id\' and \'template_name\' arguments were provided. '
                '\'template_id\' will take precedence.'
            )
    elif template_name:
        template_id = get_template_id(kwargs={'name': template_name})
    else:
        raise SaltCloudSystemExit(
            'The template_update function requires either a \'template_id\' '
            'or a \'template_name\' to be provided.'
        )

    if data:
        if path:
            log.warning(
                'Both the \'data\' and \'path\' arguments were provided. '
                '\'data\' will take precedence.'
            )
    elif path:
        with salt.utils.files.fopen(path, mode='r') as rfh:
            data = rfh.read()
    else:
        raise SaltCloudSystemExit(
            'The template_update function requires either \'data\' or a file '
            '\'path\' to be provided.'
        )

    server, user, password = _get_xml_rpc()
    auth = ':'.join([user, password])
    response = server.one.template.update(auth, int(template_id), data, int(update_number))

    ret = {
        'action': 'template.update',
        'updated': response[0],
        'template_id': response[1],
        'error_code': response[2],
    }

    return ret"
list_storage_class;"def list_storage_class(self, **kwargs):
        """"""
        list or watch objects of kind StorageClass
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_storage_class(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1StorageClassList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_storage_class_with_http_info(**kwargs)
        else:
            (data) = self.list_storage_class_with_http_info(**kwargs)
            return data"
list_items;"def list_items(queue):
    '''
    List contents of a queue
    '''
    itemstuple = _list_items(queue)
    items = [item[0] for item in itemstuple]
    return items"
register_plugin;"def register_plugin(self):
        """"""Register plugin in Spyder's main window""""""
        self.focus_changed.connect(self.main.plugin_focus_changed)
        self.main.add_dockwidget(self)
        self.main.console.set_help(self)

        self.internal_shell = self.main.console.shell
        self.console = self.main.console"
classifer_metrics;"def classifer_metrics(label, pred):
    """"""
    computes f1, precision and recall on the entity class
    """"""
    prediction = np.argmax(pred, axis=1)
    label = label.astype(int)

    pred_is_entity = prediction != not_entity_index
    label_is_entity = label != not_entity_index

    corr_pred = (prediction == label) == (pred_is_entity == True)

    #how many entities are there?
    num_entities = np.sum(label_is_entity)
    entity_preds = np.sum(pred_is_entity)

    #how many times did we correctly predict an entity?
    correct_entitites = np.sum(corr_pred[pred_is_entity])

    #precision: when we predict entity, how often are we right?
    precision = correct_entitites/entity_preds
    if entity_preds == 0:
        precision = np.nan

    #recall: of the things that were an entity, how many did we catch?
    recall = correct_entitites / num_entities
    if num_entities == 0:
        recall = np.nan
    f1 = 2 * precision * recall / (precision + recall)
    return precision, recall, f1"
add_config;"def add_config(lines):
    '''
    Add one or more config lines to the switch running config

    .. code-block:: bash

        salt '*' onyx.cmd add_config 'snmp-server community TESTSTRINGHERE rw'

    .. note::
        For more than one config added per command, lines should be a list.
    '''
    if not isinstance(lines, list):
        lines = [lines]
    try:
        enable()
        configure_terminal()
        for line in lines:
            sendline(line)

        configure_terminal_exit()
        disable()
    except TerminalException as e:
        log.error(e)
        return False
    return True"
copy_value;"def copy_value(self, orig_name, new_name):
        """"""Copy a variable""""""
        code = u""get_ipython().kernel.copy_value('%s', '%s')"" % (orig_name,
                                                                 new_name)
        if self._reading:
            self.kernel_client.input(u'!' + code)
        else:
            self.silent_execute(code)"
kill_all_processes;"def kill_all_processes(self, check_alive=True, allow_graceful=False):
        """"""Kill all of the processes.

        Note that This is slower than necessary because it calls kill, wait,
        kill, wait, ... instead of kill, kill, ..., wait, wait, ...

        Args:
            check_alive (bool): Raise an exception if any of the processes were
                already dead.
        """"""
        # Kill the raylet first. This is important for suppressing errors at
        # shutdown because we give the raylet a chance to exit gracefully and
        # clean up its child worker processes. If we were to kill the plasma
        # store (or Redis) first, that could cause the raylet to exit
        # ungracefully, leading to more verbose output from the workers.
        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:
            self._kill_process_type(
                ray_constants.PROCESS_TYPE_RAYLET,
                check_alive=check_alive,
                allow_graceful=allow_graceful)

        # We call ""list"" to copy the keys because we are modifying the
        # dictionary while iterating over it.
        for process_type in list(self.all_processes.keys()):
            self._kill_process_type(
                process_type,
                check_alive=check_alive,
                allow_graceful=allow_graceful)"
earliest_possible_date;"def earliest_possible_date():
    """"""
    The earliest date for which we can load data from this module.
    """"""
    today = pd.Timestamp('now', tz='UTC').normalize()
    # Bank of Canada only has the last 10 years of data at any given time.
    return today.replace(year=today.year - 10)"
detect_iter;"def detect_iter(self, det_iter, show_timer=False):
        """"""
        detect all images in iterator

        Parameters:
        ----------
        det_iter : DetIter
            iterator for all testing images
        show_timer : Boolean
            whether to print out detection exec time

        Returns:
        ----------
        list of detection results
        """"""
        num_images = det_iter._size
        if not isinstance(det_iter, mx.io.PrefetchingIter):
            det_iter = mx.io.PrefetchingIter(det_iter)
        start = timer()
        detections = self.mod.predict(det_iter).asnumpy()
        time_elapsed = timer() - start
        if show_timer:
            logging.info(""Detection time for {} images: {:.4f} sec"".format(
                num_images, time_elapsed))
        result = Detector.filter_positive_detections(detections)
        return result"
get_args;"def get_args(
        self,
        keep_blank_values: bool = False,
        strict_parsing: bool = False,
        encoding: str = ""utf-8"",
        errors: str = ""replace"",
    ) -> RequestParameters:
        """"""
        Method to parse `query_string` using `urllib.parse.parse_qs`.
        This methods is used by `args` property.
        Can be used directly if you need to change default parameters.
        :param keep_blank_values: flag indicating whether blank values in
            percent-encoded queries should be treated as blank strings.
            A true value indicates that blanks should be retained as blank
            strings.  The default false value indicates that blank values
            are to be ignored and treated as if they were  not included.
        :type keep_blank_values: bool
        :param strict_parsing: flag indicating what to do with parsing errors.
            If false (the default), errors are silently ignored. If true,
            errors raise a ValueError exception.
        :type strict_parsing: bool
        :param encoding: specify how to decode percent-encoded sequences
            into Unicode characters, as accepted by the bytes.decode() method.
        :type encoding: str
        :param errors: specify how to decode percent-encoded sequences
            into Unicode characters, as accepted by the bytes.decode() method.
        :type errors: str
        :return: RequestParameters
        """"""
        if not self.parsed_args[
            (keep_blank_values, strict_parsing, encoding, errors)
        ]:
            if self.query_string:
                self.parsed_args[
                    (keep_blank_values, strict_parsing, encoding, errors)
                ] = RequestParameters(
                    parse_qs(
                        qs=self.query_string,
                        keep_blank_values=keep_blank_values,
                        strict_parsing=strict_parsing,
                        encoding=encoding,
                        errors=errors,
                    )
                )

        return self.parsed_args[
            (keep_blank_values, strict_parsing, encoding, errors)
        ]"
append_result;"def append_result(self, results, num_matches):
        """"""Real-time update of search results""""""
        filename, lineno, colno, match_end, line = results

        if filename not in self.files:
            file_item = FileMatchItem(self, filename, self.sorting,
                                      self.text_color)
            file_item.setExpanded(True)
            self.files[filename] = file_item
            self.num_files += 1

        search_text = self.search_text
        title = ""'%s' - "" % search_text
        nb_files = self.num_files
        if nb_files == 0:
            text = _('String not found')
        else:
            text_matches = _('matches in')
            text_files = _('file')
            if nb_files > 1:
                text_files += 's'
            text = ""%d %s %d %s"" % (num_matches, text_matches,
                                    nb_files, text_files)
        self.set_title(title + text)

        file_item = self.files[filename]
        line = self.truncate_result(line, colno, match_end)
        item = LineMatchItem(file_item, lineno, colno, line, self.text_color)
        self.data[id(item)] = (filename, lineno, colno)"
metric_update;"def metric_update(self, project, metric_name, filter_, description):
        """"""API call:  update a metric resource.

        :type project: str
        :param project: ID of the project containing the metric.

        :type metric_name: str
        :param metric_name: the name of the metric

        :type filter_: str
        :param filter_: the advanced logs filter expression defining the
                        entries exported by the metric.

        :type description: str
        :param description: description of the metric.

        :rtype: dict
        :returns: The metric object returned from the API (converted from a
                  protobuf to a dictionary).
        """"""
        path = ""projects/%s/metrics/%s"" % (project, metric_name)
        metric_pb = LogMetric(name=path, filter=filter_, description=description)
        metric_pb = self._gapic_api.update_log_metric(path, metric_pb)
        # NOTE: LogMetric message type does not have an ``Any`` field
        #       so `MessageToDict`` can safely be used.
        return MessageToDict(metric_pb)"
get_config_file;"def get_config_file():
    """"""
    Returns the file name of the config file from which the environment
    variables are written.
    """"""
    import os
    from os.path import abspath, expanduser, join, exists

    __lib_name = get_library_name()

    assert __lib_name in [""sframe"", ""turicreate""]

    __default_config_path = join(expanduser(""~""), "".%s"" % __lib_name, ""config"")

    if ""TURI_CONFIG_FILE"" in os.environ:
        __default_config_path = abspath(expanduser(os.environ[""TURI_CONFIG_FILE""]))

        if not exists(__default_config_path):
            print((""WARNING: Config file specified in environment variable ""
                   ""'TURI_CONFIG_FILE' as ""
                   ""'%s', but this path does not exist."") % __default_config_path)

    return __default_config_path"
copy_path;"def copy_path(self, fnames=None, method=""absolute""):
        """"""Copy absolute or relative path to given file(s)/folders(s).""""""
        cb = QApplication.clipboard()
        explorer_dir = self.fsmodel.rootPath()
        if fnames is None:
            fnames = self.get_selected_filenames()
        if not isinstance(fnames, (tuple, list)):
            fnames = [fnames]
        fnames = [_fn.replace(os.sep, ""/"") for _fn in fnames]
        if len(fnames) > 1:
            if method == ""absolute"":
                clipboard_files = ',\n'.join('""' + _fn + '""' for _fn in fnames)
            elif method == ""relative"":
                clipboard_files = ',\n'.join('""' +
                                             osp.relpath(_fn, explorer_dir).
                                             replace(os.sep, ""/"") + '""'
                                             for _fn in fnames)
        else:
            if method == ""absolute"":
                clipboard_files = fnames[0]
            elif method == ""relative"":
                clipboard_files = (osp.relpath(fnames[0], explorer_dir).
                                   replace(os.sep, ""/""))
        copied_from = self.parent_widget.__class__.__name__
        if copied_from == 'ProjectExplorerWidget' and method == 'relative':
            clipboard_files = [path.strip(',""') for path in
                               clipboard_files.splitlines()]
            clipboard_files = ['/'.join(path.strip('/').split('/')[1:]) for
                               path in clipboard_files]
            if len(clipboard_files) > 1:
                clipboard_files = ',\n'.join('""' + _fn + '""' for _fn in
                                             clipboard_files)
            else:
                clipboard_files = clipboard_files[0]
        cb.setText(clipboard_files, mode=cb.Clipboard)"
restart_kernel;"def restart_kernel(self):
        """"""
        Restart the associated kernel.

        Took this code from the qtconsole project
        Licensed under the BSD license
        """"""
        sw = self.shellwidget

        if not running_under_pytest() and self.ask_before_restart:
            message = _('Are you sure you want to restart the kernel?')
            buttons = QMessageBox.Yes | QMessageBox.No
            result = QMessageBox.question(self, _('Restart kernel?'),
                                          message, buttons)
        else:
            result = None

        if (result == QMessageBox.Yes or
                running_under_pytest() or
                not self.ask_before_restart):
            if sw.kernel_manager:
                if self.infowidget.isVisible():
                    self.infowidget.hide()
                    sw.show()
                try:
                    sw.kernel_manager.restart_kernel(
                        stderr=self.stderr_handle)
                except RuntimeError as e:
                    sw._append_plain_text(
                        _('Error restarting kernel: %s\n') % e,
                        before_prompt=True
                    )
                else:
                    # For issue 6235.  IPython was changing the setting of
                    # %colors on windows by assuming it was using a dark
                    # background.  This corrects it based on the scheme.
                    self.set_color_scheme(sw.syntax_style)
                    sw._append_html(_(""<br>Restarting kernel...\n<hr><br>""),
                                    before_prompt=False)
            else:
                sw._append_plain_text(
                    _('Cannot restart a kernel not started by Spyder\n'),
                    before_prompt=True
                )"
dump_json;"def dump_json(json_info, json_file, overwrite=True):
    """"""Dump a whole json record into the given file.

    Overwrite the file if the overwrite flag set.

    Args:
        json_info (dict): Information dict to be dumped.
        json_file (str): File path to be dumped to.
        overwrite(boolean)
    """"""
    if overwrite:
        mode = ""w""
    else:
        mode = ""w+""

    try:
        with open(json_file, mode) as f:
            f.write(json.dumps(json_info))
    except BaseException as e:
        logging.error(e.message)"
copy_figure;"def copy_figure(self):
        """"""Copy figure to clipboard.""""""
        if self.fmt in ['image/png', 'image/jpeg']:
            qpixmap = QPixmap()
            qpixmap.loadFromData(self.fig, self.fmt.upper())
            QApplication.clipboard().setImage(qpixmap.toImage())
        elif self.fmt == 'image/svg+xml':
            svg_to_clipboard(self.fig)
        else:
            return

        self.blink_figure()"
image_embedding_column;"def image_embedding_column(key, module_spec):
  """"""Uses a Module to get a dense 1-D representation from the pixels of images.

  This feature column can be used on images, represented as float32 tensors of
  RGB pixel data in the range [0,1]. This can be read from a numeric_column()
  if the tf.Example input data happens to have decoded images, all with the
  same shape [height, width, 3]. More commonly, the input_fn will have code to
  explicitly decode images, resize them (possibly after performing data
  augmentation such as random crops etc.), and provide a batch of shape
  [batch_size, height, width, 3].

  The result of this feature column is the result of passing its `input`
  through the module `m` instantiated from `module_spec`, as per
  `result = m({""images"": input})`. The `result` must have dtype float32 and
  shape `[batch_size, num_features]` with a known value of num_features.

  Example:

  ```python
    image_column = hub.image_embedding_column(""embeddings"", ""/tmp/image-module"")
    feature_columns = [image_column, ...]
    estimator = tf.estimator.LinearClassifier(feature_columns, ...)
    height, width = hub.get_expected_image_size(image_column.module_spec)
    input_fn = ...  # Provides ""embeddings"" with shape [None, height, width, 3].
    estimator.train(input_fn, ...)
  ```

  Args:
    key: A string or `_FeatureColumn` identifying the input image data.
    module_spec: A string handle or a `ModuleSpec` identifying the module.

  Returns:
    `_DenseColumn` that converts from pixel data.

  Raises:
     ValueError: if module_spec is not suitable for use in this feature column.
  """"""
  module_spec = module.as_module_spec(module_spec)
  _check_module_is_image_embedding(module_spec)
  return _ImageEmbeddingColumn(key=key, module_spec=module_spec)"
while_loop;"def while_loop(cond, func, loop_vars, max_iterations=None, name=""while_loop""):
    """"""Run a while loop with user-defined computation and loop condition.

    This operator simulates a while loop which iterately does customized computation
    as long as the condition is satisfied.

    `loop_vars` is a Symbol or nested lists of Symbols on which the computation uses.

    `cond` is a user-defined function, used as the loop condition.
    It consumes `loop_vars`, and produces a scalar MXNet symbol,
    indicating the termination of the loop.
    The loop ends when `cond` returns false (zero).
    The `cond` is variadic, and its signature should be
    `cond(*loop_vars) => Symbol`.

    `func` is a user-defined function, used as the loop body.
    It also consumes `loop_vars`, and produces `step_output` and `new_loop_vars` at each step.
    In each step, `step_output` should contain the same number elements.
    Through all steps, the i-th element of `step_output` should have the same shape and dtype.
    Also, `new_loop_vars` should contain the same number of elements as `loop_vars`,
    and the corresponding element should have the same shape and dtype.
    The `func` is variadic, and its signature should be
    `func(*loop_vars) =>
    (Symbol or nested List[Symbol] step_output, Symbol or nested List[Symbol] new_loop_vars)`.

    `max_iterations` is a scalar that defines the maximum number of iterations allowed.

    This function returns two lists.
    The first list has the length of `|step_output|`,
    in which the i-th element are all i-th elements of
    `step_output` from all steps, stacked along axis 0.
    The second list has the length of `|loop_vars|`,
    which represents final states of loop variables.

    .. warning::

       For now, the axis 0 of all Symbols in the first list are `max_iterations`,
       due to lack of dynamic shape inference.

    .. warning::

       Even if `cond` is never satisfied,
       while_loop returns a list of outputs with inferred dtype and shape.
       This is different from the Symbol version,
       where in this case `step_outputs` are assumed as an empty list.

    Parameters
    ----------
    cond: a Python function.
        The loop condition.
    func: a Python function.
        The loop body.
    loop_vars: a Symbol or nested lists of Symbol.
        The initial values of the loop variables.
    max_iterations: a python int.
        Maximum number of iterations.

    Returns
    ------
    outputs: a Symbol or nested lists of Symbols
        stacked output from each step
    states: a Symbol or nested lists of Symbols
        final state

    Examples
    --------
    >>> cond = lambda i, s: i <= 5
    >>> func = lambda i, s: ([i + s], [i + 1, s + i])
    >>> loop_vars = (mx.sym.var('i'), mx.sym.var('s'))
    >>> outputs, states = mx.sym.contrib.while_loop(cond, func, loop_vars, max_iterations=10)
    """"""
    def _to_python_scalar(inputs, type_, name):
        """"""Converts ""inputs"", possibly typed mxnet NDArray, a numpy ndarray, other python types,
        to the given type
        """"""
        if hasattr(inputs, ""asscalar""):
            inputs = inputs.asscalar()
        try:
            inputs = type_(inputs)
        except:
            raise ValueError(""Cannot convert %s to python %s"" % (name, type_.__name__))
        return inputs

    def _cond_wrapper(loop_vars):
        result = cond(*loop_vars)
        if not isinstance(result, Symbol):
            raise ValueError(""Return of cond must be a Symbol"")
        return [], [result], [], []

    def _func_wrapper(loop_vars):
        """"""This wrapper unifies
             ""func: loop_vars -> new_loop_vars""
         and ""func: loop_vars -> (step_output, new_loop_vars)""
        into ""func: loop_vars -> (list of step_outputs, tuple of new_loop_vars)
        """"""
        step_output, new_loop_vars = func(*loop_vars)
        if step_output is None:
            step_output = []
        if new_loop_vars is None:
            new_loop_vars = []
        if isinstance(step_output, tuple):
            step_output = list(step_output)
        if isinstance(new_loop_vars, tuple):
            new_loop_vars = list(new_loop_vars)
        step_output, out_fmt = _flatten(step_output, ""while output"")
        new_loop_vars, var_fmt = _flatten(new_loop_vars, ""while loop_vars"")
        if len(loop_vars) != len(new_loop_vars):
            raise ValueError(""The number of loop_vars should be consistent during the loop"")
        return step_output, new_loop_vars, out_fmt, var_fmt

    def _create_subgraph(graph_vars, graph_func, subgraph_name):
        subgraph_name = _get_unique_subgraph_name(subgraph_name)
        with AttrScope(__subgraph_name__=subgraph_name):
            # create new variables with the same name,
            # them feed them to the given func
            graph_vars, var_fmt = _flatten(graph_vars, ""while loop_vars"")
            new_graph_vars = [symbol.var(_get_sym_uniq_name(sym)) for sym in graph_vars]
            new_graph_vars, _ = _regroup(new_graph_vars, var_fmt)
            outputs, final_state, out_fmt, var_fmt = graph_func(new_graph_vars)
            # first `num_out_data` elements belong to `outputs`
            # other elements belong to `final_state`
            num_out_data = len(outputs)
            num_outputs = len(outputs) + len(final_state)
            # nnvm cut-graph does not allow inputs and outputs overlap
            # so we calculate the name of inputs, and copy outputs once it overlaps with inputs
            # group all outputs of graph_func
            all_input_names = symbol.Group(outputs + final_state).list_inputs()
            in_input = lambda x: x.name in all_input_names
            in_graph = lambda x: x.list_attr().get(""__subgraph_name__"", """") == subgraph_name
            make_identity = lambda x: symbol.op.identity(x) if in_input(x) or not in_graph(x) \
                                      else x
            graph = symbol.Group(list(map(make_identity, outputs + final_state)))
        return graph, num_out_data, num_outputs, out_fmt, var_fmt

    flatten_loop_vars, init_loop_var_fmt = _flatten(loop_vars, ""while loop_vars"")
    _check_data(flatten_loop_vars, symbol.Symbol,
                ""loop_vars should be a symbol or a nested list of symbols"")

    def _union_inputs(*graphs):
        # Given a list of graphs, each whose inputs are either from loop_vars or other variables.
        # 1) calculate a list `inputs`, the union of their inputs.
        # 2) for each graph, determine in which indices their inputs reside in `inputs`
        # 3) for each variable in the input of `graph`, find which index it is
        inputs = []             # List[Symbol], result of 1)
        locs = []               # List[Tuple(List[Int], List[Int])], a list of tuples,
                                # where tuples are results of 2) and 3)
        input_id_to_loc = {}    # Dict[int, int], given id(sym), input_id_to_loc maps it
                                # to a `loc`, where inputs[loc] = sym
        for graph in graphs:
            # some loop_vars are inputs to `graph`, some are not
            name_to_loop_vars = {_get_sym_uniq_name(sym): sym for sym in flatten_loop_vars}
            # other inputs to `graph` created by cut_graph
            name_to_cut_g_syms = {sym.list_outputs()[0]: sym for sym in _cut_subgraph(graph)}
            # input_syms: all inputs to the `graph`
            name_to_input_syms = {sym.name: sym for sym in _get_graph_inputs(graph)}
            # also we collect the mapping from var's name to var's loc in loop_vars
            name_to_var_locs = {_get_sym_uniq_name(sym): i for i, sym in enumerate(flatten_loop_vars)}
            # collect arguments for each subgraph
            input_locs = []                         # results from the second step
            var_locs = [-1] * len(flatten_loop_vars)        # results from the third step
            subg_input_names = graph.list_inputs()
            assert len(set(subg_input_names)) == len(subg_input_names), \
                    ""The inputs of the subgraph don't have unique names: "" + str(subg_input_names)
            for name in subg_input_names:
                assert name in name_to_input_syms   # it should obviously hold
                # name -> sym
                if name in name_to_loop_vars:
                    sym = name_to_loop_vars[name]
                elif name in name_to_cut_g_syms:
                    sym = name_to_cut_g_syms[name]
                else:
                    sym = copy.deepcopy(name_to_input_syms[name])
                # do 2), and 1) is implicitly done
                if id(sym) in input_id_to_loc:
                    loc = input_id_to_loc[id(sym)]
                else:
                    loc = len(input_id_to_loc)
                    inputs.append(sym)
                    input_id_to_loc[id(sym)] = loc
                input_locs.append(loc)
                # do 3)
                if name in name_to_var_locs:
                    var_locs[name_to_var_locs[name]] = len(input_locs) - 1
            locs.append((input_locs, var_locs))
        return inputs, locs
    if max_iterations is None:
        raise ValueError(""max_iterations should be specified"")
    max_iterations = _to_python_scalar(max_iterations, int, ""max_iteration"")
    # It should be work as fine if loop_vars are empty I guess,
    # but it is semantically unnecessary to include this case.
    if len(loop_vars) == 0:
        raise ValueError(""loop_vars should contain at least one element"")
    # create graph for `cond'
    cond_g, num_out_data, num_outputs, _, _ = \
        _create_subgraph(loop_vars, _cond_wrapper, name + ""_cond"")
    assert num_out_data == 0
    assert num_outputs == 1
    # create graph for `func`
    func_g, num_out_data, num_outputs, out_fmt, _ = \
        _create_subgraph(loop_vars, _func_wrapper, name + ""_func"")
    # find symbols used in either cond_g or func_g
    input_syms, ((cond_input_locs, _), (func_input_locs, func_var_locs)) = \
        _union_inputs(cond_g, func_g)
    for i_th, loc in enumerate(func_var_locs, 1):
        if loc == -1:
            raise ValueError(""The %d-th loop_var doesn't involve into the computation"" % i_th)
    result = symbol._internal._while_loop(
        cond_g,
        func_g,
        *input_syms,
        max_iterations=max_iterations,
        cond_input_locs=cond_input_locs,
        func_input_locs=func_input_locs,
        func_var_locs=func_var_locs,
        num_out_data=num_out_data,
        num_outputs=num_outputs
    )
    outputs = [result[i] for i in range(num_out_data)]
    outputs, _ = _regroup(outputs, out_fmt)
    final_loop_vars = [result[i] for i in range(num_out_data, num_outputs)]
    final_loop_vars, _ = _regroup(final_loop_vars, init_loop_var_fmt)
    return outputs, final_loop_vars"
kernels_list_cli;"def kernels_list_cli(self,
                         mine=False,
                         page=1,
                         page_size=20,
                         search=None,
                         csv_display=False,
                         parent=None,
                         competition=None,
                         dataset=None,
                         user=None,
                         language=None,
                         kernel_type=None,
                         output_type=None,
                         sort_by=None):
        """""" client wrapper for kernels_list, see this function for arguments.
            Additional arguments are provided here.
             Parameters
            ==========
            csv_display: if True, print comma separated values instead of table
        """"""
        kernels = self.kernels_list(
            page=page,
            page_size=page_size,
            search=search,
            mine=mine,
            dataset=dataset,
            competition=competition,
            parent_kernel=parent,
            user=user,
            language=language,
            kernel_type=kernel_type,
            output_type=output_type,
            sort_by=sort_by)
        fields = ['ref', 'title', 'author', 'lastRunTime', 'totalVotes']
        if kernels:
            if csv_display:
                self.print_csv(kernels, fields)
            else:
                self.print_table(kernels, fields)
        else:
            print('No kernels found')"
set_elapsed_time_visible;"def set_elapsed_time_visible(self, state):
        """"""Slot to show/hide elapsed time label.""""""
        self.show_elapsed_time = state
        if self.time_label is not None:
            self.time_label.setVisible(state)"
manual_shuffle;"def manual_shuffle(self, axis, shuffle_func, lengths):
        """"""Shuffle the partitions based on the `shuffle_func`.

        Args:
            axis: The axis to shuffle across.
            shuffle_func: The function to apply before splitting the result.
            lengths: The length of each partition to split the result into.

        Returns:
             A new BaseFrameManager object, the type of object that called this.
        """"""
        if axis:
            partitions = self.row_partitions
        else:
            partitions = self.column_partitions
        func = self.preprocess_func(shuffle_func)
        result = np.array([part.shuffle(func, lengths) for part in partitions])
        return self.__constructor__(result) if axis else self.__constructor__(result.T)"
log_power;"def log_power(x, a, b, c):
    """"""""logistic power

    Parameters
    ----------
    x: int
    a: float
    b: float
    c: float

    Returns
    -------
    float
        a/(1.+(x/np.exp(b))**c)
    """"""
    return a/(1.+(x/np.exp(b))**c)"
remap_variables;"def remap_variables(fn):
    """"""
    Use fn to map the output of any variable getter.

    Args:
        fn (tf.Variable -> tf.Tensor)

    Returns:
        The current variable scope with a custom_getter that maps
        all the variables by fn.

    Example:
        .. code-block:: python

            with varreplace.remap_variables(lambda var: quantize(var)):
                x = FullyConnected('fc', x, 1000)   # fc/{W,b} will be quantized
    """"""
    def custom_getter(getter, *args, **kwargs):
        v = getter(*args, **kwargs)
        return fn(v)
    return custom_getter_scope(custom_getter)"
process_line;"def process_line(
    line,  # type: Text
    filename,  # type: str
    line_number,  # type: int
    finder=None,  # type: Optional[PackageFinder]
    comes_from=None,  # type: Optional[str]
    options=None,  # type: Optional[optparse.Values]
    session=None,  # type: Optional[PipSession]
    wheel_cache=None,  # type: Optional[WheelCache]
    use_pep517=None,  # type: Optional[bool]
    constraint=False  # type: bool
):
    # type: (...) -> Iterator[InstallRequirement]
    """"""Process a single requirements line; This can result in creating/yielding
    requirements, or updating the finder.

    For lines that contain requirements, the only options that have an effect
    are from SUPPORTED_OPTIONS_REQ, and they are scoped to the
    requirement. Other options from SUPPORTED_OPTIONS may be present, but are
    ignored.

    For lines that do not contain requirements, the only options that have an
    effect are from SUPPORTED_OPTIONS. Options from SUPPORTED_OPTIONS_REQ may
    be present, but are ignored. These lines may contain multiple options
    (although our docs imply only one is supported), and all our parsed and
    affect the finder.

    :param constraint: If True, parsing a constraints file.
    :param options: OptionParser options that we may update
    """"""
    parser = build_parser(line)
    defaults = parser.get_default_values()
    defaults.index_url = None
    if finder:
        defaults.format_control = finder.format_control
    args_str, options_str = break_args_options(line)
    # Prior to 2.7.3, shlex cannot deal with unicode entries
    if sys.version_info < (2, 7, 3):
        # https://github.com/python/mypy/issues/1174
        options_str = options_str.encode('utf8')  # type: ignore
    # https://github.com/python/mypy/issues/1174
    opts, _ = parser.parse_args(
        shlex.split(options_str), defaults)  # type: ignore

    # preserve for the nested code path
    line_comes_from = '%s %s (line %s)' % (
        '-c' if constraint else '-r', filename, line_number,
    )

    # yield a line requirement
    if args_str:
        isolated = options.isolated_mode if options else False
        if options:
            cmdoptions.check_install_build_global(options, opts)
        # get the options that apply to requirements
        req_options = {}
        for dest in SUPPORTED_OPTIONS_REQ_DEST:
            if dest in opts.__dict__ and opts.__dict__[dest]:
                req_options[dest] = opts.__dict__[dest]
        yield install_req_from_line(
            args_str, line_comes_from, constraint=constraint,
            use_pep517=use_pep517,
            isolated=isolated, options=req_options, wheel_cache=wheel_cache
        )

    # yield an editable requirement
    elif opts.editables:
        isolated = options.isolated_mode if options else False
        yield install_req_from_editable(
            opts.editables[0], comes_from=line_comes_from,
            use_pep517=use_pep517,
            constraint=constraint, isolated=isolated, wheel_cache=wheel_cache
        )

    # parse a nested requirements file
    elif opts.requirements or opts.constraints:
        if opts.requirements:
            req_path = opts.requirements[0]
            nested_constraint = False
        else:
            req_path = opts.constraints[0]
            nested_constraint = True
        # original file is over http
        if SCHEME_RE.search(filename):
            # do a url join so relative paths work
            req_path = urllib_parse.urljoin(filename, req_path)
        # original file and nested file are paths
        elif not SCHEME_RE.search(req_path):
            # do a join so relative paths work
            req_path = os.path.join(os.path.dirname(filename), req_path)
        # TODO: Why not use `comes_from='-r {} (line {})'` here as well?
        parsed_reqs = parse_requirements(
            req_path, finder, comes_from, options, session,
            constraint=nested_constraint, wheel_cache=wheel_cache
        )
        for req in parsed_reqs:
            yield req

    # percolate hash-checking option upward
    elif opts.require_hashes:
        options.require_hashes = opts.require_hashes

    # set finder options
    elif finder:
        if opts.index_url:
            finder.index_urls = [opts.index_url]
        if opts.no_index is True:
            finder.index_urls = []
        if opts.extra_index_urls:
            finder.index_urls.extend(opts.extra_index_urls)
        if opts.find_links:
            # FIXME: it would be nice to keep track of the source
            # of the find_links: support a find-links local path
            # relative to a requirements file.
            value = opts.find_links[0]
            req_dir = os.path.dirname(os.path.abspath(filename))
            relative_to_reqs_file = os.path.join(req_dir, value)
            if os.path.exists(relative_to_reqs_file):
                value = relative_to_reqs_file
            finder.find_links.append(value)
        if opts.pre:
            finder.allow_all_prereleases = True
        if opts.trusted_hosts:
            finder.secure_origins.extend(
                (""*"", host, ""*"") for host in opts.trusted_hosts)"
detect_fold_level;"def detect_fold_level(self, prev_block, block):
        """"""
        Detects fold level by looking at the block indentation.

        :param prev_block: previous text block
        :param block: current block to highlight
        """"""
        text = block.text()
        prev_lvl = TextBlockHelper().get_fold_lvl(prev_block)
        # round down to previous indentation guide to ensure contiguous block
        # fold level evolution.
        indent_len = 0
        if (prev_lvl and prev_block is not None and
           not self.editor.is_comment(prev_block)):
            # ignore commented lines (could have arbitary indentation)
            prev_text = prev_block.text()
            indent_len = (len(prev_text) - len(prev_text.lstrip())) // prev_lvl
        if indent_len == 0:
            indent_len = len(self.editor.indent_chars)

        return (len(text) - len(text.lstrip())) // indent_len"
run_main;"def run_main():
  """"""Initializes flags and calls main().""""""
  program.setup_environment()

  if getattr(tf, '__version__', 'stub') == 'stub':
    print(""TensorFlow installation not found - running with reduced feature set."",
          file=sys.stderr)

  tensorboard = program.TensorBoard(default.get_plugins(),
                                    program.get_default_assets_zip_provider())
  try:
    from absl import app
    # Import this to check that app.run() will accept the flags_parser argument.
    from absl.flags import argparse_flags
    app.run(tensorboard.main, flags_parser=tensorboard.configure)
    raise AssertionError(""absl.app.run() shouldn't return"")
  except ImportError:
    pass
  except base_plugin.FlagsError as e:
    print(""Error: %s"" % e, file=sys.stderr)
    sys.exit(1)

  tensorboard.configure(sys.argv)
  sys.exit(tensorboard.main())"
find_all_matches;"def find_all_matches(finder, ireq, pre=False):
    # type: (PackageFinder, InstallRequirement, bool) -> List[InstallationCandidate]
    """"""Find all matching dependencies using the supplied finder and the
    given ireq.

    :param finder: A package finder for discovering matching candidates.
    :type finder: :class:`~pip._internal.index.PackageFinder`
    :param ireq: An install requirement.
    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`
    :return: A list of matching candidates.
    :rtype: list[:class:`~pip._internal.index.InstallationCandidate`]
    """"""


    candidates = clean_requires_python(finder.find_all_candidates(ireq.name))
    versions = {candidate.version for candidate in candidates}
    allowed_versions = _get_filtered_versions(ireq, versions, pre)
    if not pre and not allowed_versions:
        allowed_versions = _get_filtered_versions(ireq, versions, True)
    candidates = {c for c in candidates if c.version in allowed_versions}
    return candidates"
log_path;"def log_path(cls, project, log):
        """"""Return a fully-qualified log string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/logs/{log}"", project=project, log=log
        )"
unregister_editorstack;"def unregister_editorstack(self, editorstack):
        """"""Removing editorstack only if it's not the last remaining""""""
        self.remove_last_focus_editorstack(editorstack)
        if len(self.editorstacks) > 1:
            index = self.editorstacks.index(editorstack)
            self.editorstacks.pop(index)
            return True
        else:
            # editorstack was not removed!
            return False"
register_hook;"def register_hook(self, event, hook):
        """"""Properly register a hook.""""""

        if event not in self.hooks:
            raise ValueError('Unsupported event specified, with event name ""%s""' % (event))

        if isinstance(hook, Callable):
            self.hooks[event].append(hook)
        elif hasattr(hook, '__iter__'):
            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))"
get_random;"def get_random(self):
        """"""
        Returns a random statement from the database
        """"""
        Statement = self.get_model('statement')

        statement = Statement.objects.order_by('?').first()

        if statement is None:
            raise self.EmptyDatabaseException()

        return statement"
try_one_generator;"def try_one_generator (project, name, generator, target_type, properties, sources):
    """""" Checks if generator invocation can be pruned, because it's guaranteed
        to fail. If so, quickly returns empty list. Otherwise, calls
        try_one_generator_really.
    """"""
    if __debug__:
        from .targets import ProjectTarget
        assert isinstance(project, ProjectTarget)
        assert isinstance(name, basestring) or name is None
        assert isinstance(generator, Generator)
        assert isinstance(target_type, basestring)
        assert isinstance(properties, property_set.PropertySet)
        assert is_iterable_typed(sources, virtual_target.VirtualTarget)
    source_types = []

    for s in sources:
        source_types.append (s.type ())

    viable_source_types = viable_source_types_for_generator (generator)

    if source_types and viable_source_types != ['*'] and\
           not set_.intersection (source_types, viable_source_types):
        if project.manager ().logger ().on ():
            id = generator.id ()
            project.manager ().logger ().log (__name__, ""generator '%s' pruned"" % id)
            project.manager ().logger ().log (__name__, ""source_types"" '%s' % source_types)
            project.manager ().logger ().log (__name__, ""viable_source_types '%s'"" % viable_source_types)

        return []

    else:
        return try_one_generator_really (project, name, generator, target_type, properties, sources)"
compute_certificate;"def compute_certificate(self, current_step, feed_dictionary):
    """""" Function to compute the certificate based either current value
    or dual variables loaded from dual folder """"""
    feed_dict = feed_dictionary.copy()
    nu = feed_dict[self.nu]
    second_term = self.make_m_psd(nu, feed_dict)
    tf.logging.info('Nu after modifying: ' + str(second_term))
    feed_dict.update({self.nu: second_term})
    computed_certificate = self.sess.run(self.unconstrained_objective, feed_dict=feed_dict)

    tf.logging.info('Inner step: %d, current value of certificate: %f',
                    current_step, computed_certificate)

    # Sometimes due to either overflow or instability in inverses,
    # the returned certificate is large and negative -- keeping a check
    if LOWER_CERT_BOUND < computed_certificate < 0:
      _, min_eig_val_m = self.get_lanczos_eig(feed_dict=feed_dict)
      tf.logging.info('min eig val from lanczos: ' + str(min_eig_val_m))
      input_vector_m = tf.placeholder(tf.float32, shape=(self.matrix_m_dimension, 1))
      output_vector_m = self.get_psd_product(input_vector_m)

      def np_vector_prod_fn_m(np_vector):
        np_vector = np.reshape(np_vector, [-1, 1])
        feed_dict.update({input_vector_m:np_vector})
        output_np_vector = self.sess.run(output_vector_m, feed_dict=feed_dict)
        return output_np_vector
      linear_operator_m = LinearOperator((self.matrix_m_dimension,
                                          self.matrix_m_dimension),
                                         matvec=np_vector_prod_fn_m)
      # Performing shift invert scipy operation when eig val estimate is available
      min_eig_val_m_scipy, _ = eigs(linear_operator_m, k=1, which='SR', tol=TOL)

      tf.logging.info('min eig val m from scipy: ' + str(min_eig_val_m_scipy))

      if min_eig_val_m - TOL > 0:
        tf.logging.info('Found certificate of robustness!')
        return True

    return False"
put_watch;"def put_watch(self, id, body=None, params=None):
        """"""
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_

        :arg id: Watch ID
        :arg body: The watch
        :arg active: Specify whether the watch is in/active by default
        :arg if_primary_term: only update the watch if the last operation that
            has changed the watch has the specified primary term
        :arg if_seq_no: only update the watch if the last operation that has
            changed the watch has the specified sequence number
        :arg version: Explicit version number for concurrency control
        """"""
        if id in SKIP_IN_PATH:
            raise ValueError(""Empty value passed for a required argument 'id'."")
        return self.transport.perform_request(
            ""PUT"", _make_path(""_watcher"", ""watch"", id), params=params, body=body
        )"
get_profile;"def get_profile(
        self,
        name,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Gets the specified profile.

        Example:
            >>> from google.cloud import talent_v4beta1
            >>>
            >>> client = talent_v4beta1.ProfileServiceClient()
            >>>
            >>> name = client.profile_path('[PROJECT]', '[TENANT]', '[PROFILE]')
            >>>
            >>> response = client.get_profile(name)

        Args:
            name (str): Required.

                Resource name of the profile to get.

                The format is
                ""projects/{project\_id}/tenants/{tenant\_id}/profiles/{profile\_id}"",
                for example, ""projects/api-test-project/tenants/foo/profiles/bar"".
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.talent_v4beta1.types.Profile` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""get_profile"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""get_profile""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.get_profile,
                default_retry=self._method_configs[""GetProfile""].retry,
                default_timeout=self._method_configs[""GetProfile""].timeout,
                client_info=self._client_info,
            )

        request = profile_service_pb2.GetProfileRequest(name=name)
        return self._inner_api_calls[""get_profile""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
add_job;"def add_job(self, job):
        """"""Adds a new job into the cache.
        :param dict job: The job dictionary
        :returns: True
        """"""
        self.cur.execute(""INSERT INTO jobs VALUES(?,?,?,?,?)"", (
            job[""id""], job[""description""], job[""last-run""], job[""next-run""], job[""last-run-result""]))

        return True"
get_tensor;"def get_tensor(self, name):
        """"""
        Get a tensor in this tower. The name can be:

        1. The name of the tensor without any tower prefix.

        2. A name in the input signature, if it is used when building the tower.

        In the second case, this method will return the tensor that's used as the corresponding
        input to the tower. Note that this tensor may have a different name (e.g. may be an output of a queue).
        """"""
        name = get_op_tensor_name(name)[1]
        if len(self.ns_name):
            name_with_ns = self.ns_name + ""/"" + name
        else:
            name_with_ns = name

        try:
            ret = get_op_or_tensor_by_name(name_with_ns)
        except KeyError:
            if name in self._extra_tensor_names:
                return self._extra_tensor_names[name]
            raise
        else:
            if name in self._extra_tensor_names:
                mapped_tensor = self._extra_tensor_names[name]
                logger.info(
                    ""'{}' may refer to both the Tensor/Placeholder '{}' or the input to the tower '{}'."".format(
                        name, ret.name, mapped_tensor.name) +
                    "" Assuming it is the input '{}'."".format(mapped_tensor.name))
                return mapped_tensor
            return ret"
download_file;"def download_file(self, fname, output_dir):
    """"""Downloads competition file to output_dir.""""""
    if fname not in self.competition_files:  # pylint: disable=unsupported-membership-test
      raise ValueError(""%s is not one of the competition's ""
                       ""files: %s"" % (fname, self.competition_files))
    command = [
        ""kaggle"",
        ""competitions"",
        ""download"",
        ""--file"",
        fname,
        ""--path"",
        output_dir,
        ""-c"",
        self._competition_name,
    ]
    _run_kaggle_command(command, self._competition_name)
    return os.path.join(output_dir, fname)"
sources_remove;"def sources_remove(source_uri, ruby=None, runas=None, gem_bin=None):
    '''
    Remove a gem source.

    :param source_uri: string
        The source URI to remove.
    :param gem_bin: string : None
        Full path to ``gem`` binary to use.
    :param ruby: string : None
        If RVM or rbenv are installed, the ruby version and gemset to use.
        Ignored if ``gem_bin`` is specified.
    :param runas: string : None
        The user to run gem as.

    CLI Example:

    .. code-block:: bash

        salt '*' gem.sources_remove http://rubygems.org/
    '''
    return _gem(['sources', '--remove', source_uri],
                ruby,
                gem_bin=gem_bin,
                runas=runas)"
delete_collection_namespaced_role;"def delete_collection_namespaced_role(self, namespace, **kwargs):
        """"""
        delete collection of Role
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_collection_namespaced_role(namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_collection_namespaced_role_with_http_info(namespace, **kwargs)
        else:
            (data) = self.delete_collection_namespaced_role_with_http_info(namespace, **kwargs)
            return data"
num_gpus;"def num_gpus():
    """"""Query CUDA for the number of GPUs present.

    Raises
    ------
    Will raise an exception on any CUDA error.

    Returns
    -------
    count : int
        The number of GPUs.

    """"""
    count = ctypes.c_int()
    check_call(_LIB.MXGetGPUCount(ctypes.byref(count)))
    return count.value"
select_month;"def select_month(self, month):
        """"""
        选择月份

        @2018/06/03 pandas 的索引问题导致
        https://github.com/pandas-dev/pandas/issues/21299

        因此先用set_index去重做一次index
        影响的有selects,select_time,select_month,get_bar

        @2018/06/04
        当选择的时间越界/股票不存在,raise ValueError

        @2018/06/04 pandas索引问题已经解决
        全部恢复
        """"""

        def _select_month(month):
            return self.data.loc[month, slice(None)]

        try:
            return self.new(_select_month(month), self.type, self.if_fq)
        except:
            raise ValueError('QA CANNOT GET THIS Month {} '.format(month))"
register_instances;"def register_instances(name, instances, region=None, key=None, keyid=None,
                       profile=None):
    '''
    Add EC2 instance(s) to an Elastic Load Balancer. Removing an instance from
    the ``instances`` list does not remove it from the ELB.

    name
        The name of the Elastic Load Balancer to add EC2 instances to.

    instances
        A list of EC2 instance IDs that this Elastic Load Balancer should
        distribute traffic to. This state will only ever append new instances
        to the ELB. EC2 instances already associated with this ELB will not be
        removed if they are not in the ``instances`` list.

    .. versionadded:: 2015.8.0

    .. code-block:: yaml

        add-instances:
          boto_elb.register_instances:
            - name: myloadbalancer
            - instances:
              - instance-id1
              - instance-id2
    '''
    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}
    lb = __salt__['boto_elb.exists'](name, region, key, keyid, profile)
    if not lb:
        msg = 'Could not find lb {0}'.format(name)
        log.error(msg)
        ret.update({'comment': msg, 'result': False})
        return ret

    health = __salt__['boto_elb.get_instance_health'](
            name, region, key, keyid, profile)
    nodes = [value['instance_id'] for value in health
             if value['description'] != 'Instance deregistration currently in progress.']
    new = [value for value in instances if value not in nodes]
    if not new:
        msg = 'Instance/s {0} already exist.'.format(six.text_type(instances).strip('[]'))
        log.debug(msg)
        ret.update({'comment': msg})
        return ret

    if __opts__['test']:
        ret['comment'] = 'ELB {0} is set to register : {1}.'.format(name, new)
        ret['result'] = None
        return ret

    state = __salt__['boto_elb.register_instances'](
            name, instances, region, key, keyid, profile)
    if state:
        msg = 'Load Balancer {0} has been changed'.format(name)
        log.info(msg)
        new = set().union(nodes, instances)
        ret.update({'comment': msg, 'changes': {'old': '\n'.join(nodes),
                                                'new': '\n'.join(list(new))}})
    else:
        msg = 'Load balancer {0} failed to add instances'.format(name)
        log.error(msg)
        ret.update({'comment': msg, 'result': False})
    return ret"
create_shortcuts;"def create_shortcuts(self, parent):
        """"""Create shortcuts for this widget""""""
        # Configurable
        findnext = config_shortcut(self.find_next, context='_',
                                   name='Find next', parent=parent)
        findprev = config_shortcut(self.find_previous, context='_',
                                   name='Find previous', parent=parent)
        togglefind = config_shortcut(self.show, context='_',
                                     name='Find text', parent=parent)
        togglereplace = config_shortcut(self.show_replace,
                                        context='_', name='Replace text',
                                        parent=parent)
        hide = config_shortcut(self.hide, context='_', name='hide find and replace',
                               parent=self)

        return [findnext, findprev, togglefind, togglereplace, hide]"
set_fstab;"def set_fstab(
        name,
        device,
        fstype,
        opts='defaults',
        dump=0,
        pass_num=0,
        config='/etc/fstab',
        test=False,
        match_on='auto',
        **kwargs):
    '''
    Verify that this mount is represented in the fstab, change the mount
    to match the data passed, or add the mount if it is not present.

    CLI Example:

    .. code-block:: bash

        salt '*' mount.set_fstab /mnt/foo /dev/sdz1 ext4
    '''

    # Fix the opts type if it is a list
    if isinstance(opts, list):
        opts = ','.join(opts)

    # preserve arguments for updating
    entry_args = {
        'name': name,
        'device': device.replace('\\ ', '\\040'),
        'fstype': fstype,
        'opts': opts.replace('\\ ', '\\040'),
        'dump': dump,
        'pass_num': pass_num,
    }

    lines = []
    ret = None

    # Transform match_on into list--items will be checked later
    if isinstance(match_on, list):
        pass
    elif not isinstance(match_on, six.string_types):
        msg = 'match_on must be a string or list of strings'
        raise CommandExecutionError(msg)
    elif match_on == 'auto':
        # Try to guess right criteria for auto....
        # NOTE: missing some special fstypes here
        specialFSes = frozenset([
            'none',
            'tmpfs',
            'sysfs',
            'proc',
            'fusectl',
            'debugfs',
            'securityfs',
            'devtmpfs',
            'cgroup',
            'btrfs'])

        if fstype in specialFSes:
            match_on = ['name']
        else:
            match_on = ['device']
    else:
        match_on = [match_on]

    # generate entry and criteria objects, handle invalid keys in match_on
    entry = _fstab_entry(**entry_args)
    try:
        criteria = entry.pick(match_on)

    except KeyError:
        filterFn = lambda key: key not in _fstab_entry.fstab_keys
        invalid_keys = filter(filterFn, match_on)

        msg = 'Unrecognized keys in match_on: ""{0}""'.format(invalid_keys)
        raise CommandExecutionError(msg)

    # parse file, use ret to cache status
    if not os.path.isfile(config):
        raise CommandExecutionError('Bad config file ""{0}""'.format(config))

    try:
        with salt.utils.files.fopen(config, 'r') as ifile:
            for line in ifile:
                line = salt.utils.stringutils.to_unicode(line)
                try:
                    if criteria.match(line):
                        # Note: If ret isn't None here,
                        # we've matched multiple lines
                        ret = 'present'
                        if entry.match(line):
                            lines.append(line)
                        else:
                            ret = 'change'
                            lines.append(six.text_type(entry))
                    else:
                        lines.append(line)

                except _fstab_entry.ParseError:
                    lines.append(line)

    except (IOError, OSError) as exc:
        msg = 'Couldn\'t read from {0}: {1}'
        raise CommandExecutionError(msg.format(config, exc))

    # add line if not present or changed
    if ret is None:
        lines.append(six.text_type(entry))
        ret = 'new'

    if ret != 'present':  # ret in ['new', 'change']:
        if not salt.utils.args.test_mode(test=test, **kwargs):
            try:
                with salt.utils.files.fopen(config, 'wb') as ofile:
                    # The line was changed, commit it!
                    ofile.writelines(salt.utils.data.encode(lines))
            except (IOError, OSError):
                msg = 'File not writable {0}'
                raise CommandExecutionError(msg.format(config))

    return ret"
remove_event_source;"def remove_event_source(event_source, lambda_arn, target_function, boto_session, dry=False):
    """"""
    Given an event_source dictionary, create the object and remove the event source.
    """"""

    event_source_obj, ctx, funk = get_event_source(event_source, lambda_arn, target_function, boto_session, dry=False)

    # This is slightly dirty, but necessary for using Kappa this way.
    funk.arn = lambda_arn
    if not dry:
        rule_response = event_source_obj.remove(funk)
        return rule_response
    else:
        return event_source_obj"
process_commands;"async def process_commands(self, message):
        """"""|coro|

        This function processes the commands that have been registered
        to the bot and other groups. Without this coroutine, none of the
        commands will be triggered.

        By default, this coroutine is called inside the :func:`.on_message`
        event. If you choose to override the :func:`.on_message` event, then
        you should invoke this coroutine as well.

        This is built using other low level tools, and is equivalent to a
        call to :meth:`~.Bot.get_context` followed by a call to :meth:`~.Bot.invoke`.

        This also checks if the message's author is a bot and doesn't
        call :meth:`~.Bot.get_context` or :meth:`~.Bot.invoke` if so.

        Parameters
        -----------
        message: :class:`discord.Message`
            The message to process commands for.
        """"""
        if message.author.bot:
            return

        ctx = await self.get_context(message)
        await self.invoke(ctx)"
get_supported_languages;"def get_supported_languages(
        self,
        parent=None,
        display_language_code=None,
        model=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Returns a list of supported languages for translation.

        Example:
            >>> from google.cloud import translate_v3beta1
            >>>
            >>> client = translate_v3beta1.TranslationServiceClient()
            >>>
            >>> response = client.get_supported_languages()

        Args:
            parent (str): Optional. Used for making regionalized calls.
                Format: projects/{project-id}/locations/{location-id}.
                For global calls, use projects/{project-id}/locations/global.
                If missing, the call is treated as a global call.

                Only custom model within the same location-id can be used.
                Otherwise 400 is returned.
            display_language_code (str): Optional. The language to use to return localized, human readable names
                of supported languages. If missing, default language is ENGLISH.
            model (str): Optional. Get supported languages of this model.
                The format depends on model type:
                1. Custom models:
                projects/{project-id}/locations/{location-id}/models/{model-id}.
                2. General (built-in) models:
                projects/{project-id}/locations/{location-id}/models/general/nmt
                projects/{project-id}/locations/{location-id}/models/general/base
                Returns languages supported by the specified model.
                If missing, we get supported languages of Google general NMT model.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.translate_v3beta1.types.SupportedLanguages` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""get_supported_languages"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""get_supported_languages""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.get_supported_languages,
                default_retry=self._method_configs[""GetSupportedLanguages""].retry,
                default_timeout=self._method_configs[""GetSupportedLanguages""].timeout,
                client_info=self._client_info,
            )

        request = translation_service_pb2.GetSupportedLanguagesRequest(
            parent=parent, display_language_code=display_language_code, model=model
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""get_supported_languages""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
update_zone;"def update_zone(zone_id, domain, profile, type='master', ttl=None):
    '''
    Update an existing zone.

    :param zone_id: Zone ID to update.
    :type  zone_id: ``str``

    :param domain: Zone domain name (e.g. example.com)
    :type  domain: ``str``

    :param profile: The profile key
    :type  profile: ``str``

    :param type: Zone type (master / slave).
    :type  type: ``str``

    :param ttl: TTL for new records. (optional)
    :type  ttl: ``int``

    CLI Example:

    .. code-block:: bash

        salt myminion libcloud_dns.update_zone google.com google.com profile1 type=slave
    '''
    conn = _get_driver(profile=profile)
    zone = conn.get_zone(zone_id)
    return _simple_zone(conn.update_zone(zone=zone, domain=domain, type=type, ttl=ttl))"
dqn_sym_nips;"def dqn_sym_nips(action_num, data=None, name='dqn'):
    """"""Structure of the Deep Q Network in the NIPS 2013 workshop paper:
    Playing Atari with Deep Reinforcement Learning (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)

    Parameters
    ----------
    action_num : int
    data : mxnet.sym.Symbol, optional
    name : str, optional
    """"""
    if data is None:
        net = mx.symbol.Variable('data')
    else:
        net = data
    net = mx.symbol.Convolution(data=net, name='conv1', kernel=(8, 8), stride=(4, 4), num_filter=16)
    net = mx.symbol.Activation(data=net, name='relu1', act_type=""relu"")
    net = mx.symbol.Convolution(data=net, name='conv2', kernel=(4, 4), stride=(2, 2), num_filter=32)
    net = mx.symbol.Activation(data=net, name='relu2', act_type=""relu"")
    net = mx.symbol.Flatten(data=net)
    net = mx.symbol.FullyConnected(data=net, name='fc3', num_hidden=256)
    net = mx.symbol.Activation(data=net, name='relu3', act_type=""relu"")
    net = mx.symbol.FullyConnected(data=net, name='fc4', num_hidden=action_num)
    net = mx.symbol.Custom(data=net, name=name, op_type='DQNOutput')
    return net"
convert_concat;"def convert_concat(net, node, module, builder):
    """"""Convert concat layer from mxnet to coreml.

    Parameters
    ----------
    network: net
        A mxnet network object.

    layer: node
        Node to convert.

    module: module
        An module for MXNet

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_names, output_name = _get_input_output_name(net, node, 'all')
    name = node['name']
    mode = 'CONCAT'
    builder.add_elementwise(name = name, input_names = input_names,
            output_name = output_name, mode = mode)"
safe_invoke_callback;"def safe_invoke_callback(callback, *args, **kwargs):
    """"""Invoke a callback, swallowing and logging any exceptions.""""""
    # pylint: disable=bare-except
    # We intentionally want to swallow all exceptions.
    try:
        return callback(*args, **kwargs)
    except Exception:
        _LOGGER.exception(""Error while executing Future callback."")"
data_transforms_mnist;"def data_transforms_mnist(args, mnist_mean=None, mnist_std=None):
    """""" data_transforms for mnist dataset
    """"""
    if mnist_mean is None:
        mnist_mean = [0.5]

    if mnist_std is None:
        mnist_std = [0.5]

    train_transform = transforms.Compose(
        [
            transforms.RandomCrop(28, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mnist_mean, mnist_std),
        ]
    )
    if args.cutout:
        train_transform.transforms.append(Cutout(args.cutout_length))

    valid_transform = transforms.Compose(
        [transforms.ToTensor(), transforms.Normalize(mnist_mean, mnist_std)]
    )
    return train_transform, valid_transform"
parse_args;"def parse_args():
    """"""Parse command line arguments""""""
    parser = argparse.ArgumentParser()
    parser.add_argument(""font_path"", help=""Path to ttf font file or directory containing ttf files"")
    parser.add_argument(""--loss"", help=""'ctc' or 'warpctc' loss [Default 'ctc']"", default='ctc')
    parser.add_argument(""--cpu"",
                        help=""Number of CPUs for training [Default 8]. Ignored if --gpu is specified."",
                        type=int, default=8)
    parser.add_argument(""--gpu"", help=""Number of GPUs for training [Default 0]"", type=int)
    parser.add_argument(""--num_proc"", help=""Number CAPTCHA generating processes [Default 4]"", type=int, default=4)
    parser.add_argument(""--prefix"", help=""Checkpoint prefix [Default 'ocr']"", default='ocr')
    return parser.parse_args()"
remove_existing_pidfile;"def remove_existing_pidfile(pidfile_path):
    """""" Remove the named PID file if it exists.

        Removing a PID file that doesn't already exist puts us in the
        desired state, so we ignore the condition if the file does not
        exist.

        """"""
    try:
        os.remove(pidfile_path)
    except OSError as exc:
        if exc.errno == errno.ENOENT:
            pass
        else:
            raise"
register_plugin;"def register_plugin(self):
        """"""Register plugin in Spyder's main window""""""
        self.main.add_dockwidget(self)

        self.focus_changed.connect(self.main.plugin_focus_changed)
        self.edit_goto.connect(self.main.editor.load)
        self.edit_goto[str, int, str, bool].connect(
                         lambda fname, lineno, word, processevents:
                         self.main.editor.load(fname, lineno, word,
                                               processevents=processevents))
        self.main.editor.breakpoints_saved.connect(self.set_spyder_breakpoints)
        self.main.editor.run_in_current_ipyclient.connect(self.run_script)
        self.main.editor.run_cell_in_ipyclient.connect(self.run_cell)
        self.main.workingdirectory.set_current_console_wd.connect(
                                     self.set_current_client_working_directory)

        self.tabwidget.currentChanged.connect(self.update_working_directory)

        self._remove_old_stderr_files()"
status_load;"def status_load():
    '''
    Return load

    CLI Example:

    .. code-block:: bash

        salt '*' apcups.status_load
    '''
    data = status()
    if 'LOADPCT' in data:
        load = data['LOADPCT'].split()
        if load[1].lower() == 'percent':
            return float(load[0])

    return {'Error': 'Load not available.'}"
parse_pkginfo;"def parse_pkginfo(line, osarch=None):
    '''
    A small helper to parse an rpm/repoquery command's output. Returns a
    pkginfo namedtuple.
    '''
    try:
        name, epoch, version, release, arch, repoid, install_time = line.split('_|-')
    # Handle unpack errors (should never happen with the queryformat we are
    # using, but can't hurt to be careful).
    except ValueError:
        return None

    name = resolve_name(name, arch, osarch)
    if release:
        version += '-{0}'.format(release)
    if epoch not in ('(none)', '0'):
        version = ':'.join((epoch, version))

    if install_time not in ('(none)', '0'):
        install_date = datetime.datetime.utcfromtimestamp(int(install_time)).isoformat() + ""Z""
        install_date_time_t = int(install_time)
    else:
        install_date = None
        install_date_time_t = None

    return pkginfo(name, version, arch, repoid, install_date, install_date_time_t)"
delete_tagging;"def delete_tagging(Bucket,
           region=None, key=None, keyid=None, profile=None):
    '''
    Delete the tags from the given bucket

    Returns {deleted: true} if tags were deleted and returns
    {deleted: False} if tags were not deleted.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_s3_bucket.delete_tagging my_bucket

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        conn.delete_bucket_tagging(Bucket=Bucket)
        return {'deleted': True, 'name': Bucket}
    except ClientError as e:
        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"
put_user;"def put_user(self, username, body, params=None):
        """"""
        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html>`_

        :arg username: The username of the User
        :arg body: The user to add
        :arg refresh: If `true` (the default) then refresh the affected shards
            to make this operation visible to search, if `wait_for` then wait
            for a refresh to make this operation visible to search, if `false`
            then do nothing with refreshes., valid choices are: 'true', 'false',
            'wait_for'
        """"""
        for param in (username, body):
            if param in SKIP_IN_PATH:
                raise ValueError(""Empty value passed for a required argument."")
        return self.transport.perform_request(
            ""PUT"", _make_path(""_security"", ""user"", username), params=params, body=body
        )"
select_location;"def select_location(self):
        """"""Select directory.""""""
        location = osp.normpath(getexistingdirectory(self,
                                                     _(""Select directory""),
                                                     self.location))

        if location:
            if is_writable(location):
                self.location = location
                self.update_location()"
closing_plugin;"def closing_plugin(self, cancelable=False):
        """"""Perform actions before parent main window is closed""""""
        self.dialog_manager.close_all()
        self.shell.exit_interpreter()
        return True"
clean_requires_python;"def clean_requires_python(candidates):
    """"""Get a cleaned list of all the candidates with valid specifiers in the `requires_python` attributes.""""""
    all_candidates = []
    sys_version = ""."".join(map(str, sys.version_info[:3]))
    from packaging.version import parse as parse_version

    py_version = parse_version(os.environ.get(""PIP_PYTHON_VERSION"", sys_version))
    for c in candidates:
        from_location = attrgetter(""location.requires_python"")
        requires_python = getattr(c, ""requires_python"", from_location(c))
        if requires_python:
            # Old specifications had people setting this to single digits
            # which is effectively the same as '>=digit,<digit+1'
            if requires_python.isdigit():
                requires_python = "">={0},<{1}"".format(
                    requires_python, int(requires_python) + 1
                )
            try:
                specifierset = SpecifierSet(requires_python)
            except InvalidSpecifier:
                continue
            else:
                if not specifierset.contains(py_version):
                    continue
        all_candidates.append(c)
    return all_candidates"
sys_path;"def sys_path(self):
        """"""
        The system path inside the environment

        :return: The :data:`sys.path` from the environment
        :rtype: list
        """"""

        from .vendor.vistir.compat import JSONDecodeError
        current_executable = vistir.compat.Path(sys.executable).as_posix()
        if not self.python or self.python == current_executable:
            return sys.path
        elif any([sys.prefix == self.prefix, not self.is_venv]):
            return sys.path
        cmd_args = [self.python, ""-c"", ""import json, sys; print(json.dumps(sys.path))""]
        path, _ = vistir.misc.run(cmd_args, return_object=False, nospin=True, block=True, combine_stderr=False, write_to_stdout=False)
        try:
            path = json.loads(path.strip())
        except JSONDecodeError:
            path = sys.path
        return path"
power_configuration;"def power_configuration(name, policy=None, delayType=None, delayValue=None):
    '''
    Ensures that the power configuration is configured on the system. This is
    only available on some C-Series servers.

    .. versionadded:: 2019.2.0

    name: The name of the module function to execute.

    policy(str): The action to be taken when chassis power is restored after
    an unexpected power loss. This can be one of the following:

        reset: The server is allowed to boot up normally when power is
        restored. The server can restart immediately or, optionally, after a
        fixed or random delay.

        stay-off: The server remains off until it is manually restarted.

        last-state: The server restarts and the system attempts to restore
        any processes that were running before power was lost.

    delayType(str): If the selected policy is reset, the restart can be
    delayed with this option. This can be one of the following:

        fixed: The server restarts after a fixed delay.

        random: The server restarts after a random delay.

    delayValue(int): If a fixed delay is selected, once chassis power is
    restored and the Cisco IMC has finished rebooting, the system waits for
    the specified number of seconds before restarting the server. Enter an
    integer between 0 and 240.


    SLS Example:

    .. code-block:: yaml

        reset_power:
          cimc.power_configuration:
            - policy: reset
            - delayType: fixed
            - delayValue: 0

        power_off:
          cimc.power_configuration:
            - policy: stay-off


    '''

    ret = _default_ret(name)

    power_conf = __salt__['cimc.get_power_configuration']()

    req_change = False

    try:
        power_dict = power_conf['outConfigs']['biosVfResumeOnACPowerLoss'][0]

        if policy and power_dict['vpResumeOnACPowerLoss'] != policy:
            req_change = True
        elif policy == ""reset"":
            if power_dict['delayType'] != delayType:
                req_change = True
            elif power_dict['delayType'] == ""fixed"":
                if str(power_dict['delay']) != str(delayValue):
                    req_change = True
        else:
            ret['result'] = False
            ret['comment'] = ""The power policy must be specified.""
            return ret

        if req_change:

            update = __salt__['cimc.set_power_configuration'](policy,
                                                              delayType,
                                                              delayValue)

            if update['outConfig']['biosVfResumeOnACPowerLoss'][0]['status'] != 'modified':
                ret['result'] = False
                ret['comment'] = ""Error setting power configuration.""
                return ret

            ret['changes']['before'] = power_conf
            ret['changes']['after'] = __salt__['cimc.get_power_configuration']()
            ret['comment'] = ""Power settings modified.""
        else:
            ret['comment'] = ""Power settings already configured. No changes required.""

    except Exception as err:
        ret['result'] = False
        ret['comment'] = ""Error occurred setting power settings.""
        log.error(err)
        return ret

    ret['result'] = True

    return ret"
create_cert_binding;"def create_cert_binding(name, site, hostheader='', ipaddress='*', port=443, sslflags=0):
    '''
    Assign a certificate to an IIS binding.

    .. note:

        The web binding that the certificate is being assigned to must already exist.

    :param str name: The thumbprint of the certificate.
    :param str site: The IIS site name.
    :param str hostheader: The host header of the binding.
    :param str ipaddress: The IP address of the binding.
    :param str port: The TCP port of the binding.
    :param str sslflags: Flags representing certificate type and certificate storage of the binding.

    Example of usage with only the required arguments:

    .. code-block:: yaml

        site0-cert-binding:
            win_iis.create_cert_binding:
                - name: 9988776655443322111000AAABBBCCCDDDEEEFFF
                - site: site0

    Example of usage specifying all available arguments:

    .. code-block:: yaml

        site0-cert-binding:
            win_iis.create_cert_binding:
                - name: 9988776655443322111000AAABBBCCCDDDEEEFFF
                - site: site0
                - hostheader: site0.local
                - ipaddress: 192.168.1.199
                - port: 443
                - sslflags: 1

    .. versionadded:: 2016.11.0
    '''
    ret = {'name': name,
           'changes': {},
           'comment': str(),
           'result': None}

    binding_info = _get_binding_info(hostheader, ipaddress, port)
    current_cert_bindings = __salt__['win_iis.list_cert_bindings'](site)

    if binding_info in current_cert_bindings:
        current_name = current_cert_bindings[binding_info]['certificatehash']

        if name == current_name:
            ret['comment'] = 'Certificate binding already present: {0}'.format(name)
            ret['result'] = True
            return ret
        ret['comment'] = ('Certificate binding already present with a different'
                          ' thumbprint: {0}'.format(current_name))
        ret['result'] = False
    elif __opts__['test']:
        ret['comment'] = 'Certificate binding will be created: {0}'.format(name)
        ret['changes'] = {'old': None,
                          'new': name}
    else:
        ret['comment'] = 'Created certificate binding: {0}'.format(name)
        ret['changes'] = {'old': None,
                          'new': name}
        ret['result'] = __salt__['win_iis.create_cert_binding'](name, site, hostheader,
                                                                ipaddress, port, sslflags)
    return ret"
dataset_exists;"def dataset_exists(self, dataset):
        """"""Returns whether the given dataset exists.
        If regional location is specified for the dataset, that is also checked
        to be compatible with the remote dataset, otherwise an exception is thrown.

           :param dataset:
           :type dataset: BQDataset
        """"""

        try:
            response = self.client.datasets().get(projectId=dataset.project_id,
                                                  datasetId=dataset.dataset_id).execute()
            if dataset.location is not None:
                fetched_location = response.get('location')
                if dataset.location != fetched_location:
                    raise Exception('''Dataset already exists with regional location {}. Can't use {}.'''.format(
                        fetched_location if fetched_location is not None else 'unspecified',
                        dataset.location))

        except http.HttpError as ex:
            if ex.resp.status == 404:
                return False
            raise

        return True"
get_installed_distributions;"def get_installed_distributions(local_only=True,
                                skip=stdlib_pkgs,
                                include_editables=True,
                                editables_only=False,
                                user_only=False):
    # type: (bool, Container[str], bool, bool, bool) -> List[Distribution]
    """"""
    Return a list of installed Distribution objects.

    If ``local_only`` is True (default), only return installations
    local to the current virtualenv, if in a virtualenv.

    ``skip`` argument is an iterable of lower-case project names to
    ignore; defaults to stdlib_pkgs

    If ``include_editables`` is False, don't report editables.

    If ``editables_only`` is True , only report editables.

    If ``user_only`` is True , only report installations in the user
    site directory.

    """"""
    if local_only:
        local_test = dist_is_local
    else:
        def local_test(d):
            return True

    if include_editables:
        def editable_test(d):
            return True
    else:
        def editable_test(d):
            return not dist_is_editable(d)

    if editables_only:
        def editables_only_test(d):
            return dist_is_editable(d)
    else:
        def editables_only_test(d):
            return True

    if user_only:
        user_test = dist_in_usersite
    else:
        def user_test(d):
            return True

    # because of pkg_resources vendoring, mypy cannot find stub in typeshed
    return [d for d in pkg_resources.working_set  # type: ignore
            if local_test(d) and
            d.key not in skip and
            editable_test(d) and
            editables_only_test(d) and
            user_test(d)
            ]"
delete_events;"def delete_events(
        self,
        project_name,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Deletes all error events of a given project.

        Example:
            >>> from google.cloud import errorreporting_v1beta1
            >>>
            >>> client = errorreporting_v1beta1.ErrorStatsServiceClient()
            >>>
            >>> project_name = client.project_path('[PROJECT]')
            >>>
            >>> response = client.delete_events(project_name)

        Args:
            project_name (str): [Required] The resource name of the Google Cloud Platform project.
                Written as ``projects/`` plus the `Google Cloud Platform project
                ID <https://support.google.com/cloud/answer/6158840>`__. Example:
                ``projects/my-project-123``.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.errorreporting_v1beta1.types.DeleteEventsResponse` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""delete_events"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""delete_events""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.delete_events,
                default_retry=self._method_configs[""DeleteEvents""].retry,
                default_timeout=self._method_configs[""DeleteEvents""].timeout,
                client_info=self._client_info,
            )

        request = error_stats_service_pb2.DeleteEventsRequest(project_name=project_name)
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""project_name"", project_name)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""delete_events""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
download_gcs_file;"def download_gcs_file(path, out_fname=None, prefix_filter=None):
  """"""Download a file from GCS, optionally to a file.""""""
  url = posixpath.join(GCS_BUCKET, path)
  if prefix_filter:
    url += ""?prefix=%s"" % prefix_filter
  stream = bool(out_fname)
  resp = requests.get(url, stream=stream)
  if not resp.ok:
    raise ValueError(""GCS bucket inaccessible"")
  if out_fname:
    with tf.io.gfile.GFile(out_fname, ""wb"") as f:
      for chunk in resp.iter_content(1024):
        f.write(chunk)
  else:
    return resp.content"
random_crop_and_flip;"def random_crop_and_flip(x, pad_rows=4, pad_cols=4):
  """"""Augment a batch by randomly cropping and horizontally flipping it.""""""
  rows = tf.shape(x)[1]
  cols = tf.shape(x)[2]
  channels = x.get_shape()[3]

  def _rand_crop_img(img):
    """"""Randomly crop an individual image""""""
    return tf.random_crop(img, [rows, cols, channels])

  # Some of these ops are only on CPU.
  # This function will often be called with the device set to GPU.
  # We need to set it to CPU temporarily to avoid an exception.
  with tf.device('/CPU:0'):
    x = tf.image.resize_image_with_crop_or_pad(x, rows + pad_rows,
                                               cols + pad_cols)
    x = tf.map_fn(_rand_crop_img, x)
    x = tf.image.random_flip_left_right(x)
  return x"
get_blob;"def get_blob(storage_conn=None, **kwargs):
    '''
    .. versionadded:: 2015.8.0

    Download a blob
    '''
    if not storage_conn:
        storage_conn = get_storage_conn(opts=kwargs)

    if 'container' not in kwargs:
        raise SaltSystemExit(code=42, msg='The blob container name must be specified as ""container""')

    if 'name' not in kwargs:
        raise SaltSystemExit(code=42, msg='The blob name must be specified as ""name""')

    if 'local_path' not in kwargs and 'return_content' not in kwargs:
        raise SaltSystemExit(
            code=42,
            msg='Either a local path needs to be passed in as ""local_path"", '
            'or ""return_content"" to return the blob contents directly'
        )

    blob_kwargs = {
        'container_name': kwargs['container'],
        'blob_name': kwargs['name'],
        'snapshot': kwargs.get('snapshot', None),
        'x_ms_lease_id': kwargs.get('lease_id', None),
        'progress_callback': kwargs.get('progress_callback', None),
        'max_connections': kwargs.get('max_connections', 1),
        'max_retries': kwargs.get('max_retries', 5),
        'retry_wait': kwargs.get('retry_wait', 1),
    }

    if 'local_path' in kwargs:
        data = storage_conn.get_blob_to_path(
            file_path=kwargs['local_path'],
            open_mode=kwargs.get('open_mode', 'wb'),
            **blob_kwargs
        )
    elif 'return_content' in kwargs:
        data = storage_conn.get_blob_to_bytes(
            **blob_kwargs
        )

    return data"
patch_namespaced_ingress;"def patch_namespaced_ingress(self, name, namespace, body, **kwargs):
        """"""
        partially update the specified Ingress
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.patch_namespaced_ingress(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Ingress (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param object body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).
        :param bool force: Force is going to \""force\"" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.
        :return: NetworkingV1beta1Ingress
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.patch_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.patch_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)
            return data"
start_file;"def start_file(filename):
    """"""
    Generalized os.startfile for all platforms supported by Qt

    This function is simply wrapping QDesktopServices.openUrl

    Returns True if successfull, otherwise returns False.
    """"""
    from qtpy.QtCore import QUrl
    from qtpy.QtGui import QDesktopServices

    # We need to use setUrl instead of setPath because this is the only
    # cross-platform way to open external files. setPath fails completely on
    # Mac and doesn't open non-ascii files on Linux.
    # Fixes Issue 740
    url = QUrl()
    url.setUrl(filename)
    return QDesktopServices.openUrl(url)"
convert_dense;"def convert_dense(builder, layer, input_names, output_names, keras_layer):
    """"""Convert a dense layer from keras to coreml.

    Parameters
    keras_layer: layer
    ----------
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_name, output_name = (input_names[0], output_names[0])

    has_bias = keras_layer.bias
    # Get the weights from keras
    W = keras_layer.get_weights ()[0].T
    Wb = keras_layer.get_weights ()[1].T if has_bias else None

    builder.add_inner_product(name = layer,
            W = W,
            b = Wb,
            input_channels = keras_layer.input_dim,
            output_channels = keras_layer.output_dim,
            has_bias = has_bias,
            input_name = input_name,
            output_name = output_name)"
init_lsh;"def init_lsh(self):
    """"""
    Initializes locality-sensitive hashing with FALCONN to find nearest neighbors in training data.
    """"""
    self.query_objects = {
    }  # contains the object that can be queried to find nearest neighbors at each layer.
    # mean of training data representation per layer (that needs to be substracted before LSH).
    self.centers = {}
    for layer in self.layers:
      assert self.nb_tables >= self.neighbors

      # Normalize all the lenghts, since we care about the cosine similarity.
      self.train_activations_lsh[layer] /= np.linalg.norm(
          self.train_activations_lsh[layer], axis=1).reshape(-1, 1)

      # Center the dataset and the queries: this improves the performance of LSH quite a bit.
      center = np.mean(self.train_activations_lsh[layer], axis=0)
      self.train_activations_lsh[layer] -= center
      self.centers[layer] = center

      # LSH parameters
      params_cp = falconn.LSHConstructionParameters()
      params_cp.dimension = len(self.train_activations_lsh[layer][1])
      params_cp.lsh_family = falconn.LSHFamily.CrossPolytope
      params_cp.distance_function = falconn.DistanceFunction.EuclideanSquared
      params_cp.l = self.nb_tables
      params_cp.num_rotations = 2  # for dense set it to 1; for sparse data set it to 2
      params_cp.seed = 5721840
      # we want to use all the available threads to set up
      params_cp.num_setup_threads = 0
      params_cp.storage_hash_table = falconn.StorageHashTable.BitPackedFlatHashTable

      # we build 18-bit hashes so that each table has
      # 2^18 bins; this is a good choice since 2^18 is of the same
      # order of magnitude as the number of data points
      falconn.compute_number_of_hash_functions(self.number_bits, params_cp)

      print('Constructing the LSH table')
      table = falconn.LSHIndex(params_cp)
      table.setup(self.train_activations_lsh[layer])

      # Parse test feature vectors and find k nearest neighbors
      query_object = table.construct_query_object()
      query_object.set_num_probes(self.nb_tables)
      self.query_objects[layer] = query_object"
set_embedding;"def set_embedding(self, *embeddings):
        """"""Attaches one or more embeddings to the indexed text tokens.


        Parameters
        ----------
        embeddings : None or tuple of :class:`gluonnlp.embedding.TokenEmbedding` instances
            The embedding to be attached to the indexed tokens. If a tuple of multiple embeddings
            are provided, their embedding vectors will be concatenated for the same token.
        """"""

        if len(embeddings) == 1 and embeddings[0] is None:
            self._embedding = None
            return

        for embs in embeddings:
            assert isinstance(embs, emb.TokenEmbedding), \
                'The argument `embeddings` must be an instance or a list of instances of ' \
                '`gluonnlp.embedding.TokenEmbedding`.'
            assert embs.idx_to_vec is not None, \
                'For all specified `embeddings`, `embeddings.idx_to_vec` must be initialized. ' \
                'Use eg. `emb[emb.unknown_token] = nd.zeros(emsize)` to initialize, ' \
                'where `emsize` is the desired embedding dimensionality.'

        assert all([embs.unknown_token for embs in embeddings]) or \
            all([not embs.unknown_token for embs in embeddings]), \
            'Either all or none of the TokenEmbeddings must have an ' \
            'unknown_token set.'

        new_embedding = emb.TokenEmbedding(self.unknown_token, allow_extend=False)
        new_embedding._token_to_idx = self.token_to_idx
        new_embedding._idx_to_token = self.idx_to_token

        new_vec_len = sum(embs.idx_to_vec.shape[1] for embs in embeddings)
        new_idx_to_vec = nd.zeros(shape=(len(self), new_vec_len))

        col_start = 0
        # Concatenate all the embedding vectors in embedding.
        for embs in embeddings:
            if embs and embs.idx_to_vec is not None:
                col_end = col_start + embs.idx_to_vec.shape[1]
                # Cancatenate vectors of the unknown token.
                new_idx_to_vec[0, col_start:col_end] = embs.idx_to_vec[0]
                new_idx_to_vec[1:, col_start:col_end] = embs[self._idx_to_token[1:]]
                col_start = col_end

        new_embedding._idx_to_vec = new_idx_to_vec
        self._embedding = new_embedding"
get_word_index;"def get_word_index(tokens, char_index):
    '''
    Given word return word index.
    '''
    for (i, token) in enumerate(tokens):
        if token['char_end'] == 0:
            continue
        if token['char_begin'] <= char_index and char_index <= token['char_end']:
            return i
    return 0"
sources_list;"def sources_list(ruby=None, runas=None, gem_bin=None):
    '''
    List the configured gem sources.

    :param gem_bin: string : None
        Full path to ``gem`` binary to use.
    :param ruby: string : None
        If RVM or rbenv are installed, the ruby version and gemset to use.
        Ignored if ``gem_bin`` is specified.
    :param runas: string : None
        The user to run gem as.

    CLI Example:

    .. code-block:: bash

        salt '*' gem.sources_list
    '''
    ret = _gem(['sources'], ruby, gem_bin=gem_bin, runas=runas)
    return [] if ret is False else ret.splitlines()[2:]"
find_signature_input_colocation_error;"def find_signature_input_colocation_error(signature_name, inputs):
  """"""Returns error message for colocation of signature inputs, or None if ok.""""""
  for input_name, tensor in inputs.items():
    expected_colocation_groups = [tf.compat.as_bytes(""loc:@"" + tensor.op.name)]
    if tensor.op.colocation_groups() != expected_colocation_groups:
      return (
          ""A tensor x used as input in a signature must not be subject to a ""
          ""tf.colocate_with(y) constraint. (The reverse would be allowed.)\n""
          ""Details: tensor '%s' appears as input '%s' of signature '%s' ""
          ""but has Tensor.op.colocation_groups() == %s"" %
          (tensor, input_name, signature_name, tensor.op.colocation_groups()))
  return None"
modify_write;"def modify_write(self, write_pb, **unused_kwargs):
        """"""Modify a ``Write`` protobuf based on the state of this write option.

        The ``last_update_time`` is added to ``write_pb`` as an ""update time""
        precondition. When set, the target document must exist and have been
        last updated at that time.

        Args:
            write_pb (google.cloud.firestore_v1beta1.types.Write): A
                ``Write`` protobuf instance to be modified with a precondition
                determined by the state of this option.
            unused_kwargs (Dict[str, Any]): Keyword arguments accepted by
                other subclasses that are unused here.
        """"""
        current_doc = types.Precondition(update_time=self._last_update_time)
        write_pb.current_document.CopyFrom(current_doc)"
refresh_table;"def refresh_table(self):
        """"""Refresh variable table""""""
        if self.is_visible and self.isVisible():
            self.shellwidget.refresh_namespacebrowser()
            try:
                self.editor.resizeRowToContents()
            except TypeError:
                pass"
sine_wave;"def sine_wave(frequency):
  """"""Emit a sine wave at the given frequency.""""""
  xs = tf.reshape(tf.range(_samples(), dtype=tf.float32), [1, _samples(), 1])
  ts = xs / FLAGS.sample_rate
  return tf.sin(2 * math.pi * frequency * ts)"
save_function_tuple;"def save_function_tuple(self, func):
        """"""  Pickles an actual func object.

        A func comprises: code, globals, defaults, closure, and dict.  We
        extract and save these, injecting reducing functions at certain points
        to recreate the func object.  Keep in mind that some of these pieces
        can contain a ref to the func itself.  Thus, a naive save on these
        pieces could trigger an infinite loop of save's.  To get around that,
        we first create a skeleton func object using just the code (this is
        safe, since this won't contain a ref to the func), and memoize it as
        soon as it's created.  The other stuff can then be filled in later.
        """"""
        if is_tornado_coroutine(func):
            self.save_reduce(_rebuild_tornado_coroutine, (func.__wrapped__,),
                             obj=func)
            return

        save = self.save
        write = self.write

        code, f_globals, defaults, closure_values, dct, base_globals = self.extract_func_data(func)

        save(_fill_function)  # skeleton function updater
        write(pickle.MARK)    # beginning of tuple that _fill_function expects

        self._save_subimports(
            code,
            itertools.chain(f_globals.values(), closure_values or ()),
        )

        # create a skeleton function object and memoize it
        save(_make_skel_func)
        save((
            code,
            len(closure_values) if closure_values is not None else -1,
            base_globals,
        ))
        write(pickle.REDUCE)
        self.memoize(func)

        # save the rest of the func data needed by _fill_function
        save(f_globals)
        save(defaults)
        save(dct)
        save(func.__module__)
        save(closure_values)
        write(pickle.TUPLE)
        write(pickle.REDUCE)"
start_handshake;"def start_handshake(self, timeout):
        """"""
        Tells `Packetizer` that the handshake process started.
        Starts a book keeping timer that can signal a timeout in the
        handshake process.

        :param float timeout: amount of seconds to wait before timing out
        """"""
        if not self.__timer:
            self.__timer = threading.Timer(float(timeout), self.read_timer)
            self.__timer.start()"
cmd_implementation;"def cmd_implementation(self, events_lib, top_level_cmd_name, subcmd_name, *args, **kwargs):
        """"""
        calls for value substitution in the event json and returns the
        customized json as a string

        Parameters
        ----------
        events_lib
        top_level_cmd_name: string
            the name of the service
        subcmd_name: string
            the name of the event under the service
        args: tuple
            any arguments passed in before kwargs
        kwargs: dict
            the keys and values for substitution in the json
        Returns
        -------
        event: string
            returns the customized event json as a string
        """"""
        event = events_lib.generate_event(top_level_cmd_name, subcmd_name, kwargs)
        click.echo(event)
        return event"
create_window;"def create_window(self):
        """"""Create a QMainWindow instance containing this plugin.""""""
        self.undocked_window = window = PluginWindow(self)
        window.setAttribute(Qt.WA_DeleteOnClose)
        icon = self.get_plugin_icon()
        if is_text_string(icon):
            icon = self.get_icon(icon)
        window.setWindowIcon(icon)
        window.setWindowTitle(self.get_plugin_title())
        window.setCentralWidget(self)
        window.resize(self.size())
        self.refresh_plugin()

        self.dockwidget.setFloating(False)
        self.dockwidget.setVisible(False)

        window.show()"
update_search_space;"def update_search_space(self, search_space):
        """"""TODO: this is urgly, we put all the initialization work in this method, because initialization relies
        on search space, also because update_search_space is called at the beginning.
        NOTE: updating search space is not supported.

        Parameters
        ----------
        search_space:
            search space
        """"""
        if not self.update_ss_done:
            self.categorical_dict = generate_scenario(search_space)
            if self.categorical_dict is None:
                raise RuntimeError('categorical dict is not correctly returned after parsing search space.')
            self.optimizer = self._main_cli()
            self.smbo_solver = self.optimizer.solver
            self.loguniform_key = {key for key in search_space.keys() if search_space[key]['_type'] == 'loguniform'}
            self.update_ss_done = True
        else:
            self.logger.warning('update search space is not supported.')"
tenant_get;"def tenant_get(tenant_id=None, name=None, profile=None,
               **connection_args):
    '''
    Return a specific tenants (keystone tenant-get)

    CLI Examples:

    .. code-block:: bash

        salt '*' keystone.tenant_get c965f79c4f864eaaa9c3b41904e67082
        salt '*' keystone.tenant_get tenant_id=c965f79c4f864eaaa9c3b41904e67082
        salt '*' keystone.tenant_get name=nova
    '''
    kstone = auth(profile, **connection_args)
    ret = {}

    if name:
        for tenant in getattr(kstone, _TENANTS, None).list():
            if tenant.name == name:
                tenant_id = tenant.id
                break
    if not tenant_id:
        return {'Error': 'Unable to resolve tenant id'}
    tenant = getattr(kstone, _TENANTS, None).get(tenant_id)
    ret[tenant.name] = dict((value, getattr(tenant, value)) for value in dir(tenant)
                            if not value.startswith('_') and
                            isinstance(getattr(tenant, value), (six.string_types, dict, bool)))
    return ret"
make_kind_check;"def make_kind_check(python_types, numpy_kind):
    """"""
    Make a function that checks whether a scalar or array is of a given kind
    (e.g. float, int, datetime, timedelta).
    """"""
    def check(value):
        if hasattr(value, 'dtype'):
            return value.dtype.kind == numpy_kind
        return isinstance(value, python_types)
    return check"
create_env_error_message;"def create_env_error_message(error, show_traceback, using_user_site):
    """"""Format an error message for an EnvironmentError

    It may occur anytime during the execution of the install command.
    """"""
    parts = []

    # Mention the error if we are not going to show a traceback
    parts.append(""Could not install packages due to an EnvironmentError"")
    if not show_traceback:
        parts.append("": "")
        parts.append(str(error))
    else:
        parts.append(""."")

    # Spilt the error indication from a helper message (if any)
    parts[-1] += ""\n""

    # Suggest useful actions to the user:
    #  (1) using user site-packages or (2) verifying the permissions
    if error.errno == errno.EACCES:
        user_option_part = ""Consider using the `--user` option""
        permissions_part = ""Check the permissions""

        if not using_user_site:
            parts.extend([
                user_option_part, "" or "",
                permissions_part.lower(),
            ])
        else:
            parts.append(permissions_part)
        parts.append("".\n"")

    return """".join(parts).strip() + ""\n"""
load_lang_conf;"def load_lang_conf():
    """"""
    Load language setting from language config file if it exists, otherwise
    try to use the local settings if Spyder provides a translation, or
    return the default if no translation provided.
    """"""
    if osp.isfile(LANG_FILE):
        with open(LANG_FILE, 'r') as f:
            lang = f.read()
    else:
        lang = get_interface_language()
        save_lang_conf(lang)

    # Save language again if it's been disabled
    if lang.strip('\n') in DISABLED_LANGUAGES:
        lang = DEFAULT_LANGUAGE
        save_lang_conf(lang)

    return lang"
build_finished;"def build_finished(app, exception):
    ''' Generate a ``sitemap.txt`` from the collected HTML page links.

    '''
    filename = join(app.outdir, ""sitemap.txt"")

    links_iter = status_iterator(sorted(app.sitemap_links),
                                 'adding links to sitemap... ',
                                 'brown',
                                 len(app.sitemap_links),
                                 app.verbosity)

    try:
        with open(filename, 'w') as f:
            for link in links_iter:
                f.write(""%s\n"" % link)
    except OSError as e:
        raise SphinxError('cannot write sitemap.txt, reason: %s' % e)"
run_program;"def run_program(program, args=None, **subprocess_kwargs):
    """"""
    Run program in a separate process.

    NOTE: returns the process object created by
    `subprocess.Popen()`. This can be used with
    `proc.communicate()` for example.

    If 'shell' appears in the kwargs, it must be False,
    otherwise ProgramError will be raised.

    If only the program name is given and not the full path,
    a lookup will be performed to find the program. If the
    lookup fails, ProgramError will be raised.

    Note that stdin, stdout and stderr will be set by default
    to PIPE unless specified in subprocess_kwargs.

    :str program: The name of the program to run.
    :list args: The program arguments.
    :subprocess_kwargs: These will be passed to subprocess.Popen.
    """"""
    if 'shell' in subprocess_kwargs and subprocess_kwargs['shell']:
        raise ProgramError(
                ""This function is only for non-shell programs, ""
                ""use run_shell_command() instead."")
    fullcmd = find_program(program)
    if not fullcmd:
        raise ProgramError(""Program %s was not found"" % program)
    # As per subprocess, we make a complete list of prog+args
    fullcmd = [fullcmd] + (args or [])
    for stream in ['stdin', 'stdout', 'stderr']:
        subprocess_kwargs.setdefault(stream, subprocess.PIPE)
    subprocess_kwargs = alter_subprocess_kwargs_by_platform(
            **subprocess_kwargs)
    return subprocess.Popen(fullcmd, **subprocess_kwargs)"
cancel_operation;"def cancel_operation(
        self, name, retry=gapic_v1.method.DEFAULT, timeout=gapic_v1.method.DEFAULT
    ):
        """"""Starts asynchronous cancellation on a long-running operation.

        The server makes a best effort to cancel the operation, but success is
        not guaranteed. Clients can use :meth:`get_operation` or service-
        specific methods to check whether the cancellation succeeded or whether
        the operation completed despite cancellation. On successful
        cancellation, the operation is not deleted; instead, it becomes an
        operation with an ``Operation.error`` value with a
        ``google.rpc.Status.code`` of ``1``, corresponding to
        ``Code.CANCELLED``.

        Example:
            >>> from google.api_core import operations_v1
            >>> api = operations_v1.OperationsClient()
            >>> name = ''
            >>> api.cancel_operation(name)

        Args:
            name (str): The name of the operation resource to be cancelled.
            retry (google.api_core.retry.Retry): The retry strategy to use
                when invoking the RPC. If unspecified, the default retry from
                the client configuration will be used. If ``None``, then this
                method will not retry the RPC at all.
            timeout (float): The amount of time in seconds to wait for the RPC
                to complete. Note that if ``retry`` is used, this timeout
                applies to each individual attempt and the overall time it
                takes for this method to complete may be longer. If
                unspecified, the the default timeout in the client
                configuration is used. If ``None``, then the RPC method will
                not time out.

        Raises:
            google.api_core.exceptions.MethodNotImplemented: If the server
                does not support this method. Services are not required to
                implement this method.
            google.api_core.exceptions.GoogleAPICallError: If an error occurred
                while invoking the RPC, the appropriate ``GoogleAPICallError``
                subclass will be raised.
        """"""
        # Create the request object.
        request = operations_pb2.CancelOperationRequest(name=name)
        self._cancel_operation(request, retry=retry, timeout=timeout)"
read_response;"def read_response(self, delegate: httputil.HTTPMessageDelegate) -> Awaitable[bool]:
        """"""Read a single HTTP response.

        Typical client-mode usage is to write a request using `write_headers`,
        `write`, and `finish`, and then call ``read_response``.

        :arg delegate: a `.HTTPMessageDelegate`

        Returns a `.Future` that resolves to a bool after the full response has
        been read. The result is true if the stream is still open.
        """"""
        if self.params.decompress:
            delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)
        return self._read_message(delegate)"
keep_impute;"def keep_impute(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):
    """""" The model is revaluated for each test sample with the non-important features set to an imputed value.

    Note that the imputation is done using a multivariate normality assumption on the dataset. This depends on
    being able to estimate the full data covariance matrix (and inverse) accuractly. So X_train.shape[0] should
    be significantly bigger than X_train.shape[1].
    """"""

    X_train, X_test = to_array(X_train, X_test)

    # how many features to mask
    assert X_train.shape[1] == X_test.shape[1]

    # keep nkeep top features for each test explanation
    C = np.cov(X_train.T)
    C += np.eye(C.shape[0]) * 1e-6
    X_test_tmp = X_test.copy()
    yp_masked_test = np.zeros(y_test.shape)
    tie_breaking_noise = const_rand(X_train.shape[1], random_state) * 1e-6
    mean_vals = X_train.mean(0)
    for i in range(len(y_test)):
        if nkeep[i] < X_test.shape[1]:
            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)
            observe_inds = ordering[:nkeep[i]]
            impute_inds = ordering[nkeep[i]:]
            
            # impute missing data assuming it follows a multivariate normal distribution
            Coo_inv = np.linalg.inv(C[observe_inds,:][:,observe_inds])
            Cio = C[impute_inds,:][:,observe_inds]
            impute = mean_vals[impute_inds] + Cio @ Coo_inv @ (X_test[i, observe_inds] - mean_vals[observe_inds])
            
            X_test_tmp[i, impute_inds] = impute

    yp_masked_test = trained_model.predict(X_test_tmp)

    return metric(y_test, yp_masked_test)"
next_trials;"def next_trials(self):
        """"""Provides a batch of Trial objects to be queued into the TrialRunner.

        A batch ends when self._trial_generator returns None.

        Returns:
            trials (list): Returns a list of trials.
        """"""
        trials = []

        for trial in self._trial_generator:
            if trial is None:
                return trials
            trials += [trial]

        self._finished = True
        return trials"
convert_bidirectional;"def convert_bidirectional(builder, layer, input_names, output_names, keras_layer):
    """"""
    Convert a bidirectional layer from keras to coreml.
    Currently assumes the units are LSTMs.

    Parameters
    ----------
    keras_layer: layer
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""

    input_size = keras_layer.input_shape[-1]

    lstm_layer = keras_layer.forward_layer
    if (type(lstm_layer) != _keras.layers.recurrent.LSTM):
        raise TypeError('Bidirectional layers only supported with LSTM')

    if lstm_layer.go_backwards:
        raise TypeError(' \'go_backwards\' mode not supported with Bidirectional layers')

    output_all = keras_layer.return_sequences
    hidden_size = lstm_layer.units

    # Keras: I C F O; W_x, W_h, b
    # CoreML: I F O G; W_h and W_x are separated
    # Keras has all forward weights, followed by backward in the same order
    W_h, W_x, b = ([], [], [])
    keras_W_h = keras_layer.forward_layer.get_weights()[1].T
    W_h.append(keras_W_h[0 * hidden_size:][:hidden_size])
    W_h.append(keras_W_h[1 * hidden_size:][:hidden_size])
    W_h.append(keras_W_h[3 * hidden_size:][:hidden_size])
    W_h.append(keras_W_h[2 * hidden_size:][:hidden_size])

    keras_W_x = keras_layer.forward_layer.get_weights()[0].T
    W_x.append(keras_W_x[0 * hidden_size:][:hidden_size])
    W_x.append(keras_W_x[1 * hidden_size:][:hidden_size])
    W_x.append(keras_W_x[3 * hidden_size:][:hidden_size])
    W_x.append(keras_W_x[2 * hidden_size:][:hidden_size])

    if keras_layer.forward_layer.use_bias:
        keras_b = keras_layer.forward_layer.get_weights()[2]
        b.append(keras_b[0 * hidden_size:][:hidden_size])
        b.append(keras_b[1 * hidden_size:][:hidden_size])
        b.append(keras_b[3 * hidden_size:][:hidden_size])
        b.append(keras_b[2 * hidden_size:][:hidden_size])

    if len(b) == 0:
        b = None

    W_h_back, W_x_back, b_back = ([],[],[])
    keras_W_h = keras_layer.backward_layer.get_weights()[1].T
    W_h_back.append(keras_W_h[0 * hidden_size:][:hidden_size])
    W_h_back.append(keras_W_h[1 * hidden_size:][:hidden_size])
    W_h_back.append(keras_W_h[3 * hidden_size:][:hidden_size])
    W_h_back.append(keras_W_h[2 * hidden_size:][:hidden_size])

    keras_W_x = keras_layer.backward_layer.get_weights()[0].T
    W_x_back.append(keras_W_x[0 * hidden_size:][:hidden_size])
    W_x_back.append(keras_W_x[1 * hidden_size:][:hidden_size])
    W_x_back.append(keras_W_x[3 * hidden_size:][:hidden_size])
    W_x_back.append(keras_W_x[2 * hidden_size:][:hidden_size])

    if keras_layer.backward_layer.use_bias:
        keras_b = keras_layer.backward_layer.get_weights()[2]
        b_back.append(keras_b[0 * hidden_size:][:hidden_size])
        b_back.append(keras_b[1 * hidden_size:][:hidden_size])
        b_back.append(keras_b[3 * hidden_size:][:hidden_size])
        b_back.append(keras_b[2 * hidden_size:][:hidden_size])
    if len(b_back) == 0:
        b_back = None

    if (b == None and b_back != None) or (b != None and b_back == None):
        raise ValueError('Unsupported Bi-directional LSTM configuration. Bias must be enabled/disabled for both directions.')

    # Set activation type
    inner_activation_str = _get_recurrent_activation_name_from_keras(lstm_layer.recurrent_activation)
    activation_str = _get_recurrent_activation_name_from_keras(lstm_layer.activation)

    output_name_1 = output_names[0]
    if hasattr(keras_layer, 'merge_mode'):
        merge_mode = keras_layer.merge_mode
        if merge_mode not in ['concat','sum','mul','ave']:
            raise NotImplementedError('merge_mode \'%s\' in Bidirectional LSTM not supported currently' % merge_mode)
        if merge_mode != 'concat':
            output_name_1 += '_concatenated_bilstm_output'

    # Add to the network
    builder.add_bidirlstm(
        name = layer,
        W_h = W_h, W_x = W_x, b = b,
        W_h_back = W_h_back, W_x_back = W_x_back, b_back = b_back,
        hidden_size=hidden_size,
        input_size=input_size,
        input_names=input_names,
        output_names=[output_name_1] + output_names[1:],
        inner_activation = inner_activation_str,
        cell_state_update_activation = activation_str,
        output_activation = activation_str,
        forget_bias = lstm_layer.unit_forget_bias,
        output_all = output_all)

    if output_name_1 != output_names[0]:
        mode = 'CONCAT'
        if merge_mode == 'sum':
            mode = 'ADD'
        elif merge_mode == 'ave':
            mode = 'AVE'
        elif merge_mode == 'mul':
            mode = 'MULTIPLY'
        builder.add_split(name = layer + '_split',
                          input_name= output_name_1,
                          output_names= [output_names[0] + '_forward', output_names[0] + '_backward'])
        builder.add_elementwise(name = layer + '_elementwise',
                                input_names = [output_names[0] + '_forward', output_names[0] + '_backward'],
                                output_name = output_names[0],
                                mode = mode)"
extract_features;"def extract_features(self, dataset, missing_value_action='auto'):
        """"""
        For each example in the dataset, extract the leaf indices of
        each tree as features.

        For multiclass classification, each leaf index contains #num_class
        numbers.

        The returned feature vectors can be used as input to train another
        supervised learning model such as a
        :py:class:`~turicreate.logistic_classifier.LogisticClassifier`,
        an :py:class:`~turicreate.svm_classifier.SVMClassifier`, or a

        Parameters
        ----------
        dataset : SFrame
            Dataset of new observations. Must include columns with the same
            names as the features used for model training, but does not require
            a target column. Additional columns are ignored.

        missing_value_action: str, optional
            Action to perform when missing values are encountered. This can be
            one of:

            - 'auto': Choose a model dependent missing value policy.
            - 'impute': Proceed with evaluation by filling in the missing
                        values with the mean of the training data. Missing
                        values are also imputed if an entire column of data is
                        missing during evaluation.
            - 'none': Treat missing value as is. Model must be able to handle
                      missing value.
            - 'error' : Do not proceed with prediction and terminate with
                        an error message.

        Returns
        -------
        out : SArray
            An SArray of dtype array.array containing extracted features.

        Examples
        --------
        >>> data =  turicreate.SFrame(
            'https://static.turi.com/datasets/regression/houses.csv')

        >>> # Regression Tree Models
        >>> data['regression_tree_features'] = model.extract_features(data)

        >>> # Classification Tree Models
        >>> data['classification_tree_features'] = model.extract_features(data)
        """"""
        _raise_error_if_not_sframe(dataset, ""dataset"")
        if missing_value_action == 'auto':
            missing_value_action = select_default_missing_value_policy(self,
                    'extract_features')

        return self.__proxy__.extract_features(dataset, missing_value_action)"
load_data_and_labels;"def load_data_and_labels():
    """"""Loads MR polarity data from files, splits the data into words and generates labels.
    Returns split sentences and labels.
    """"""
    # download dataset
    get_chinese_text()

    # Load data from files
    positive_examples = list(codecs.open(""./data/pos.txt"", ""r"", ""utf-8"").readlines())
    positive_examples = [s.strip() for s in positive_examples]
    positive_examples = [pe for pe in positive_examples if len(pe) < 100]
    negative_examples = list(codecs.open(""./data/neg.txt"", ""r"", ""utf-8"").readlines())
    negative_examples = [s.strip() for s in negative_examples]
    negative_examples = [ne for ne in negative_examples if len(ne) < 100]
    # Split by words
    x_text = positive_examples + negative_examples
    # x_text = [clean_str(sent) for sent in x_text]
    x_text = [list(s) for s in x_text]

    # Generate labels
    positive_labels = [[0, 1] for _ in positive_examples]
    negative_labels = [[1, 0] for _ in negative_examples]
    y = np.concatenate([positive_labels, negative_labels], 0)
    return [x_text, y]"
log_inference;"def log_inference(batch_id, batch_num, metric, step_loss, log_interval):
    """"""Generate and print out the log message for inference.
    """"""
    metric_nm, metric_val = metric.get()
    if not isinstance(metric_nm, list):
        metric_nm = [metric_nm]
        metric_val = [metric_val]

    eval_str = '[Batch %d/%d] loss=%.4f, metrics:' + \
               ','.join([i + ':%.4f' for i in metric_nm])
    logging.info(eval_str, batch_id + 1, batch_num, \
                 step_loss / log_interval, \
                 *metric_val)"
get_event_noblock;"def get_event_noblock(self):
        '''
        Get the raw event without blocking or any other niceties
        '''
        assert self._run_io_loop_sync

        if not self.cpub:
            if not self.connect_pub():
                return None
        raw = self.subscriber.read_sync(timeout=0)
        if raw is None:
            return None
        mtag, data = self.unpack(raw, self.serial)
        return {'data': data, 'tag': mtag}"
compare_params;"def compare_params(defined, existing, return_old_value=False):
    '''
    .. versionadded:: 2017.7

    Compares Zabbix object definition against existing Zabbix object.

    :param defined: Zabbix object definition taken from sls file.
    :param existing: Existing Zabbix object taken from result of an API call.
    :param return_old_value: Default False. If True, returns dict(""old""=old_val, ""new""=new_val) for rollback purpose.
    :return: Params that are different from existing object. Result extended by
        object ID can be passed directly to Zabbix API update method.
    '''
    # Comparison of data types
    if not isinstance(defined, type(existing)):
        raise SaltException('Zabbix object comparison failed (data type mismatch). Expecting {0}, got {1}. '
                            'Existing value: ""{2}"", defined value: ""{3}"").'.format(type(existing),
                                                                                   type(defined),
                                                                                   existing,
                                                                                   defined))

    # Comparison of values
    if not salt.utils.data.is_iter(defined):
        if six.text_type(defined) != six.text_type(existing) and return_old_value:
            return {'new': six.text_type(defined), 'old': six.text_type(existing)}
        elif six.text_type(defined) != six.text_type(existing) and not return_old_value:
            return six.text_type(defined)

    # Comparison of lists of values or lists of dicts
    if isinstance(defined, list):
        if len(defined) != len(existing):
            log.info('Different list length!')
            return {'new': defined, 'old': existing} if return_old_value else defined
        else:
            difflist = []
            for ditem in defined:
                d_in_e = []
                for eitem in existing:
                    comp = compare_params(ditem, eitem, return_old_value)
                    if return_old_value:
                        d_in_e.append(comp['new'])
                    else:
                        d_in_e.append(comp)
                if all(d_in_e):
                    difflist.append(ditem)
            # If there is any difference in a list then whole defined list must be returned and provided for update
            if any(difflist) and return_old_value:
                return {'new': defined, 'old': existing}
            elif any(difflist) and not return_old_value:
                return defined

    # Comparison of dicts
    if isinstance(defined, dict):
        try:
            # defined must be a subset of existing to be compared
            if set(defined) <= set(existing):
                intersection = set(defined) & set(existing)
                diffdict = {'new': {}, 'old': {}} if return_old_value else {}
                for i in intersection:
                    comp = compare_params(defined[i], existing[i], return_old_value)
                    if return_old_value:
                        if comp or (not comp and isinstance(comp, list)):
                            diffdict['new'].update({i: defined[i]})
                            diffdict['old'].update({i: existing[i]})
                    else:
                        if comp or (not comp and isinstance(comp, list)):
                            diffdict.update({i: defined[i]})
                return diffdict

            return {'new': defined, 'old': existing} if return_old_value else defined

        except TypeError:
            raise SaltException('Zabbix object comparison failed (data type mismatch). Expecting {0}, got {1}. '
                                'Existing value: ""{2}"", defined value: ""{3}"").'.format(type(existing),
                                                                                       type(defined),
                                                                                       existing,
                                                                                       defined))"
borrow_optimizer;"def borrow_optimizer(self, shared_module):
        """"""Borrows optimizer from a shared module. Used in bucketing, where exactly the same
        optimizer (esp. kvstore) is used.

        Parameters
        ----------
        shared_module : Module
        """"""
        assert shared_module.optimizer_initialized
        self._optimizer = shared_module._optimizer
        self._kvstore = shared_module._kvstore
        self._update_on_kvstore = shared_module._update_on_kvstore
        self._updater = shared_module._updater
        self.optimizer_initialized = True"
load_last_visible_toolbars;"def load_last_visible_toolbars(self):
        """"""Loads the last visible toolbars from the .ini file.""""""
        toolbars_names = CONF.get('main', 'last_visible_toolbars', default=[])

        if toolbars_names:
            dic = {}
            for toolbar in self.toolbarslist:
                dic[toolbar.objectName()] = toolbar

            toolbars = []
            for name in toolbars_names:
                if name in dic:
                    toolbars.append(dic[name])
            self.visible_toolbars = toolbars
        else:
            self.get_visible_toolbars()
        self._update_show_toolbars_action()"
conditional_symbols;"def conditional_symbols(node):
    '''
    Group lhs and rhs into conditional, stable and undefined
    :param node: ast node 
    
    :returns: tuple of (conditional_lhs, stable_lhs),(conditional_rhs, stable_rhs), undefined
    
    '''

    gen = ConditionalSymbolVisitor()
    gen.visit(node)
    lhs = gen.cond_lhs, gen.stable_lhs
    rhs = gen.cond_rhs, gen.stable_rhs
    undefined = gen.undefined
    return lhs, rhs, undefined"
any_has_focus;"def any_has_focus(self):
        """"""Returns if tour or any of its components has focus.""""""
        f = (self.hasFocus() or self.parent.hasFocus() or 
             self.tips.hasFocus() or self.canvas.hasFocus())
        return f"
competition_submit_cli;"def competition_submit_cli(self,
                               file_name,
                               message,
                               competition,
                               competition_opt=None,
                               quiet=False):
        """""" submit a competition using the client. Arguments are same as for
            competition_submit, except for extra arguments provided here.
             Parameters
            ==========
            competition_opt: an alternative competition option provided by cli
        """"""
        competition = competition or competition_opt
        try:
            submit_result = self.competition_submit(file_name, message,
                                                    competition, quiet)
        except ApiException as e:
            if e.status == 404:
                print('Could not find competition - please verify that you '
                      'entered the correct competition ID and that the '
                      'competition is still accepting submissions.')
                return None
            else:
                raise e
        return submit_result"
get_abbr_impl;"def get_abbr_impl():
    # type: () -> str
    """"""Return abbreviated implementation name.""""""
    if hasattr(sys, 'pypy_version_info'):
        pyimpl = 'pp'
    elif sys.platform.startswith('java'):
        pyimpl = 'jy'
    elif sys.platform == 'cli':
        pyimpl = 'ip'
    else:
        pyimpl = 'cp'
    return pyimpl"
convert_merge;"def convert_merge(builder, layer, input_names, output_names, keras_layer):
    """"""
    Convert concat layer from keras to coreml.

    Parameters
    ----------
    keras_layer: layer
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    output_name = output_names[0]

    mode = _get_elementwise_name_from_keras_layer(keras_layer)
    builder.add_elementwise(name = layer, input_names = input_names,
            output_name = output_name, mode = mode)"
compare_models;"def compare_models(dataset, models, model_names=None, user_sample=1.0,
                   metric='auto',
                   target=None,
                   exclude_known_for_precision_recall=True,
                   make_plot=False,
                   verbose=True,
                   **kwargs):
    """"""
    Compare the prediction or recommendation performance of recommender models
    on a common test dataset.

    Models that are trained to predict ratings are compared separately from
    models that are trained without target ratings.  The ratings prediction
    models are compared on root-mean-squared error, and the rest are compared on
    precision-recall.

    Parameters
    ----------
    dataset : SFrame
        The dataset to use for model evaluation.

    models : list[recommender models]
        List of trained recommender models.

    model_names : list[str], optional
        List of model name strings for display.

    user_sample : float, optional
        Sampling proportion of unique users to use in estimating model
        performance. Defaults to 1.0, i.e. use all users in the dataset.

    metric : str, {'auto', 'rmse', 'precision_recall'}, optional
        Metric for the evaluation. The default automatically splits
        models into two groups with their default evaluation metric respectively:
        'rmse' for models trained with a target, and 'precision_recall'
        otherwise.

    target : str, optional
        The name of the target column for evaluating rmse. If the model is
        trained with a target column, the default is to using the same column.
        If the model is trained without a target column and `metric='rmse'`,
        then this option must be provided by user.

    exclude_known_for_precision_recall : bool, optional
        A useful option when `metric='precision_recall'`. Recommender models
        automatically exclude items seen in the training data from the
        final recommendation list. If the input evaluation `dataset` is the
        same as the data used for training the models, set this option to False.


    verbose : bool, optional
        If true, print the progress.

    Returns
    -------
    out : list[SFrame]
        A list of results where each one is an sframe of evaluation results of
        the respective model on the given dataset

    Examples
    --------
    If you have created two ItemSimilarityRecommenders ``m1`` and ``m2`` and have
    an :class:`~turicreate.SFrame` ``test_data``, then you may compare the
    performance of the two models on test data using:

    >>> import turicreate
    >>> train_data = turicreate.SFrame({'user_id': [""0"", ""0"", ""0"", ""1"", ""1"", ""2"", ""2"", ""2""],
    ...                               'item_id': [""a"", ""c"", ""e"", ""b"", ""f"", ""b"", ""c"", ""d""]})
    >>> test_data = turicreate.SFrame({'user_id': [""0"", ""0"", ""1"", ""1"", ""1"", ""2"", ""2""],
    ...                              'item_id': [""b"", ""d"", ""a"", ""c"", ""e"", ""a"", ""e""]})
    >>> m1 = turicreate.item_similarity_recommender.create(train_data)
    >>> m2 = turicreate.item_similarity_recommender.create(train_data, only_top_k=1)
    >>> turicreate.recommender.util.compare_models(test_data, [m1, m2], model_names=[""m1"", ""m2""])

    The evaluation metric is automatically set to 'precision_recall', and the
    evaluation will be based on recommendations that exclude items seen in the
    training data.

    If you want to evaluate on the original training set:

    >>> turicreate.recommender.util.compare_models(train_data, [m1, m2],
    ...                                     exclude_known_for_precision_recall=False)

    Suppose you have four models, two trained with a target rating column, and
    the other two trained without a target. By default, the models are put into
    two different groups with ""rmse"", and ""precision-recall"" as the evaluation
    metric respectively.

    >>> train_data2 = turicreate.SFrame({'user_id': [""0"", ""0"", ""0"", ""1"", ""1"", ""2"", ""2"", ""2""],
    ...                                'item_id': [""a"", ""c"", ""e"", ""b"", ""f"", ""b"", ""c"", ""d""],
    ...                                'rating': [1, 3, 4, 5, 3, 4, 2, 5]})
    >>> test_data2 = turicreate.SFrame({'user_id': [""0"", ""0"", ""1"", ""1"", ""1"", ""2"", ""2""],
    ...                               'item_id': [""b"", ""d"", ""a"", ""c"", ""e"", ""a"", ""e""],
    ...                               'rating': [3, 5, 4, 4, 3, 5, 2]})
    >>> m3 = turicreate.factorization_recommender.create(train_data2, target='rating')
    >>> m4 = turicreate.factorization_recommender.create(train_data2, target='rating')
    >>> turicreate.recommender.util.compare_models(test_data2, [m3, m4])

    To compare all four models using the same 'precision_recall' metric, you can
    do:

    >>> turicreate.recommender.util.compare_models(test_data2, [m1, m2, m3, m4],
    ...                                          metric='precision_recall')
    """"""

    num_models = len(models)

    if model_names is None:
        model_names = ['M' + str(i) for i in range(len(models))]

    if num_models < 1:
        raise ValueError(""Must pass in at least one recommender model to \
                           evaluate"")

    if model_names is not None and len(model_names) != num_models:
        raise ValueError(""Must pass in the same number of model names as \
                          models"")

    # if we are asked to sample the users, come up with a list of unique users
    if user_sample < 1.0:
        user_id_name = models[0].user_id
        if user_id_name is None:
            raise ValueError(""user_id not set in model(s)"")
        user_sa = dataset[user_id_name]
        unique_users = list(user_sa.unique())
        nusers = len(unique_users)
        ntake = int(round(user_sample * nusers))

        _random.shuffle(unique_users)

        users = unique_users[:ntake]
        print(""compare_models: using"", ntake, ""users to estimate model performance"")
        users = frozenset(users)
        ix = [u in users for u in dataset[user_id_name]]
        dataset_subset = dataset[_SArray(ix) == True]
    else:
        dataset_subset = dataset

    results = []
    for (m, mname) in zip(models, model_names):
        if verbose:
            print('PROGRESS: Evaluate model %s' % mname)
        r = m.evaluate(dataset_subset,
                       metric,
                       exclude_known_for_precision_recall,
                       target,
                       verbose=verbose,
                       cutoffs=list(range(1,11,1))+list(range(11,50,5)),
                       **kwargs)
        results.append(r)

    return results"
project_get;"def project_get(project_id=None, name=None, profile=None, **connection_args):
    '''
    Return a specific projects (keystone project-get)
    Overrides keystone tenant-get form api V2.
    For keystone api V3 only.

    .. versionadded:: 2016.11.0

    project_id
        The project id.

    name
        The project name.

    profile
        Configuration profile - if configuration for multiple openstack accounts required.

    CLI Examples:

    .. code-block:: bash

        salt '*' keystone.project_get c965f79c4f864eaaa9c3b41904e67082
        salt '*' keystone.project_get project_id=c965f79c4f864eaaa9c3b41904e67082
        salt '*' keystone.project_get name=nova
    '''
    auth(profile, **connection_args)

    if _OS_IDENTITY_API_VERSION > 2:
        return tenant_get(tenant_id=project_id, name=name, profile=None, **connection_args)
    else:
        return False"
make_multipart;"def make_multipart(self, content_disposition=None, content_type=None,
                       content_location=None):
        """"""
        Makes this request field into a multipart request field.

        This method overrides ""Content-Disposition"", ""Content-Type"" and
        ""Content-Location"" headers to the request parameter.

        :param content_type:
            The 'Content-Type' of the request body.
        :param content_location:
            The 'Content-Location' of the request body.

        """"""
        self.headers['Content-Disposition'] = content_disposition or 'form-data'
        self.headers['Content-Disposition'] += '; '.join([
            '', self._render_parts(
                (('name', self._name), ('filename', self._filename))
            )
        ])
        self.headers['Content-Type'] = content_type
        self.headers['Content-Location'] = content_location"
set_metadata;"def set_metadata(candidates, traces, dependencies, pythons):
    """"""Add ""metadata"" to candidates based on the dependency tree.

    Metadata for a candidate includes markers and a specifier for Python
    version requirements.

    :param candidates: A key-candidate mapping. Candidates in the mapping will
        have their markers set.
    :param traces: A graph trace (produced by `traces.trace_graph`) providing
        information about dependency relationships between candidates.
    :param dependencies: A key-collection mapping containing what dependencies
        each candidate in `candidates` requested.
    :param pythons: A key-str mapping containing Requires-Python information
        of each candidate.

    Keys in mappings and entries in the trace are identifiers of a package, as
    implemented by the `identify` method of the resolver's provider.

    The candidates are modified in-place.
    """"""
    metasets_mapping = _calculate_metasets_mapping(
        dependencies, pythons, copy.deepcopy(traces),
    )
    for key, candidate in candidates.items():
        candidate.markers = _format_metasets(metasets_mapping[key])"
count_tokens;"def count_tokens(tokens, to_lower=False, counter=None):
    r""""""Counts tokens in the specified string.

    For token_delim='(td)' and seq_delim='(sd)', a specified string of two sequences of tokens may
    look like::

        (td)token1(td)token2(td)token3(td)(sd)(td)token4(td)token5(td)(sd)


    Parameters
    ----------
    tokens : list of str
        A source list of tokens.
    to_lower : bool, default False
        Whether to convert the source source_str to the lower case.
    counter : Counter or None, default None
        The Counter instance to be updated with the counts of `tokens`. If
        None, return a new Counter instance counting tokens from `tokens`.

    Returns
    -------
    The `counter` Counter instance after being updated with the token
    counts of `source_str`. If `counter` is None, return a new Counter
    instance counting tokens from `source_str`.

    Examples
    --------
    >>> import re
    >>> source_str = ' Life is great ! \n life is good . \n'
    >>> source_str_tokens = filter(None, re.split(' |\n', source_str))
    >>> gluonnlp.data.count_tokens(source_str_tokens)
    Counter({'is': 2, 'Life': 1, 'great': 1, '!': 1, 'life': 1, 'good': 1, '.': 1})

    """"""
    if to_lower:
        tokens = [t.lower() for t in tokens]

    if counter is None:
        return Counter(tokens)
    else:
        counter.update(tokens)
        return counter"
encryption_configuration;"def encryption_configuration(self):
        """"""google.cloud.bigquery.table.EncryptionConfiguration: Custom
        encryption configuration for the table.

        Custom encryption configuration (e.g., Cloud KMS keys) or :data:`None`
        if using default encryption.

        See `protecting data with Cloud KMS keys
        <https://cloud.google.com/bigquery/docs/customer-managed-encryption>`_
        in the BigQuery documentation.
        """"""
        prop = self._properties.get(""encryptionConfiguration"")
        if prop is not None:
            prop = EncryptionConfiguration.from_api_repr(prop)
        return prop"
bundle_attacks;"def bundle_attacks(sess, model, x, y, attack_configs, goals, report_path,
                   attack_batch_size=BATCH_SIZE, eval_batch_size=BATCH_SIZE):
  """"""
  Runs attack bundling.
  Users of cleverhans may call this function but are more likely to call
  one of the recipes above.

  Reference: https://openreview.net/forum?id=H1g0piA9tQ

  :param sess: tf.session.Session
  :param model: cleverhans.model.Model
  :param x: numpy array containing clean example inputs to attack
  :param y: numpy array containing true labels
  :param attack_configs: list of AttackConfigs to run
  :param goals: list of AttackGoals to run
    The bundler works through the goals in order, until each is satisfied.
    Some goals may never be satisfied, in which case the bundler will run
    forever, updating the report on disk as it goes.
  :param report_path: str, the path the report will be saved to
  :param attack_batch_size: int, batch size for generating adversarial examples
  :param eval_batch_size: int, batch size for evaluating the model on clean / adversarial examples
  :returns:
    adv_x: The adversarial examples, in the same format as `x`
    run_counts: dict mapping each AttackConfig to a numpy array reporting
      how many times that AttackConfig was run on each example
  """"""
  assert isinstance(sess, tf.Session)
  assert isinstance(model, Model)
  assert all(isinstance(attack_config, AttackConfig) for attack_config
             in attack_configs)
  assert all(isinstance(goal, AttackGoal) for goal in goals)
  assert isinstance(report_path, six.string_types)
  if x.shape[0] != y.shape[0]:
    raise ValueError(""Number of input examples does not match number of labels"")

  # Note: no need to precompile attacks, correctness_and_confidence
  # caches them

  run_counts = {}
  for attack_config in attack_configs:
    run_counts[attack_config] = np.zeros(x.shape[0], dtype=np.int64)

  # TODO: make an interface to pass this in if it has already been computed
  # elsewhere
  _logger.info(""Running on clean data to initialize the report..."")
  packed = correctness_and_confidence(sess, model, x, y, batch_size=eval_batch_size,
                                      devices=devices)
  _logger.info(""...done"")
  correctness, confidence = packed
  _logger.info(""Accuracy: "" + str(correctness.mean()))
  report = ConfidenceReport()
  report['clean'] = ConfidenceReportEntry(correctness, confidence)

  adv_x = x.copy()

  for goal in goals:
    bundle_attacks_with_goal(sess, model, x, y, adv_x, attack_configs,
                             run_counts,
                             goal, report, report_path,
                             attack_batch_size=attack_batch_size, eval_batch_size=eval_batch_size)

  # Many users will set `goals` to make this run forever, so the return
  # statement is not the primary way to get information out.
  return adv_x, run_counts"
read_img;"def read_img(path):
    """""" Reads image specified by path into numpy.ndarray""""""
    img = cv2.resize(cv2.imread(path, 0), (80, 30)).astype(np.float32) / 255
    img = np.expand_dims(img.transpose(1, 0), 0)
    return img"
match_rank;"def match_rank (self, ps):
        """""" Returns true if the generator can be run with the specified
            properties.
        """"""
        # See if generator's requirements are satisfied by
        # 'properties'.  Treat a feature name in requirements
        # (i.e. grist-only element), as matching any value of the
        # feature.
        assert isinstance(ps, property_set.PropertySet)
        all_requirements = self.requirements ()

        property_requirements = []
        feature_requirements = []
        # This uses strings because genenator requirements allow
        # the '<feature>' syntax without value and regular validation
        # is not happy about that.
        for r in all_requirements:
            if get_value (r):
                property_requirements.append (r)

            else:
                feature_requirements.append (r)

        return all(ps.get(get_grist(s)) == [get_value(s)] for s in property_requirements) \
               and all(ps.get(get_grist(s)) for s in feature_requirements)"
get_pending_servermanager;"def get_pending_servermanager():
    '''
    Determine whether there are pending Server Manager tasks that require a
    reboot.

    .. versionadded:: 2016.11.0

    Returns:
        bool: ``True`` if there are pending Server Manager tasks, otherwise
        ``False``

    CLI Example:

    .. code-block:: bash

        salt '*' system.get_pending_servermanager
    '''
    vname = 'CurrentRebootAttempts'
    key = r'SOFTWARE\Microsoft\ServerManager'

    # There are situations where it's possible to have '(value not set)' as
    # the value data, and since an actual reboot won't be pending in that
    # instance, just catch instances where we try unsuccessfully to cast as int.

    reg_ret = __utils__['reg.read_value']('HKLM', key, vname)

    if reg_ret['success']:
        log.debug('Found key: %s', key)

        try:
            if int(reg_ret['vdata']) > 0:
                return True
        except ValueError:
            pass
    else:
        log.debug('Unable to access key: %s', key)
    return False"
weak_lru_cache;"def weak_lru_cache(maxsize=100):
    """"""Weak least-recently-used cache decorator.

    If *maxsize* is set to None, the LRU features are disabled and the cache
    can grow without bound.

    Arguments to the cached function must be hashable. Any that are weak-
    referenceable will be stored by weak reference.  Once any of the args have
    been garbage collected, the entry will be removed from the cache.

    View the cache statistics named tuple (hits, misses, maxsize, currsize)
    with f.cache_info().  Clear the cache and statistics with f.cache_clear().

    See:  http://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used

    """"""
    class desc(lazyval):
        def __get__(self, instance, owner):
            if instance is None:
                return self
            try:
                return self._cache[instance]
            except KeyError:
                inst = ref(instance)

                @_weak_lru_cache(maxsize)
                @wraps(self._get)
                def wrapper(*args, **kwargs):
                    return self._get(inst(), *args, **kwargs)

                self._cache[instance] = wrapper
                return wrapper

        @_weak_lru_cache(maxsize)
        def __call__(self, *args, **kwargs):
            return self._get(*args, **kwargs)

    return desc"
list_endpoints_for_all_namespaces;"def list_endpoints_for_all_namespaces(self, **kwargs):
        """"""
        list or watch objects of kind Endpoints
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_endpoints_for_all_namespaces(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str pretty: If 'true', then the output is pretty printed.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1EndpointsList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_endpoints_for_all_namespaces_with_http_info(**kwargs)
        else:
            (data) = self.list_endpoints_for_all_namespaces_with_http_info(**kwargs)
            return data"
annotated_dataset_path;"def annotated_dataset_path(cls, project, dataset, annotated_dataset):
        """"""Return a fully-qualified annotated_dataset string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/datasets/{dataset}/annotatedDatasets/{annotated_dataset}"",
            project=project,
            dataset=dataset,
            annotated_dataset=annotated_dataset,
        )"
insert_option_group;"def insert_option_group(self, idx, *args, **kwargs):
        """"""Insert an OptionGroup at a given position.""""""
        group = self.add_option_group(*args, **kwargs)

        self.option_groups.pop()
        self.option_groups.insert(idx, group)

        return group"
new_project;"def new_project(self, project_id, name=None, labels=None):
        """"""Create a project bound to the current client.

        Use :meth:`Project.reload() \
        <google.cloud.resource_manager.project.Project.reload>` to retrieve
        project metadata after creating a
        :class:`~google.cloud.resource_manager.project.Project` instance.

        .. note:

            This does not make an API call.

        :type project_id: str
        :param project_id: The ID for this project.

        :type name: str
        :param name: The display name of the project.

        :type labels: dict
        :param labels: A list of labels associated with the project.

        :rtype: :class:`~google.cloud.resource_manager.project.Project`
        :returns: A new instance of a
                  :class:`~google.cloud.resource_manager.project.Project`
                  **without** any metadata loaded.
        """"""
        return Project(project_id=project_id, client=self, name=name, labels=labels)"
set_standby_timeout;"def set_standby_timeout(timeout, power='ac', scheme=None):
    '''
    Set the standby timeout in minutes for the given power scheme

    Args:
        timeout (int):
            The amount of time in minutes before the computer sleeps

        power (str):
            Set the value for AC or DC power. Default is ``ac``. Valid options
            are:

                - ``ac`` (AC Power)
                - ``dc`` (Battery)

        scheme (str):
            The scheme to use, leave as ``None`` to use the current. Default is
            ``None``. This can be the GUID or the Alias for the Scheme. Known
            Aliases are:

                - ``SCHEME_BALANCED`` - Balanced
                - ``SCHEME_MAX`` - Power saver
                - ``SCHEME_MIN`` - High performance

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    CLI Example:

    .. code-block:: bash

        # Sets the system standby timeout to 30 minutes on Battery
        salt '*' powercfg.set_standby_timeout 30 power=dc
    '''
    return _set_powercfg_value(
        scheme=scheme,
        sub_group='SUB_SLEEP',
        setting_guid='STANDBYIDLE',
        power=power,
        value=timeout)"
get_each_method_maximun_cpu_mem;"def get_each_method_maximun_cpu_mem(self):
        """"""获取每个方法中的cpu和内存耗费最值点.""""""
        # 本函数用于丰富self.method_exec_info的信息:存入cpu、mem最值点
        self.method_exec_info = deepcopy(self.data.get(""method_exec_info"", []))
        method_exec_info = deepcopy(self.method_exec_info)  # 用来辅助循环
        method_index, cpu_max, cpu_max_time, mem_max, mem_max_time = 0, 0, 0, 0, 0  # 临时变量
        self.max_mem = 0
        for index, timestamp in enumerate(self.timestamp_list):
            # method_exec_info是按顺序的,逐个遍历找出每个method_exec_info中的cpu和mem的最值点和timestamp:
            start, end = method_exec_info[0][""start_time""], method_exec_info[0][""end_time""]
            if timestamp < start:
                # 方法正式start之前的数据，不能参与方法内的cpu、mem计算，直接忽略此条数据
                continue
            elif timestamp <= end:
                # 方法执行期间的数据,纳入最值比较:
                if self.cpu_axis[index] > cpu_max:
                    cpu_max, cpu_max_time = self.cpu_axis[index], timestamp
                if self.mem_axis[index] > mem_max:
                    mem_max, mem_max_time = self.mem_axis[index], timestamp
                continue
            else:
                # 本次方法筛选完毕，保存本方法的最值cpu和mem
                if cpu_max_time != 0 and mem_max_time != 0:
                    self.method_exec_info[method_index].update({""cpu_max"": cpu_max, ""mem_max"": mem_max, ""cpu_max_time"": cpu_max_time, ""mem_max_time"": mem_max_time})
                # 保存最大的内存，后面绘图时用
                if mem_max > self.max_mem:
                    self.max_mem = mem_max
                cpu_max, mem_max = 0, 0  # 临时变量
                # 准备进行下一个方法的检查，发现已经检查完则正式结束
                del method_exec_info[0]
                if method_exec_info:
                    method_index += 1  # 进行下一个方法时:当前方法的序号+1
                    continue
                else:
                    break"
update_multi_precision;"def update_multi_precision(self, index, weight, grad, state):
        """"""Updates the given parameter using the corresponding gradient and state.
        Mixed precision version.

        Parameters
        ----------
        index : int
            The unique index of the parameter into the individual learning
            rates and weight decays. Learning rates and weight decay
            may be set via `set_lr_mult()` and `set_wd_mult()`, respectively.
        weight : NDArray
            The parameter to be updated.
        grad : NDArray
            The gradient of the objective with respect to this parameter.
        state : any obj
            The state returned by `create_state()`.
        """"""
        if self.multi_precision and weight.dtype == numpy.float16:
            # Wrapper for mixed precision
            weight_master_copy = state[0]
            original_state = state[1]
            grad32 = grad.astype(numpy.float32)
            self.update(index, weight_master_copy, grad32, original_state)
            cast(weight_master_copy, dtype=weight.dtype, out=weight)
        else:
            self.update(index, weight, grad, state)"
load_data;"def load_data(self, profdatafile):
        """"""Load profiler data saved by profile/cProfile module""""""
        import pstats
        try:
            stats_indi = [pstats.Stats(profdatafile), ]
        except (OSError, IOError):
            return
        self.profdata = stats_indi[0]
        
        if self.compare_file is not None:
            try:
                stats_indi.append(pstats.Stats(self.compare_file))
            except (OSError, IOError) as e:
                QMessageBox.critical(
                    self, _(""Error""),
                    _(""Error when trying to load profiler results""))
                logger.debug(""Error when calling pstats, {}"".format(e))
                self.compare_file = None
        map(lambda x: x.calc_callees(), stats_indi)
        self.profdata.calc_callees()
        self.stats1 = stats_indi
        self.stats = stats_indi[0].stats"
proto_value_for_feature;"def proto_value_for_feature(example, feature_name):
  """"""Get the value of a feature from Example regardless of feature type.""""""
  feature = get_example_features(example)[feature_name]
  if feature is None:
    raise ValueError('Feature {} is not on example proto.'.format(feature_name))
  feature_type = feature.WhichOneof('kind')
  if feature_type is None:
    raise ValueError('Feature {} on example proto has no declared type.'.format(
        feature_name))
  return getattr(feature, feature_type).value"
last_timestamp;"def last_timestamp(self, event_key=None):
    """"""Obtain the last timestamp.

    Args:
      event_key: the type key of the sought events (e.g., constants.NAN_KEY). If
      None, includes all event type keys.

    Returns:
      Last (latest) timestamp of all the events of the given type (or all
        event types if event_key is None).
    """"""
    if event_key is None:
      timestamps = [self._trackers[key].first_timestamp
                    for key in self._trackers]
      return max(timestamp for timestamp in timestamps if timestamp >= 0)
    else:
      return self._trackers[event_key].last_timestamp"
board_type;"def board_type(self):
        """"""
        [str] 板块类别，’MainBoard’ - 主板,’GEM’ - 创业板（股票专用）
        """"""
        try:
            return self.__dict__[""board_type""]
        except (KeyError, ValueError):
            raise AttributeError(
                ""Instrument(order_book_id={}) has no attribute 'board_type' "".format(self.order_book_id)
            )"
get_plugin_tabwidget;"def get_plugin_tabwidget(self, plugin):
        """"""Get the tabwidget of the plugin's current tab manager.""""""
        # The tab widget is named ""tabs"" in the editor plugin while it is
        # named ""tabwidget"" in the notebook plugin.
        try:
            tabwidget = plugin.get_current_tab_manager().tabs
        except AttributeError:
            tabwidget = plugin.get_current_tab_manager().tabwidget

        return tabwidget"
list_conf;"def list_conf(conf_file=default_conf, log_file=None, include_unset=False):
    '''
    Show parsed configuration

    .. versionadded:: 2018.3.0

    conf_file : string
        path to logadm.conf, defaults to /etc/logadm.conf
    log_file : string
        optional show only one log file
    include_unset : boolean
        include unset flags in output

    CLI Example:

    .. code-block:: bash

        salt '*' logadm.list_conf
        salt '*' logadm.list_conf log=/var/log/syslog
        salt '*' logadm.list_conf include_unset=False
    '''
    cfg = _parse_conf(conf_file)
    cfg_parsed = {}

    ## parse all options
    for entry in cfg:
        log_cfg = _parse_options(entry, cfg[entry], include_unset)
        cfg_parsed[log_cfg['log_file'] if 'log_file' in log_cfg else log_cfg['entryname']] = log_cfg

    ## filter
    if log_file and log_file in cfg_parsed:
        return {log_file: cfg_parsed[log_file]}
    elif log_file:
        return {log_file: 'not found in {}'.format(conf_file)}
    else:
        return cfg_parsed"
get_tags;"def get_tags(filesystemid,
             keyid=None,
             key=None,
             profile=None,
             region=None,
             **kwargs):
    '''
    Return the tags associated with an EFS instance.

    filesystemid
        (string) - ID of the file system whose tags to list

    returns
        (list) - list of tags as key/value pairs

    CLI Example:

    .. code-block:: bash

        salt 'my-minion' boto_efs.get_tags efs-id
    '''
    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)
    response = client.describe_tags(FileSystemId=filesystemid)
    result = response[""Tags""]

    while ""NextMarker"" in response:
        response = client.describe_tags(FileSystemId=filesystemid,
                                        Marker=response[""NextMarker""])
        result.extend(response[""Tags""])

    return result"
get_argparser;"def get_argparser():
    """"""Argument parser""""""
    parser = argparse.ArgumentParser(description='BERT pretraining example.')
    parser.add_argument('--num_steps', type=int, default=20, help='Number of optimization steps')
    parser.add_argument('--num_buckets', type=int, default=1,
                        help='Number of buckets for variable length sequence sampling')
    parser.add_argument('--dtype', type=str, default='float32', help='data dtype')
    parser.add_argument('--batch_size', type=int, default=8, help='Batch size per GPU.')
    parser.add_argument('--accumulate', type=int, default=1,
                        help='Number of batches for gradient accumulation. '
                             'The effective batch size = batch_size * accumulate.')
    parser.add_argument('--use_avg_len', action='store_true',
                        help='Use average length information for the bucket sampler. '
                             'The batch size is approximately the number of tokens in the batch')
    parser.add_argument('--batch_size_eval', type=int, default=8,
                        help='Batch size per GPU for evaluation.')
    parser.add_argument('--dataset_name', type=str, default='book_corpus_wiki_en_uncased',
                        help='The dataset from which the vocabulary is created. Options include '
                             'book_corpus_wiki_en_uncased, book_corpus_wiki_en_cased. '
                             'Default is book_corpus_wiki_en_uncased')
    parser.add_argument('--pretrained', action='store_true',
                        help='Load the pretrained model released by Google.')
    parser.add_argument('--model', type=str, default='bert_12_768_12',
                        help='Model to run pre-training on. '
                             'Options are bert_12_768_12, bert_24_1024_16')
    parser.add_argument('--data', type=str, default=None,
                        help='Path to training data. Training is skipped if not set.')
    parser.add_argument('--data_eval', type=str, default=None,
                        help='Path to evaluation data. Evaluation is skipped if not set.')
    parser.add_argument('--ckpt_dir', type=str, required=True,
                        help='Path to checkpoint directory')
    parser.add_argument('--start_step', type=int, default=0,
                        help='Start optimization step from the checkpoint.')
    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--warmup_ratio', type=float, default=0.1,
                        help='ratio of warmup steps used in NOAM\'s stepsize schedule')
    parser.add_argument('--log_interval', type=int, default=10, help='Report interval')
    parser.add_argument('--ckpt_interval', type=int, default=250000, help='Checkpoint interval')
    parser.add_argument('--dummy_data_len', type=int, default=None,
                        help='If provided, a data batch of target sequence length is '
                             'used. For benchmarking purpuse only.')
    parser.add_argument('--seed', type=int, default=0, help='Random seed')
    parser.add_argument('--verbose', action='store_true', help='verbose logging')
    parser.add_argument('--profile', type=str, default=None,
                        help='output profiling result to the target file')
    return parser"
get_minions;"def get_minions():
    '''
    Return a list of minions
    '''
    with _get_serv(ret=None, commit=True) as cur:

        sql = '''SELECT DISTINCT id
                FROM `salt_returns`'''

        cur.execute(sql)
        data = cur.fetchall()
        ret = []
        for minion in data:
            ret.append(minion[0])
        return ret"
region_option;"def region_option(f):
    """"""
    Configures --region option for CLI

    :param f: Callback Function to be passed to Click
    """"""
    def callback(ctx, param, value):
        state = ctx.ensure_object(Context)
        state.region = value
        return value

    return click.option('--region',
                        expose_value=False,
                        help='Set the AWS Region of the service (e.g. us-east-1).',
                        callback=callback)(f)"
generate_parameters;"def generate_parameters(self, parameter_id):
        """"""Returns a dict of trial (hyper-)parameters, as a serializable object.

        Parameters
        ----------
        parameter_id : int
    
        Returns
        -------
        config : dict
        """"""
        if not self.population:
            raise RuntimeError('The population is empty')
        pos = -1
        for i in range(len(self.population)):
            if self.population[i].result is None:
                pos = i
                break
        if pos != -1:
            indiv = copy.deepcopy(self.population[pos])
            self.population.pop(pos)
            total_config = indiv.config
        else:
            random.shuffle(self.population)
            if self.population[0].result < self.population[1].result:
                self.population[0] = self.population[1]

            # mutation
            space = json2space(self.searchspace_json,
                               self.population[0].config)
            is_rand = dict()
            mutation_pos = space[random.randint(0, len(space)-1)]
            for i in range(len(self.space)):
                is_rand[self.space[i]] = (self.space[i] == mutation_pos)
            config = json2paramater(
                self.searchspace_json, is_rand, self.random_state, self.population[0].config)
            self.population.pop(1)
            # remove ""_index"" from config and save params-id

            total_config = config
        self.total_data[parameter_id] = total_config
        config = _split_index(total_config)
        return config"
clip_boxes;"def clip_boxes(boxes, im_shape):
    """"""
    Clip boxes to image boundaries.
    :param boxes: [N, 4* num_classes]
    :param im_shape: tuple of 2
    :return: [N, 4* num_classes]
    """"""
    # x1 >= 0
    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
    # y1 >= 0
    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
    # x2 < im_shape[1]
    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
    # y2 < im_shape[0]
    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
    return boxes"
check_keypoint;"def check_keypoint(kp, rows, cols):
    """"""Check if keypoint coordinates are in range [0, 1)""""""
    for name, value, size in zip(['x', 'y'], kp[:2], [cols, rows]):
        if not 0 <= value < size:
            raise ValueError(
                'Expected {name} for keypoint {kp} '
                'to be in the range [0.0, {size}], got {value}.'.format(
                    kp=kp,
                    name=name,
                    value=value,
                    size=size
                )
            )"
get_config;"def get_config(self):
        """"""Return configurations of MaxBoltzmannQPolicy

        # Returns
            Dict of config
        """"""
        config = super(MaxBoltzmannQPolicy, self).get_config()
        config['eps'] = self.eps
        config['tau'] = self.tau
        config['clip'] = self.clip
        return config"
load_repo;"def load_repo(client, path=None, index='git'):
    """"""
    Parse a git repository with all it's commits and load it into elasticsearch
    using `client`. If the index doesn't exist it will be created.
    """"""
    path = dirname(dirname(abspath(__file__))) if path is None else path
    repo_name = basename(path)
    repo = git.Repo(path)

    create_git_index(client, index)

    # we let the streaming bulk continuously process the commits as they come
    # in - since the `parse_commits` function is a generator this will avoid
    # loading all the commits into memory
    for ok, result in streaming_bulk(
            client,
            parse_commits(repo.refs.master.commit, repo_name),
            index=index,
            doc_type='doc',
            chunk_size=50 # keep the batch sizes small for appearances only
        ):
        action, result = result.popitem()
        doc_id = '/%s/doc/%s' % (index, result['_id'])
        # process the information from ES whether the document has been
        # successfully indexed
        if not ok:
            print('Failed to %s document %s: %r' % (action, doc_id, result))
        else:
            print(doc_id)"
create_marker_table;"def create_marker_table(self):
        """"""
        Create marker table if it doesn't exist.
        Use a separate connection since the transaction might have to be reset.
        """"""
        connection = self.connect()
        try:
            connection.execute_non_query(
                """""" CREATE TABLE {marker_table} (
                        id            BIGINT    NOT NULL IDENTITY(1,1),
                        update_id     VARCHAR(128)  NOT NULL,
                        target_table  VARCHAR(128),
                        inserted      DATETIME DEFAULT(GETDATE()),
                        PRIMARY KEY (update_id)
                    )
                """"""
                .format(marker_table=self.marker_table)
            )
        except _mssql.MSSQLDatabaseException as e:
            # Table already exists code
            if e.number == 2714:
                pass
            else:
                raise
        connection.close()"
target_distribution;"def target_distribution(self, samples):
        """"""posterior probability
        
        Parameters
        ----------
        samples: list
            a collection of sample, it's a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,
            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}
        
        Returns
        -------
        float
            posterior probability
        """"""
        curr_likelihood = self.likelihood(samples)
        curr_prior = self.prior(samples)
        ret = np.ones(NUM_OF_INSTANCE)
        for i in range(NUM_OF_INSTANCE):
            ret[i] = curr_likelihood[i] * curr_prior[i]
        return ret"
orchestrate_single;"def orchestrate_single(fun, name, test=None, queue=False, pillar=None, **kwargs):
    '''
    Execute a single state orchestration routine

    .. versionadded:: 2015.5.0

    CLI Example:

    .. code-block:: bash

        salt-run state.orchestrate_single fun=salt.wheel name=key.list_all
    '''
    if pillar is not None and not isinstance(pillar, dict):
        raise SaltInvocationError(
            'Pillar data must be formatted as a dictionary'
        )
    __opts__['file_client'] = 'local'
    minion = salt.minion.MasterMinion(__opts__)
    running = minion.functions['state.single'](
            fun,
            name,
            test=None,
            queue=False,
            pillar=pillar,
            **kwargs)
    ret = {minion.opts['id']: running}
    __jid_event__.fire_event({'data': ret, 'outputter': 'highstate'}, 'progress')
    return ret"
export_model;"def export_model(module_spec, class_count, saved_model_dir):
  """"""Exports model for serving.

  Args:
    module_spec: The hub.ModuleSpec for the image module being used.
    class_count: The number of classes.
    saved_model_dir: Directory in which to save exported model and variables.
  """"""
  # The SavedModel should hold the eval graph.
  sess, in_image, _, _, _, _ = build_eval_session(module_spec, class_count)
  with sess.graph.as_default() as graph:
    tf.saved_model.simple_save(
        sess,
        saved_model_dir,
        inputs={'image': in_image},
        outputs={'prediction': graph.get_tensor_by_name('final_result:0')},
        legacy_init_op=tf.group(tf.tables_initializer(), name='legacy_init_op')
    )"
fancy_handler;"def fancy_handler(signum, frame, spinner):
    """"""Signal handler, used to gracefully shut down the ``spinner`` instance
    when specified signal is received by the process running the ``spinner``.

    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``
    function for more details.
    """"""
    spinner.red.fail(""✘"")
    spinner.stop()
    sys.exit(0)"
get_backend_type;"def get_backend_type(self, location):
        # type: (str) -> Optional[Type[VersionControl]]
        """"""
        Return the type of the version control backend if found at given
        location, e.g. vcs.get_backend_type('/path/to/vcs/checkout')
        """"""
        for vc_type in self._registry.values():
            if vc_type.controls_location(location):
                logger.debug('Determine that %s uses VCS: %s',
                             location, vc_type.name)
                return vc_type
        return None"
arg_type;"def arg_type(arg_names, kwargs):
  """"""
  Returns a hashable summary of the types of arg_names within kwargs.
  :param arg_names: tuple containing names of relevant arguments
  :param kwargs: dict mapping string argument names to values.
    These must be values for which we can create a tf placeholder.
    Currently supported: numpy darray or something that can ducktype it
  returns:
    API contract is to return a hashable object describing all
    structural consequences of argument values that can otherwise
    be fed into a graph of fixed structure.
    Currently this is implemented as a tuple of tuples that track:
      - whether each argument was passed
      - whether each argument was passed and not None
      - the dtype of each argument
    Callers shouldn't rely on the exact structure of this object,
    just its hashability and one-to-one mapping between graph structures.
  """"""
  assert isinstance(arg_names, tuple)
  passed = tuple(name in kwargs for name in arg_names)
  passed_and_not_none = []
  for name in arg_names:
    if name in kwargs:
      passed_and_not_none.append(kwargs[name] is not None)
    else:
      passed_and_not_none.append(False)
  passed_and_not_none = tuple(passed_and_not_none)
  dtypes = []
  for name in arg_names:
    if name not in kwargs:
      dtypes.append(None)
      continue
    value = kwargs[name]
    if value is None:
      dtypes.append(None)
      continue
    assert hasattr(value, 'dtype'), type(value)
    dtype = value.dtype
    if not isinstance(dtype, np.dtype):
      dtype = dtype.as_np_dtype
    assert isinstance(dtype, np.dtype)
    dtypes.append(dtype)
  dtypes = tuple(dtypes)
  return (passed, passed_and_not_none, dtypes)"
retry_target;"def retry_target(target, predicate, sleep_generator, deadline, on_error=None):
    """"""Call a function and retry if it fails.

    This is the lowest-level retry helper. Generally, you'll use the
    higher-level retry helper :class:`Retry`.

    Args:
        target(Callable): The function to call and retry. This must be a
            nullary function - apply arguments with `functools.partial`.
        predicate (Callable[Exception]): A callable used to determine if an
            exception raised by the target should be considered retryable.
            It should return True to retry or False otherwise.
        sleep_generator (Iterable[float]): An infinite iterator that determines
            how long to sleep between retries.
        deadline (float): How long to keep retrying the target.
        on_error (Callable): A function to call while processing a retryable
            exception.  Any error raised by this function will *not* be
            caught.

    Returns:
        Any: the return value of the target function.

    Raises:
        google.api_core.RetryError: If the deadline is exceeded while retrying.
        ValueError: If the sleep generator stops yielding values.
        Exception: If the target raises a method that isn't retryable.
    """"""
    if deadline is not None:
        deadline_datetime = datetime_helpers.utcnow() + datetime.timedelta(
            seconds=deadline
        )
    else:
        deadline_datetime = None

    last_exc = None

    for sleep in sleep_generator:
        try:
            return target()

        # pylint: disable=broad-except
        # This function explicitly must deal with broad exceptions.
        except Exception as exc:
            if not predicate(exc):
                raise
            last_exc = exc
            if on_error is not None:
                on_error(exc)

        now = datetime_helpers.utcnow()
        if deadline_datetime is not None and deadline_datetime < now:
            six.raise_from(
                exceptions.RetryError(
                    ""Deadline of {:.1f}s exceeded while calling {}"".format(
                        deadline, target
                    ),
                    last_exc,
                ),
                last_exc,
            )

        _LOGGER.debug(
            ""Retrying due to {}, sleeping {:.1f}s ..."".format(last_exc, sleep)
        )
        time.sleep(sleep)

    raise ValueError(""Sleep generator stopped yielding sleep values."")"
get_min_eig_vec_proxy;"def get_min_eig_vec_proxy(self, use_tf_eig=False):
    """"""Computes the min eigen value and corresponding vector of matrix M.

    Args:
      use_tf_eig: Whether to use tf's default full eigen decomposition
    Returns:
      eig_vec: Minimum absolute eigen value
      eig_val: Corresponding eigen vector
    """"""
    if use_tf_eig:
      # If smoothness parameter is too small, essentially no smoothing
      # Just output the eigen vector corresponding to min
      return tf.cond(self.smooth_placeholder < 1E-8,
                     self.tf_min_eig_vec,
                     self.tf_smooth_eig_vec)

    # Using autograph to automatically handle
    # the control flow of minimum_eigen_vector
    min_eigen_tf = autograph.to_graph(utils.minimum_eigen_vector)

    def _vector_prod_fn(x):
      return self.dual_object.get_psd_product(x)

    estimated_eigen_vector = min_eigen_tf(
        x=self.eig_init_vec_placeholder,
        num_steps=self.eig_num_iter_placeholder,
        learning_rate=self.params['eig_learning_rate'],
        vector_prod_fn=_vector_prod_fn)
    return estimated_eigen_vector"
sge_submit;"def sge_submit(nslave, worker_args, worker_envs):
    """"""
      customized submit script, that submit nslave jobs, each must contain args as parameter
      note this can be a lambda function containing additional parameters in input
      Parameters
         nslave number of slave process to start up
         args arguments to launch each job
              this usually includes the parameters of master_uri and parameters passed into submit
    """"""
    env_arg = ','.join(['%s=\""%s\""' % (k, str(v)) for k, v in worker_envs.items()])
    cmd = 'qsub -cwd -t 1-%d -S /bin/bash' % nslave
    if args.queue != 'default':
        cmd += '-q %s' % args.queue
    cmd += ' -N %s ' % args.jobname
    cmd += ' -e %s -o %s' % (args.logdir, args.logdir)
    cmd += ' -pe orte %d' % (args.vcores)
    cmd += ' -v %s,PATH=${PATH}:.' % env_arg
    cmd += ' %s %s' % (runscript, ' '.join(args.command + worker_args))
    print cmd
    subprocess.check_call(cmd, shell = True)
    print 'Waiting for the jobs to get up...'"
save_historylog;"def save_historylog(self):
        """"""Save current history log (all text in console)""""""
        title = _(""Save history log"")
        self.redirect_stdio.emit(False)
        filename, _selfilter = getsavefilename(self, title,
                    self.historylog_filename, ""%s (*.log)"" % _(""History logs""))
        self.redirect_stdio.emit(True)
        if filename:
            filename = osp.normpath(filename)
            try:
                encoding.write(to_text_string(self.get_text_with_eol()),
                               filename)
                self.historylog_filename = filename
                CONF.set('main', 'historylog_filename', filename)
            except EnvironmentError:
                pass"
maybe_resume_consumer;"def maybe_resume_consumer(self):
        """"""Check the current load and resume the consumer if needed.""""""
        # If we have been paused by flow control, check and see if we are
        # back within our limits.
        #
        # In order to not thrash too much, require us to have passed below
        # the resume threshold (80% by default) of each flow control setting
        # before restarting.
        if self._consumer is None or not self._consumer.is_paused:
            return

        if self.load < self.flow_control.resume_threshold:
            self._consumer.resume()
        else:
            _LOGGER.debug(""Did not resume, current load is %s"", self.load)"
ensure_str;"def ensure_str(s, encoding=""utf-8"", errors=""strict""):
    """"""Coerce *s* to `str`.

    To keep six with lower version, see Issue 4169, we copy this function
    from six == 1.12.0.

    TODO(yuhguo): remove this function when six >= 1.12.0.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if six.PY3:
        text_type = str
        binary_type = bytes
    else:
        text_type = unicode  # noqa: F821
        binary_type = str
    if not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    if six.PY2 and isinstance(s, text_type):
        s = s.encode(encoding, errors)
    elif six.PY3 and isinstance(s, binary_type):
        s = s.decode(encoding, errors)
    return s"
srv_data;"def srv_data(target, port, prio=10, weight=10):
    '''
    Generate SRV record data
    :param target:
    :param port:
    :param prio:
    :param weight:
    :return:
    '''
    return _rec2data(prio, weight, port, target)"
get_backend_symbol;"def get_backend_symbol(self, backend):
        """"""Return symbol for target backend.

        Parameters
        ----------
        backend : str
            The backend names.

        Returns
        -------
        out : Symbol
            The created Symbol for target backend.
        """"""
        out = SymbolHandle()
        check_call(_LIB.MXGenBackendSubgraph(self.handle, c_str(backend), ctypes.byref(out)))
        return Symbol(out)"
confusion_matrix;"def confusion_matrix(targets, predictions):
    r""""""
    Compute the confusion matrix for classifier predictions.

    Parameters
    ----------
    targets : SArray
        Ground truth class labels (cannot be of type float).

    predictions : SArray
        The prediction that corresponds to each target value.
        This vector must have the same length as ``targets``. The predictions
        SArray cannot be of type float.

    Returns
    -------
    out : SFrame
        An SFrame containing counts for 'target_label', 'predicted_label' and
        'count' corresponding to each pair of true and predicted labels.

    See Also
    --------
    accuracy

    Examples
    --------
    >>> targets = turicreate.SArray([0, 1, 1, 0])
    >>> predictions = turicreate.SArray([1, 0, 1, 0])

    >>> turicreate.evaluation.confusion_matrix(targets, predictions)
    """"""

    _supervised_evaluation_error_checking(targets, predictions)
    _check_same_type_not_float(targets, predictions)
    return _turicreate.extensions._supervised_streaming_evaluator(targets,
                       predictions, ""confusion_matrix_no_map"", {})"
event_return;"def event_return(events):
    '''
    Return event to mysql server

    Requires that configuration be enabled via 'event_return'
    option in master config.
    '''
    with _get_serv(events, commit=True) as cur:
        for event in events:
            tag = event.get('tag', '')
            data = event.get('data', '')
            sql = '''INSERT INTO `salt_events` (`tag`, `data`, `master_id`)
                     VALUES (%s, %s, %s)'''
            cur.execute(sql, (tag, salt.utils.json.dumps(data), __opts__['id']))"
replace_certificate_signing_request;"def replace_certificate_signing_request(self, name, body, **kwargs):
        """"""
        replace the specified CertificateSigningRequest
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_certificate_signing_request(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the CertificateSigningRequest (required)
        :param V1beta1CertificateSigningRequest body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1beta1CertificateSigningRequest
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_certificate_signing_request_with_http_info(name, body, **kwargs)
        else:
            (data) = self.replace_certificate_signing_request_with_http_info(name, body, **kwargs)
            return data"
get_image_list;"def get_image_list(self, name, dir_structure='original'):
        """"""
        Args:
            name (str): 'train' or 'val' or 'test'
            dir_structure (str): same as in :meth:`ILSVRC12.__init__()`.
        Returns:
            list: list of (image filename, label)
        """"""
        assert name in ['train', 'val', 'test']
        assert dir_structure in ['original', 'train']
        add_label_to_fname = (name != 'train' and dir_structure != 'original')
        if add_label_to_fname:
            synset = self.get_synset_1000()

        fname = os.path.join(self.dir, name + '.txt')
        assert os.path.isfile(fname), fname
        with open(fname) as f:
            ret = []
            for line in f.readlines():
                name, cls = line.strip().split()
                cls = int(cls)

                if add_label_to_fname:
                    name = os.path.join(synset[cls], name)

                ret.append((name.strip(), cls))
        assert len(ret), fname
        return ret"
format_stackdriver_json;"def format_stackdriver_json(record, message):
    """"""Helper to format a LogRecord in in Stackdriver fluentd format.

        :rtype: str
        :returns: JSON str to be written to the log file.
    """"""
    subsecond, second = math.modf(record.created)

    payload = {
        ""message"": message,
        ""timestamp"": {""seconds"": int(second), ""nanos"": int(subsecond * 1e9)},
        ""thread"": record.thread,
        ""severity"": record.levelname,
    }

    return json.dumps(payload)"
get_similarity_task_tokens;"def get_similarity_task_tokens(args):
    """"""Returns a set of all tokens occurring the evaluation datasets.""""""
    tokens = set()
    for _, _, dataset in iterate_similarity_datasets(args):
        tokens.update(
            itertools.chain.from_iterable((d[0], d[1]) for d in dataset))
    return tokens"
replace_grist;"def replace_grist (features, new_grist):
    """""" Replaces the grist of a string by a new one.
        Returns the string with the new grist.
    """"""
    assert is_iterable_typed(features, basestring) or isinstance(features, basestring)
    assert isinstance(new_grist, basestring)
    # this function is used a lot in the build phase and the original implementation
    # was extremely slow; thus some of the weird-looking optimizations for this function.
    single_item = False
    if isinstance(features, str):
        features = [features]
        single_item = True

    result = []
    for feature in features:
        # '<feature>value' -> ('<feature', '>', 'value')
        # 'something' -> ('something', '', '')
        # '<toolset>msvc/<feature>value' -> ('<toolset', '>', 'msvc/<feature>value')
        grist, split, value = feature.partition('>')
        # if a partition didn't occur, then grist is just 'something'
        # set the value to be the grist
        if not value and not split:
            value = grist
        result.append(new_grist + value)

    if single_item:
        return result[0]
    return result"
tensor_info_proto_maps_match;"def tensor_info_proto_maps_match(map_a, map_b):
  """"""Whether two signature inputs/outputs match in dtype, shape and sparsity.

  Args:
    map_a: A proto map<string,TensorInfo>.
    map_b: A proto map<string,TensorInfo>.

  Returns:
    A boolean whether `map_a` and `map_b` tensors have the same dtype, shape and
    sparsity.
  """"""
  iter_a = sorted(parse_tensor_info_map(map_a).items())
  iter_b = sorted(parse_tensor_info_map(map_b).items())
  if len(iter_a) != len(iter_b):
    return False  # Mismatch count.
  for info_a, info_b in zip(iter_a, iter_b):
    if info_a[0] != info_b[0]:
      return False  # Mismatch keys.
    if _is_sparse(info_a[1]) != _is_sparse(info_b[1]):
      return False
    if info_a[1].dtype != info_b[1].dtype:
      return False
    if not _shape_match(info_a[1].get_shape(), info_b[1].get_shape()):
      return False
  return True"
create_namespaced_local_subject_access_review;"def create_namespaced_local_subject_access_review(self, namespace, body, **kwargs):
        """"""
        create a LocalSubjectAccessReview
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.create_namespaced_local_subject_access_review(namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1LocalSubjectAccessReview body: (required)
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :param str pretty: If 'true', then the output is pretty printed.
        :return: V1LocalSubjectAccessReview
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.create_namespaced_local_subject_access_review_with_http_info(namespace, body, **kwargs)
        else:
            (data) = self.create_namespaced_local_subject_access_review_with_http_info(namespace, body, **kwargs)
            return data"
modify_write;"def modify_write(self, write_pb, **unused_kwargs):
        """"""Modify a ``Write`` protobuf based on the state of this write option.

        If:

        * ``exists=True``, adds a precondition that requires existence
        * ``exists=False``, adds a precondition that requires non-existence

        Args:
            write_pb (google.cloud.firestore_v1beta1.types.Write): A
                ``Write`` protobuf instance to be modified with a precondition
                determined by the state of this option.
            unused_kwargs (Dict[str, Any]): Keyword arguments accepted by
                other subclasses that are unused here.
        """"""
        current_doc = types.Precondition(exists=self._exists)
        write_pb.current_document.CopyFrom(current_doc)"
create_app;"def create_app(name, site, sourcepath, apppool=None):
    '''
    Create an IIS application.

    .. note:

        This function only validates against the application name, and will return True
        even if the application already exists with a different configuration. It will not
        modify the configuration of an existing application.

    :param str name: The IIS application.
    :param str site: The IIS site name.
    :param str sourcepath: The physical path.
    :param str apppool: The name of the IIS application pool.

    Example of usage with only the required arguments:

    .. code-block:: yaml

        site0-v1-app:
            win_iis.create_app:
                - name: v1
                - site: site0
                - sourcepath: C:\\inetpub\\site0\\v1

    Example of usage specifying all available arguments:

    .. code-block:: yaml

        site0-v1-app:
            win_iis.create_app:
                - name: v1
                - site: site0
                - sourcepath: C:\\inetpub\\site0\\v1
                - apppool: site0
    '''
    ret = {'name': name,
           'changes': {},
           'comment': str(),
           'result': None}

    current_apps = __salt__['win_iis.list_apps'](site)

    if name in current_apps:
        ret['comment'] = 'Application already present: {0}'.format(name)
        ret['result'] = True
    elif __opts__['test']:
        ret['comment'] = 'Application will be created: {0}'.format(name)
        ret['changes'] = {'old': None,
                          'new': name}
    else:
        ret['comment'] = 'Created application: {0}'.format(name)
        ret['changes'] = {'old': None,
                          'new': name}
        ret['result'] = __salt__['win_iis.create_app'](name, site, sourcepath,
                                                       apppool)
    return ret"
add_configurations;"def add_configurations(self, experiments):
        """"""Chains generator given experiment specifications.

        Arguments:
            experiments (Experiment | list | dict): Experiments to run.
        """"""
        experiment_list = convert_to_experiment_list(experiments)
        for experiment in experiment_list:
            self._trial_generator = itertools.chain(
                self._trial_generator,
                self._generate_trials(experiment.spec, experiment.name))"
split_params;"def split_params(sym, params):
        """"""Helper function to split params dictionary into args and aux params

        Parameters
        ----------
        sym : :class:`~mxnet.symbol.Symbol`
            MXNet symbol object
        params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`
            Dict of converted parameters stored in ``mxnet.ndarray.NDArray`` format

        Returns
        -------
        arg_params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`
            Dict of converted parameters stored in ``mxnet.ndarray.NDArray`` format
        aux_params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`
            Dict of converted parameters stored in ``mxnet.ndarray.NDArray`` format
        """"""
        arg_params = {}
        aux_params = {}
        for args in sym.list_arguments():
            if args in params:
                arg_params.update({args: nd.array(params[args])})
        for aux in sym.list_auxiliary_states():
            if aux in params:
                aux_params.update({aux: nd.array(params[aux])})
        return arg_params, aux_params"
read_namespaced_replica_set_scale;"def read_namespaced_replica_set_scale(self, name, namespace, **kwargs):
        """"""
        read scale of the specified ReplicaSet
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_replica_set_scale(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Scale (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :return: V1Scale
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_replica_set_scale_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_replica_set_scale_with_http_info(name, namespace, **kwargs)
            return data"
has_any_role;"def has_any_role(*items):
    r""""""A :func:`.check` that is added that checks if the member invoking the
    command has **any** of the roles specified. This means that if they have
    one out of the three roles specified, then this check will return `True`.

    Similar to :func:`.has_role`\, the names or IDs passed in must be exact.

    This check raises one of two special exceptions, :exc:`.MissingAnyRole` if the user
    is missing all roles, or :exc:`.NoPrivateMessage` if it is used in a private message.
    Both inherit from :exc:`.CheckFailure`.

    .. versionchanged:: 1.1.0

        Raise :exc:`.MissingAnyRole` or :exc:`.NoPrivateMessage`
        instead of generic :exc:`.CheckFailure`

    Parameters
    -----------
    items: List[Union[:class:`str`, :class:`int`]]
        An argument list of names or IDs to check that the member has roles wise.

    Example
    --------

    .. code-block:: python3

        @bot.command()
        @commands.has_any_role('Library Devs', 'Moderators', 492212595072434186)
        async def cool(ctx):
            await ctx.send('You are cool indeed')
    """"""
    def predicate(ctx):
        if not isinstance(ctx.channel, discord.abc.GuildChannel):
            raise NoPrivateMessage()

        getter = functools.partial(discord.utils.get, ctx.author.roles)
        if any(getter(id=item) is not None if isinstance(item, int) else getter(name=item) is not None for item in items):
            return True
        raise MissingAnyRole(items)

    return check(predicate)"
columnwise_summary;"def columnwise_summary(sf):
    """"""
    Plots a columnwise summary of the sframe provided as input, 
    and returns the resulting Plot object.
    
    The function supports SFrames.

    Parameters
    ----------
    sf : SFrame
      The data to get a columnwise summary for.
    
    Returns
    -------
    out : Plot
      A :class: Plot object that is the columnwise summary plot.

    Examples
    --------
    Make a columnwise summary of an SFrame.

    >>> x = turicreate.SArray([1,2,3,4,5])
    >>> s = turicreate.SArray(['a','b','c','a','a'])
    >>> sf_test = turicreate.SFrame([x,x,x,x,s,s,s,x,s,x,s,s,s,x,x])
    >>> colsum = turicreate.visualization.columnwise_summary(sf_test)
    """"""
    if not isinstance(sf, tc.data_structures.sframe.SFrame):
        raise ValueError(""turicreate.visualization.columnwise_summary "" + 
            ""supports SFrame"")
    plt_ref = tc.extensions.plot_columnwise_summary(sf)
    return Plot(plt_ref)"
replace_csi_driver;"def replace_csi_driver(self, name, body, **kwargs):
        """"""
        replace the specified CSIDriver
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_csi_driver(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the CSIDriver (required)
        :param V1beta1CSIDriver body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1beta1CSIDriver
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_csi_driver_with_http_info(name, body, **kwargs)
        else:
            (data) = self.replace_csi_driver_with_http_info(name, body, **kwargs)
            return data"
set_editor_doc;"def set_editor_doc(self, doc, force_refresh=False):
        """"""
        Use the help plugin to show docstring dictionary computed
        with introspection plugin from the Editor plugin
        """"""
        if (self.locked and not force_refresh):
            return
        self.switch_to_editor_source()
        self._last_editor_doc = doc
        self.object_edit.setText(doc['obj_text'])

        if self.rich_help:
            self.render_sphinx_doc(doc)
        else:
            self.set_plain_text(doc, is_code=False)

        if self.dockwidget is not None:
            self.dockwidget.blockSignals(True)
        self.__eventually_raise_help(doc['docstring'], force=force_refresh)
        if self.dockwidget is not None:
            self.dockwidget.blockSignals(False)"
invalid_request_content;"def invalid_request_content(message):
        """"""
        Creates a Lambda Service InvalidRequestContent Response

        Parameters
        ----------
        message str
            Message to be added to the body of the response

        Returns
        -------
        Flask.Response
            A response object representing the InvalidRequestContent Error
        """"""
        exception_tuple = LambdaErrorResponses.InvalidRequestContentException

        return BaseLocalService.service_response(
            LambdaErrorResponses._construct_error_response_body(LambdaErrorResponses.USER_ERROR, message),
            LambdaErrorResponses._construct_headers(exception_tuple[0]),
            exception_tuple[1]
        )"
generate_scenario;"def generate_scenario(ss_content):
    """"""Generate the scenario. The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and 
    can be constructed either by providing an actual scenario-object, or by specifing the options in a scenario file.
    
    Reference: https://automl.github.io/SMAC3/stable/options.html

    The format of the scenario file is one option per line:
    OPTION1 = VALUE1
    OPTION2 = VALUE2
    ...

    Parameters
    ----------
    abort_on_first_run_crash: bool
        If true, SMAC will abort if the first run of the target algorithm crashes. Default: True, 
        because trials reported to nni tuner would always in success state
    algo: function
        Specifies the target algorithm call that SMAC will optimize. Interpreted as a bash-command.
        Not required by tuner, but required by nni's training service for running trials
    always_race_default:
        Race new incumbents always against default configuration
    cost_for_crash:
        Defines the cost-value for crashed runs on scenarios with quality as run-obj. Default: 2147483647.0.
        Trials reported to nni tuner would always in success state
    cutoff_time:
        Maximum runtime, after which the target algorithm is cancelled. `Required if *run_obj* is runtime`
    deterministic: bool
        If true, the optimization process will be repeatable.
    execdir:
        Specifies the path to the execution-directory. Default: .
        Trials are executed by nni's training service
    feature_file:
        Specifies the file with the instance-features.
        No features specified or feature file is not supported
    initial_incumbent:
        DEFAULT is the default from the PCS. Default: DEFAULT. Must be from: [‘DEFAULT’, ‘RANDOM’].
    input_psmac_dirs:
        For parallel SMAC, multiple output-directories are used.
        Parallelism is supported by nni
    instance_file:
        Specifies the file with the training-instances. Not supported
    intensification_percentage:
        The fraction of time to be used on intensification (versus choice of next Configurations). Default: 0.5.
        Not supported, trials are controlled by nni's training service and kill be assessor
    maxR: int
        Maximum number of calls per configuration. Default: 2000.
    memory_limit:
        Maximum available memory the target algorithm can occupy before being cancelled.
    minR: int
        Minimum number of calls per configuration. Default: 1.
    output_dir:
        Specifies the output-directory for all emerging files, such as logging and results.
        Default: smac3-output_2018-01-22_15:05:56_807070.
    overall_obj:
    	PARX, where X is an integer defining the penalty imposed on timeouts (i.e. runtimes that exceed the cutoff-time).
        Timeout is not supported
    paramfile:
        Specifies the path to the PCS-file.
    run_obj:
        Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well.
        Must be from: [‘runtime’, ‘quality’].
    runcount_limit: int
        Maximum number of algorithm-calls during optimization. Default: inf.
        Use default because this is controlled by nni
    shared_model:
        Whether to run SMAC in parallel mode. Parallelism is supported by nni
    test_instance_file:
        Specifies the file with the test-instances. Instance is not supported
    tuner-timeout:
        Maximum amount of CPU-time used for optimization. Not supported
    wallclock_limit: int
        Maximum amount of wallclock-time used for optimization. Default: inf.
        Use default because this is controlled by nni

    Returns
    -------
    Scenario:
        The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and can be constructed
        either by providing an actual scenario-object, or by specifing the options in a scenario file
    """"""
    with open('scenario.txt', 'w') as sce_fd:
        sce_fd.write('deterministic = 0\n')
        #sce_fd.write('output_dir = \n')
        sce_fd.write('paramfile = param_config_space.pcs\n')
        sce_fd.write('run_obj = quality\n')

    return generate_pcs(ss_content)"
resolve_deps;"def resolve_deps(
    deps,
    which,
    project,
    sources=None,
    python=False,
    clear=False,
    pre=False,
    allow_global=False,
    req_dir=None
):
    """"""Given a list of dependencies, return a resolved list of dependencies,
    using pip-tools -- and their hashes, using the warehouse API / pip.
    """"""
    index_lookup = {}
    markers_lookup = {}
    python_path = which(""python"", allow_global=allow_global)
    if not os.environ.get(""PIP_SRC""):
        os.environ[""PIP_SRC""] = project.virtualenv_src_location
    backup_python_path = sys.executable
    results = []
    resolver = None
    if not deps:
        return results, resolver
    # First (proper) attempt:
    req_dir = req_dir if req_dir else os.environ.get(""req_dir"", None)
    if not req_dir:
        from .vendor.vistir.path import create_tracked_tempdir
        req_dir = create_tracked_tempdir(prefix=""pipenv-"", suffix=""-requirements"")
    with HackedPythonVersion(python_version=python, python_path=python_path):
        try:
            results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
                deps,
                index_lookup,
                markers_lookup,
                project,
                sources,
                clear,
                pre,
                req_dir=req_dir,
            )
        except RuntimeError:
            # Don't exit here, like usual.
            results = None
    # Second (last-resort) attempt:
    if results is None:
        with HackedPythonVersion(
            python_version=""."".join([str(s) for s in sys.version_info[:3]]),
            python_path=backup_python_path,
        ):
            try:
                # Attempt to resolve again, with different Python version information,
                # particularly for particularly particular packages.
                results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
                    deps,
                    index_lookup,
                    markers_lookup,
                    project,
                    sources,
                    clear,
                    pre,
                    req_dir=req_dir,
                )
            except RuntimeError:
                sys.exit(1)
    return results, resolver"
check_tool_aux;"def check_tool_aux(command):
    """""" Checks if 'command' can be found either in path
        or is a full name to an existing file.
    """"""
    assert isinstance(command, basestring)
    dirname = os.path.dirname(command)
    if dirname:
        if os.path.exists(command):
            return command
        # Both NT and Cygwin will run .exe files by their unqualified names.
        elif on_windows() and os.path.exists(command + '.exe'):
            return command
        # Only NT will run .bat files by their unqualified names.
        elif os_name() == 'NT' and os.path.exists(command + '.bat'):
            return command
    else:
        paths = path.programs_path()
        if path.glob(paths, [command]):
            return command"
get_cname;"def get_cname(name=None, canonical=None, return_fields=None, **api_opts):
    '''
    Get CNAME information.

    CLI Examples:

    .. code-block:: bash

        salt-call infoblox.get_cname name=example.example.com
        salt-call infoblox.get_cname canonical=example-ha-0.example.com
    '''
    infoblox = _get_infoblox(**api_opts)
    o = infoblox.get_cname(name=name, canonical=canonical, return_fields=return_fields)
    return o"
get_config;"def get_config(config_path):
    """"""Retrieve the config from the specified path, returning a config dict.""""""
    if not os.path.exists(config_path):
        raise ConfigDoesNotExistException

    logger.debug('config_path is {0}'.format(config_path))
    with io.open(config_path, encoding='utf-8') as file_handle:
        try:
            yaml_dict = poyo.parse_string(file_handle.read())
        except poyo.exceptions.PoyoException as e:
            raise InvalidConfiguration(
                'Unable to parse YAML file {}. Error: {}'
                ''.format(config_path, e)
            )

    config_dict = merge_configs(DEFAULT_CONFIG, yaml_dict)

    raw_replay_dir = config_dict['replay_dir']
    config_dict['replay_dir'] = _expand_path(raw_replay_dir)

    raw_cookies_dir = config_dict['cookiecutters_dir']
    config_dict['cookiecutters_dir'] = _expand_path(raw_cookies_dir)

    return config_dict"
extend_dict_key_value;"def extend_dict_key_value(
        in_dict,
        keys,
        value,
        delimiter=DEFAULT_TARGET_DELIM,
        ordered_dict=False):
    '''
    Ensures that in_dict contains the series of recursive keys defined in keys.
    Also extends the list, that is at the end of `in_dict` traversed with `keys`,
    with `value`.

    :param dict in_dict: The dictionary to work with
    :param str keys: The delimited string with one or more keys.
    :param any value: The value to extend the nested dict-key with.
    :param str delimiter: The delimiter to use in `keys`. Defaults to ':'.
    :param bool ordered_dict: Create OrderedDicts if keys are missing.
                              Default: create regular dicts.

    :return dict: Though it updates in_dict in-place.
    '''
    dict_pointer, last_key = _dict_rpartition(
        in_dict,
        keys,
        delimiter=delimiter,
        ordered_dict=ordered_dict)
    if last_key not in dict_pointer or dict_pointer[last_key] is None:
        dict_pointer[last_key] = []
    try:
        dict_pointer[last_key].extend(value)
    except AttributeError:
        raise SaltInvocationError('The last key contains a {}, which cannot extend.'
                                  ''.format(type(dict_pointer[last_key])))
    except TypeError:
        raise SaltInvocationError('Cannot extend {} with a {}.'
                                  ''.format(type(dict_pointer[last_key]), type(value)))
    return in_dict"
configure_logger;"def configure_logger(stream_level='DEBUG', debug_file=None):
    """"""Configure logging for cookiecutter.

    Set up logging to stdout with given level. If ``debug_file`` is given set
    up logging to file with DEBUG level.
    """"""
    # Set up 'cookiecutter' logger
    logger = logging.getLogger('cookiecutter')
    logger.setLevel(logging.DEBUG)

    # Remove all attached handlers, in case there was
    # a logger with using the name 'cookiecutter'
    del logger.handlers[:]

    # Create a file handler if a log file is provided
    if debug_file is not None:
        debug_formatter = logging.Formatter(LOG_FORMATS['DEBUG'])
        file_handler = logging.FileHandler(debug_file)
        file_handler.setLevel(LOG_LEVELS['DEBUG'])
        file_handler.setFormatter(debug_formatter)
        logger.addHandler(file_handler)

    # Get settings based on the given stream_level
    log_formatter = logging.Formatter(LOG_FORMATS[stream_level])
    log_level = LOG_LEVELS[stream_level]

    # Create a stream handler
    stream_handler = logging.StreamHandler(stream=sys.stdout)
    stream_handler.setLevel(log_level)
    stream_handler.setFormatter(log_formatter)
    logger.addHandler(stream_handler)

    return logger"
create_optimizer;"def create_optimizer(name, **kwargs):
        """"""Instantiates an optimizer with a given name and kwargs.

        .. note:: We can use the alias `create` for ``Optimizer.create_optimizer``.

        Parameters
        ----------
        name: str
            Name of the optimizer. Should be the name
            of a subclass of Optimizer. Case insensitive.

        kwargs: dict
            Parameters for the optimizer.

        Returns
        -------
        Optimizer
            An instantiated optimizer.

        Examples
        --------
        >>> sgd = mx.optimizer.Optimizer.create_optimizer('sgd')
        >>> type(sgd)
        <class 'mxnet.optimizer.SGD'>
        >>> adam = mx.optimizer.create('adam', learning_rate=.1)
        >>> type(adam)
        <class 'mxnet.optimizer.Adam'>
        """"""
        if name.lower() in Optimizer.opt_registry:
            return Optimizer.opt_registry[name.lower()](**kwargs)
        else:
            raise ValueError('Cannot find optimizer %s' % name)"
bigtable_admins;"def bigtable_admins(self):
        """"""Access to bigtable.admin role memebers

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_admins_policy]
            :end-before: [END bigtable_admins_policy]
        """"""
        result = set()
        for member in self._bindings.get(BIGTABLE_ADMIN_ROLE, ()):
            result.add(member)
        return frozenset(result)"
get_mount_targets;"def get_mount_targets(filesystemid=None,
                      mounttargetid=None,
                      keyid=None,
                      key=None,
                      profile=None,
                      region=None,
                      **kwargs):
    '''
    Get all the EFS mount point properties for a specific filesystemid or
    the properties for a specific mounttargetid. One or the other must be
    specified

    filesystemid
        (string) - ID of the file system whose mount targets to list
                   Must be specified if mounttargetid is not

    mounttargetid
        (string) - ID of the mount target to have its properties returned
                   Must be specified if filesystemid is not

    returns
        (list[dict]) - list of all mount point properties

    CLI Example:

    .. code-block:: bash

        salt 'my-minion' boto_efs.get_mount_targets
    '''

    result = None
    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)

    if filesystemid:
        response = client.describe_mount_targets(FileSystemId=filesystemid)
        result = response[""MountTargets""]
        while ""NextMarker"" in response:
            response = client.describe_mount_targets(FileSystemId=filesystemid,
                                                 Marker=response[""NextMarker""])
            result.extend(response[""MountTargets""])
    elif mounttargetid:
        response = client.describe_mount_targets(MountTargetId=mounttargetid)
        result = response[""MountTargets""]

    return result"
first_timestamp;"def first_timestamp(self, event_key=None):
    """"""Obtain the first timestamp.

    Args:
      event_key: the type key of the sought events (e.g., constants.NAN_KEY).
      If None, includes all event type keys.

    Returns:
      First (earliest) timestamp of all the events of the given type (or all
        event types if event_key is None).
    """"""
    if event_key is None:
      timestamps = [self._trackers[key].first_timestamp
                    for key in self._trackers]
      return min(timestamp for timestamp in timestamps if timestamp >= 0)
    else:
      return self._trackers[event_key].first_timestamp"
ensure_object;"def ensure_object(self, object_type):
        """"""Like :meth:`find_object` but sets the innermost object to a
        new instance of `object_type` if it does not exist.
        """"""
        rv = self.find_object(object_type)
        if rv is None:
            self.obj = rv = object_type()
        return rv"
request_chunked;"def request_chunked(self, method, url, body=None, headers=None):
        """"""
        Alternative to the common request method, which sends the
        body with chunked encoding and not as one block
        """"""
        headers = HTTPHeaderDict(headers if headers is not None else {})
        skip_accept_encoding = 'accept-encoding' in headers
        skip_host = 'host' in headers
        self.putrequest(
            method,
            url,
            skip_accept_encoding=skip_accept_encoding,
            skip_host=skip_host
        )
        for header, value in headers.items():
            self.putheader(header, value)
        if 'transfer-encoding' not in headers:
            self.putheader('Transfer-Encoding', 'chunked')
        self.endheaders()

        if body is not None:
            stringish_types = six.string_types + (bytes,)
            if isinstance(body, stringish_types):
                body = (body,)
            for chunk in body:
                if not chunk:
                    continue
                if not isinstance(chunk, bytes):
                    chunk = chunk.encode('utf8')
                len_str = hex(len(chunk))[2:]
                self.send(len_str.encode('utf-8'))
                self.send(b'\r\n')
                self.send(chunk)
                self.send(b'\r\n')

        # After the if clause, to always have a closed body
        self.send(b'0\r\n\r\n')"
list_instances;"def list_instances(self):
        """"""List instances owned by the project.

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_list_instances]
            :end-before: [END bigtable_list_instances]

        :rtype: tuple
        :returns:
            (instances, failed_locations), where 'instances' is list of
            :class:`google.cloud.bigtable.instance.Instance`, and
            'failed_locations' is a list of locations which could not
            be resolved.
        """"""
        resp = self.instance_admin_client.list_instances(self.project_path)
        instances = [Instance.from_pb(instance, self) for instance in resp.instances]
        return instances, resp.failed_locations"
read_json;"def read_json(cls, filename):
        """"""
        Construct an SArray from a json file or glob of json files.
        The json file must contain a list of dictionaries. The returned
        SArray type will be of dict type

        Parameters
        ----------
        filename : str
          The filename or glob to load into an SArray.

        Examples
        --------
        Construct an SArray from a local JSON file named 'data.json':

        >>> turicreate.SArray.read_json('/data/data.json')

        Construct an SArray from all JSON files /data/data*.json

        >>> turicreate.SArray.read_json('/data/data*.json')

        """"""
        proxy = UnitySArrayProxy()
        proxy.load_from_json_record_files(_make_internal_url(filename))
        return cls(_proxy = proxy)"
context_menu_requested;"def context_menu_requested(self, event):
        """"""Popup context menu.""""""
        if self.fig:
            pos = QPoint(event.x(), event.y())
            context_menu = QMenu(self)
            context_menu.addAction(ima.icon('editcopy'), ""Copy Image"",
                                   self.copy_figure,
                                   QKeySequence(
                                       get_shortcut('plots', 'copy')))
            context_menu.popup(self.mapToGlobal(pos))"
get_value;"def get_value (property):
    """""" Gets the value of a property, that is, the part following the grist, if any.
    """"""
    assert is_iterable_typed(property, basestring) or isinstance(property, basestring)
    return replace_grist (property, '')"
ext_pillar;"def ext_pillar(minion_id,  # pylint: disable=W0613
               pillar,  # pylint: disable=W0613
               command):
    '''
    Execute a command and read the output as YAMLEX
    '''
    try:
        command = command.replace('%s', minion_id)
        return deserialize(__salt__['cmd.run']('{0}'.format(command)))
    except Exception:
        log.critical('YAML data from %s failed to parse', command)
        return {}"
get_impl_ver;"def get_impl_ver():
    # type: () -> str
    """"""Return implementation version.""""""
    impl_ver = get_config_var(""py_version_nodot"")
    if not impl_ver or get_abbr_impl() == 'pp':
        impl_ver = ''.join(map(str, get_impl_version_info()))
    return impl_ver"
add_softmax;"def add_softmax(self, name, input_name, output_name):
        """"""
        Add a softmax layer to the model.

        Parameters
        ----------
        name: str
            The name of this layer.
        input_name: str
            The input blob name of this layer.
        output_name: str
            The output blob name of this layer.

        See Also
        --------
        add_activation, add_inner_product, add_convolution
        """"""

        spec = self.spec
        nn_spec = self.nn_spec

        # Add a new layer
        spec_layer = nn_spec.layers.add()
        spec_layer.name = name
        spec_layer.input.append(input_name)
        spec_layer.output.append(output_name)
        spec_layer_params = spec_layer.softmax.MergeFromString(b'')"
get_exception;"def get_exception(self):
        """"""
        Return any exception that happened during the last server request.
        This can be used to fetch more specific error information after using
        calls like `start_client`.  The exception (if any) is cleared after
        this call.

        :return:
            an exception, or ``None`` if there is no stored exception.

        .. versionadded:: 1.1
        """"""
        self.lock.acquire()
        try:
            e = self.saved_exception
            self.saved_exception = None
            return e
        finally:
            self.lock.release()"
document_did_save_notification;"def document_did_save_notification(self, params):
        """"""
        Handle the textDocument/didSave message received from an LSP server.
        """"""
        text = None
        if 'text' in params:
            text = params['text']
        params = {
            'textDocument': {
                'uri': path_as_uri(params['file'])
            }
        }
        if text is not None:
            params['text'] = text
        return params"
read_binary_matrix;"def read_binary_matrix(filename):
  """"""Reads and returns binary formatted matrix stored in filename.

  The file format is described on the data set page:
  https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/

  Args:
    filename: String with path to the file.

  Returns:
    Numpy array contained in the file.
  """"""
  with tf.io.gfile.GFile(filename, ""rb"") as f:
    s = f.read()

    # Data is stored in little-endian byte order.
    int32_dtype = np.dtype(""int32"").newbyteorder(""<"")

    # The first 4 bytes contain a magic code that specifies the data type.
    magic = int(np.frombuffer(s, dtype=int32_dtype, count=1))
    if magic == 507333717:
      data_dtype = np.dtype(""uint8"")  # uint8 does not have a byte order.
    elif magic == 507333716:
      data_dtype = np.dtype(""int32"").newbyteorder(""<"")
    else:
      raise ValueError(""Invalid magic value for data type!"")

    # The second 4 bytes contain an int32 with the number of dimensions of the
    # stored array.
    ndim = int(np.frombuffer(s, dtype=int32_dtype, count=1, offset=4))

    # The next ndim x 4 bytes contain the shape of the array in int32.
    dims = np.frombuffer(s, dtype=int32_dtype, count=ndim, offset=8)

    # If the array has less than three dimensions, three int32 are still used to
    # save the shape info (remaining int32 are simply set to 1). The shape info
    # hence uses max(3, ndim) bytes.
    bytes_used_for_shape_info = max(3, ndim) * 4

    # The remaining bytes are the array.
    data = np.frombuffer(
        s, dtype=data_dtype, offset=8 + bytes_used_for_shape_info)
  return data.reshape(tuple(dims))"
map_exp_ids;"def map_exp_ids(self, exp, positions=False):
        """"""Maps ids to words or word-position strings.

        Args:
            exp: list of tuples [(id, weight), (id,weight)]
            positions: if True, also return word positions

        Returns:
            list of tuples (word, weight), or (word_positions, weight) if
            examples: ('bad', 1) or ('bad_3-6-12', 1)
        """"""
        if positions:
            exp = [('%s_%s' % (
                self.indexed_string.word(x[0]),
                '-'.join(
                    map(str,
                        self.indexed_string.string_position(x[0])))), x[1])
                   for x in exp]
        else:
            exp = [(self.indexed_string.word(x[0]), x[1]) for x in exp]
        return exp"
get_agent_class;"def get_agent_class(alg):
    """"""Returns the class of a known agent given its name.""""""

    try:
        return _get_agent_class(alg)
    except ImportError:
        from ray.rllib.agents.mock import _agent_import_failed
        return _agent_import_failed(traceback.format_exc())"
join_images;"def join_images(img_files, out_file):
    """"""Join the list of images into the out file""""""
    images = [PIL.Image.open(f) for f in img_files]
    joined = PIL.Image.new(
        'RGB',
        (sum(i.size[0] for i in images), max(i.size[1] for i in images))
    )
    left = 0
    for img in images:
        joined.paste(im=img, box=(left, 0))
        left = left + img.size[0]
    joined.save(out_file)"
get_flat_size;"def get_flat_size(self):
        """"""Returns the total length of all of the flattened variables.

        Returns:
            The length of all flattened variables concatenated.
        """"""
        return sum(
            np.prod(v.get_shape().as_list()) for v in self.variables.values())"
create_issue;"def create_issue(project,
                 summary,
                 description,
                 template_engine='jinja',
                 context=None,
                 defaults=None,
                 saltenv='base',
                 issuetype='Bug',
                 priority='Normal',
                 labels=None,
                 assignee=None,
                 server=None,
                 username=None,
                 password=None,
                 **kwargs):
    '''
    Create a JIRA issue using the named settings. Return the JIRA ticket ID.

    project
        The name of the project to attach the JIRA ticket to.

    summary
        The summary (title) of the JIRA ticket. When the ``template_engine``
        argument is set to a proper value of an existing Salt template engine
        (e.g., ``jinja``, ``mako``, etc.) it will render the ``summary`` before
        creating the ticket.

    description
        The full body description of the JIRA ticket. When the ``template_engine``
        argument is set to a proper value of an existing Salt template engine
        (e.g., ``jinja``, ``mako``, etc.) it will render the ``description`` before
        creating the ticket.

    template_engine: ``jinja``
        The name of the template engine to be used to render the values of the
        ``summary`` and ``description`` arguments. Default: ``jinja``.

    context: ``None``
        The context to pass when rendering the ``summary`` and ``description``.
        This argument is ignored when ``template_engine`` is set as ``None``

    defaults: ``None``
        Default values to pass to the Salt rendering pipeline for the
        ``summary`` and ``description`` arguments.
        This argument is ignored when ``template_engine`` is set as ``None``.

    saltenv: ``base``
        The Salt environment name (for the rendering system).

    issuetype: ``Bug``
        The type of the JIRA ticket. Default: ``Bug``.

    priority: ``Normal``
        The priority of the JIRA ticket. Default: ``Normal``.

    labels: ``None``
        A list of labels to add to the ticket.

    assignee: ``None``
        The name of the person to assign the ticket to.

    CLI Examples:

    .. code-block:: bash

        salt '*' jira.create_issue NET 'Ticket title' 'Ticket description'
        salt '*' jira.create_issue NET 'Issue on {{ opts.id }}' 'Error detected on {{ opts.id }}' template_engine=jinja
    '''
    if template_engine:
        summary = __salt__['file.apply_template_on_contents'](summary,
                                                              template=template_engine,
                                                              context=context,
                                                              defaults=defaults,
                                                              saltenv=saltenv)
        description = __salt__['file.apply_template_on_contents'](description,
                                                                  template=template_engine,
                                                                  context=context,
                                                                  defaults=defaults,
                                                                  saltenv=saltenv)
    jira_ = _get_jira(server=server,
                      username=username,
                      password=password)
    if not labels:
        labels = []
    data = {
        'project': {
            'key': project
        },
        'summary': summary,
        'description': description,
        'issuetype': {
            'name': issuetype
        },
        'priority': {
            'name': priority
        },
        'labels': labels
    }
    data.update(clean_kwargs(**kwargs))
    issue = jira_.create_issue(data)
    issue_key = str(issue)
    if assignee:
        assign_issue(issue_key, assignee)
    return issue_key"
decoded_output_boxes;"def decoded_output_boxes(self):
        """""" Returns: N x #class x 4 """"""
        anchors = tf.tile(tf.expand_dims(self.proposals.boxes, 1),
                          [1, cfg.DATA.NUM_CLASS, 1])   # N x #class x 4
        decoded_boxes = decode_bbox_target(
            self.box_logits / self.bbox_regression_weights,
            anchors
        )
        return decoded_boxes"
init_params;"def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,
                    allow_missing=False, force_init=False, allow_extra=False):
        """"""Initializes the parameters and auxiliary states.

        Parameters
        ----------
        initializer : Initializer
            Called to initialize parameters if needed.
        arg_params : dict
            If not ``None``, should be a dictionary of existing arg_params. Initialization
            will be copied from that.
        aux_params : dict
            If not ``None``, should be a dictionary of existing aux_params. Initialization
            will be copied from that.
        allow_missing : bool
            If ``True``, params could contain missing values, and the initializer will be
            called to fill those missing params.
        force_init : bool
            If ``True``, will force re-initialize even if already initialized.
        allow_extra : boolean, optional
            Whether allow extra parameters that are not needed by symbol.
            If this is True, no error will be thrown when arg_params or aux_params
            contain extra parameters that is not needed by the executor.
        """"""
        if self.params_initialized and not force_init:
            warnings.warn(""Parameters already initialized and force_init=False. ""
                          ""init_params call ignored."", stacklevel=2)
            return
        assert self.binded, 'call bind before initializing the parameters'

        def _impl(name, arr, cache):
            """"""Internal helper for parameter initialization""""""
            if cache is not None:
                if name in cache:
                    cache_arr = cache[name]

                    # just in case the cached array is just the target itself
                    if cache_arr is not arr:
                        cache_arr.copyto(arr)
                else:
                    if not allow_missing:
                        raise RuntimeError(""%s is not presented"" % name)
                    if initializer is not None:
                        initializer(name, arr)
            else:
                initializer(name, arr)

        attrs = self._symbol.attr_dict()
        for name, arr in sorted(self._arg_params.items()):
            desc = InitDesc(name, attrs.get(name, None))
            _impl(desc, arr, arg_params)

        for name, arr in sorted(self._aux_params.items()):
            desc = InitDesc(name, attrs.get(name, None))
            _impl(desc, arr, aux_params)

        self.params_initialized = True
        self._params_dirty = False

        # copy the initialized parameters to devices
        self._exec_group.set_params(self._arg_params, self._aux_params,
                                    allow_extra=allow_extra)"
top_matches;"def top_matches(self, top):
        '''
        Search through the top high data for matches and return the states
        that this minion needs to execute.

        Returns:
        {'saltenv': ['state1', 'state2', ...]}
        '''
        matches = {}
        for saltenv, body in six.iteritems(top):
            if self.opts['pillarenv']:
                if saltenv != self.opts['pillarenv']:
                    continue
            for match, data in six.iteritems(body):
                if self.matchers['confirm_top.confirm_top'](
                        match,
                        data,
                        self.opts.get('nodegroups', {}),
                        ):
                    if saltenv not in matches:
                        matches[saltenv] = env_matches = []
                    else:
                        env_matches = matches[saltenv]
                    for item in data:
                        if isinstance(item, six.string_types) and item not in env_matches:
                            env_matches.append(item)
        return matches"
zipline_root;"def zipline_root(environ=None):
    """"""
    Get the root directory for all zipline-managed files.

    For testing purposes, this accepts a dictionary to interpret as the os
    environment.

    Parameters
    ----------
    environ : dict, optional
        A dict to interpret as the os environment.

    Returns
    -------
    root : string
        Path to the zipline root dir.
    """"""
    if environ is None:
        environ = os.environ

    root = environ.get('ZIPLINE_ROOT', None)
    if root is None:
        root = expanduser('~/.zipline')

    return root"
get_frames_mouth;"def get_frames_mouth(self, detector, predictor, frames):
        """"""
        Get frames using mouth crop
        """"""
        mouth_width = 100
        mouth_height = 50
        horizontal_pad = 0.19
        normalize_ratio = None
        mouth_frames = []
        for frame in frames:
            dets = detector(frame, 1)
            shape = None
            for det in dets:
                shape = predictor(frame, det)
                i = -1
            if shape is None: # Detector doesn't detect face, just return None
                return [None]
            mouth_points = []
            for part in shape.parts():
                i += 1
                if i < 48: # Only take mouth region
                    continue
                mouth_points.append((part.x, part.y))
            np_mouth_points = np.array(mouth_points)

            mouth_centroid = np.mean(np_mouth_points[:, -2:], axis=0)

            if normalize_ratio is None:
                mouth_left = np.min(np_mouth_points[:, :-1]) * (1.0 - horizontal_pad)
                mouth_right = np.max(np_mouth_points[:, :-1]) * (1.0 + horizontal_pad)

                normalize_ratio = mouth_width / float(mouth_right - mouth_left)

            new_img_shape = (int(frame.shape[0] * normalize_ratio),
                             int(frame.shape[1] * normalize_ratio))
            resized_img = imresize(frame, new_img_shape)

            mouth_centroid_norm = mouth_centroid * normalize_ratio

            mouth_l = int(mouth_centroid_norm[0] - mouth_width / 2)
            mouth_r = int(mouth_centroid_norm[0] + mouth_width / 2)
            mouth_t = int(mouth_centroid_norm[1] - mouth_height / 2)
            mouth_b = int(mouth_centroid_norm[1] + mouth_height / 2)

            mouth_crop_image = resized_img[mouth_t:mouth_b, mouth_l:mouth_r]

            mouth_frames.append(mouth_crop_image)
        return mouth_frames"
back_make;"def back_make(dev, cache_mode='writeback', force=False, attach=True, bucket_size=None):
    '''
    Create a backing device for attachment to a set.
    Because the block size must be the same, a cache set already needs to exist.

    CLI example:

    .. code-block:: bash

        salt '*' bcache.back_make sdc cache_mode=writeback attach=True


    :param cache_mode: writethrough, writeback, writearound or none.
    :param force: Overwrite existing bcaches
    :param attach: Immediately attach the backing device to the set
    :param bucket_size: Size of a bucket (see kernel doc)
    '''
    # pylint: disable=too-many-return-statements
    cache = uuid()

    if not cache:
        log.error('No bcache set found')
        return False
    elif _sysfs_attr(_bcpath(dev)):
        if not force:
            log.error('%s already contains a bcache. Wipe it manually or use force', dev)
            return False
        elif uuid(dev) and not detach(dev):
            return False
        elif not stop(dev):
            return False

    dev = _devpath(dev)
    block_size = _size_map(_fssys('block_size'))
    # You might want to override, we pick the cache set's as sane default
    if bucket_size is None:
        bucket_size = _size_map(_fssys('bucket_size'))

    cmd = 'make-bcache --block {0} --bucket {1} --{2} --bdev {3}'.format(block_size, bucket_size, cache_mode, dev)
    if force:
        cmd += ' --wipe-bcache'

    if not _run_all(cmd, 'error', 'Error creating backing device {0}: %s'.format(dev)):
        return False
    elif not _sysfs_attr('fs/bcache/register', _devpath(dev),
                         'error', 'Error registering backing device {0}'.format(dev)):
        return False
    elif not _wait(lambda: _sysfs_attr(_bcpath(dev)) is not False,
                   'error', 'Backing device {0} did not register'.format(dev)):
        return False
    elif attach:
        return attach_(dev)

    return True"
predict_topk;"def predict_topk(self, dataset, output_type=""probability"", k=3,
        batch_size=None):
        """"""
        Return top-k predictions for the ``dataset``, using the trained model.
        Predictions are returned as an SFrame with three columns: `id`,
        `class`, and `probability` or `rank`, depending on the ``output_type``
        parameter.

        Parameters
        ----------
        dataset : SFrame | SArray | turicreate.Image | list
            Drawings to be classified.
            If dataset is an SFrame, it must include columns with the same
            names as the features used for model training, but does not require
            a target column. Additional columns are ignored.

        output_type : {'probability', 'rank'}, optional
            Choose the return type of the prediction:

            - `probability`: Probability associated with each label in the 
                             prediction.
            - `rank`       : Rank associated with each label in the prediction.
            
        k : int, optional
            Number of classes to return for each input example.

        batch_size : int, optional
            If you are getting memory errors, try decreasing this value. If you
            have a powerful computer, increasing this value may improve
            performance.

        Returns
        -------
        out : SFrame
            An SFrame with model predictions.

        See Also
        --------
        predict, evaluate

        Examples
        --------
        >>> pred = m.predict_topk(validation_data, k=3)
        >>> pred
        +----+-------+-------------------+
        | id | class |   probability     |
        +----+-------+-------------------+
        | 0  |   4   |   0.995623886585  |
        | 0  |   9   |  0.0038311756216  |
        | 0  |   7   | 0.000301006948575 |
        | 1  |   1   |   0.928708016872  |
        | 1  |   3   |  0.0440889261663  |
        | 1  |   2   |  0.0176190119237  |
        | 2  |   3   |   0.996967732906  |
        | 2  |   2   |  0.00151345680933 |
        | 2  |   7   | 0.000637513934635 |
        | 3  |   1   |   0.998070061207  |
        | .. |  ...  |        ...        |
        +----+-------+-------------------+
        [35688 rows x 3 columns]
        """"""
        _tkutl._check_categorical_option_type(""output_type"", output_type, 
            [""probability"", ""rank""])
        
        if not isinstance(k, int): 
            raise TypeError(""'k' must be an integer >= 1"")
        if k <= 0: 
            raise ValueError(""'k' must be >= 1"")
        if batch_size is not None and not isinstance(batch_size, int):
            raise TypeError(""'batch_size' must be an integer >= 1"")
        if batch_size is not None and batch_size < 1:
            raise ValueError(""'batch_size' must be >= 1"")

        prob_vector = self.predict(
            dataset, output_type='probability_vector', batch_size=batch_size)

        classes = self.classes
        if output_type == 'probability':
            results = prob_vector.apply(lambda p: [
                        {'class': classes[i], 'probability': p[i]}
                        for i in reversed(_np.argsort(p)[-k:])]
                      )
        else:
            assert(output_type == 'rank')
            results = prob_vector.apply(lambda p: [
                        {'class': classes[index], 'rank': rank}
                        for rank, index in enumerate(reversed(_np.argsort(p)[-k:]))]
                      )

        results = _tc.SFrame({'X': results})
        results = results.add_row_number()
        results = results.stack('X', new_column_name='X')
        results = results.unpack('X', column_name_prefix='')
        return results"
before_invoke;"def before_invoke(self, coro):
        """"""A decorator that registers a coroutine as a pre-invoke hook.

        A pre-invoke hook is called directly before the command is
        called. This makes it a useful function to set up database
        connections or any type of set up required.

        This pre-invoke hook takes a sole parameter, a :class:`.Context`.

        See :meth:`.Bot.before_invoke` for more info.

        Parameters
        -----------
        coro: :ref:`coroutine <coroutine>`
            The coroutine to register as the pre-invoke hook.

        Raises
        -------
        TypeError
            The coroutine passed is not actually a coroutine.
        """"""
        if not asyncio.iscoroutinefunction(coro):
            raise TypeError('The pre-invoke hook must be a coroutine.')

        self._before_invoke = coro
        return coro"
create_namespaced_lease;"def create_namespaced_lease(self, namespace, body, **kwargs):
        """"""
        create a Lease
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.create_namespaced_lease(namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V1Lease body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1Lease
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.create_namespaced_lease_with_http_info(namespace, body, **kwargs)
        else:
            (data) = self.create_namespaced_lease_with_http_info(namespace, body, **kwargs)
            return data"
handle_qbytearray;"def handle_qbytearray(obj, encoding):
    """"""Qt/Python2/3 compatibility helper.""""""
    if isinstance(obj, QByteArray):
        obj = obj.data()

    return to_text_string(obj, encoding=encoding)"
convert_dense;"def convert_dense(builder, layer, input_names, output_names, keras_layer):
    """"""
    Convert a dense layer from keras to coreml.

    Parameters
    keras_layer: layer
    ----------
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""
    # Get input and output names
    input_name, output_name = (input_names[0], output_names[0])

    has_bias = keras_layer.use_bias
    # Get the weights from keras
    W = keras_layer.get_weights ()[0].T
    Wb = keras_layer.get_weights ()[1].T if has_bias else None
    output_channels, input_channels = W.shape

    builder.add_inner_product(name = layer,
            W = W,
            b = Wb,
            input_channels = input_channels,
            output_channels = output_channels,
            has_bias = has_bias,
            input_name = input_name,
            output_name = output_name)"
setup_toolbar;"def setup_toolbar(self):
        """"""Setup toolbar""""""
        load_button = create_toolbutton(self, text=_('Import data'),
                                        icon=ima.icon('fileimport'),
                                        triggered=lambda: self.import_data())
        self.save_button = create_toolbutton(self, text=_(""Save data""),
                            icon=ima.icon('filesave'),
                            triggered=lambda: self.save_data(self.filename))
        self.save_button.setEnabled(False)
        save_as_button = create_toolbutton(self,
                                           text=_(""Save data as...""),
                                           icon=ima.icon('filesaveas'),
                                           triggered=self.save_data)
        reset_namespace_button = create_toolbutton(
                self, text=_(""Remove all variables""),
                icon=ima.icon('editdelete'), triggered=self.reset_namespace)

        return [load_button, self.save_button, save_as_button,
                reset_namespace_button]"
device_path;"def device_path(cls, project, location, registry, device):
        """"""Return a fully-qualified device string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/locations/{location}/registries/{registry}/devices/{device}"",
            project=project,
            location=location,
            registry=registry,
            device=device,
        )"
expand_abbreviations;"def expand_abbreviations(template, abbreviations):
    """"""Expand abbreviations in a template name.

    :param template: The project template name.
    :param abbreviations: Abbreviation definitions.
    """"""
    if template in abbreviations:
        return abbreviations[template]

    # Split on colon. If there is no colon, rest will be empty
    # and prefix will be the whole template
    prefix, sep, rest = template.partition(':')
    if prefix in abbreviations:
        return abbreviations[prefix].format(rest)

    return template"
get_interface;"def get_interface(iface):
    '''
    Returns details about given interface.

    CLI Example:

    .. code-block:: bash

        salt '*' ip.get_interface eth0
    '''
    _interfaces = get_interfaces_details()
    for _interface in _interfaces['interfaces']:
        if _interface['connectionid'] == iface:
            return _dict_to_string(_interface)
    return None"
parse_version_info;"def parse_version_info(version):
    """"""Parse version string to a VersionInfo instance.

    :param version: version string
    :return: a :class:`VersionInfo` instance
    :rtype: :class:`VersionInfo`

    >>> import semver
    >>> version_info = semver.parse_version_info(""3.4.5-pre.2+build.4"")
    >>> version_info.major
    3
    >>> version_info.minor
    4
    >>> version_info.patch
    5
    >>> version_info.prerelease
    'pre.2'
    >>> version_info.build
    'build.4'
    """"""
    parts = parse(version)
    version_info = VersionInfo(
            parts['major'], parts['minor'], parts['patch'],
            parts['prerelease'], parts['build'])

    return version_info"
servicegroup_server_enable;"def servicegroup_server_enable(sg_name, s_name, s_port, **connection_args):
    '''
    Enable a server:port member of a servicegroup

    CLI Example:

    .. code-block:: bash

        salt '*' netscaler.servicegroup_server_enable 'serviceGroupName' 'serverName' 'serverPort'
    '''
    ret = True
    server = _servicegroup_get_server(sg_name, s_name, s_port, **connection_args)
    if server is None:
        return False
    nitro = _connect(**connection_args)
    if nitro is None:
        return False
    try:
        NSServiceGroup.enable_server(nitro, server)
    except NSNitroError as error:
        log.debug('netscaler module error - NSServiceGroup.enable_server() failed: %s', error)
        ret = False
    _disconnect(nitro)
    return ret"
fixup_namespace_packages;"def fixup_namespace_packages(path_item, parent=None):
    """"""Ensure that previously-declared namespace packages include path_item""""""
    _imp.acquire_lock()
    try:
        for package in _namespace_packages.get(parent, ()):
            subpath = _handle_ns(package, path_item)
            if subpath:
                fixup_namespace_packages(subpath, package)
    finally:
        _imp.release_lock()"
set_value;"def set_value(self, key, value):
        # type: (str, Any) -> None
        """"""Modify a value in the configuration.
        """"""
        self._ensure_have_load_only()

        fname, parser = self._get_parser_to_modify()

        if parser is not None:
            section, name = _disassemble_key(key)

            # Modify the parser and the configuration
            if not parser.has_section(section):
                parser.add_section(section)
            parser.set(section, name, value)

        self._config[self.load_only][key] = value
        self._mark_as_modified(fname, parser)"
get_dir;"def get_dir(self, objtxt):
        """"""Return dir(object)""""""
        obj, valid = self._eval(objtxt)
        if valid:
            return getobjdir(obj)"
fetch_user_profile;"async def fetch_user_profile(self, user_id):
        """"""|coro|

        Gets an arbitrary user's profile. This can only be used by non-bot accounts.

        Parameters
        ------------
        user_id: :class:`int`
            The ID of the user to fetch their profile for.

        Raises
        -------
        Forbidden
            Not allowed to fetch profiles.
        HTTPException
            Fetching the profile failed.

        Returns
        --------
        :class:`.Profile`
            The profile of the user.
        """"""

        state = self._connection
        data = await self.http.get_user_profile(user_id)

        def transform(d):
            return state._get_guild(int(d['id']))

        since = data.get('premium_since')
        mutual_guilds = list(filter(None, map(transform, data.get('mutual_guilds', []))))
        user = data['user']
        return Profile(flags=user.get('flags', 0),
                       premium_since=utils.parse_time(since),
                       mutual_guilds=mutual_guilds,
                       user=User(data=user, state=state),
                       connected_accounts=data['connected_accounts'])"
auth_interactive;"def auth_interactive(self, username, handler, submethods=""""):
        """"""
        Authenticate to the server interactively.  A handler is used to answer
        arbitrary questions from the server.  On many servers, this is just a
        dumb wrapper around PAM.

        This method will block until the authentication succeeds or fails,
        peroidically calling the handler asynchronously to get answers to
        authentication questions.  The handler may be called more than once
        if the server continues to ask questions.

        The handler is expected to be a callable that will handle calls of the
        form: ``handler(title, instructions, prompt_list)``.  The ``title`` is
        meant to be a dialog-window title, and the ``instructions`` are user
        instructions (both are strings).  ``prompt_list`` will be a list of
        prompts, each prompt being a tuple of ``(str, bool)``.  The string is
        the prompt and the boolean indicates whether the user text should be
        echoed.

        A sample call would thus be:
        ``handler('title', 'instructions', [('Password:', False)])``.

        The handler should return a list or tuple of answers to the server's
        questions.

        If the server requires multi-step authentication (which is very rare),
        this method will return a list of auth types permissible for the next
        step.  Otherwise, in the normal case, an empty list is returned.

        :param str username: the username to authenticate as
        :param callable handler: a handler for responding to server questions
        :param str submethods: a string list of desired submethods (optional)
        :return:
            list of auth types permissible for the next stage of
            authentication (normally empty).

        :raises: `.BadAuthenticationType` -- if public-key authentication isn't
            allowed by the server for this user
        :raises: `.AuthenticationException` -- if the authentication failed
        :raises: `.SSHException` -- if there was a network error

        .. versionadded:: 1.5
        """"""
        if (not self.active) or (not self.initial_kex_done):
            # we should never try to authenticate unless we're on a secure link
            raise SSHException(""No existing session"")
        my_event = threading.Event()
        self.auth_handler = AuthHandler(self)
        self.auth_handler.auth_interactive(
            username, handler, my_event, submethods
        )
        return self.auth_handler.wait_for_response(my_event)"
replace_volume_attachment;"def replace_volume_attachment(self, name, body, **kwargs):
        """"""
        replace the specified VolumeAttachment
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_volume_attachment(name, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the VolumeAttachment (required)
        :param V1VolumeAttachment body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V1VolumeAttachment
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_volume_attachment_with_http_info(name, body, **kwargs)
        else:
            (data) = self.replace_volume_attachment_with_http_info(name, body, **kwargs)
            return data"
autosave_all;"def autosave_all(self):
        """"""Autosave all opened files.""""""
        for index in range(self.stack.get_stack_count()):
            self.autosave(index)"
get_system_memory;"def get_system_memory():
    """"""Return the total amount of system memory in bytes.

    Returns:
        The total amount of system memory in bytes.
    """"""
    # Try to accurately figure out the memory limit if we are in a docker
    # container. Note that this file is not specific to Docker and its value is
    # often much larger than the actual amount of memory.
    docker_limit = None
    memory_limit_filename = ""/sys/fs/cgroup/memory/memory.limit_in_bytes""
    if os.path.exists(memory_limit_filename):
        with open(memory_limit_filename, ""r"") as f:
            docker_limit = int(f.read())

    # Use psutil if it is available.
    psutil_memory_in_bytes = None
    try:
        import psutil
        psutil_memory_in_bytes = psutil.virtual_memory().total
    except ImportError:
        pass

    if psutil_memory_in_bytes is not None:
        memory_in_bytes = psutil_memory_in_bytes
    elif sys.platform == ""linux"" or sys.platform == ""linux2"":
        # Handle Linux.
        bytes_in_kilobyte = 1024
        memory_in_bytes = vmstat(""total memory"") * bytes_in_kilobyte
    else:
        # Handle MacOS.
        memory_in_bytes = sysctl([""sysctl"", ""hw.memsize""])

    if docker_limit is not None:
        return min(docker_limit, memory_in_bytes)
    else:
        return memory_in_bytes"
create_event;"def create_event(name, details, service_key, profile):
    '''
    Create an event on the PagerDuty service

    .. code-block:: yaml

        server-warning-message:
          pagerduty.create_event:
            - name: 'This is a server warning message'
            - details: 'This is a much more detailed message'
            - service_key: 9abcd123456789efabcde362783cdbaf
            - profile: my-pagerduty-account

    The following parameters are required:

    name
        This is a short description of the event.

    details
        This can be a more detailed description of the event.

    service_key
        This key can be found by using pagerduty.list_services.

    profile
        This refers to the configuration profile to use to connect to the
        PagerDuty service.
    '''
    ret = {'name': name,
           'changes': {},
           'result': None,
           'comment': ''}
    if __opts__['test']:
        ret['comment'] = 'Need to create event: {0}'.format(name)
        return ret
    __salt__['pagerduty.create_event'](
        description=name,
        details=details,
        service_key=service_key,
        profile=profile,
    )
    ret['result'] = True
    ret['comment'] = 'Created event: {0}'.format(name)
    return ret"
normalize_locale;"def normalize_locale(loc):
    '''
    Format a locale specifier according to the format returned by `locale -a`.
    '''
    comps = split_locale(loc)
    comps['territory'] = comps['territory'].upper()
    comps['codeset'] = comps['codeset'].lower().replace('-', '')
    comps['charmap'] = ''
    return join_locale(comps)"
get_inspection_units;"def get_inspection_units(logdir='', event_file='', tag=''):
  """"""Returns a list of InspectionUnit objects given either logdir or event_file.

  If logdir is given, the number of InspectionUnits should equal the
  number of directories or subdirectories that contain event files.

  If event_file is given, the number of InspectionUnits should be 1.

  Args:
    logdir: A log directory that contains event files.
    event_file: Or, a particular event file path.
    tag: An optional tag name to query for.

  Returns:
    A list of InspectionUnit objects.
  """"""
  if logdir:
    subdirs = io_wrapper.GetLogdirSubdirectories(logdir)
    inspection_units = []
    for subdir in subdirs:
      generator = itertools.chain(*[
          generator_from_event_file(os.path.join(subdir, f))
          for f in tf.io.gfile.listdir(subdir)
          if io_wrapper.IsTensorFlowEventsFile(os.path.join(subdir, f))
      ])
      inspection_units.append(InspectionUnit(
          name=subdir,
          generator=generator,
          field_to_obs=get_field_to_observations_map(generator, tag)))
    if inspection_units:
      print('Found event files in:\n{}\n'.format('\n'.join(
          [u.name for u in inspection_units])))
    elif io_wrapper.IsTensorFlowEventsFile(logdir):
      print(
          'It seems that {} may be an event file instead of a logdir. If this '
          'is the case, use --event_file instead of --logdir to pass '
          'it in.'.format(logdir))
    else:
      print('No event files found within logdir {}'.format(logdir))
    return inspection_units
  elif event_file:
    generator = generator_from_event_file(event_file)
    return [InspectionUnit(
        name=event_file,
        generator=generator,
        field_to_obs=get_field_to_observations_map(generator, tag))]
  return []"
get_config_status;"def get_config_status():
    '''
    Get the status of the current DSC Configuration

    Returns:
        dict: A dictionary representing the status of the current DSC
            Configuration on the machine

    CLI Example:

    .. code-block:: bash

        salt '*' dsc.get_config_status
    '''
    cmd = 'Get-DscConfigurationStatus | ' \
          'Select-Object -Property HostName, Status, MetaData, ' \
          '@{Name=""StartDate"";Expression={Get-Date ($_.StartDate) -Format g}}, ' \
          'Type, Mode, RebootRequested, NumberofResources'
    try:
        return _pshell(cmd, ignore_retcode=True)
    except CommandExecutionError as exc:
        if 'No status information available' in exc.info['stderr']:
            raise CommandExecutionError('Not Configured')
        raise"
load_object;"def load_object(filename):
    """"""Unpickles and decompresses the given filename and returns the created object.""""""
    logging.info('loading {}...'.format(filename))
    try:
        with gzip.GzipFile(filename, 'rb') as f:
            buf = ''
            while True:
                data = f.read()
                if data == '':
                    break
                buf += data
            return pickle.loads(buf)
    except Exception as e:
        logging.error('load failure: {}'.format(e))
        raise"
matches_requirement;"def matches_requirement(self, req):
        """"""
        Say if this instance matches (fulfills) a requirement.
        :param req: The requirement to match.
        :rtype req: str
        :return: True if it matches, else False.
        """"""
        # Requirement may contain extras - parse to lose those
        # from what's passed to the matcher
        r = parse_requirement(req)
        scheme = get_scheme(self.metadata.scheme)
        try:
            matcher = scheme.matcher(r.requirement)
        except UnsupportedVersionError:
            # XXX compat-mode if cannot read the version
            logger.warning('could not read version %r - using name only',
                           req)
            name = req.split()[0]
            matcher = scheme.matcher(name)

        name = matcher.key   # case-insensitive

        result = False
        for p in self.provides:
            p_name, p_ver = parse_name_and_version(p)
            if p_name != name:
                continue
            try:
                result = matcher.match(p_ver)
                break
            except UnsupportedVersionError:
                pass
        return result"
parse_def;"def parse_def(self, text):
        """"""Parse the function definition text.""""""
        self.__init__()

        if not is_start_of_function(text):
            return

        self.func_indent = get_indent(text)

        text = text.strip()
        text = text.replace('\r\n', '')
        text = text.replace('\n', '')

        return_type_re = re.search(r'->[ ]*([a-zA-Z0-9_,()\[\] ]*):$', text)
        if return_type_re:
            self.return_type_annotated = return_type_re.group(1)
            text_end = text.rfind(return_type_re.group(0))
        else:
            self.return_type_annotated = None
            text_end = len(text)

        pos_args_start = text.find('(') + 1
        pos_args_end = text.rfind(')', pos_args_start, text_end)

        self.args_text = text[pos_args_start:pos_args_end]

        args_list = self.split_args_text_to_list(self.args_text)
        if args_list is not None:
            self.has_info = True
            self.split_arg_to_name_type_value(args_list)"
print_config_value;"def print_config_value(self, name, prefix='- ', separator=': '):
        """"""print a single configuration value, based on a prefix and separator

           Parameters
           ==========
           name: the key of the config valur in self.config_values to print
           prefix: the prefix to print
           separator: the separator to use (default is : )
        """"""

        value_out = 'None'
        if name in self.config_values and self.config_values[name] is not None:
            value_out = self.config_values[name]
        print(prefix + name + separator + value_out)"
swarm_init;"def swarm_init(advertise_addr=str,
               listen_addr=int,
               force_new_cluster=bool):
    '''
    Initalize Docker on Minion as a Swarm Manager

    advertise_addr
        The ip of the manager

    listen_addr
        Listen address used for inter-manager communication,
        as well as determining the networking interface used
        for the VXLAN Tunnel Endpoint (VTEP).
        This can either be an address/port combination in
        the form 192.168.1.1:4567,
        or an interface followed by a port number,
        like eth0:4567

    force_new_cluster
        Force a new cluster if True is passed

    CLI Example:

    .. code-block:: bash

        salt '*' swarm.swarm_init advertise_addr='192.168.50.10' listen_addr='0.0.0.0' force_new_cluster=False
    '''
    try:
        salt_return = {}
        __context__['client'].swarm.init(advertise_addr,
                                         listen_addr,
                                         force_new_cluster)
        output = 'Docker swarm has been initialized on {0} ' \
                 'and the worker/manager Join token is below'.format(__context__['server_name'])
        salt_return.update({'Comment': output,
                            'Tokens': swarm_tokens()})
    except TypeError:
        salt_return = {}
        salt_return.update({'Error': 'Please make sure you are passing advertise_addr, '
                                     'listen_addr and force_new_cluster correctly.'})
    return salt_return"
get_lenet;"def get_lenet():
    """""" A lenet style net, takes difference of each frame as input.
    """"""
    source = mx.sym.Variable(""data"")
    source = (source - 128) * (1.0/128)
    frames = mx.sym.SliceChannel(source, num_outputs=30)
    diffs = [frames[i+1] - frames[i] for i in range(29)]
    source = mx.sym.Concat(*diffs)
    net = mx.sym.Convolution(source, kernel=(5, 5), num_filter=40)
    net = mx.sym.BatchNorm(net, fix_gamma=True)
    net = mx.sym.Activation(net, act_type=""relu"")
    net = mx.sym.Pooling(net, pool_type=""max"", kernel=(2,2), stride=(2,2))
    net = mx.sym.Convolution(net, kernel=(3, 3), num_filter=40)
    net = mx.sym.BatchNorm(net, fix_gamma=True)
    net = mx.sym.Activation(net, act_type=""relu"")
    net = mx.sym.Pooling(net, pool_type=""max"", kernel=(2,2), stride=(2,2))
    # first fullc
    flatten = mx.symbol.Flatten(net)
    flatten = mx.symbol.Dropout(flatten)
    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=600)
    # Name the final layer as softmax so it auto matches the naming of data iterator
    # Otherwise we can also change the provide_data in the data iter
    return mx.symbol.LogisticRegressionOutput(data=fc1, name='softmax')"
set_config;"def set_config(name, xpath=None, value=None, commit=False):
    '''
    Sets a Palo Alto XPATH to a specific value. This will always overwrite the existing value, even if it is not
    changed.

    You can add or create a new object at a specified location in the configuration hierarchy. Use the xpath parameter
    to specify the location of the object in the configuration

    name: The name of the module function to execute.

    xpath(str): The XPATH of the configuration API tree to control.

    value(str): The XML value to set. This must be a child to the XPATH.

    commit(bool): If true the firewall will commit the changes, if false do not commit changes.

    SLS Example:

    .. code-block:: yaml

        panos/hostname:
            panos.set_config:
              - xpath: /config/devices/entry[@name='localhost.localdomain']/deviceconfig/system
              - value: <hostname>foobar</hostname>
              - commit: True

    '''
    ret = _default_ret(name)

    result, msg = _set_config(xpath, value)

    ret.update({
        'comment': msg,
        'result': result
    })

    if not result:
        return ret

    if commit is True:
        ret.update({
            'commit': __salt__['panos.commit'](),
            'result': True
        })

    return ret"
handle_market_close;"def handle_market_close(self, dt, data_portal):
        """"""Handles the close of the given day.

        Parameters
        ----------
        dt : Timestamp
            The most recently completed simulation datetime.
        data_portal : DataPortal
            The current data portal.

        Returns
        -------
        A daily perf packet.
        """"""
        completed_session = self._current_session

        if self.emission_rate == 'daily':
            # this method is called for both minutely and daily emissions, but
            # this chunk of code here only applies for daily emissions. (since
            # it's done every minute, elsewhere, for minutely emission).
            self.sync_last_sale_prices(dt, data_portal)

        session_ix = self._session_count
        # increment the day counter before we move markers forward.
        self._session_count += 1

        packet = {
            'period_start': self._first_session,
            'period_end': self._last_session,
            'capital_base': self._capital_base,
            'daily_perf': {
                'period_open': self._market_open,
                'period_close': dt,
            },
            'cumulative_perf': {
                'period_open': self._first_session,
                'period_close': self._last_session,
            },
            'progress': self._progress(self),
            'cumulative_risk_metrics': {},
        }
        ledger = self._ledger
        ledger.end_of_session(session_ix)
        self.end_of_session(
            packet,
            ledger,
            completed_session,
            session_ix,
            data_portal,
        )

        return packet"
set_logger_dir;"def set_logger_dir(dirname, action=None):
    """"""
    Set the directory for global logging.

    Args:
        dirname(str): log directory
        action(str): an action of [""k"",""d"",""q""] to be performed
            when the directory exists. Will ask user by default.

                ""d"": delete the directory. Note that the deletion may fail when
                the directory is used by tensorboard.

                ""k"": keep the directory. This is useful when you resume from a
                previous training and want the directory to look as if the
                training was not interrupted.
                Note that this option does not load old models or any other
                old states for you. It simply does nothing.

    """"""
    global LOG_DIR, _FILE_HANDLER
    if _FILE_HANDLER:
        # unload and close the old file handler, so that we may safely delete the logger directory
        _logger.removeHandler(_FILE_HANDLER)
        del _FILE_HANDLER

    def dir_nonempty(dirname):
        # If directory exists and nonempty (ignore hidden files), prompt for action
        return os.path.isdir(dirname) and len([x for x in os.listdir(dirname) if x[0] != '.'])

    if dir_nonempty(dirname):
        if not action:
            _logger.warn(""""""\
Log directory {} exists! Use 'd' to delete it. """""".format(dirname))
            _logger.warn(""""""\
If you're resuming from a previous run, you can choose to keep it.
Press any other key to exit. """""")
        while not action:
            action = input(""Select Action: k (keep) / d (delete) / q (quit):"").lower().strip()
        act = action
        if act == 'b':
            backup_name = dirname + _get_time_str()
            shutil.move(dirname, backup_name)
            info(""Directory '{}' backuped to '{}'"".format(dirname, backup_name))  # noqa: F821
        elif act == 'd':
            shutil.rmtree(dirname, ignore_errors=True)
            if dir_nonempty(dirname):
                shutil.rmtree(dirname, ignore_errors=False)
        elif act == 'n':
            dirname = dirname + _get_time_str()
            info(""Use a new log directory {}"".format(dirname))  # noqa: F821
        elif act == 'k':
            pass
        else:
            raise OSError(""Directory {} exits!"".format(dirname))
    LOG_DIR = dirname
    from .fs import mkdir_p
    mkdir_p(dirname)
    _set_file(os.path.join(dirname, 'log.log'))"
log_text;"def log_text(self, text, **kw):
        """"""Add a text entry to be logged during :meth:`commit`.

        :type text: str
        :param text: the text entry

        :type kw: dict
        :param kw: (optional) additional keyword arguments for the entry.
                   See :class:`~google.cloud.logging.entries.LogEntry`.
        """"""
        self.entries.append(TextEntry(payload=text, **kw))"
set_zone;"def set_zone(timezone):
    '''
    Sets the timezone using the tzutil.

    Args:
        timezone (str): A valid timezone

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    Raises:
        CommandExecutionError: If invalid timezone is passed

    CLI Example:

    .. code-block:: bash

        salt '*' timezone.set_zone 'America/Denver'
    '''
    # if it's one of the key's just use it
    if timezone.lower() in mapper.win_to_unix:
        win_zone = timezone

    elif timezone.lower() in mapper.unix_to_win:
        # if it's one of the values, use the key
        win_zone = mapper.get_win(timezone)

    else:
        # Raise error because it's neither key nor value
        raise CommandExecutionError('Invalid timezone passed: {0}'.format(timezone))

    # Set the value
    cmd = ['tzutil', '/s', win_zone]
    res = __salt__['cmd.run_all'](cmd, python_shell=False)
    if res['retcode']:
        raise CommandExecutionError('tzutil encountered an error setting '
                                    'timezone: {0}'.format(timezone),
                                    info=res)
    return zone_compare(timezone)"
create_async_dynamodb_table;"def create_async_dynamodb_table(self, table_name, read_capacity, write_capacity):
        """"""
        Create the DynamoDB table for async task return values
        """"""
        try:
            dynamodb_table = self.dynamodb_client.describe_table(TableName=table_name)
            return False, dynamodb_table

        # catch this exception (triggered if the table doesn't exist)
        except botocore.exceptions.ClientError:
            dynamodb_table = self.dynamodb_client.create_table(
                AttributeDefinitions=[
                    {
                        'AttributeName': 'id',
                        'AttributeType': 'S'
                    }
                ],
                TableName=table_name,
                KeySchema=[
                    {
                        'AttributeName': 'id',
                        'KeyType': 'HASH'
                    },
                ],
                ProvisionedThroughput = {
                    'ReadCapacityUnits': read_capacity,
                    'WriteCapacityUnits': write_capacity
                }
            )
            if dynamodb_table:
                try:
                    self._set_async_dynamodb_table_ttl(table_name)
                except botocore.exceptions.ClientError:
                    # this fails because the operation is async, so retry
                    time.sleep(10)
                    self._set_async_dynamodb_table_ttl(table_name)

        return True, dynamodb_table"
extract_features;"def extract_features(self, dataset, feature, batch_size=64, verbose=False):
        """"""
        Parameters
        ----------
        dataset: SFrame
            SFrame of images
        """"""
        from ._mxnet._mx_sframe_iter import SFrameImageIter as _SFrameImageIter
        from six.moves.queue import Queue as _Queue
        from threading import Thread as _Thread
        import turicreate as _tc
        import array

        if len(dataset) == 0:
            return _tc.SArray([], array.array)

        batch_size = min(len(dataset), batch_size)
        # Make a data iterator
        dataIter = _SFrameImageIter(sframe=dataset, data_field=[feature], batch_size=batch_size, image_shape=self.image_shape)

        # Setup the MXNet model
        model = MXFeatureExtractor._get_mx_module(self.ptModel.mxmodel,
                self.data_layer, self.feature_layer, self.context, self.image_shape, batch_size)

        out = _tc.SArrayBuilder(dtype = array.array)
        progress = { 'num_processed' : 0, 'total' : len(dataset) }
        if verbose:
            print(""Performing feature extraction on resized images..."")

        # Encapsulates the work done by the MXNet model for a single batch
        def handle_request(batch):
            model.forward(batch)
            mx_out = [array.array('d',m) for m in model.get_outputs()[0].asnumpy()]
            if batch.pad != 0:
                # If batch size is not evenly divisible by the length, it will loop back around.
                # We don't want that.
                mx_out = mx_out[:-batch.pad]
            return mx_out

        # Copies the output from MXNet into the SArrayBuilder and emits progress
        def consume_response(mx_out):
            out.append_multiple(mx_out)

            progress['num_processed'] += len(mx_out)
            if verbose:
                print('Completed {num_processed:{width}d}/{total:{width}d}'.format(
                    width = len(str(progress['total'])), **progress))

        # Create a dedicated thread for performing MXNet work, using two FIFO
        # queues for communication back and forth with this thread, with the
        # goal of keeping MXNet busy throughout.
        request_queue = _Queue()
        response_queue = _Queue()
        def mx_worker():
            while True:
                batch = request_queue.get()  # Consume request
                if batch is None:
                    # No more work remains. Allow the thread to finish.
                    return
                response_queue.put(handle_request(batch))  # Produce response
        mx_worker_thread = _Thread(target=mx_worker)
        mx_worker_thread.start()

        try:
            # Attempt to have two requests in progress at any one time (double
            # buffering), so that the iterator is creating one batch while MXNet
            # performs inference on the other.
            if dataIter.has_next:
                request_queue.put(next(dataIter))  # Produce request
                while dataIter.has_next:
                    request_queue.put(next(dataIter))  # Produce request
                    consume_response(response_queue.get())
                consume_response(response_queue.get())
        finally:
            # Tell the worker thread to shut down.
            request_queue.put(None)

        return out.close()"
kernel_output;"def kernel_output(self, user_name, kernel_slug, **kwargs):  # noqa: E501
        """"""Download the latest output from a kernel  # noqa: E501

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.kernel_output(user_name, kernel_slug, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str user_name: Kernel owner (required)
        :param str kernel_slug: Kernel name (required)
        :return: Result
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501
        else:
            (data) = self.kernel_output_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501
            return data"
parse_csr;"def parse_csr():
    """"""
    Parse certificate signing request for domains
    """"""
    LOGGER.info(""Parsing CSR..."")
    cmd = [
        'openssl', 'req',
        '-in', os.path.join(gettempdir(), 'domain.csr'),
        '-noout',
        '-text'
    ]
    devnull = open(os.devnull, 'wb')
    out = subprocess.check_output(cmd, stderr=devnull)
    domains = set([])
    common_name = re.search(r""Subject:.*? CN\s?=\s?([^\s,;/]+)"", out.decode('utf8'))
    if common_name is not None:
        domains.add(common_name.group(1))
    subject_alt_names = re.search(r""X509v3 Subject Alternative Name: \n +([^\n]+)\n"", out.decode('utf8'), re.MULTILINE | re.DOTALL)
    if subject_alt_names is not None:
        for san in subject_alt_names.group(1).split("", ""):
            if san.startswith(""DNS:""):
                domains.add(san[4:])

    return domains"
fetch_invite;"async def fetch_invite(self, *, with_counts=True):
        """"""|coro|

        Retrieves an :class:`Invite` from a invite URL or ID.
        This is the same as :meth:`Client.get_invite`; the invite
        code is abstracted away.

        Parameters
        -----------
        with_counts: :class:`bool`
            Whether to include count information in the invite. This fills the
            :attr:`Invite.approximate_member_count` and :attr:`Invite.approximate_presence_count`
            fields.

        Returns
        --------
        :class:`Invite`
            The invite from the URL/ID.
        """"""
        if self._invite:
            invite_id = resolve_invite(self._invite)
            data = await self._state.http.get_invite(invite_id, with_counts=with_counts)
            return Invite.from_incomplete(state=self._state, data=data)"
get_python_args;"def get_python_args(fname, python_args, interact, debug, end_args):
    """"""Construct Python interpreter arguments""""""
    p_args = []
    if python_args is not None:
        p_args += python_args.split()
    if interact:
        p_args.append('-i')
    if debug:
        p_args.extend(['-m', 'pdb'])
    if fname is not None:
        if os.name == 'nt' and debug:
            # When calling pdb on Windows, one has to replace backslashes by
            # slashes to avoid confusion with escape characters (otherwise, 
            # for example, '\t' will be interpreted as a tabulation):
            p_args.append(osp.normpath(fname).replace(os.sep, '/'))
        else:
            p_args.append(fname)
    if end_args:
        p_args.extend(shell_split(end_args))
    return p_args"
create_partitions;"def create_partitions(self, new_partitions, **kwargs):
        """"""
        Create additional partitions for the given topics.

        The future result() value is None.

        :param list(NewPartitions) new_partitions: New partitions to be created.
        :param float operation_timeout: Set broker's operation timeout in seconds,
                  controlling how long the CreatePartitions request will block
                  on the broker waiting for the partition creation to propagate
                  in the cluster. A value of 0 returns immediately. Default: 0
        :param float request_timeout: Set the overall request timeout in seconds,
                  including broker lookup, request transmission, operation time
                  on broker, and response. Default: `socket.timeout.ms*1000.0`
        :param bool validate_only: Tell broker to only validate the request,
                  without creating the partitions. Default: False

        :returns: a dict of futures for each topic, keyed by the topic name.
        :rtype: dict(<topic_name, future>)

        :raises KafkaException: Operation failed locally or on broker.
        :raises TypeException: Invalid input.
        :raises ValueException: Invalid input.
        """"""

        f, futmap = AdminClient._make_futures([x.topic for x in new_partitions],
                                              None,
                                              AdminClient._make_topics_result)

        super(AdminClient, self).create_partitions(new_partitions, f, **kwargs)

        return futmap"
pack_weights;"def pack_weights(self, args):
        """"""Pack separate weight matrices into a single packed
        weight.

        Parameters
        ----------
        args : dict of str -> NDArray
            Dictionary containing unpacked weights.

        Returns
        -------
        args : dict of str -> NDArray
            Dictionary with packed weights associated with
            this cell.
        """"""
        args = args.copy()
        if not self._gate_names:
            return args
        for group_name in ['i2h', 'h2h']:
            weight = []
            bias = []
            for gate in self._gate_names:
                wname = '%s%s%s_weight'%(self._prefix, group_name, gate)
                weight.append(args.pop(wname))
                bname = '%s%s%s_bias'%(self._prefix, group_name, gate)
                bias.append(args.pop(bname))
            args['%s%s_weight'%(self._prefix, group_name)] = ndarray.concatenate(weight)
            args['%s%s_bias'%(self._prefix, group_name)] = ndarray.concatenate(bias)
        return args"
new_prompt;"def new_prompt(self, prompt):
        """"""
        Print a new prompt and save its (line, index) position
        """"""
        if self.get_cursor_line_column()[1] != 0:
            self.write('\n')
        self.write(prompt, prompt=True)
        # now we update our cursor giving end of prompt
        self.current_prompt_pos = self.get_position('cursor')
        self.ensureCursorVisible()
        self.new_input_line = False"
make_insecure_stub;"def make_insecure_stub(stub_class, host, port=None):
    """"""Makes an insecure stub for an RPC service.

    Uses / depends on gRPC.

    :type stub_class: type
    :param stub_class: A gRPC stub type for a given service.

    :type host: str
    :param host: The host for the service. May also include the port
                 if ``port`` is unspecified.

    :type port: int
    :param port: (Optional) The port for the service.

    :rtype: object, instance of ``stub_class``
    :returns: The stub object used to make gRPC requests to a given API.
    """"""
    if port is None:
        target = host
    else:
        # NOTE: This assumes port != http_client.HTTPS_PORT:
        target = ""%s:%d"" % (host, port)
    channel = grpc.insecure_channel(target)
    return stub_class(channel)"
run_cell;"def run_cell(self, code, cell_name, filename, run_cell_copy):
        """"""Run cell in current or dedicated client.""""""

        def norm(text):
            return remove_backslashes(to_text_string(text))

        self.run_cell_filename = filename

        # Select client to execute code on it
        client = self.get_client_for_file(filename)
        if client is None:
            client = self.get_current_client()

        is_internal_kernel = False
        if client is not None:
            # Internal kernels, use runcell
            if client.get_kernel() is not None and not run_cell_copy:
                line = (to_text_string(""{}('{}','{}')"")
                            .format(to_text_string('runcell'),
                                (to_text_string(cell_name).replace(""\\"",""\\\\"")
                                    .replace(""'"", r""\'"")),
                                norm(filename).replace(""'"", r""\'"")))
                is_internal_kernel = True

            # External kernels and run_cell_copy, just execute the code
            else:
                line = code.strip()

            try:
                if client.shellwidget._executing:
                    # Don't allow multiple executions when there's
                    # still an execution taking place
                    # Fixes issue 7293
                    pass
                elif client.shellwidget._reading:
                    client.shellwidget._append_html(
                        _(""<br><b>Exit the debugger before trying to ""
                          ""run a cell in this console.</b>\n<hr><br>""),
                        before_prompt=True)
                    return
                else:
                    if is_internal_kernel:
                        client.shellwidget.silent_execute(
                            to_text_string('get_ipython().cell_code = '
                                           '""""""{}""""""')
                                .format(to_text_string(code)
                                .replace('\\', r'\\')
                                .replace('""""""', r'\""\""\""')))
                    self.execute_code(line)
            except AttributeError:
                pass
            self.visibility_changed(True)
            self.raise_()
        else:
            # XXX: not sure it can really happen
            QMessageBox.warning(self, _('Warning'),
                                _(""No IPython console is currently available ""
                                  ""to run <b>{}</b>.<br><br>Please open a new ""
                                  ""one and try again.""
                                  ).format(osp.basename(filename)),
                                QMessageBox.Ok)"
get_data;"def get_data(query_func, load_data=False, save_data=False):
    """"""Gets data from query_func, optionally saving that data to a file; or loads data from a file.""""""
    if hasattr(query_func, '__name__'):
        func_name = query_func.__name__
    elif hasattr(query_func, 'func'):
        func_name = query_func.func.__name__

    pickle_file = '{}.pickle'.format(func_name)

    if load_data:
        data = load_object(pickle_file)
    else:
        data = query_func()
        if save_data:
            save_object(pickle_file, data)
    return data"
copartition_datasets;"def copartition_datasets(self, axis, other, left_func, right_func):
        """"""Copartition two BlockPartitions objects.

        Args:
            axis: The axis to copartition.
            other: The other BlockPartitions object to copartition with.
            left_func: The function to apply to left. If None, just use the dimension
                of self (based on axis).
            right_func: The function to apply to right. If None, check the dimensions of
                other and use the identity function if splitting needs to happen.

        Returns:
            A tuple of BlockPartitions objects, left and right.
        """"""
        if left_func is None:
            new_self = self
        else:
            new_self = self.map_across_full_axis(axis, left_func)

        # This block of code will only shuffle if absolutely necessary. If we do need to
        # shuffle, we use the identity function and then reshuffle.
        if right_func is None:
            if axis == 0 and not np.array_equal(
                other.block_lengths, new_self.block_lengths
            ):
                new_other = other.manual_shuffle(
                    axis, lambda x: x, new_self.block_lengths
                )
            elif axis == 1 and not np.array_equal(
                other.block_widths, new_self.block_widths
            ):
                new_other = other.manual_shuffle(
                    axis, lambda x: x, new_self.block_widths
                )
            else:
                new_other = other
        # Most of the time, we will be given an operation to do. We perform that with
        # manual_shuffle.
        else:
            new_other = other.manual_shuffle(
                axis,
                right_func,
                new_self.block_lengths if axis == 0 else new_self.block_widths,
            )
        return new_self, new_other"
read_namespaced_service_account;"def read_namespaced_service_account(self, name, namespace, **kwargs):
        """"""
        read the specified ServiceAccount
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_service_account(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ServiceAccount (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.
        :return: V1ServiceAccount
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_service_account_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_service_account_with_http_info(name, namespace, **kwargs)
            return data"
additional_options;"def additional_options(self, is_pylab=False, is_sympy=False):
        """"""
        Additional options for shell widgets that are not defined
        in JupyterWidget config options
        """"""
        options = dict(
            pylab=self.get_option('pylab'),
            autoload_pylab=self.get_option('pylab/autoload'),
            sympy=self.get_option('symbolic_math'),
            show_banner=self.get_option('show_banner')
        )

        if is_pylab is True:
            options['autoload_pylab'] = True
            options['sympy'] = False
        if is_sympy is True:
            options['autoload_pylab'] = False
            options['sympy'] = True

        return options"
check_triggers;"def check_triggers(self, price, dt):
        """"""
        Update internal state based on price triggers and the
        trade event's price.
        """"""
        stop_reached, limit_reached, sl_stop_reached = \
            self.check_order_triggers(price)
        if (stop_reached, limit_reached) \
                != (self.stop_reached, self.limit_reached):
            self.dt = dt
        self.stop_reached = stop_reached
        self.limit_reached = limit_reached
        if sl_stop_reached:
            # Change the STOP LIMIT order into a LIMIT order
            self.stop = None"
update_adjustments;"def update_adjustments(self, adjustments, method):
        """"""
        Merge ``adjustments`` with existing adjustments, handling index
        collisions according to ``method``.

        Parameters
        ----------
        adjustments : dict[int -> list[Adjustment]]
            The mapping of row indices to lists of adjustments that should be
            appended to existing adjustments.
        method : {'append', 'prepend'}
            How to handle index collisions. If 'append', new adjustments will
            be applied after previously-existing adjustments. If 'prepend', new
            adjustments will be applied before previously-existing adjustments.
        """"""
        try:
            merge_func = _merge_methods[method]
        except KeyError:
            raise ValueError(
                ""Invalid merge method %s\n""
                ""Valid methods are: %s"" % (method, ', '.join(_merge_methods))
            )

        self.adjustments = merge_with(
            merge_func,
            self.adjustments,
            adjustments,
        )"
create_webhook;"async def create_webhook(self, *, name, avatar=None, reason=None):
        """"""|coro|

        Creates a webhook for this channel.

        Requires :attr:`~.Permissions.manage_webhooks` permissions.

        .. versionchanged:: 1.1.0
            Added the ``reason`` keyword-only parameter.

        Parameters
        -------------
        name: :class:`str`
            The webhook's name.
        avatar: Optional[:class:`bytes`]
            A :term:`py:bytes-like object` representing the webhook's default avatar.
            This operates similarly to :meth:`~ClientUser.edit`.
        reason: Optional[:class:`str`]
            The reason for creating this webhook. Shows up in the audit logs.

        Raises
        -------
        HTTPException
            Creating the webhook failed.
        Forbidden
            You do not have permissions to create a webhook.

        Returns
        --------
        :class:`Webhook`
            The created webhook.
        """"""

        if avatar is not None:
            avatar = utils._bytes_to_base64_data(avatar)

        data = await self._state.http.create_webhook(self.id, name=str(name), avatar=avatar, reason=reason)
        return Webhook.from_state(data, state=self._state)"
run_hook;"def run_hook(hook_name, project_dir, context):
    """"""
    Try to find and execute a hook from the specified project directory.

    :param hook_name: The hook to execute.
    :param project_dir: The directory to execute the script from.
    :param context: Cookiecutter project context.
    """"""
    script = find_hook(hook_name)
    if script is None:
        logger.debug('No {} hook found'.format(hook_name))
        return
    logger.debug('Running hook {}'.format(hook_name))
    run_script_with_context(script, project_dir, context)"
get_value;"def get_value(self, sid, dt, field):
        """"""
        Parameters
        ----------
        sid : int
            The asset identifier.
        day : datetime64-like
            Midnight of the day for which data is requested.
        colname : string
            The price field. e.g. ('open', 'high', 'low', 'close', 'volume')

        Returns
        -------
        float
            The spot price for colname of the given sid on the given day.
            Raises a NoDataOnDate exception if the given day and sid is before
            or after the date range of the equity.
            Returns -1 if the day is within the date range, but the price is
            0.
        """"""
        ix = self.sid_day_index(sid, dt)
        price = self._spot_col(field)[ix]
        if field != 'volume':
            if price == 0:
                return nan
            else:
                return price * 0.001
        else:
            return price"
delete_dataset;"def delete_dataset(
        self, dataset, delete_contents=False, retry=DEFAULT_RETRY, not_found_ok=False
    ):
        """"""Delete a dataset.

        See
        https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/delete

        Args
            dataset (Union[ \
                :class:`~google.cloud.bigquery.dataset.Dataset`, \
                :class:`~google.cloud.bigquery.dataset.DatasetReference`, \
                str, \
            ]):
                A reference to the dataset to delete. If a string is passed
                in, this method attempts to create a dataset reference from a
                string using
                :func:`google.cloud.bigquery.dataset.DatasetReference.from_string`.
            delete_contents (boolean):
                (Optional) If True, delete all the tables in the dataset. If
                False and the dataset contains tables, the request will fail.
                Default is False.
            retry (:class:`google.api_core.retry.Retry`):
                (Optional) How to retry the RPC.
            not_found_ok (bool):
                Defaults to ``False``. If ``True``, ignore ""not found"" errors
                when deleting the dataset.
        """"""
        if isinstance(dataset, str):
            dataset = DatasetReference.from_string(
                dataset, default_project=self.project
            )

        if not isinstance(dataset, (Dataset, DatasetReference)):
            raise TypeError(""dataset must be a Dataset or a DatasetReference"")

        params = {}
        if delete_contents:
            params[""deleteContents""] = ""true""

        try:
            self._call_api(
                retry, method=""DELETE"", path=dataset.path, query_params=params
            )
        except google.api_core.exceptions.NotFound:
            if not not_found_ok:
                raise"
hybrid_forward;"def hybrid_forward(self, F, center, context, center_words):
        """"""SkipGram forward pass.

        Parameters
        ----------
        center : mxnet.nd.NDArray or mxnet.sym.Symbol
            Sparse CSR array of word / subword indices of shape (batch_size,
            len(token_to_idx) + num_subwords). Embedding for center words are
            computed via F.sparse.dot between the CSR center array and the
            weight matrix.
        context : mxnet.nd.NDArray or mxnet.sym.Symbol
            Dense array of context words of shape (batch_size, ). Also used for
            row-wise independently masking negatives equal to one of context.
        center_words : mxnet.nd.NDArray or mxnet.sym.Symbol
            Dense array of center words of shape (batch_size, ). Only used for
            row-wise independently masking negatives equal to one of
            center_words.
        """"""

        # negatives sampling
        negatives = []
        mask = []
        for _ in range(self._kwargs['num_negatives']):
            negatives.append(self.negatives_sampler(center_words))
            mask_ = negatives[-1] != center_words
            mask_ = F.stack(mask_, (negatives[-1] != context))
            mask.append(mask_.min(axis=0))

        negatives = F.stack(*negatives, axis=1)
        mask = F.stack(*mask, axis=1).astype(np.float32)

        # center - context pairs
        emb_center = self.embedding(center).expand_dims(1)
        emb_context = self.embedding_out(context).expand_dims(2)
        pred_pos = F.batch_dot(emb_center, emb_context).squeeze()
        loss_pos = (F.relu(pred_pos) - pred_pos + F.Activation(
            -F.abs(pred_pos), act_type='softrelu')) / (mask.sum(axis=1) + 1)

        # center - negatives pairs
        emb_negatives = self.embedding_out(negatives).reshape(
            (-1, self._kwargs['num_negatives'],
             self._kwargs['output_dim'])).swapaxes(1, 2)
        pred_neg = F.batch_dot(emb_center, emb_negatives).squeeze()
        mask = mask.reshape((-1, self._kwargs['num_negatives']))
        loss_neg = (F.relu(pred_neg) + F.Activation(
            -F.abs(pred_neg), act_type='softrelu')) * mask
        loss_neg = loss_neg.sum(axis=1) / (mask.sum(axis=1) + 1)

        return loss_pos + loss_neg"
list_audit_sink;"def list_audit_sink(self, **kwargs):
        """"""
        list or watch objects of kind AuditSink
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_audit_sink(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1alpha1AuditSinkList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_audit_sink_with_http_info(**kwargs)
        else:
            (data) = self.list_audit_sink_with_http_info(**kwargs)
            return data"
combine_comments;"def combine_comments(comments):
    '''
    Given a list of comments, strings, a single comment or a single string,
    return a single string of text containing all of the comments, prepending
    the '#' and joining with newlines as necessary.
    '''
    if not isinstance(comments, list):
        comments = [comments]
    ret = []
    for comment in comments:
        if not isinstance(comment, six.string_types):
            comment = str(comment)
        # Normalize for any spaces (or lack thereof) after the #
        ret.append('# {0}\n'.format(comment.lstrip('#').lstrip()))
    return ''.join(ret)"
rmtree_errorhandler;"def rmtree_errorhandler(func, path, exc_info):
    """"""On Windows, the files in .svn are read-only, so when rmtree() tries to
    remove them, an exception is thrown.  We catch that here, remove the
    read-only attribute, and hopefully continue without problems.""""""
    # if file type currently read only
    if os.stat(path).st_mode & stat.S_IREAD:
        # convert to read/write
        os.chmod(path, stat.S_IWRITE)
        # use the original function to repeat the operation
        func(path)
        return
    else:
        raise"
set_rich_text_html;"def set_rich_text_html(self, html_text, base_url):
        """"""Set rich text""""""
        self.rich_text.set_html(html_text, base_url)
        self.save_text([self.rich_text.set_html, html_text, base_url])"
pixel_data;"def pixel_data(self):
        """"""
        Returns the pixel data stored in the Image object.

        Returns
        -------
        out : numpy.array
            The pixel data of the Image object. It returns a multi-dimensional
            numpy array, where the shape of the array represents the shape of
            the image (height, weight, channels).

        See Also
        --------
        width, channels, height

        Examples
        --------
        >>> img = turicreate.Image('https://static.turi.com/datasets/images/sample.jpg')
        >>> image_array = img.pixel_data
        """"""

        from .. import extensions as _extensions
        data = _np.zeros((self.height, self.width, self.channels), dtype=_np.uint8)
        _extensions.image_load_to_numpy(self, data.ctypes.data, data.strides)
        if self.channels == 1:
            data = data.squeeze(2)
        return data"
set_states;"def set_states(self, states=None, value=None):
        """"""Set value for states. Only one of states & value can be specified.

        Parameters
        ----------
        states : list of list of NDArrays
            source states arrays formatted like [[state1_dev1, state1_dev2],
            [state2_dev1, state2_dev2]].
        value : number
            a single scalar value for all state arrays.
        """"""
        if states is not None:
            assert value is None, ""Only one of states & value can be specified.""
            _load_general(states, self.state_arrays, (0,)*len(states))
        else:
            assert value is not None, ""At least one of states & value must be specified.""
            assert states is None, ""Only one of states & value can be specified.""
            for d_dst in self.state_arrays:
                for dst in d_dst:
                    dst[:] = value"
file_dict;"def file_dict(*packages, **kwargs):
    '''
    List the files that belong to a package, sorted by group. Not specifying
    any packages will return a list of _every_ file on the system's rpm
    database (not generally recommended).

    root
        use root as top level directory (default: ""/"")

    CLI Examples:

    .. code-block:: bash

        salt '*' lowpkg.file_dict httpd
        salt '*' lowpkg.file_dict httpd postfix
        salt '*' lowpkg.file_dict
    '''
    errors = []
    ret = {}
    pkgs = {}
    cmd = ['rpm']
    if kwargs.get('root'):
        cmd.extend(['--root', kwargs['root']])
    cmd.extend(['-q' if packages else '-qa',
                '--queryformat', r'%{NAME} %{VERSION}\n'])
    if packages:
        cmd.extend(packages)
    out = __salt__['cmd.run'](cmd, output_loglevel='trace', python_shell=False)
    for line in salt.utils.itertools.split(out, '\n'):
        if 'is not installed' in line:
            errors.append(line)
            continue
        comps = line.split()
        pkgs[comps[0]] = {'version': comps[1]}
    for pkg in pkgs:
        cmd = ['rpm']
        if kwargs.get('root'):
            cmd.extend(['--root', kwargs['root']])
        cmd.extend(['-ql', pkg])
        out = __salt__['cmd.run'](
            ['rpm', '-ql', pkg],
            output_loglevel='trace',
            python_shell=False)
        ret[pkg] = out.splitlines()
    return {'errors': errors, 'packages': ret}"
generate_simpleaccount;"def generate_simpleaccount(self):
        """"""make a simple account with a easier way
        如果当前user中没有创建portfolio, 则创建一个portfolio,并用此portfolio创建一个account
        如果已有一个或多个portfolio,则使用第一个portfolio来创建一个account
        """"""
        if len(self.portfolio_list.keys()) < 1:
            po = self.new_portfolio()
        else:
            po = list(self.portfolio_list.values())[0]
        ac = po.new_account()
        return ac, po"
create_database;"def create_database(self):
        """"""
        Populate the database with the tables.
        """"""
        from chatterbot.ext.sqlalchemy_app.models import Base
        Base.metadata.create_all(self.engine)"
override_stage_config_setting;"def override_stage_config_setting(self, key, val):
        """"""
        Forcefully override a setting set by zappa_settings (for the current stage only)
        :param key: settings key
        :param val: value
        """"""
        self._stage_config_overrides = getattr(self, '_stage_config_overrides', {})
        self._stage_config_overrides.setdefault(self.api_stage, {})[key] = val"
get_weights;"def get_weights(self):
        """"""Returns a dictionary containing the weights of the network.

        Returns:
            Dictionary mapping variable names to their weights.
        """"""
        self._check_sess()
        return {
            k: v.eval(session=self.sess)
            for k, v in self.variables.items()
        }"
shorten_paths;"def shorten_paths(path_list, is_unsaved):
    """"""
    Takes a list of paths and tries to ""intelligently"" shorten them all. The
    aim is to make it clear to the user where the paths differ, as that is
    likely what they care about. Note that this operates on a list of paths
    not on individual paths.

    If the path ends in an actual file name, it will be trimmed off.
    """"""
    # TODO: at the end, if the path is too long, should do a more dumb kind of
    # shortening, but not completely dumb.

    # Convert the path strings to a list of tokens and start building the
    # new_path using the drive
    path_list = path_list[:]  # Make a local copy
    new_path_list = []

    for ii, (path, is_unsav) in enumerate(zip(path_list, is_unsaved)):
        if is_unsav:
            new_path_list.append(_('unsaved file'))
            path_list[ii] = None
        else:
            drive, path = osp.splitdrive(osp.dirname(path))
            new_path_list.append(drive + osp.sep)
            path_list[ii] = [part for part in path.split(osp.sep) if part]

    def recurse_level(level_idx):
        sep = os.sep

        # If toks are all empty we need not have recursed here
        if not any(level_idx.values()):
            return

        # Firstly, find the longest common prefix for all in the level
        # s = len of longest common prefix
        sample_toks = list(level_idx.values())[0]
        if not sample_toks:
            s = 0
        else:
            for s, sample_val in enumerate(sample_toks):
                if not all(len(toks) > s and toks[s] == sample_val
                           for toks in level_idx.values()):
                    break

        # Shorten longest common prefix
        if s == 0:
            short_form = ''
        else:
            if s == 1:
                short_form = sample_toks[0]
            elif s == 2:
                short_form = sample_toks[0] + sep + sample_toks[1]
            else:
                short_form = ""..."" + sep + sample_toks[s-1]
            for idx in level_idx:
                new_path_list[idx] += short_form + sep
                level_idx[idx] = level_idx[idx][s:]

        # Group the remaining bit after the common prefix, shorten, and recurse
        while level_idx:
            k, group = 0, level_idx  # k is length of the group's common prefix
            while True:
                # Abort if we've gone beyond end of one or more in the group
                prospective_group = {idx: toks for idx, toks
                                     in group.items() if len(toks) == k}
                if prospective_group:
                    if k == 0:  # we spit out the group with no suffix
                        group = prospective_group
                    break
                # Only keep going if all n still match on the kth token
                _, sample_toks = next(iteritems(group))
                prospective_group = {idx: toks for idx, toks
                                     in group.items()
                                     if toks[k] == sample_toks[k]}
                if len(prospective_group) == len(group) or k == 0:
                    group = prospective_group
                    k += 1
                else:
                    break
            _, sample_toks = next(iteritems(group))
            if k == 0:
                short_form = ''
            elif k == 1:
                short_form = sample_toks[0]
            elif k == 2:
                short_form = sample_toks[0] + sep + sample_toks[1]
            else:  # k > 2
                short_form = sample_toks[0] + ""..."" + sep + sample_toks[k-1]
            for idx in group.keys():
                new_path_list[idx] += short_form + (sep if k > 0 else '')
                del level_idx[idx]
            recurse_level({idx: toks[k:] for idx, toks in group.items()})

    recurse_level({i: pl for i, pl in enumerate(path_list) if pl})

    return [path.rstrip(os.sep) for path in new_path_list]"
analyze_one_classification_result;"def analyze_one_classification_result(storage_client, file_path,
                                      adv_batch, dataset_batches,
                                      dataset_meta):
  """"""Reads and analyzes one classification result.

  This method reads file with classification result and counts
  how many images were classified correctly and incorrectly,
  how many times target class was hit and total number of images.

  Args:
    storage_client: instance of CompetitionStorageClient
    file_path: result file path
    adv_batch: AversarialBatches.data[adv_batch_id]
      adv_batch_id is stored in each ClassificationBatch entity
    dataset_batches: instance of DatasetBatches
    dataset_meta: instance of DatasetMetadata

  Returns:
    Tuple of (count_correctly_classified, count_errors, count_hit_target_class,
    num_images)
  """"""
  class_result = read_classification_results(storage_client, file_path)
  if class_result is None:
    return 0, 0, 0, 0
  adv_images = adv_batch['images']
  dataset_batch_images = (
      dataset_batches.data[adv_batch['dataset_batch_id']]['images'])
  count_correctly_classified = 0
  count_errors = 0
  count_hit_target_class = 0
  num_images = 0
  for adv_img_id, label in iteritems(class_result):
    if adv_img_id not in adv_images:
      continue
    num_images += 1
    clean_image_id = adv_images[adv_img_id]['clean_image_id']
    dataset_image_id = (
        dataset_batch_images[clean_image_id]['dataset_image_id'])
    if label == dataset_meta.get_true_label(dataset_image_id):
      count_correctly_classified += 1
    else:
      count_errors += 1
    if label == dataset_meta.get_target_class(dataset_image_id):
      count_hit_target_class += 1
  return (count_correctly_classified, count_errors,
          count_hit_target_class, num_images)"
set_monitor_timeout;"def set_monitor_timeout(timeout, power='ac', scheme=None):
    '''
    Set the monitor timeout in minutes for the given power scheme

    Args:
        timeout (int):
            The amount of time in minutes before the monitor will timeout

        power (str):
            Set the value for AC or DC power. Default is ``ac``. Valid options
            are:

                - ``ac`` (AC Power)
                - ``dc`` (Battery)

        scheme (str):
            The scheme to use, leave as ``None`` to use the current. Default is
            ``None``. This can be the GUID or the Alias for the Scheme. Known
            Aliases are:

                - ``SCHEME_BALANCED`` - Balanced
                - ``SCHEME_MAX`` - Power saver
                - ``SCHEME_MIN`` - High performance

    Returns:
        bool: ``True`` if successful, otherwise ``False``

    CLI Example:

    .. code-block:: bash

        # Sets the monitor timeout to 30 minutes
        salt '*' powercfg.set_monitor_timeout 30
    '''
    return _set_powercfg_value(
        scheme=scheme,
        sub_group='SUB_VIDEO',
        setting_guid='VIDEOIDLE',
        power=power,
        value=timeout)"
submit_order;"def submit_order(id_or_ins, amount, side, price=None, position_effect=None):
    """"""
    通用下单函数，策略可以通过该函数自由选择参数下单。

    :param id_or_ins: 下单标的物
    :type id_or_ins: :class:`~Instrument` object | `str`

    :param float amount: 下单量，需为正数

    :param side: 多空方向，多（SIDE.BUY）或空（SIDE.SELL）
    :type side: :class:`~SIDE` enum

    :param float price: 下单价格，默认为None，表示市价单

    :param position_effect: 开平方向，开仓（POSITION_EFFECT.OPEN），平仓（POSITION.CLOSE）或平今（POSITION_EFFECT.CLOSE_TODAY），交易股票不需要该参数
    :type position_effect: :class:`~POSITION_EFFECT` enum

    :return: :class:`~Order` object | None

    :example:

    .. code-block:: python

        # 购买 2000 股的平安银行股票，并以市价单发送：
        submit_order('000001.XSHE', 2000, SIDE.BUY)
        # 平 10 份 RB1812 多方向的今仓，并以 4000 的价格发送限价单
        submit_order('RB1812', 10, SIDE.SELL, price=4000, position_effect=POSITION_EFFECT.CLOSE_TODAY)

    """"""
    order_book_id = assure_order_book_id(id_or_ins)
    env = Environment.get_instance()
    if (
        env.config.base.run_type != RUN_TYPE.BACKTEST
        and env.get_instrument(order_book_id).type == ""Future""
    ):
        if ""88"" in order_book_id:
            raise RQInvalidArgument(
                _(u""Main Future contracts[88] are not supported in paper trading."")
            )
        if ""99"" in order_book_id:
            raise RQInvalidArgument(
                _(u""Index Future contracts[99] are not supported in paper trading."")
            )
    style = cal_style(price, None)
    market_price = env.get_last_price(order_book_id)
    if not is_valid_price(market_price):
        user_system_log.warn(
            _(u""Order Creation Failed: [{order_book_id}] No market data"").format(
                order_book_id=order_book_id
            )
        )
        return

    amount = int(amount)

    order = Order.__from_create__(
        order_book_id=order_book_id,
        quantity=amount,
        side=side,
        style=style,
        position_effect=position_effect,
    )

    if order.type == ORDER_TYPE.MARKET:
        order.set_frozen_price(market_price)
    if env.can_submit_order(order):
        env.broker.submit_order(order)
        return order"
set_layout_settings;"def set_layout_settings(self, settings, dont_goto=None):
        """"""Restore layout state for the splitter panels.

        Apply the settings to restore a saved layout within the editor.  If
        the splitsettings key doesn't exist, then return without restoring
        any settings.

        The current EditorSplitter (self) calls split() for each element
        in split_settings, thus recreating the splitter panels from the saved
        state.  split() also clones the editorstack, which is then
        iterated over to restore the saved line numbers on each file.

        The size and positioning of each splitter panel is restored from
        hexstate.

        Args:
            settings: A dictionary with keys {hexstate, sizes, orientation}
                    that define the layout for the EditorSplitter panels.
            dont_goto: Defaults to None, which positions the cursor to the
                    end of the editor.  If there's a value, positions the
                    cursor on the saved line number for each editor.
        """"""
        splitsettings = settings.get('splitsettings')
        if splitsettings is None:
            return
        splitter = self
        editor = None
        for index, (is_vertical, cfname, clines) in enumerate(splitsettings):
            if index > 0:
                splitter.split(Qt.Vertical if is_vertical else Qt.Horizontal)
                splitter = splitter.widget(1)
            editorstack = splitter.widget(0)
            for index, finfo in enumerate(editorstack.data):
                editor = finfo.editor
                # TODO: go_to_line is not working properly (the line it jumps
                # to is not the corresponding to that file). This will be fixed
                # in a future PR (which will fix issue #3857)
                if dont_goto is not None:
                    # skip go to line for first file because is already there
                    pass
                else:
                    try:
                        editor.go_to_line(clines[index])
                    except IndexError:
                        pass
        hexstate = settings.get('hexstate')
        if hexstate is not None:
            self.restoreState( QByteArray().fromHex(
                    str(hexstate).encode('utf-8')) )
        sizes = settings.get('sizes')
        if sizes is not None:
            self.setSizes(sizes)
        if editor is not None:
            editor.clearFocus()
            editor.setFocus()"
check_installed_files;"def check_installed_files(self):
        """"""
        Checks that the hashes and sizes of the files in ``RECORD`` are
        matched by the files themselves. Returns a (possibly empty) list of
        mismatches. Each entry in the mismatch list will be a tuple consisting
        of the path, 'exists', 'size' or 'hash' according to what didn't match
        (existence is checked first, then size, then hash), the expected
        value and the actual value.
        """"""
        mismatches = []
        record_path = os.path.join(self.path, 'installed-files.txt')
        if os.path.exists(record_path):
            for path, _, _ in self.list_installed_files():
                if path == record_path:
                    continue
                if not os.path.exists(path):
                    mismatches.append((path, 'exists', True, False))
        return mismatches"
list_volume_attachment;"def list_volume_attachment(self, **kwargs):
        """"""
        list or watch objects of kind VolumeAttachment
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_volume_attachment(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1VolumeAttachmentList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_volume_attachment_with_http_info(**kwargs)
        else:
            (data) = self.list_volume_attachment_with_http_info(**kwargs)
            return data"
remove_mentions;"def remove_mentions(self, string):
        """"""Removes mentions from the string to prevent abuse.

        This includes ``@everyone``, ``@here``, member mentions and role mentions.
        """"""

        def replace(obj, *, transforms=self.MENTION_TRANSFORMS):
            return transforms.get(obj.group(0), '@invalid')

        return self.MENTION_PATTERN.sub(replace, string)"
delete_collection_pod_security_policy;"def delete_collection_pod_security_policy(self, **kwargs):
        """"""
        delete collection of PodSecurityPolicy
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_collection_pod_security_policy(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_collection_pod_security_policy_with_http_info(**kwargs)
        else:
            (data) = self.delete_collection_pod_security_policy_with_http_info(**kwargs)
            return data"
get_user_config;"def get_user_config(config_file=None, default_config=False):
    """"""Return the user config as a dict.

    If ``default_config`` is True, ignore ``config_file`` and return default
    values for the config parameters.

    If a path to a ``config_file`` is given, that is different from the default
    location, load the user config from that.

    Otherwise look up the config file path in the ``COOKIECUTTER_CONFIG``
    environment variable. If set, load the config from this path. This will
    raise an error if the specified path is not valid.

    If the environment variable is not set, try the default config file path
    before falling back to the default config values.
    """"""
    # Do NOT load a config. Return defaults instead.
    if default_config:
        return copy.copy(DEFAULT_CONFIG)

    # Load the given config file
    if config_file and config_file is not USER_CONFIG_PATH:
        return get_config(config_file)

    try:
        # Does the user set up a config environment variable?
        env_config_file = os.environ['COOKIECUTTER_CONFIG']
    except KeyError:
        # Load an optional user config if it exists
        # otherwise return the defaults
        if os.path.exists(USER_CONFIG_PATH):
            return get_config(USER_CONFIG_PATH)
        else:
            return copy.copy(DEFAULT_CONFIG)
    else:
        # There is a config environment variable. Try to load it.
        # Do not check for existence, so invalid file paths raise an error.
        return get_config(env_config_file)"
avg_grads;"def avg_grads(tower_grads):
  """"""Calculate the average gradient for each shared variable across all
  towers.
  Note that this function provides a synchronization point across all towers.
  Args:
    tower_grads: List of lists of (gradient, variable) tuples. The outer list
      is over individual gradients. The inner list is over the gradient
      calculation for each tower.
  Returns:
     List of pairs of (gradient, variable) where the gradient has been
     averaged across all towers.

  Modified from this tutorial: https://tinyurl.com/n3jr2vm
  """"""
  if len(tower_grads) == 1:
    return tower_grads[0]
  average_grads = []
  for grad_and_vars in zip(*tower_grads):
    # Note that each grad_and_vars looks like the following:
    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
    grads = [g for g, _ in grad_and_vars]

    # Average over the 'tower' dimension.
    grad = tf.add_n(grads) / len(grads)

    # Keep in mind that the Variables are redundant because they are shared
    # across towers. So .. we will just return the first tower's pointer to
    # the Variable.
    v = grad_and_vars[0][1]
    assert all(v is grad_and_var[1] for grad_and_var in grad_and_vars)
    grad_and_var = (grad, v)
    average_grads.append(grad_and_var)
  return average_grads"
replace_namespaced_horizontal_pod_autoscaler_status;"def replace_namespaced_horizontal_pod_autoscaler_status(self, name, namespace, body, **kwargs):
        """"""
        replace status of the specified HorizontalPodAutoscaler
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.replace_namespaced_horizontal_pod_autoscaler_status(name, namespace, body, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the HorizontalPodAutoscaler (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param V2beta1HorizontalPodAutoscaler body: (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.
        :return: V2beta1HorizontalPodAutoscaler
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.replace_namespaced_horizontal_pod_autoscaler_status_with_http_info(name, namespace, body, **kwargs)
        else:
            (data) = self.replace_namespaced_horizontal_pod_autoscaler_status_with_http_info(name, namespace, body, **kwargs)
            return data"
verify_token;"def verify_token(self, token):
        '''
        If token is valid Then returns user name associated with token
        Else False.
        '''
        try:
            result = self.resolver.get_token(token)
        except Exception as ex:
            raise EauthAuthenticationError(
                ""Token validation failed with {0}."".format(repr(ex)))

        return result"
resolve_invite;"def resolve_invite(invite):
    """"""
    Resolves an invite from a :class:`Invite`, URL or ID

    Parameters
    -----------
    invite: Union[:class:`Invite`, :class:`Object`, :class:`str`]
        The invite.

    Returns
    --------
    :class:`str`
        The invite code.
    """"""
    from .invite import Invite  # circular import
    if isinstance(invite, Invite) or isinstance(invite, Object):
        return invite.id
    else:
        rx = r'(?:https?\:\/\/)?discord(?:\.gg|app\.com\/invite)\/(.+)'
        m = re.match(rx, invite)
        if m:
            return m.group(1)
    return invite"
compile_pillar;"def compile_pillar(self):
        '''
        Return the pillar data from the master
        '''
        load = {'id': self.minion_id,
                'grains': self.grains,
                'saltenv': self.opts['saltenv'],
                'pillarenv': self.opts['pillarenv'],
                'pillar_override': self.pillar_override,
                'extra_minion_data': self.extra_minion_data,
                'ver': '2',
                'cmd': '_pillar'}
        if self.ext:
            load['ext'] = self.ext
        ret_pillar = self.channel.crypted_transfer_decode_dictentry(load,
                                                                    dictkey='pillar',
                                                                    )

        if not isinstance(ret_pillar, dict):
            log.error(
                'Got a bad pillar from master, type %s, expecting dict: %s',
                type(ret_pillar).__name__, ret_pillar
            )
            return {}
        return ret_pillar"
handle_report_metric_data;"def handle_report_metric_data(self, data):
        """"""reveice the metric data and update Bayesian optimization with final result

        Parameters
        ----------
        data:
            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.

        Raises
        ------
        ValueError
            Data type not supported
        """"""
        logger.debug('handle report metric data = %s', data)

        assert 'value' in data
        value = extract_scalar_reward(data['value'])
        if self.optimize_mode is OptimizeMode.Maximize:
            reward = -value
        else:
            reward = value
        assert 'parameter_id' in data
        s, i, _ = data['parameter_id'].split('_')

        logger.debug('bracket id = %s, metrics value = %s, type = %s', s, value, data['type'])
        s = int(s)

        assert 'type' in data
        if data['type'] == 'FINAL':
            # and PERIODICAL metric are independent, thus, not comparable.
            assert 'sequence' in data
            self.brackets[s].set_config_perf(
                int(i), data['parameter_id'], sys.maxsize, value)
            self.completed_hyper_configs.append(data)
       
            _parameters = self.parameters[data['parameter_id']]
            _parameters.pop(_KEY)
            # update BO with loss, max_s budget, hyperparameters
            self.cg.new_result(loss=reward, budget=data['sequence'], parameters=_parameters, update_model=True)
        elif data['type'] == 'PERIODICAL':
            self.brackets[s].set_config_perf(
                int(i), data['parameter_id'], data['sequence'], value)
        else:
            raise ValueError(
                'Data type not supported: {}'.format(data['type']))"
volume_attach;"def volume_attach(provider, names, **kwargs):
    '''
    Attach volume to a server

    CLI Example:

    .. code-block:: bash

        salt minionname cloud.volume_attach my-nova myblock server_name=myserver device='/dev/xvdf'

    '''
    client = _get_client()
    info = client.extra_action(provider=provider, names=names, action='volume_attach', **kwargs)
    return info"
process_splits;"def process_splits(self, splits):
        """"""Processes a list of splits by modifying any positions as needed.

        Parameters
        ----------
        splits: list[(Asset, float)]
            A list of splits. Each split is a tuple of (asset, ratio).
        """"""
        leftover_cash = self.position_tracker.handle_splits(splits)
        if leftover_cash > 0:
            self._cash_flow(leftover_cash)"
get_field;"def get_field(self, field):
        """"""Return the dataset corresponds to the provided key.

        Example::
            a = np.ones((2,2))
            b = np.zeros((2,2))
            np.savez('data.npz', a=a, b=b)
            dataset = NumpyDataset('data.npz')
            data_a = dataset.get_field('a')
            data_b = dataset.get_field('b')

        Parameters
        ----------
        field : str
            The name of the field to retrieve.
        """"""
        idx = self._keys.index(field)
        return self._data[idx]"
usage_plan_present;"def usage_plan_present(name, plan_name, description=None, throttle=None, quota=None, region=None, key=None, keyid=None,
                       profile=None):
    '''
    Ensure the spcifieda usage plan with the corresponding metrics is deployed

    .. versionadded:: 2017.7.0

    name
        name of the state

    plan_name
        [Required] name of the usage plan

    throttle
        [Optional] throttling parameters expressed as a dictionary.
        If provided, at least one of the throttling parameters must be present

        rateLimit
            rate per second at which capacity bucket is populated

        burstLimit
            maximum rate allowed

    quota
        [Optional] quota on the number of api calls permitted by the plan.
        If provided, limit and period must be present

        limit
            [Required] number of calls permitted per quota period

        offset
            [Optional] number of calls to be subtracted from the limit at the beginning of the period

        period
            [Required] period to which quota applies. Must be DAY, WEEK or MONTH

    .. code-block:: yaml

        UsagePlanPresent:
          boto_apigateway.usage_plan_present:
            - plan_name: my_usage_plan
            - throttle:
                rateLimit: 70
                burstLimit: 100
            - quota:
                limit: 1000
                offset: 0
                period: DAY
            - profile: my_profile

    '''
    func_params = locals()

    ret = {'name': name,
           'result': True,
           'comment': '',
           'changes': {}
           }

    try:
        common_args = dict([('region', region),
                            ('key', key),
                            ('keyid', keyid),
                            ('profile', profile)])

        existing = __salt__['boto_apigateway.describe_usage_plans'](name=plan_name, **common_args)
        if 'error' in existing:
            ret['result'] = False
            ret['comment'] = 'Failed to describe existing usage plans'
            return ret

        if not existing['plans']:
            # plan does not exist, we need to create it
            if __opts__['test']:
                ret['comment'] = 'a new usage plan {0} would be created'.format(plan_name)
                ret['result'] = None
                return ret

            result = __salt__['boto_apigateway.create_usage_plan'](name=plan_name,
                                                                   description=description,
                                                                   throttle=throttle,
                                                                   quota=quota,
                                                                   **common_args)
            if 'error' in result:
                ret['result'] = False
                ret['comment'] = 'Failed to create a usage plan {0}, {1}'.format(plan_name, result['error'])
                return ret

            ret['changes']['old'] = {'plan': None}
            ret['comment'] = 'A new usage plan {0} has been created'.format(plan_name)

        else:
            # need an existing plan modified to match given value
            plan = existing['plans'][0]
            needs_updating = False

            modifiable_params = (('throttle', ('rateLimit', 'burstLimit')), ('quota', ('limit', 'offset', 'period')))
            for p, fields in modifiable_params:
                for f in fields:
                    actual_param = {} if func_params.get(p) is None else func_params.get(p)
                    if plan.get(p, {}).get(f, None) != actual_param.get(f, None):
                        needs_updating = True
                        break

            if not needs_updating:
                ret['comment'] = 'usage plan {0} is already in a correct state'.format(plan_name)
                ret['result'] = True
                return ret

            if __opts__['test']:
                ret['comment'] = 'a new usage plan {0} would be updated'.format(plan_name)
                ret['result'] = None
                return ret

            result = __salt__['boto_apigateway.update_usage_plan'](plan['id'],
                                                                   throttle=throttle,
                                                                   quota=quota,
                                                                   **common_args)
            if 'error' in result:
                ret['result'] = False
                ret['comment'] = 'Failed to update a usage plan {0}, {1}'.format(plan_name, result['error'])
                return ret

            ret['changes']['old'] = {'plan': plan}
            ret['comment'] = 'usage plan {0} has been updated'.format(plan_name)

        newstate = __salt__['boto_apigateway.describe_usage_plans'](name=plan_name, **common_args)
        if 'error' in existing:
            ret['result'] = False
            ret['comment'] = 'Failed to describe existing usage plans after updates'
            return ret

        ret['changes']['new'] = {'plan': newstate['plans'][0]}

    except (ValueError, IOError) as e:
        ret['result'] = False
        ret['comment'] = '{0}'.format(e.args)

    return ret"
parse_statement;"def parse_statement(self):
        """"""Parse a single statement.""""""
        token = self.stream.current
        if token.type != 'name':
            self.fail('tag name expected', token.lineno)
        self._tag_stack.append(token.value)
        pop_tag = True
        try:
            if token.value in _statement_keywords:
                return getattr(self, 'parse_' + self.stream.current.value)()
            if token.value == 'call':
                return self.parse_call_block()
            if token.value == 'filter':
                return self.parse_filter_block()
            ext = self.extensions.get(token.value)
            if ext is not None:
                return ext(self)

            # did not work out, remove the token we pushed by accident
            # from the stack so that the unknown tag fail function can
            # produce a proper error message.
            self._tag_stack.pop()
            pop_tag = False
            self.fail_unknown_tag(token.value, token.lineno)
        finally:
            if pop_tag:
                self._tag_stack.pop()"
pod_present;"def pod_present(
        name,
        namespace='default',
        metadata=None,
        spec=None,
        source='',
        template='',
        **kwargs):
    '''
    Ensures that the named pod is present inside of the specified
    namespace with the given metadata and spec.
    If the pod exists it will be replaced.

    name
        The name of the pod.

    namespace
        The namespace holding the pod. The 'default' one is going to be
        used unless a different one is specified.

    metadata
        The metadata of the pod object.

    spec
        The spec of the pod object.

    source
        A file containing the definition of the pod (metadata and
        spec) in the official kubernetes format.

    template
        Template engine to be used to render the source file.
    '''
    ret = {'name': name,
           'changes': {},
           'result': False,
           'comment': ''}

    if (metadata or spec) and source:
        return _error(
            ret,
            '\'source\' cannot be used in combination with \'metadata\' or '
            '\'spec\''
        )

    if metadata is None:
        metadata = {}

    if spec is None:
        spec = {}

    pod = __salt__['kubernetes.show_pod'](name, namespace, **kwargs)

    if pod is None:
        if __opts__['test']:
            ret['result'] = None
            ret['comment'] = 'The pod is going to be created'
            return ret
        res = __salt__['kubernetes.create_pod'](name=name,
                                                namespace=namespace,
                                                metadata=metadata,
                                                spec=spec,
                                                source=source,
                                                template=template,
                                                saltenv=__env__,
                                                **kwargs)
        ret['changes']['{0}.{1}'.format(namespace, name)] = {
            'old': {},
            'new': res}
    else:
        if __opts__['test']:
            ret['result'] = None
            return ret

        # TODO: fix replace_namespaced_pod validation issues
        ret['comment'] = 'salt is currently unable to replace a pod without ' \
            'deleting it. Please perform the removal of the pod requiring ' \
            'the \'pod_absent\' state if this is the desired behaviour.'
        ret['result'] = False
        return ret

    ret['changes'] = {
        'metadata': metadata,
        'spec': spec
    }
    ret['result'] = True
    return ret"
convert_leakyrelu;"def convert_leakyrelu(net, node, module, builder):
    """"""Convert a leakyrelu layer from mxnet to coreml.

    Parameters
    ----------
    network: net
        A mxnet network object.

    layer: node
        Node to convert.

    module: module
        An module for MXNet

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """"""

    input_name, output_name = _get_input_output_name(net, node)
    name = node['name']
    inputs = node['inputs']
    args, _ = module.get_params()
    mx_non_linearity = _get_attrs(node)['act_type']
    if mx_non_linearity == 'elu':
        non_linearity = 'ELU'
        slope = _get_attrs(node)['slope'] if 'slope' in _get_attrs(node) else 0.25
        params = slope
    elif mx_non_linearity == 'leaky':
        non_linearity = 'LEAKYRELU'
        slope = _get_attrs(node)['slope'] if 'slope' in _get_attrs(node) else 0.25
        params = [slope]
    elif mx_non_linearity == 'prelu':
        non_linearity = 'PRELU'
        params = args[_get_node_name(net, inputs[1][0])].asnumpy()
    else:
        raise TypeError('Unknown activation type %s' % mx_non_linearity)
    builder.add_activation(name = name,
                           non_linearity = non_linearity,
                           input_name = input_name,
                           output_name = output_name,
                           params = params)"
register_plugin;"def register_plugin(self):
        """"""Register plugin in Spyder's main window""""""
        self.pylint.treewidget.sig_edit_goto.connect(self.main.editor.load)
        self.pylint.redirect_stdio.connect(
            self.main.redirect_internalshell_stdio)
        self.main.add_dockwidget(self)
        
        pylint_act = create_action(self, _(""Run static code analysis""),
                                   triggered=self.run_pylint)
        pylint_act.setEnabled(is_module_installed('pylint'))
        self.register_shortcut(pylint_act, context=""Pylint"",
                               name=""Run analysis"")
        
        self.main.source_menu_actions += [MENU_SEPARATOR, pylint_act]
        self.main.editor.pythonfile_dependent_actions += [pylint_act]"
disassociate_route_table;"def disassociate_route_table(association_id, region=None, key=None, keyid=None, profile=None):
    '''
    Dissassociates a route table.

    association_id
        The Route Table Association ID to disassociate

    CLI Example:

    .. code-block:: bash

        salt myminion boto_vpc.disassociate_route_table 'rtbassoc-d8ccddba'

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        if conn.disassociate_route_table(association_id):
            log.info('Route table with association id %s has been disassociated.', association_id)
            return {'disassociated': True}
        else:
            log.warning('Route table with association id %s has not been disassociated.', association_id)
            return {'disassociated': False}
    except BotoServerError as e:
        return {'disassociated': False, 'error': __utils__['boto.get_error'](e)}"
datasets_download;"def datasets_download(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501
        """"""Download dataset file  # noqa: E501

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.datasets_download(owner_slug, dataset_slug, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str owner_slug: Dataset owner (required)
        :param str dataset_slug: Dataset name (required)
        :param str dataset_version_number: Dataset version number
        :return: Result
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.datasets_download_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501
        else:
            (data) = self.datasets_download_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501
            return data"
read_namespaced_deployment_scale;"def read_namespaced_deployment_scale(self, name, namespace, **kwargs):
        """"""
        read scale of the specified Deployment
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_deployment_scale(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Scale (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :return: V1Scale
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_deployment_scale_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_deployment_scale_with_http_info(name, namespace, **kwargs)
            return data"
get_menu_actions;"def get_menu_actions(self):
        """"""Returns a list of menu actions""""""
        items = self.selectedItems()
        actions = self.get_actions_from_items(items)
        if actions:
            actions.append(None)
        actions += self.common_actions
        return actions"
iter_editorstacks;"def iter_editorstacks(self):
        """"""Return the editor stacks for this splitter and every first child.

        Note: If a splitter contains more than one splitter as a direct
              child, only the first child's editor stack is included.

        Returns:
            List of tuples containing (EditorStack instance, orientation).
        """"""
        editorstacks = [(self.widget(0), self.orientation())]
        if self.count() > 1:
            editorsplitter = self.widget(1)
            editorstacks += editorsplitter.iter_editorstacks()
        return editorstacks"
handle_trial_end;"def handle_trial_end(self, data):
        """"""receive the information of trial end and generate next configuaration.

        Parameters
        ----------
        data: dict()
            it has three keys: trial_job_id, event, hyper_params
            trial_job_id: the id generated by training service
            event: the job's state
            hyper_params: the hyperparameters (a string) generated and returned by tuner
        """"""
        logger.debug('Tuner handle trial end, result is %s', data)

        hyper_params = json_tricks.loads(data['hyper_params'])
        s, i, _ = hyper_params['parameter_id'].split('_')
        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))

        if hyper_configs is not None:
            logger.debug(
                'bracket %s next round %s, hyper_configs: %s', s, i, hyper_configs)
            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs
            for _ in range(self.credit):
                self._request_one_trial_job()
        # Finish this bracket and generate a new bracket
        elif self.brackets[int(s)].no_more_trial:
            self.curr_s -= 1
            self.generate_new_bracket()
            for _ in range(self.credit):
                self._request_one_trial_job()"
zone_exists;"def zone_exists(zone, region=None, key=None, keyid=None, profile=None,
                retry_on_rate_limit=None, rate_limit_retries=None,
                retry_on_errors=True, error_retries=5):
    '''
    Check for the existence of a Route53 hosted zone.

    .. versionadded:: 2015.8.0

    CLI Example::

        salt myminion boto_route53.zone_exists example.org
    '''
    if region is None:
        region = 'universal'

    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)

    if retry_on_rate_limit or rate_limit_retries is not None:
        salt.utils.versions.warn_until(
            'Neon',
            'The \'retry_on_rate_limit\' and \'rate_limit_retries\' arguments '
            'have been deprecated in favor of \'retry_on_errors\' and '
            '\'error_retries\' respectively. Their functionality will be '
            'removed, as such, their usage is no longer required.'
        )
        if retry_on_rate_limit is not None:
            retry_on_errors = retry_on_rate_limit
        if rate_limit_retries is not None:
            error_retries = rate_limit_retries

    while error_retries > 0:
        try:
            return bool(conn.get_zone(zone))

        except DNSServerError as e:
            if retry_on_errors:
                if 'Throttling' == e.code:
                    log.debug('Throttled by AWS API.')
                elif 'PriorRequestNotComplete' == e.code:
                    log.debug('The request was rejected by AWS API.\
                              Route 53 was still processing a prior request')
                time.sleep(3)
                error_retries -= 1
                continue
            raise e"
format_specifier;"def format_specifier(ireq):
    """"""
    Generic formatter for pretty printing the specifier part of
    InstallRequirements to the terminal.
    """"""
    # TODO: Ideally, this is carried over to the pip library itself
    specs = ireq.specifier._specs if ireq.req is not None else []
    specs = sorted(specs, key=lambda x: x._spec[1])
    return "","".join(str(s) for s in specs) or ""<any>"""
server_url_for_websocket_url;"def server_url_for_websocket_url(url):
    ''' Convert an ``ws(s)`` URL for a Bokeh server into the appropriate
    ``http(s)`` URL for the websocket endpoint.

    Args:
        url (str):
            An ``ws(s)`` URL ending in ``/ws``

    Returns:
        str:
            The corresponding ``http(s)`` URL.

    Raises:
        ValueError:
            If the input URL is not of the proper form.

    '''
    if url.startswith(""ws:""):
        reprotocoled = ""http"" + url[2:]
    elif url.startswith(""wss:""):
        reprotocoled = ""https"" + url[3:]
    else:
        raise ValueError(""URL has non-websocket protocol "" + url)
    if not reprotocoled.endswith(""/ws""):
        raise ValueError(""websocket URL does not end in /ws"")
    return reprotocoled[:-2]"
high_limit;"def high_limit(self):
        '涨停价'
        return self.groupby(level=1).close.apply(lambda x: round((x.shift(1) + 0.0002)*1.1, 2)).sort_index()"
define_macro;"def define_macro(out_f, (name, args, body), undefine=False, check=True):
    """"""Generate a macro definition or undefinition""""""
    if undefine:
        out_f.write(
            '#undef {0}\n'
            .format(macro_name(name))
        )
    else:
        if args:
            arg_list = '({0})'.format(', '.join(args))
        else:
            arg_list = ''

        if check:
            out_f.write(
                '#ifdef {0}\n'
                '#  error {0} already defined.\n'
                '#endif\n'
                .format(macro_name(name))
            )

        out_f.write(
            '#define {0}{1} {2}\n'.format(macro_name(name), arg_list, body)
        )"
parent_directory;"def parent_directory(self):
        """"""Change working directory to parent directory""""""
        self.chdir(os.path.join(getcwd_or_home(), os.path.pardir))"
sort_index;"def sort_index(self, **kwargs):
        """"""Sorts the data with respect to either the columns or the indices.

        Returns:
            DataManager containing the data sorted by columns or indices.
        """"""
        axis = kwargs.pop(""axis"", 0)
        index = self.columns if axis else self.index

        # sort_index can have ascending be None and behaves as if it is False.
        # sort_values cannot have ascending be None. Thus, the following logic is to
        # convert the ascending argument to one that works with sort_values
        ascending = kwargs.pop(""ascending"", True)
        if ascending is None:
            ascending = False
        kwargs[""ascending""] = ascending

        def sort_index_builder(df, **kwargs):
            if axis:
                df.columns = index
            else:
                df.index = index
            return df.sort_index(axis=axis, **kwargs)

        func = self._prepare_method(sort_index_builder, **kwargs)
        new_data = self._map_across_full_axis(axis, func)
        if axis:
            new_columns = pandas.Series(self.columns).sort_values(**kwargs)
            new_index = self.index
        else:
            new_index = pandas.Series(self.index).sort_values(**kwargs)
            new_columns = self.columns
        return self.__constructor__(
            new_data, new_index, new_columns, self.dtypes.copy()
        )"
export_assets;"def export_assets(
        self,
        parent,
        output_config,
        read_time=None,
        asset_types=None,
        content_type=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Exports assets with time and resource types to a given Cloud Storage
        location. The output format is newline-delimited JSON. This API
        implements the ``google.longrunning.Operation`` API allowing you to keep
        track of the export.

        Example:
            >>> from google.cloud import asset_v1
            >>>
            >>> client = asset_v1.AssetServiceClient()
            >>>
            >>> # TODO: Initialize `parent`:
            >>> parent = ''
            >>>
            >>> # TODO: Initialize `output_config`:
            >>> output_config = {}
            >>>
            >>> response = client.export_assets(parent, output_config)
            >>>
            >>> def callback(operation_future):
            ...     # Handle result.
            ...     result = operation_future.result()
            >>>
            >>> response.add_done_callback(callback)
            >>>
            >>> # Handle metadata.
            >>> metadata = response.metadata()

        Args:
            parent (str): Required. The relative name of the root asset. This can only be an
                organization number (such as ""organizations/123""), a project ID (such as
                ""projects/my-project-id""), or a project number (such as ""projects/12345""),
                or a folder number (such as ""folders/123"").
            output_config (Union[dict, ~google.cloud.asset_v1.types.OutputConfig]): Required. Output configuration indicating where the results will be output
                to. All results will be in newline delimited JSON format.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.asset_v1.types.OutputConfig`
            read_time (Union[dict, ~google.cloud.asset_v1.types.Timestamp]): Timestamp to take an asset snapshot. This can only be set to a timestamp
                between 2018-10-02 UTC (inclusive) and the current time. If not specified,
                the current time will be used. Due to delays in resource data collection
                and indexing, there is a volatile window during which running the same
                query may get different results.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.asset_v1.types.Timestamp`
            asset_types (list[str]): A list of asset types of which to take a snapshot for. For example:
                ""compute.googleapis.com/Disk"". If specified, only matching assets will
                be returned. See `Introduction to Cloud Asset
                Inventory <https://cloud.google.com/resource-manager/docs/cloud-asset-inventory/overview>`__
                for all supported asset types.
            content_type (~google.cloud.asset_v1.types.ContentType): Asset content type. If not specified, no content but the asset name will be
                returned.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.asset_v1.types._OperationFuture` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""export_assets"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""export_assets""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.export_assets,
                default_retry=self._method_configs[""ExportAssets""].retry,
                default_timeout=self._method_configs[""ExportAssets""].timeout,
                client_info=self._client_info,
            )

        request = asset_service_pb2.ExportAssetsRequest(
            parent=parent,
            output_config=output_config,
            read_time=read_time,
            asset_types=asset_types,
            content_type=content_type,
        )
        operation = self._inner_api_calls[""export_assets""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )
        return google.api_core.operation.from_gapic(
            operation,
            self.transport._operations_client,
            asset_service_pb2.ExportAssetsResponse,
            metadata_type=asset_service_pb2.ExportAssetsRequest,
        )"
collect_env_info;"def collect_env_info():
    """"""
    Returns:
        str - a table contains important information about the environment
    """"""
    data = []
    data.append((""sys.platform"", sys.platform))
    data.append((""Python"", sys.version.replace(""\n"", """")))
    data.append((""Tensorpack"", __git_version__))
    data.append((""Numpy"", np.__version__))

    data.append((""TensorFlow"", tfv1.VERSION + ""/"" + tfv1.GIT_VERSION))
    data.append((""TF Compiler Version"", tfv1.COMPILER_VERSION))
    has_cuda = tf.test.is_built_with_cuda()
    data.append((""TF CUDA support"", has_cuda))

    try:
        from tensorflow.python.framework import test_util
        data.append((""TF MKL support"", test_util.IsMklEnabled()))
    except Exception:
        pass

    try:
        from tensorflow.python.framework import test_util
        data.append((""TF XLA support"", test_util.is_xla_enabled()))
    except Exception:
        pass

    if has_cuda:
        data.append((""Nvidia Driver"", find_library(""nvidia-ml"")))
        data.append((""CUDA"", find_library(""cudart"")))
        data.append((""CUDNN"", find_library(""cudnn"")))
        data.append((""NCCL"", find_library(""nccl"")))

        # List devices with NVML
        data.append(
            (""CUDA_VISIBLE_DEVICES"",
             os.environ.get(""CUDA_VISIBLE_DEVICES"", str(None))))
        try:
            devs = defaultdict(list)
            with NVMLContext() as ctx:
                for idx, dev in enumerate(ctx.devices()):
                    devs[dev.name()].append(str(idx))

            for devname, devids in devs.items():
                data.append(
                    (""GPU "" + "","".join(devids), devname))
        except Exception:
            data.append((""GPU"", ""Not found with NVML""))

    vram = psutil.virtual_memory()
    data.append((""Free RAM"", ""{:.2f}/{:.2f} GB"".format(vram.available / 1024**3, vram.total / 1024**3)))
    data.append((""CPU Count"", psutil.cpu_count()))

    # Other important dependencies:
    try:
        import horovod
        data.append((""horovod"", horovod.__version__))
    except ImportError:
        pass

    try:
        import cv2
        data.append((""cv2"", cv2.__version__))
    except ImportError:
        pass

    import msgpack
    data.append((""msgpack"", ""."".join([str(x) for x in msgpack.version])))

    has_prctl = True
    try:
        import prctl
        _ = prctl.set_pdeathsig  # noqa
    except Exception:
        has_prctl = False
    data.append((""python-prctl"", has_prctl))

    return tabulate(data)"
get_pillar;"def get_pillar(opts, grains, minion_id, saltenv=None, ext=None, funcs=None,
               pillar_override=None, pillarenv=None, extra_minion_data=None):
    '''
    Return the correct pillar driver based on the file_client option
    '''
    file_client = opts['file_client']
    if opts.get('master_type') == 'disable' and file_client == 'remote':
        file_client = 'local'
    ptype = {
        'remote': RemotePillar,
        'local': Pillar
    }.get(file_client, Pillar)
    # If local pillar and we're caching, run through the cache system first
    log.debug('Determining pillar cache')
    if opts['pillar_cache']:
        log.info('Compiling pillar from cache')
        log.debug('get_pillar using pillar cache with ext: %s', ext)
        return PillarCache(opts, grains, minion_id, saltenv, ext=ext, functions=funcs,
                pillar_override=pillar_override, pillarenv=pillarenv)
    return ptype(opts, grains, minion_id, saltenv, ext, functions=funcs,
                 pillar_override=pillar_override, pillarenv=pillarenv,
                 extra_minion_data=extra_minion_data)"
gather_data;"def gather_data(self):
        """"""
        Gather data about files which may be recovered.

        The data is stored in self.data as a list of tuples with the data
        pertaining to the original file and the autosave file. Each element of
        the tuple is a dict as returned by gather_file_data().
        """"""
        self.data = []
        try:
            FileNotFoundError
        except NameError:  # Python 2
            FileNotFoundError = OSError
        # In Python 3, easier to use os.scandir()
        try:
            for name in os.listdir(self.autosave_dir):
                full_name = osp.join(self.autosave_dir, name)
                if osp.isdir(full_name):
                    continue
                for orig, autosave in self.autosave_mapping.items():
                    if autosave == full_name:
                        orig_dict = gather_file_data(orig)
                        break
                else:
                    orig_dict = None
                autosave_dict = gather_file_data(full_name)
                self.data.append((orig_dict, autosave_dict))
        except FileNotFoundError:  # autosave dir does not exist
            pass
        self.data.sort(key=recovery_data_key_function)
        self.num_enabled = len(self.data)"
get_table;"def get_table(self, table, retry=DEFAULT_RETRY):
        """"""Fetch the table referenced by ``table``.

        Args:
            table (Union[ \
                :class:`~google.cloud.bigquery.table.Table`, \
                :class:`~google.cloud.bigquery.table.TableReference`, \
                str, \
            ]):
                A reference to the table to fetch from the BigQuery API.
                If a string is passed in, this method attempts to create a
                table reference from a string using
                :func:`google.cloud.bigquery.table.TableReference.from_string`.
            retry (:class:`google.api_core.retry.Retry`):
                (Optional) How to retry the RPC.

        Returns:
            google.cloud.bigquery.table.Table:
                A ``Table`` instance.
        """"""
        table_ref = _table_arg_to_table_ref(table, default_project=self.project)
        api_response = self._call_api(retry, method=""GET"", path=table_ref.path)
        return Table.from_api_repr(api_response)"
add_evaluation_step;"def add_evaluation_step(result_tensor, ground_truth_tensor):
  """"""Inserts the operations we need to evaluate the accuracy of our results.

  Args:
    result_tensor: The new final node that produces results.
    ground_truth_tensor: The node we feed ground truth data
    into.

  Returns:
    Tuple of (evaluation step, prediction).
  """"""
  with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
      prediction = tf.argmax(result_tensor, 1)
      correct_prediction = tf.equal(prediction, ground_truth_tensor)
    with tf.name_scope('accuracy'):
      evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  tf.summary.scalar('accuracy', evaluation_step)
  return evaluation_step, prediction"
build_graph;"def build_graph(self, image, label):
        """"""This function should build the model which takes the input variables
        and return cost at the end""""""

        # In tensorflow, inputs to convolution function are assumed to be
        # NHWC. Add a single channel here.
        image = tf.expand_dims(image, 3)

        image = image * 2 - 1   # center the pixels values at zero

        # The context manager `argscope` sets the default option for all the layers under
        # this context. Here we use 32 channel convolution with shape 3x3
        with argscope([tf.layers.conv2d], padding='same', activation=tf.nn.relu):
            l = tf.layers.conv2d(image, 32, 3, name='conv0')
            l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')
            l = tf.layers.conv2d(l, 32, 3, name='conv1')
            l = tf.layers.conv2d(l, 32, 3, name='conv2')
            l = tf.layers.max_pooling2d(l, 2, 2, padding='valid')
            l = tf.layers.conv2d(l, 32, 3, name='conv3')
            l = tf.layers.flatten(l)
            l = tf.layers.dense(l, 512, activation=tf.nn.relu, name='fc0')
            l = tf.layers.dropout(l, rate=0.5,
                                  training=get_current_tower_context().is_training)
        logits = tf.layers.dense(l, 10, activation=tf.identity, name='fc1')

        # a vector of length B with loss of each sample
        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)
        cost = tf.reduce_mean(cost, name='cross_entropy_loss')  # the average cross-entropy loss

        correct = tf.cast(tf.nn.in_top_k(logits, label, 1), tf.float32, name='correct')
        accuracy = tf.reduce_mean(correct, name='accuracy')

        # This will monitor training error & accuracy (in a moving average fashion). The value will be automatically
        # 1. written to tensosrboard
        # 2. written to stat.json
        # 3. printed after each epoch
        train_error = tf.reduce_mean(1 - correct, name='train_error')
        summary.add_moving_summary(train_error, accuracy)

        # Use a regex to find parameters to apply weight decay.
        # Here we apply a weight decay on all W (weight matrix) of all fc layers
        # If you don't like regex, you can certainly define the cost in any other methods.
        wd_cost = tf.multiply(1e-5,
                              regularize_cost('fc.*/kernel', tf.nn.l2_loss),
                              name='regularize_loss')
        total_cost = tf.add_n([wd_cost, cost], name='total_cost')
        summary.add_moving_summary(cost, wd_cost, total_cost)

        # monitor histogram of all weight (of conv and fc layers) in tensorboard
        summary.add_param_summary(('.*/kernel', ['histogram', 'rms']))
        # the function should return the total cost to be optimized
        return total_cost"
add_tile;"def add_tile(self, tile_source, **kw):
        ''' Adds new ``TileRenderer`` into ``Plot.renderers``

        Args:
            tile_source (TileSource) : a tile source instance which contain tileset configuration

        Keyword Arguments:
            Additional keyword arguments are passed on as-is to the tile renderer

        Returns:
            TileRenderer : TileRenderer

        '''
        tile_renderer = TileRenderer(tile_source=tile_source, **kw)
        self.renderers.append(tile_renderer)
        return tile_renderer"
get_yml_content;"def get_yml_content(file_path):
    '''Load yaml file content'''
    try:
        with open(file_path, 'r') as file:
            return yaml.load(file, Loader=yaml.Loader)
    except yaml.scanner.ScannerError as err:
        print_error('yaml file format error!')
        exit(1)
    except Exception as exception:
        print_error(exception)
        exit(1)"
write_sid;"def write_sid(self, sid, df, invalid_data_behavior='warn'):
        """"""
        Write the OHLCV data for the given sid.
        If there is no bcolz ctable yet created for the sid, create it.
        If the length of the bcolz ctable is not exactly to the date before
        the first day provided, fill the ctable with 0s up to that date.

        Parameters
        ----------
        sid : int
            The asset identifer for the data being written.
        df : pd.DataFrame
            DataFrame of market data with the following characteristics.
            columns : ('open', 'high', 'low', 'close', 'volume')
                open : float64
                high : float64
                low  : float64
                close : float64
                volume : float64|int64
            index : DatetimeIndex of market minutes.
        """"""
        cols = {
            'open': df.open.values,
            'high': df.high.values,
            'low': df.low.values,
            'close': df.close.values,
            'volume': df.volume.values,
        }
        dts = df.index.values
        # Call internal method, since DataFrame has already ensured matching
        # index and value lengths.
        self._write_cols(sid, dts, cols, invalid_data_behavior)"
set_expire;"def set_expire(name, date):
    '''
    Sets the date on which the account expires. The user will not be able to
    login after this date. Date format is mm/dd/yyyy

    :param str name: The name of the user account

    :param datetime date: The date the account will expire. Format must be
        mm/dd/yyyy.

    :return: True if successful, False if not
    :rtype: bool

    :raises: CommandExecutionError on user not found or any other unknown error

    CLI Example:

    .. code-block:: bash

        salt '*' shadow.set_expire username 07/23/2015
    '''
    _set_account_policy(
        name, 'usingHardExpirationDate=1 hardExpireDateGMT={0}'.format(date))

    return get_expire(name) == date"
search_complete;"def search_complete(self, completed):
        """"""Current search thread has finished""""""
        self.result_browser.set_sorting(ON)
        self.find_options.ok_button.setEnabled(True)
        self.find_options.stop_button.setEnabled(False)
        self.status_bar.hide()
        self.result_browser.expandAll()
        if self.search_thread is None:
            return
        self.sig_finished.emit()
        found = self.search_thread.get_results()
        self.stop_and_reset_thread()
        if found is not None:
            results, pathlist, nb, error_flag = found
            self.result_browser.show()"
get_main_chain;"def get_main_chain(self):
        """"""Returns the main chain node ID list.""""""
        pre_node = {}
        distance = {}
        for i in range(self.n_nodes):
            distance[i] = 0
            pre_node[i] = i
        for i in range(self.n_nodes - 1):
            for u in range(self.n_nodes):
                for v, _ in self.adj_list[u]:
                    if distance[u] + 1 > distance[v]:
                        distance[v] = distance[u] + 1
                        pre_node[v] = u
        temp_id = 0
        for i in range(self.n_nodes):
            if distance[i] > distance[temp_id]:
                temp_id = i
        ret = []
        for i in range(self.n_nodes + 5):
            ret.append(temp_id)
            if pre_node[temp_id] == temp_id:
                break
            temp_id = pre_node[temp_id]
        assert temp_id == pre_node[temp_id]
        ret.reverse()
        return ret"
ensure_str;"def ensure_str(s, encoding='utf-8', errors='strict'):
    """"""Coerce *s* to `str`.

    For Python 2:
      - `unicode` -> encoded to `str`
      - `str` -> `str`

    For Python 3:
      - `str` -> `str`
      - `bytes` -> decoded to `str`
    """"""
    if not isinstance(s, (text_type, binary_type)):
        raise TypeError(""not expecting type '%s'"" % type(s))
    if PY2 and isinstance(s, text_type):
        s = s.encode(encoding, errors)
    elif PY3 and isinstance(s, binary_type):
        s = s.decode(encoding, errors)
    return s"
remove_dns_challenge_txt;"def remove_dns_challenge_txt(self, zone_id, domain, txt_challenge):
        """"""
        Remove DNS challenge TXT.
        """"""
        print(""Deleting DNS challenge.."")
        resp = self.route53.change_resource_record_sets(
            HostedZoneId=zone_id,
            ChangeBatch=self.get_dns_challenge_change_batch('DELETE', domain, txt_challenge)
        )

        return resp"
option_changed;"def option_changed(self, option, value):
        """"""Option has changed""""""
        setattr(self, to_text_string(option), value)
        self.shellwidget.set_namespace_view_settings()
        self.refresh_table()"
normalize_layout;"def normalize_layout(layout, min_percentile=1, max_percentile=99, relative_margin=0.1):
    """"""Removes outliers and scales layout to between [0,1].""""""

    # compute percentiles
    mins = np.percentile(layout, min_percentile, axis=(0))
    maxs = np.percentile(layout, max_percentile, axis=(0))

    # add margins
    mins -= relative_margin * (maxs - mins)
    maxs += relative_margin * (maxs - mins)

    # `clip` broadcasts, `[None]`s added only for readability
    clipped = np.clip(layout, mins, maxs)

    # embed within [0,1] along both axes
    clipped -= clipped.min(axis=0)
    clipped /= clipped.max(axis=0)

    return clipped"
update_multiarray_shape_range;"def update_multiarray_shape_range(spec, feature_name, shape_range):
    """"""
    Annotate an input or output MLMultiArray feature in a Neural Network spec
    to accommodate a range of shapes

    :param spec: MLModel
        The MLModel spec containing the feature

    :param feature_name: str
        The name of the feature for which to add shape range
        information. If the feature is not found in the input or output
        descriptions then an exception is thrown

    :param shape_range: NeuralNetworkMultiArrayShapeRange
        A NeuralNetworkMultiArrayShapeRange object with the populated shape
        range information. The shape_range object must either contain only
        shape information for channel or channel, height and width. If
        the object is invalid then an exception is thrown

    Examples
    --------
    .. sourcecode:: python

        >>> import coremltools
        >>> from coremltools.models.neural_network import flexible_shape_utils
        >>> spec = coremltools.utils.load_spec('mymodel.mlmodel')
        >>> shape_range = flexible_shape_utils.NeuralNetworkMultiArrayShapeRange()
        >>> shape_range.add_channel_range((1, 3))
        >>> shape_range.add_width_range((128, 256))
        >>> shape_range.add_height_range((128, 256))
        >>> flexible_shape_utils.update_multiarray_shape_range(spec, feature_name='my_multiarray_featurename', shape_range=shape_range)

    :return:
        None. The spec is updated
    """"""
    if not isinstance(shape_range, NeuralNetworkMultiArrayShapeRange):
        raise Exception('Shape range should be of type MultiArrayShapeRange')

    shape_range.validate_array_shape_range()
    feature = _get_feature(spec, feature_name)

    if feature.type.WhichOneof('Type') != 'multiArrayType':
        raise Exception('Trying to update shape range for '
                        'a non-multiArray feature type')

    # Add channel range
    feature.type.multiArrayType.ClearField('ShapeFlexibility')
    s = feature.type.multiArrayType.shapeRange.sizeRanges.add()
    s.lowerBound = shape_range.get_channel_range().lowerBound
    s.upperBound = shape_range.get_channel_range().upperBound

    if shape_range.get_shape_range_dims() > 1:
        # Add height range
        s = feature.type.multiArrayType.shapeRange.sizeRanges.add()
        s.lowerBound = shape_range.get_height_range().lowerBound
        s.upperBound = shape_range.get_height_range().upperBound
        # Add width range
        s = feature.type.multiArrayType.shapeRange.sizeRanges.add()
        s.lowerBound = shape_range.get_width_range().lowerBound
        s.upperBound = shape_range.get_width_range().upperBound

    # Bump up specification version
    spec.specificationVersion = max(_MINIMUM_FLEXIBLE_SHAPES_SPEC_VERSION,
                                    spec.specificationVersion)"
send_msg;"def send_msg(recipient,
             message,
             subject='Message from Salt',
             sender=None,
             server=None,
             use_ssl='True',
             username=None,
             password=None,
             profile=None,
             attachments=None):
    '''
    Send a message to an SMTP recipient. Designed for use in states.

    CLI Examples:

    .. code-block:: bash

        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' profile='my-smtp-account'
        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' username='myuser' password='verybadpass' sender='admin@example.com' server='smtp.domain.com'
        salt '*' smtp.send_msg 'admin@example.com' 'This is a salt module test' username='myuser' password='verybadpass' sender='admin@example.com' server='smtp.domain.com' attachments=""['/var/log/messages']""
    '''
    if profile:
        creds = __salt__['config.option'](profile)
        server = creds.get('smtp.server')
        use_ssl = creds.get('smtp.tls')
        sender = creds.get('smtp.sender')
        username = creds.get('smtp.username')
        password = creds.get('smtp.password')

    if attachments:
        msg = email.mime.multipart.MIMEMultipart()
        msg.attach(email.mime.text.MIMEText(message))
    else:
        msg = email.mime.text.MIMEText(message)
    msg['Subject'] = subject
    msg['From'] = sender
    msg['To'] = recipient
    recipients = [r.strip() for r in recipient.split(',')]

    try:
        if use_ssl in ['True', 'true']:
            smtpconn = smtplib.SMTP_SSL(server)
        else:
            smtpconn = smtplib.SMTP(server)

    except socket.gaierror as _error:
        log.debug(""Exception: %s"", _error)
        return False

    if use_ssl not in ('True', 'true'):
        smtpconn.ehlo()
        if smtpconn.has_extn('STARTTLS'):
            try:
                smtpconn.starttls()
            except smtplib.SMTPHeloError:
                log.debug(""The server didn’t reply properly \
                        to the HELO greeting."")
                return False
            except smtplib.SMTPException:
                log.debug(""The server does not support the STARTTLS extension."")
                return False
            except RuntimeError:
                log.debug(""SSL/TLS support is not available \
                        to your Python interpreter."")
                return False
            smtpconn.ehlo()

    if username and password:
        try:
            smtpconn.login(username, password)
        except smtplib.SMTPAuthenticationError as _error:
            log.debug(""SMTP Authentication Failure"")
            return False

    if attachments:
        for f in attachments:
            name = os.path.basename(f)
            with salt.utils.files.fopen(f, 'rb') as fin:
                att = email.mime.application.MIMEApplication(fin.read(), Name=name)
            att['Content-Disposition'] = 'attachment; filename=""{0}""'.format(name)
            msg.attach(att)

    try:
        smtpconn.sendmail(sender, recipients, msg.as_string())
    except smtplib.SMTPRecipientsRefused:
        log.debug(""All recipients were refused."")
        return False
    except smtplib.SMTPHeloError:
        log.debug(""The server didn’t reply properly to the HELO greeting."")
        return False
    except smtplib.SMTPSenderRefused:
        log.debug(""The server didn’t accept the %s."", sender)
        return False
    except smtplib.SMTPDataError:
        log.debug(""The server replied with an unexpected error code."")
        return False

    smtpconn.quit()
    return True"
clear_cache;"def clear_cache(self, index=None, params=None):
        """"""
        Clear either all caches or specific cached associated with one ore more indices.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-clearcache.html>`_

        :arg index: A comma-separated list of index name to limit the operation
        :arg allow_no_indices: Whether to ignore if a wildcard indices
            expression resolves into no concrete indices. (This includes `_all`
            string or when no indices have been specified)
        :arg expand_wildcards: Whether to expand wildcard expression to concrete
            indices that are open, closed or both., default 'open', valid
            choices are: 'open', 'closed', 'none', 'all'
        :arg field_data: Clear field data
        :arg fielddata: Clear field data
        :arg fields: A comma-separated list of fields to clear when using the
            `field_data` parameter (default: all)
        :arg ignore_unavailable: Whether specified concrete indices should be
            ignored when unavailable (missing or closed)
        :arg query: Clear query caches
        :arg recycler: Clear the recycler cache
        :arg request: Clear request cache
        :arg request_cache: Clear request cache
        """"""
        return self.transport.perform_request(
            ""POST"", _make_path(index, ""_cache"", ""clear""), params=params
        )"
get_sessions;"def get_sessions(self, app_path):
        ''' Gets all currently active sessions for an application.

        Args:
            app_path (str) :
                The configured application path for the application to return
                sessions for.

        Returns:
            list[ServerSession]

        '''
        if app_path not in self._applications:
            raise ValueError(""Application %s does not exist on this server"" % app_path)
        return list(self._applications[app_path].sessions)"
worker_thread;"def worker_thread(thread_name, run_freq,  gui_queue):
    """"""
    A worker thrread that communicates with the GUI
    These threads can call functions that block withouth affecting the GUI (a good thing)
    Note that this function is the code started as each thread. All threads are identical in this way
    :param thread_name: Text name used  for displaying info
    :param run_freq: How often the thread should run in milliseconds
    :param gui_queue: Queue used to communicate with the GUI
    :return:
    """"""
    print('Starting thread - {} that runds every {} ms'.format(thread_name, run_freq))
    for i in itertools.count():                             # loop forever, keeping count in i as it loops
        time.sleep(run_freq/1000)                           # sleep for a while
        gui_queue.put('{} - {}'.format(thread_name, i))     # put a message into queue for GUI
        print('..')"
require_initialized;"def require_initialized(exception):
    """"""
    Decorator for API methods that should only be called after
    TradingAlgorithm.initialize.  `exception` will be raised if the method is
    called before initialize has completed.

    Examples
    --------
    @require_initialized(SomeException(""Don't do that!""))
    def method(self):
        # Do stuff that should only be allowed after initialize.
    """"""
    def decorator(method):
        @wraps(method)
        def wrapped_method(self, *args, **kwargs):
            if not self.initialized:
                raise exception
            return method(self, *args, **kwargs)
        return wrapped_method
    return decorator"
remote_addr;"def remote_addr(self):
        """"""Attempt to return the original client ip based on X-Forwarded-For
        or X-Real-IP. If HTTP headers are unavailable or untrusted, returns
        an empty string.

        :return: original client ip.
        """"""
        if not hasattr(self, ""_remote_addr""):
            if self.app.config.PROXIES_COUNT == 0:
                self._remote_addr = """"
            elif self.app.config.REAL_IP_HEADER and self.headers.get(
                self.app.config.REAL_IP_HEADER
            ):
                self._remote_addr = self.headers[
                    self.app.config.REAL_IP_HEADER
                ]
            elif self.app.config.FORWARDED_FOR_HEADER:
                forwarded_for = self.headers.get(
                    self.app.config.FORWARDED_FOR_HEADER, """"
                ).split("","")
                remote_addrs = [
                    addr
                    for addr in [addr.strip() for addr in forwarded_for]
                    if addr
                ]
                if self.app.config.PROXIES_COUNT == -1:
                    self._remote_addr = remote_addrs[0]
                elif len(remote_addrs) >= self.app.config.PROXIES_COUNT:
                    self._remote_addr = remote_addrs[
                        -self.app.config.PROXIES_COUNT
                    ]
                else:
                    self._remote_addr = """"
            else:
                self._remote_addr = """"
        return self._remote_addr"
trigger_events;"def trigger_events(events, loop):
    """"""Trigger event callbacks (functions or async)

    :param events: one or more sync or async functions to execute
    :param loop: event loop
    """"""
    for event in events:
        result = event(loop)
        if isawaitable(result):
            loop.run_until_complete(result)"
namespace_present;"def namespace_present(name, **kwargs):
    '''
    Ensures that the named namespace is present.

    name
        The name of the namespace.

    '''
    ret = {'name': name,
           'changes': {},
           'result': False,
           'comment': ''}

    namespace = __salt__['kubernetes.show_namespace'](name, **kwargs)

    if namespace is None:
        if __opts__['test']:
            ret['result'] = None
            ret['comment'] = 'The namespace is going to be created'
            return ret

        res = __salt__['kubernetes.create_namespace'](name, **kwargs)
        ret['result'] = True
        ret['changes']['namespace'] = {
            'old': {},
            'new': res}
    else:
        ret['result'] = True if not __opts__['test'] else None
        ret['comment'] = 'The namespace already exists'

    return ret"
get_managed;"def get_managed(
        name,
        template,
        source,
        source_hash,
        source_hash_name,
        user,
        group,
        mode,
        attrs,
        saltenv,
        context,
        defaults,
        skip_verify=False,
        **kwargs):
    '''
    Return the managed file data for file.managed

    name
        location where the file lives on the server

    template
        template format

    source
        managed source file

    source_hash
        hash of the source file

    source_hash_name
        When ``source_hash`` refers to a remote file, this specifies the
        filename to look for in that file.

        .. versionadded:: 2016.3.5

    user
        Owner of file

    group
        Group owner of file

    mode
        Permissions of file

    attrs
        Attributes of file

        .. versionadded:: 2018.3.0

    context
        Variables to add to the template context

    defaults
        Default values of for context_dict

    skip_verify
        If ``True``, hash verification of remote file sources (``http://``,
        ``https://``, ``ftp://``) will be skipped, and the ``source_hash``
        argument will be ignored.

        .. versionadded:: 2016.3.0

    CLI Example:

    .. code-block:: bash

        salt '*' file.get_managed /etc/httpd/conf.d/httpd.conf jinja salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' None root root '755' base None None
    '''
    # Copy the file to the minion and templatize it
    sfn = ''
    source_sum = {}

    def _get_local_file_source_sum(path):
        '''
        DRY helper for getting the source_sum value from a locally cached
        path.
        '''
        return {'hsum': get_hash(path, form='sha256'), 'hash_type': 'sha256'}

    # If we have a source defined, let's figure out what the hash is
    if source:
        urlparsed_source = _urlparse(source)
        if urlparsed_source.scheme in salt.utils.files.VALID_PROTOS:
            parsed_scheme = urlparsed_source.scheme
        else:
            parsed_scheme = ''
        parsed_path = os.path.join(
                urlparsed_source.netloc, urlparsed_source.path).rstrip(os.sep)
        unix_local_source = parsed_scheme in ('file', '')

        if parsed_scheme == '':
            parsed_path = sfn = source
            if not os.path.exists(sfn):
                msg = 'Local file source {0} does not exist'.format(sfn)
                return '', {}, msg
        elif parsed_scheme == 'file':
            sfn = parsed_path
            if not os.path.exists(sfn):
                msg = 'Local file source {0} does not exist'.format(sfn)
                return '', {}, msg

        if parsed_scheme and parsed_scheme.lower() in string.ascii_lowercase:
            parsed_path = ':'.join([parsed_scheme, parsed_path])
            parsed_scheme = 'file'

        if parsed_scheme == 'salt':
            source_sum = __salt__['cp.hash_file'](source, saltenv)
            if not source_sum:
                return '', {}, 'Source file {0} not found in saltenv \'{1}\''.format(source, saltenv)
        elif not source_hash and unix_local_source:
            source_sum = _get_local_file_source_sum(parsed_path)
        elif not source_hash and source.startswith(os.sep):
            # This should happen on Windows
            source_sum = _get_local_file_source_sum(source)
        else:
            if not skip_verify:
                if source_hash:
                    try:
                        source_sum = get_source_sum(name,
                                                    source,
                                                    source_hash,
                                                    source_hash_name,
                                                    saltenv)
                    except CommandExecutionError as exc:
                        return '', {}, exc.strerror
                else:
                    msg = (
                        'Unable to verify upstream hash of source file {0}, '
                        'please set source_hash or set skip_verify to True'
                        .format(salt.utils.url.redact_http_basic_auth(source))
                    )
                    return '', {}, msg

    if source and (template or parsed_scheme in salt.utils.files.REMOTE_PROTOS):
        # Check if we have the template or remote file cached
        cache_refetch = False
        cached_dest = __salt__['cp.is_cached'](source, saltenv)
        if cached_dest and (source_hash or skip_verify):
            htype = source_sum.get('hash_type', 'sha256')
            cached_sum = get_hash(cached_dest, form=htype)
            if skip_verify:
                # prev: if skip_verify or cached_sum == source_sum['hsum']:
                # but `cached_sum == source_sum['hsum']` is elliptical as prev if
                sfn = cached_dest
                source_sum = {'hsum': cached_sum, 'hash_type': htype}
            elif cached_sum != source_sum.get('hsum', __opts__['hash_type']):
                cache_refetch = True
            else:
                sfn = cached_dest

        # If we didn't have the template or remote file, or the file has been
        # updated and the cache has to be refreshed, download the file.
        if not sfn or cache_refetch:
            try:
                sfn = __salt__['cp.cache_file'](
                    source,
                    saltenv,
                    source_hash=source_sum.get('hsum'))
            except Exception as exc:
                # A 404 or other error code may raise an exception, catch it
                # and return a comment that will fail the calling state.
                _source = salt.utils.url.redact_http_basic_auth(source)
                return '', {}, 'Failed to cache {0}: {1}'.format(_source, exc)

        # If cache failed, sfn will be False, so do a truth check on sfn first
        # as invoking os.path.exists() on a bool raises a TypeError.
        if not sfn or not os.path.exists(sfn):
            _source = salt.utils.url.redact_http_basic_auth(source)
            return sfn, {}, 'Source file \'{0}\' not found'.format(_source)
        if sfn == name:
            raise SaltInvocationError(
                'Source file cannot be the same as destination'
            )

        if template:
            if template in salt.utils.templates.TEMPLATE_REGISTRY:
                context_dict = defaults if defaults else {}
                if context:
                    context_dict = salt.utils.dictupdate.merge(context_dict, context)
                data = salt.utils.templates.TEMPLATE_REGISTRY[template](
                    sfn,
                    name=name,
                    source=source,
                    user=user,
                    group=group,
                    mode=mode,
                    attrs=attrs,
                    saltenv=saltenv,
                    context=context_dict,
                    salt=__salt__,
                    pillar=__pillar__,
                    grains=__opts__['grains'],
                    opts=__opts__,
                    **kwargs)
            else:
                return sfn, {}, ('Specified template format {0} is not supported'
                                 ).format(template)

            if data['result']:
                sfn = data['data']
                hsum = get_hash(sfn, form='sha256')
                source_sum = {'hash_type': 'sha256',
                              'hsum': hsum}
            else:
                __clean_tmp(sfn)
                return sfn, {}, data['data']

    return sfn, source_sum, ''"
competitions_submissions_list;"def competitions_submissions_list(self, id, **kwargs):  # noqa: E501
        """"""List competition submissions  # noqa: E501

        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.competitions_submissions_list(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: Competition name (required)
        :param int page: Page number
        :return: Result
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501
        else:
            (data) = self.competitions_submissions_list_with_http_info(id, **kwargs)  # noqa: E501
            return data"
crop_and_resize;"def crop_and_resize(image, boxes, box_ind, crop_size, pad_border=True):
    """"""
    Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes.

    Args:
        image: NCHW
        boxes: nx4, x1y1x2y2
        box_ind: (n,)
        crop_size (int):
    Returns:
        n,C,size,size
    """"""
    assert isinstance(crop_size, int), crop_size
    boxes = tf.stop_gradient(boxes)

    # TF's crop_and_resize produces zeros on border
    if pad_border:
        # this can be quite slow
        image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode='SYMMETRIC')
        boxes = boxes + 1

    @under_name_scope()
    def transform_fpcoor_for_tf(boxes, image_shape, crop_shape):
        """"""
        The way tf.image.crop_and_resize works (with normalized box):
        Initial point (the value of output[0]): x0_box * (W_img - 1)
        Spacing: w_box * (W_img - 1) / (W_crop - 1)
        Use the above grid to bilinear sample.

        However, what we want is (with fpcoor box):
        Spacing: w_box / W_crop
        Initial point: x0_box + spacing/2 - 0.5
        (-0.5 because bilinear sample (in my definition) assumes floating point coordinate
         (0.0, 0.0) is the same as pixel value (0, 0))

        This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize

        Returns:
            y1x1y2x2
        """"""
        x0, y0, x1, y1 = tf.split(boxes, 4, axis=1)

        spacing_w = (x1 - x0) / tf.cast(crop_shape[1], tf.float32)
        spacing_h = (y1 - y0) / tf.cast(crop_shape[0], tf.float32)

        imshape = [tf.cast(image_shape[0] - 1, tf.float32), tf.cast(image_shape[1] - 1, tf.float32)]
        nx0 = (x0 + spacing_w / 2 - 0.5) / imshape[1]
        ny0 = (y0 + spacing_h / 2 - 0.5) / imshape[0]

        nw = spacing_w * tf.cast(crop_shape[1] - 1, tf.float32) / imshape[1]
        nh = spacing_h * tf.cast(crop_shape[0] - 1, tf.float32) / imshape[0]

        return tf.concat([ny0, nx0, ny0 + nh, nx0 + nw], axis=1)

    # Expand bbox to a minium size of 1
    # boxes_x1y1, boxes_x2y2 = tf.split(boxes, 2, axis=1)
    # boxes_wh = boxes_x2y2 - boxes_x1y1
    # boxes_center = tf.reshape((boxes_x2y2 + boxes_x1y1) * 0.5, [-1, 2])
    # boxes_newwh = tf.maximum(boxes_wh, 1.)
    # boxes_x1y1new = boxes_center - boxes_newwh * 0.5
    # boxes_x2y2new = boxes_center + boxes_newwh * 0.5
    # boxes = tf.concat([boxes_x1y1new, boxes_x2y2new], axis=1)

    image_shape = tf.shape(image)[2:]
    boxes = transform_fpcoor_for_tf(boxes, image_shape, [crop_size, crop_size])
    image = tf.transpose(image, [0, 2, 3, 1])   # nhwc
    ret = tf.image.crop_and_resize(
        image, boxes, tf.cast(box_ind, tf.int32),
        crop_size=[crop_size, crop_size])
    ret = tf.transpose(ret, [0, 3, 1, 2])   # ncss
    return ret"
delete_topics;"def delete_topics(self, topics, **kwargs):
        """"""
        Delete topics.

        The future result() value is None.

        :param list(str) topics: Topics to mark for deletion.
        :param float operation_timeout: Set broker's operation timeout in seconds,
                  controlling how long the DeleteTopics request will block
                  on the broker waiting for the topic deletion to propagate
                  in the cluster. A value of 0 returns immediately. Default: 0
        :param float request_timeout: Set the overall request timeout in seconds,
                  including broker lookup, request transmission, operation time
                  on broker, and response. Default: `socket.timeout.ms*1000.0`

        :returns: a dict of futures for each topic, keyed by the topic name.
        :rtype: dict(<topic_name, future>)

        :raises KafkaException: Operation failed locally or on broker.
        :raises TypeException: Invalid input.
        :raises ValueException: Invalid input.
        """"""

        f, futmap = AdminClient._make_futures(topics, None,
                                              AdminClient._make_topics_result)

        super(AdminClient, self).delete_topics(topics, f, **kwargs)

        return futmap"
add_graph_copy;"def add_graph_copy(self, graph, tags=None):
    """"""Adds a copy of Graph with the specified set of tags.""""""
    with graph.as_default():
      # Remove default attrs so that Modules created by a tensorflow version
      # with ops that have new attrs that are left to their default values can
      # still be loaded by older versions unware of those attributes.
      meta_graph = tf_v1.train.export_meta_graph(strip_default_attrs=True)
      _export_tags(meta_graph, tags)
      _export_signatures(meta_graph)
      _export_module_attachments(meta_graph)
    self._proto.meta_graphs.extend([meta_graph])"
delete_route;"def delete_route(route_table_id=None, destination_cidr_block=None,
                 route_table_name=None, region=None, key=None,
                 keyid=None, profile=None):
    '''
    Deletes a route.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_vpc.delete_route 'rtb-1f382e7d' '10.0.0.0/16'

    '''

    if not _exactly_one((route_table_name, route_table_id)):
        raise SaltInvocationError('One (but not both) of route_table_id or route_table_name '
                                  'must be provided.')

    if destination_cidr_block is None:
        raise SaltInvocationError('destination_cidr_block is required.')

    try:
        if route_table_name:
            route_table_id = _get_resource_id('route_table', route_table_name,
                                              region=region, key=key,
                                              keyid=keyid, profile=profile)
            if not route_table_id:
                return {'created': False,
                        'error': {'message': 'route table {0} does not exist.'.format(route_table_name)}}
    except BotoServerError as e:
        return {'created': False, 'error': __utils__['boto.get_error'](e)}

    return _delete_resource(resource='route', resource_id=route_table_id,
                            destination_cidr_block=destination_cidr_block,
                            region=region, key=key,
                            keyid=keyid, profile=profile)"
get_authenticated_user;"async def get_authenticated_user(
        self, http_client: httpclient.AsyncHTTPClient = None
    ) -> Dict[str, Any]:
        """"""Fetches the authenticated user data upon redirect.

        This method should be called by the handler that receives the
        redirect from the `authenticate_redirect()` method (which is
        often the same as the one that calls it; in that case you would
        call `get_authenticated_user` if the ``openid.mode`` parameter
        is present and `authenticate_redirect` if it is not).

        The result of this method will generally be used to set a cookie.

        .. versionchanged:: 6.0

            The ``callback`` argument was removed. Use the returned
            awaitable object instead.
        """"""
        handler = cast(RequestHandler, self)
        # Verify the OpenID response via direct request to the OP
        args = dict(
            (k, v[-1]) for k, v in handler.request.arguments.items()
        )  # type: Dict[str, Union[str, bytes]]
        args[""openid.mode""] = u""check_authentication""
        url = self._OPENID_ENDPOINT  # type: ignore
        if http_client is None:
            http_client = self.get_auth_http_client()
        resp = await http_client.fetch(
            url, method=""POST"", body=urllib.parse.urlencode(args)
        )
        return self._on_authentication_verified(resp)"
data_iter;"def data_iter(batch_size, num_embed, pre_trained_word2vec=False):
    """"""Construct data iter

    Parameters
    ----------
    batch_size: int
    num_embed: int
    pre_trained_word2vec: boolean
                        identify the pre-trained layers or not
    Returns
    ----------
    train_set: DataIter
                Train DataIter
    valid: DataIter
                Valid DataIter
    sentences_size: int
                array dimensions
    embedded_size: int
                array dimensions
    vocab_size: int
                array dimensions
    """"""
    print('Loading data...')
    if pre_trained_word2vec:
        word2vec = data_helpers.load_pretrained_word2vec('data/rt.vec')
        x, y = data_helpers.load_data_with_word2vec(word2vec)
        # reshape for convolution input
        x = np.reshape(x, (x.shape[0], 1, x.shape[1], x.shape[2]))
        embedded_size = x.shape[-1]
        sentences_size = x.shape[2]
        vocabulary_size = -1
    else:
        x, y, vocab, vocab_inv = data_helpers.load_data()
        embedded_size = num_embed
        sentences_size = x.shape[1]
        vocabulary_size = len(vocab)

    # randomly shuffle data
    np.random.seed(10)
    shuffle_indices = np.random.permutation(np.arange(len(y)))
    x_shuffled = x[shuffle_indices]
    y_shuffled = y[shuffle_indices]

    # split train/valid set
    x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]
    y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]
    print('Train/Valid split: %d/%d' % (len(y_train), len(y_dev)))
    print('train shape:', x_train.shape)
    print('valid shape:', x_dev.shape)
    print('sentence max words', sentences_size)
    print('embedding size', embedded_size)
    print('vocab size', vocabulary_size)

    train_set = mx.io.NDArrayIter(
        x_train, y_train, batch_size, shuffle=True)
    valid = mx.io.NDArrayIter(
        x_dev, y_dev, batch_size)

    return train_set, valid, sentences_size, embedded_size, vocabulary_size"
delete_table;"def delete_table(self, table):
        """"""Deletes a table, if it exists.

           :param table:
           :type table: BQTable
        """"""

        if not self.table_exists(table):
            return

        self.client.tables().delete(projectId=table.project_id,
                                    datasetId=table.dataset_id,
                                    tableId=table.table_id).execute()"
get_timestamped_export_dir;"def get_timestamped_export_dir(export_dir_base):
  """"""Builds a path to a new subdirectory within the base directory.

  Each export is written into a new subdirectory named using the
  current time.  This guarantees monotonically increasing version
  numbers even across multiple runs of the pipeline.
  The timestamp used is the number of seconds since epoch UTC.

  Args:
    export_dir_base: A string containing a directory to write the exported
        graph and checkpoints.
  Returns:
    The full path of the new subdirectory (which is not actually created yet).

  Raises:
    RuntimeError: if repeated attempts fail to obtain a unique timestamped
      directory name.
  """"""
  attempts = 0
  while attempts < MAX_DIRECTORY_CREATION_ATTEMPTS:
    export_timestamp = int(time.time())

    export_dir = os.path.join(
        tf.compat.as_bytes(export_dir_base),
        tf.compat.as_bytes(str(export_timestamp)))
    if not tf_v1.gfile.Exists(export_dir):
      # Collisions are still possible (though extremely unlikely): this
      # directory is not actually created yet, but it will be almost
      # instantly on return from this function.
      return export_dir
    time.sleep(1)
    attempts += 1
    logging.warn(
        ""Export directory %s already exists; retrying (attempt %d/%d)"",
        export_dir, attempts, MAX_DIRECTORY_CREATION_ATTEMPTS)
  raise RuntimeError(""Failed to obtain a unique export directory name after ""
                     ""%d attempts."".MAX_DIRECTORY_CREATION_ATTEMPTS)"
set_editor_ids_order;"def set_editor_ids_order(self, ordered_editor_ids):
        """"""
        Order the root file items in the Outline Explorer following the
        provided list of editor ids.
        """"""
        if self.ordered_editor_ids != ordered_editor_ids:
            self.ordered_editor_ids = ordered_editor_ids
            if self.sort_files_alphabetically is False:
                self.__sort_toplevel_items()"
pairwise_iou;"def pairwise_iou(boxlist1, boxlist2):
    """"""Computes pairwise intersection-over-union between box collections.

    Args:
      boxlist1: Nx4 floatbox
      boxlist2: Mx4

    Returns:
      a tensor with shape [N, M] representing pairwise iou scores.
    """"""
    intersections = pairwise_intersection(boxlist1, boxlist2)
    areas1 = area(boxlist1)
    areas2 = area(boxlist2)
    unions = (
        tf.expand_dims(areas1, 1) + tf.expand_dims(areas2, 0) - intersections)
    return tf.where(
        tf.equal(intersections, 0.0),
        tf.zeros_like(intersections), tf.truediv(intersections, unions))"
load_audio;"def load_audio(path, with_path=True, recursive=True, ignore_failure=True, random_order=False):
    """"""
    Loads WAV file(s) from a path.

    Parameters
    ----------
    path : str
        Path to WAV files to be loaded.

    with_path : bool, optional
        Indicates whether a path column is added to the returned SFrame.

    recursive : bool, optional
        Indicates whether ``load_audio`` should do a recursive directory traversal,
        or only load audio files directly under ``path``.

    ignore_failure : bool, optional
        If True, only print warnings for failed files and keep loading the remaining
        audio files.

    random_order : bool, optional
        Load audio files in random order.

    Returns
    -------
    out : SFrame
        Returns an SFrame with either an 'audio' column or both an 'audio' and
        a 'path' column. The 'audio' column is a column of dictionaries.

        Each dictionary contains two items. One item is the sample rate, in
        samples per second (int type). The other item will be the data in a numpy
        array. If the wav file has a single channel, the array will have a single
        dimension. If there are multiple channels, the array will have shape
        (L,C) where L is the number of samples and C is the number of channels.

    Examples
    --------
    >>> audio_path = ""~/Documents/myAudioFiles/""
    >>> audio_sframe = tc.audio_analysis.load_audio(audio_path, recursive=True)
    """"""
    from scipy.io import wavfile as _wavfile

    all_wav_files = []

    if _fnmatch(path, '*.wav'):    # single file
        all_wav_files.append(path)
    elif recursive:
        for (dir_path, _, file_names) in _os.walk(path):
            for cur_file in file_names:
                if _fnmatch(cur_file, '*.wav'):
                    all_wav_files.append(dir_path + '/' + cur_file)
    else:
        all_wav_files = _glob(path + '/*.wav')

    if random_order:
        _shuffle(all_wav_files)

    result_builder = _tc.SFrameBuilder(column_types=[dict, str], column_names=['audio', 'path'])
    for cur_file_path in all_wav_files:
        try:
            sample_rate, data = _wavfile.read(cur_file_path)
        except Exception as e:
            error_string = ""Could not read {}: {}"".format(cur_file_path, e)
            if not ignore_failure:
                raise _ToolkitError(error_string)
            else:
                print(error_string)
                continue

        result_builder.append([{'sample_rate': sample_rate, 'data': data}, cur_file_path])

    result = result_builder.close()
    if not with_path:
        del result['path']
    return result"
get_restart_power_failure;"def get_restart_power_failure():
    '''
    Displays whether 'restart on power failure' is on or off if supported

    :return: A string value representing the ""restart on power failure"" settings
    :rtype: string

    CLI Example:

    .. code-block:: bash

        salt '*' power.get_restart_power_failure
    '''
    ret = salt.utils.mac_utils.execute_return_result(
        'systemsetup -getrestartpowerfailure')
    return salt.utils.mac_utils.validate_enabled(
        salt.utils.mac_utils.parse_return(ret)) == 'on'"
find_text;"def find_text(self, text, changed=True, forward=True, case=False,
                  words=False, regexp=False):
        """"""Find text""""""
        cursor = self.textCursor()
        findflag = QTextDocument.FindFlag()

        if not forward:
            findflag = findflag | QTextDocument.FindBackward

        if case:
            findflag = findflag | QTextDocument.FindCaseSensitively

        moves = [QTextCursor.NoMove]
        if forward:
            moves += [QTextCursor.NextWord, QTextCursor.Start]
            if changed:
                if to_text_string(cursor.selectedText()):
                    new_position = min([cursor.selectionStart(),
                                        cursor.selectionEnd()])
                    cursor.setPosition(new_position)
                else:
                    cursor.movePosition(QTextCursor.PreviousWord)
        else:
            moves += [QTextCursor.End]

        if regexp:
            text = to_text_string(text)
        else:
            text = re.escape(to_text_string(text))

        if QT55_VERSION:
            pattern = QRegularExpression(u""\\b{}\\b"".format(text) if words else
                                         text)
            if case:
                pattern.setPatternOptions(
                    QRegularExpression.CaseInsensitiveOption)
        else:
            pattern = QRegExp(u""\\b{}\\b"".format(text)
                              if words else text, Qt.CaseSensitive if case else
                              Qt.CaseInsensitive, QRegExp.RegExp2)

        for move in moves:
            cursor.movePosition(move)
            if regexp and '\\n' in text:
                # Multiline regular expression
                found_cursor = self.find_multiline_pattern(pattern, cursor,
                                                           findflag)
            else:
                # Single line find: using the QTextDocument's find function,
                # probably much more efficient than ours
                found_cursor = self.document().find(pattern, cursor, findflag)
            if found_cursor is not None and not found_cursor.isNull():
                self.setTextCursor(found_cursor)
                return True

        return False"
save_checkpoint;"def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params):
    """"""Checkpoint the model data into file.

    Parameters
    ----------
    prefix : str
        Prefix of model name.
    epoch : int
        The epoch number of the model.
    symbol : Symbol
        The input Symbol.
    arg_params : dict of str to NDArray
        Model parameter, dict of name to NDArray of net's weights.
    aux_params : dict of str to NDArray
        Model parameter, dict of name to NDArray of net's auxiliary states.
    Notes
    -----
    - ``prefix-symbol.json`` will be saved for symbol.
    - ``prefix-epoch.params`` will be saved for parameters.
    """"""
    if symbol is not None:
        symbol.save('%s-symbol.json' % prefix)

    save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
    save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})
    param_name = '%s-%04d.params' % (prefix, epoch)
    nd.save(param_name, save_dict)
    logging.info('Saved checkpoint to \""%s\""', param_name)"
convert_cols;"def convert_cols(cols, scale_factor, sid, invalid_data_behavior):
    """"""Adapt OHLCV columns into uint32 columns.

    Parameters
    ----------
    cols : dict
        A dict mapping each column name (open, high, low, close, volume)
        to a float column to convert to uint32.
    scale_factor : int
        Factor to use to scale float values before converting to uint32.
    sid : int
        Sid of the relevant asset, for logging.
    invalid_data_behavior : str
        Specifies behavior when data cannot be converted to uint32.
        If 'raise', raises an exception.
        If 'warn', logs a warning and filters out incompatible values.
        If 'ignore', silently filters out incompatible values.
    """"""
    scaled_opens = (np.nan_to_num(cols['open']) * scale_factor).round()
    scaled_highs = (np.nan_to_num(cols['high']) * scale_factor).round()
    scaled_lows = (np.nan_to_num(cols['low']) * scale_factor).round()
    scaled_closes = (np.nan_to_num(cols['close']) * scale_factor).round()

    exclude_mask = np.zeros_like(scaled_opens, dtype=bool)

    for col_name, scaled_col in [
        ('open', scaled_opens),
        ('high', scaled_highs),
        ('low', scaled_lows),
        ('close', scaled_closes),
    ]:
        max_val = scaled_col.max()

        try:
            check_uint32_safe(max_val, col_name)
        except ValueError:
            if invalid_data_behavior == 'raise':
                raise

            if invalid_data_behavior == 'warn':
                logger.warn(
                    'Values for sid={}, col={} contain some too large for '
                    'uint32 (max={}), filtering them out',
                    sid, col_name, max_val,
                )

            # We want to exclude all rows that have an unsafe value in
            # this column.
            exclude_mask &= (scaled_col >= np.iinfo(np.uint32).max)

    # Convert all cols to uint32.
    opens = scaled_opens.astype(np.uint32)
    highs = scaled_highs.astype(np.uint32)
    lows = scaled_lows.astype(np.uint32)
    closes = scaled_closes.astype(np.uint32)
    volumes = cols['volume'].astype(np.uint32)

    # Exclude rows with unsafe values by setting to zero.
    opens[exclude_mask] = 0
    highs[exclude_mask] = 0
    lows[exclude_mask] = 0
    closes[exclude_mask] = 0
    volumes[exclude_mask] = 0

    return opens, highs, lows, closes, volumes"
read_namespaced_ingress;"def read_namespaced_ingress(self, name, namespace, **kwargs):
        """"""
        read the specified Ingress
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_namespaced_ingress(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the Ingress (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.
        :return: NetworkingV1beta1Ingress
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_namespaced_ingress_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.read_namespaced_ingress_with_http_info(name, namespace, **kwargs)
            return data"
time_machine;"def time_machine(host, mode):
    """"""Query archive.org.""""""
    now = datetime.datetime.now()
    to = str(now.year) + str(now.day) + str(now.month)
    if now.month > 6:
    	fro = str(now.year) + str(now.day) + str(now.month - 6)
    else:
    	fro = str(now.year - 1) + str(now.day) + str(now.month + 6)
    url = ""http://web.archive.org/cdx/search?url=%s&matchType=%s&collapse=urlkey&fl=original&filter=mimetype:text/html&filter=statuscode:200&output=json&from=%s&to=%s"" % (host, mode, fro, to)
    response = get(url).text
    parsed = json.loads(response)[1:]
    urls = []
    for item in parsed:
        urls.append(item[0])
    return urls"
sorted_outrows;"def sorted_outrows(outrows):
    # type: (Iterable[InstalledCSVRow]) -> List[InstalledCSVRow]
    """"""
    Return the given rows of a RECORD file in sorted order.

    Each row is a 3-tuple (path, hash, size) and corresponds to a record of
    a RECORD file (see PEP 376 and PEP 427 for details).  For the rows
    passed to this function, the size can be an integer as an int or string,
    or the empty string.
    """"""
    # Normally, there should only be one row per path, in which case the
    # second and third elements don't come into play when sorting.
    # However, in cases in the wild where a path might happen to occur twice,
    # we don't want the sort operation to trigger an error (but still want
    # determinism).  Since the third element can be an int or string, we
    # coerce each element to a string to avoid a TypeError in this case.
    # For additional background, see--
    # https://github.com/pypa/pip/issues/5868
    return sorted(outrows, key=lambda row: tuple(str(x) for x in row))"
port_policy_absent;"def port_policy_absent(name, sel_type=None, protocol=None, port=None):
    '''
    .. versionadded:: 2019.2.0

    Makes sure an SELinux port policy for a given port, protocol and SELinux context type is absent.

    name
        The protocol and port spec. Can be formatted as ``(tcp|udp)/(port|port-range)``.

    sel_type
        The SELinux Type. Optional; can be used in determining if policy is present,
        ignored by ``semanage port --delete``.

    protocol
        The protocol for the port, ``tcp`` or ``udp``. Required if name is not formatted.

    port
        The port or port range. Required if name is not formatted.
    '''
    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''}
    old_state = __salt__['selinux.port_get_policy'](
        name=name,
        sel_type=sel_type,
        protocol=protocol,
        port=port, )
    if not old_state:
        ret.update({'result': True,
                    'comment': 'SELinux policy for ""{0}"" already absent '.format(name) +
                               'with specified sel_type ""{0}"", protocol ""{1}"" and port ""{2}"".'.format(
                                   sel_type, protocol, port)})
        return ret
    if __opts__['test']:
        ret.update({'result': None})
    else:
        delete_ret = __salt__['selinux.port_delete_policy'](
            name=name,
            protocol=protocol,
            port=port, )
        if delete_ret['retcode'] != 0:
            ret.update({'comment': 'Error deleting policy: {0}'.format(delete_ret)})
        else:
            ret.update({'result': True})
            new_state = __salt__['selinux.port_get_policy'](
                name=name,
                sel_type=sel_type,
                protocol=protocol,
                port=port, )
            ret['changes'].update({'old': old_state, 'new': new_state})
    return ret"
extract_zip;"def extract_zip(zip_name, exclude_term=None):
    """"""Extracts a zip file to its containing directory.""""""

    zip_dir = os.path.dirname(os.path.abspath(zip_name))

    try:
        with zipfile.ZipFile(zip_name) as z:

            # write each zipped file out if it isn't a directory
            files = [zip_file for zip_file in z.namelist() if not zip_file.endswith('/')]

            print('Extracting %i files from %r.' % (len(files), zip_name))
            for zip_file in files:

                # remove any provided extra directory term from zip file
                if exclude_term:
                    dest_file = zip_file.replace(exclude_term, '')
                else:
                    dest_file = zip_file

                dest_file = os.path.normpath(os.path.join(zip_dir, dest_file))
                dest_dir = os.path.dirname(dest_file)

                # make directory if it does not exist
                if not os.path.isdir(dest_dir):
                    os.makedirs(dest_dir)

                # read file from zip, then write to new directory
                data = z.read(zip_file)
                with open(dest_file, 'wb') as f:
                    f.write(encode_utf8(data))

    except zipfile.error as e:
        print(""Bad zipfile (%r): %s"" % (zip_name, e))
        raise e"
get_output_shapes;"def get_output_shapes(self):
        """"""Get the shapes of the outputs.""""""
        outputs = self.execs[0].outputs
        shapes = [out.shape for out in outputs]

        concat_shapes = []
        for key, the_shape, axis in zip(self.symbol.list_outputs(), shapes, self.output_layouts):
            the_shape = list(the_shape)
            if axis >= 0:
                the_shape[axis] = self.batch_size
            concat_shapes.append((key, tuple(the_shape)))
        return concat_shapes"
get_symbol;"def get_symbol(num_classes, num_layers, image_shape, num_group=32, conv_workspace=256, dtype='float32', **kwargs):
    """"""
    Adapted from https://github.com/tornadomeet/ResNet/blob/master/train_resnet.py
    Original author Wei Wu
    """"""
    image_shape = [int(l) for l in image_shape.split(',')]
    (nchannel, height, width) = image_shape
    if height <= 32:
        num_stages = 3
        if (num_layers-2) % 9 == 0 and num_layers >= 164:
            per_unit = [(num_layers-2)//9]
            filter_list = [16, 64, 128, 256]
            bottle_neck = True
        elif (num_layers-2) % 6 == 0 and num_layers < 164:
            per_unit = [(num_layers-2)//6]
            filter_list = [16, 16, 32, 64]
            bottle_neck = False
        else:
            raise ValueError(""no experiments done on num_layers {}, you can do it yourself"".format(num_layers))
        units = per_unit * num_stages
    else:
        if num_layers >= 50:
            filter_list = [64, 256, 512, 1024, 2048]
            bottle_neck = True
        else:
            filter_list = [64, 64, 128, 256, 512]
            bottle_neck = False
        num_stages = 4
        if num_layers == 18:
            units = [2, 2, 2, 2]
        elif num_layers == 34:
            units = [3, 4, 6, 3]
        elif num_layers == 50:
            units = [3, 4, 6, 3]
        elif num_layers == 101:
            units = [3, 4, 23, 3]
        elif num_layers == 152:
            units = [3, 8, 36, 3]
        elif num_layers == 200:
            units = [3, 24, 36, 3]
        elif num_layers == 269:
            units = [3, 30, 48, 8]
        else:
            raise ValueError(""no experiments done on num_layers {}, you can do it yourself"".format(num_layers))

    return resnext(units      = units,
                  num_stages  = num_stages,
                  filter_list = filter_list,
                  num_classes = num_classes,
                  num_group   = num_group,
                  image_shape = image_shape,
                  bottle_neck = bottle_neck,
                  workspace   = conv_workspace,
                  dtype       = dtype)"
set_regressor_interface_params;"def set_regressor_interface_params(spec, features, output_features):
    """""" Common utilities to set the regressor interface params.
    """"""
    if output_features is None:
        output_features = [(""predicted_class"", datatypes.Double())]
    else:
        output_features = _fm.process_or_validate_features(output_features, 1)

    if len(output_features) != 1:
        raise ValueError(""Provided output features for a regressor must be ""
                    ""one Double feature."")

    if output_features[0][1] != datatypes.Double():
        raise ValueError(""Output type of a regressor must be a Double."")

    prediction_name = output_features[0][0]
    spec.description.predictedFeatureName = prediction_name

    # Normalize the features list.
    features = _fm.process_or_validate_features(features)

    # add input and output features
    for cur_input_name, feature_type in features:
        input_ = spec.description.input.add()
        input_.name = cur_input_name
        datatypes._set_datatype(input_.type, feature_type)

    output_ = spec.description.output.add()
    output_.name = prediction_name
    datatypes._set_datatype(output_.type, 'Double')
    return spec"
get_coding;"def get_coding(text, force_chardet=False):
    """"""
    Function to get the coding of a text.
    @param text text to inspect (string)
    @return coding string
    """"""
    if not force_chardet:
        for line in text.splitlines()[:2]:
            try:
                result = CODING_RE.search(to_text_string(line))
            except UnicodeDecodeError:
                # This could fail because to_text_string assume the text
                # is utf8-like and we don't know the encoding to give
                # it to to_text_string
                pass
            else:
                if result:
                    codec = result.group(1)
                    # sometimes we find a false encoding that can
                    # result in errors
                    if codec in CODECS:
                        return codec

    # Fallback using chardet
    if is_binary_string(text):
        detector = UniversalDetector()
        for line in text.splitlines()[:2]:
            detector.feed(line)
            if detector.done: break

        detector.close()
        return detector.result['encoding']

    return None"
import_module;"def import_module(module_name):
    """"""Helper function to import module""""""
    import sys, os
    import importlib
    sys.path.append(os.path.dirname(__file__))
    return importlib.import_module(module_name)"
get_cache_subnet_group;"def get_cache_subnet_group(name, region=None, key=None, keyid=None,
                           profile=None):
    '''
    Get information about a cache subnet group.

    CLI example::

        salt myminion boto_elasticache.get_cache_subnet_group mycache_subnet_group
    '''
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)

    try:
        csg = conn.describe_cache_subnet_groups(name)
        csg = csg['DescribeCacheSubnetGroupsResponse']
        csg = csg['DescribeCacheSubnetGroupsResult']['CacheSubnetGroups'][0]
    except boto.exception.BotoServerError as e:
        msg = 'Failed to get cache subnet group {0}.'.format(name)
        log.error(msg)
        log.debug(e)
        return False
    except (IndexError, TypeError, KeyError):
        msg = 'Failed to get cache subnet group {0} (2).'.format(name)
        log.error(msg)
        return False
    ret = {}
    for key, val in six.iteritems(csg):
        if key == 'CacheSubnetGroupName':
            ret['cache_subnet_group_name'] = val
        elif key == 'CacheSubnetGroupDescription':
            ret['cache_subnet_group_description'] = val
        elif key == 'VpcId':
            ret['vpc_id'] = val
        elif key == 'Subnets':
            ret['subnets'] = []
            for subnet in val:
                _subnet = {}
                _subnet['subnet_id'] = subnet['SubnetIdentifier']
                _az = subnet['SubnetAvailabilityZone']['Name']
                _subnet['subnet_availability_zone'] = _az
                ret['subnets'].append(_subnet)
        else:
            ret[key] = val
    return ret"
prev_close;"def prev_close(self):
        """"""
        [float] 昨日收盘价
        """"""
        try:
            return self._data['prev_close']
        except (ValueError, KeyError):
            pass

        if self._prev_close is None:
            trading_dt = Environment.get_instance().trading_dt
            data_proxy = Environment.get_instance().data_proxy
            self._prev_close = data_proxy.get_prev_close(self._instrument.order_book_id, trading_dt)
        return self._prev_close"
column_families;"def column_families(self):
        """"""List[:class:`~.external_config.BigtableColumnFamily`]: List of
        column families to expose in the table schema along with their types.

        See
        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.tableDefinitions.(key).bigtableOptions.columnFamilies
        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externalDataConfiguration.bigtableOptions.columnFamilies
        """"""
        prop = self._properties.get(""columnFamilies"", [])
        return [BigtableColumnFamily.from_api_repr(cf) for cf in prop]"
root_item_selected;"def root_item_selected(self, item):
        """"""Root item has been selected: expanding it and collapsing others""""""
        if self.show_all_files:
            return
        for root_item in self.get_top_level_items():
            if root_item is item:
                self.expandItem(root_item)
            else:
                self.collapseItem(root_item)"
get_host_vsan_system;"def get_host_vsan_system(service_instance, host_ref, hostname=None):
    '''
    Returns a host's vsan system

    service_instance
        Service instance to the host or vCenter

    host_ref
        Refernce to ESXi host

    hostname
        Name of ESXi host. Default value is None.
    '''
    if not hostname:
        hostname = salt.utils.vmware.get_managed_object_name(host_ref)
    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(
        path='configManager.vsanSystem',
        type=vim.HostSystem,
        skip=False)
    objs = salt.utils.vmware.get_mors_with_properties(
        service_instance, vim.HostVsanSystem, property_list=['config.enabled'],
        container_ref=host_ref, traversal_spec=traversal_spec)
    if not objs:
        raise VMwareObjectRetrievalError('Host\'s \'{0}\' VSAN system was '
                                         'not retrieved'.format(hostname))
    log.trace('[%s] Retrieved VSAN system', hostname)
    return objs[0]['object']"
edit_distance_matrix;"def edit_distance_matrix(train_x, train_y=None):
    """"""Calculate the edit distance.
    Args:
        train_x: A list of neural architectures.
        train_y: A list of neural architectures.
    Returns:
        An edit-distance matrix.
    """"""
    if train_y is None:
        ret = np.zeros((train_x.shape[0], train_x.shape[0]))
        for x_index, x in enumerate(train_x):
            for y_index, y in enumerate(train_x):
                if x_index == y_index:
                    ret[x_index][y_index] = 0
                elif x_index < y_index:
                    ret[x_index][y_index] = edit_distance(x, y)
                else:
                    ret[x_index][y_index] = ret[y_index][x_index]
        return ret
    ret = np.zeros((train_x.shape[0], train_y.shape[0]))
    for x_index, x in enumerate(train_x):
        for y_index, y in enumerate(train_y):
            ret[x_index][y_index] = edit_distance(x, y)
    return ret"
dict_filter;"def dict_filter(d, exclude=[]):
    """"""
    Exclude specified keys from a nested dict
    """"""

    if isinstance(d, list):
        ret = []
        for e in d:
            ret.append(dict_filter(e, exclude))
        return ret
    elif isinstance(d, dict):
        ret = {}
        for k, v in d.items():
            if isinstance(k, builtin_str):
                k = str(k)

            assert isinstance(k, str)
            if k in exclude:
                continue
            ret[k] = dict_filter(v, exclude)
        return ret

    return d"
translate_bytes;"def translate_bytes(val):
    '''
    These values can be expressed as an integer number of bytes, or a string
    expression (i.e. 100mb, 1gb, etc.).
    '''
    try:
        val = int(val)
    except (TypeError, ValueError):
        if not isinstance(val, six.string_types):
            val = six.text_type(val)
    return val"
namespace_absent;"def namespace_absent(name, **kwargs):
    '''
    Ensures that the named namespace is absent.

    name
        The name of the namespace
    '''

    ret = {'name': name,
           'changes': {},
           'result': False,
           'comment': ''}

    namespace = __salt__['kubernetes.show_namespace'](name, **kwargs)

    if namespace is None:
        ret['result'] = True if not __opts__['test'] else None
        ret['comment'] = 'The namespace does not exist'
        return ret

    if __opts__['test']:
        ret['comment'] = 'The namespace is going to be deleted'
        ret['result'] = None
        return ret

    res = __salt__['kubernetes.delete_namespace'](name, **kwargs)
    if (
            res['code'] == 200 or
            (
                isinstance(res['status'], six.string_types) and
                'Terminating' in res['status']
            ) or
            (
                isinstance(res['status'], dict) and
                res['status']['phase'] == 'Terminating'
            )):
        ret['result'] = True
        ret['changes'] = {
            'kubernetes.namespace': {
                'new': 'absent', 'old': 'present'}}
        if res['message']:
            ret['comment'] = res['message']
        else:
            ret['comment'] = 'Terminating'
    else:
        ret['comment'] = 'Something went wrong, response: {0}'.format(res)

    return ret"
retrieve_all;"def retrieve_all(self, sids, default_none=False):
        """"""
        Retrieve all assets in `sids`.

        Parameters
        ----------
        sids : iterable of int
            Assets to retrieve.
        default_none : bool
            If True, return None for failed lookups.
            If False, raise `SidsNotFound`.

        Returns
        -------
        assets : list[Asset or None]
            A list of the same length as `sids` containing Assets (or Nones)
            corresponding to the requested sids.

        Raises
        ------
        SidsNotFound
            When a requested sid is not found and default_none=False.
        """"""
        sids = list(sids)
        hits, missing, failures = {}, set(), []
        for sid in sids:
            try:
                asset = self._asset_cache[sid]
                if not default_none and asset is None:
                    # Bail early if we've already cached that we don't know
                    # about an asset.
                    raise SidsNotFound(sids=[sid])
                hits[sid] = asset
            except KeyError:
                missing.add(sid)

        # All requests were cache hits.  Return requested sids in order.
        if not missing:
            return [hits[sid] for sid in sids]

        update_hits = hits.update

        # Look up cache misses by type.
        type_to_assets = self.group_by_type(missing)

        # Handle failures
        failures = {failure: None for failure in type_to_assets.pop(None, ())}
        update_hits(failures)
        self._asset_cache.update(failures)

        if failures and not default_none:
            raise SidsNotFound(sids=list(failures))

        # We don't update the asset cache here because it should already be
        # updated by `self.retrieve_equities`.
        update_hits(self.retrieve_equities(type_to_assets.pop('equity', ())))
        update_hits(
            self.retrieve_futures_contracts(type_to_assets.pop('future', ()))
        )

        # We shouldn't know about any other asset types.
        if type_to_assets:
            raise AssertionError(
                ""Found asset types: %s"" % list(type_to_assets.keys())
            )

        return [hits[sid] for sid in sids]"
export_csv;"def export_csv(self, filename, delimiter=',', line_terminator='\n',
            header=True, quote_level=csv.QUOTE_NONNUMERIC, double_quote=True,
            escape_char='\\', quote_char='\""', na_rep='',
            file_header='', file_footer='', line_prefix='',
            _no_prefix_on_first_value=False, **kwargs):
        """"""
        Writes an SFrame to a CSV file.

        Parameters
        ----------
        filename : string
            The location to save the CSV.

        delimiter : string, optional
            This describes the delimiter used for writing csv files.

        line_terminator: string, optional
            The newline character

        header : bool, optional
            If true, the column names are emitted as a header.

        quote_level: csv.QUOTE_ALL | csv.QUOTE_NONE | csv.QUOTE_NONNUMERIC, optional
            The quoting level. If csv.QUOTE_ALL, every field is quoted.
            if csv.quote_NONE, no field is quoted. If csv.QUOTE_NONNUMERIC, only
            non-numeric fileds are quoted. csv.QUOTE_MINIMAL is interpreted as
            csv.QUOTE_NONNUMERIC.

        double_quote : bool, optional
            If True, quotes are escaped as two consecutive quotes

        escape_char : string, optional
            Character which begins a C escape sequence

        quote_char: string, optional
            Character used to quote fields

        na_rep: string, optional
            The value used to denote a missing value.

        file_header: string, optional
            A string printed to the start of the file

        file_footer: string, optional
            A string printed to the end of the file

        line_prefix: string, optional
            A string printed at the start of each value line
        """"""
        # Pandas argument compatibility
        if ""sep"" in kwargs:
            delimiter = kwargs['sep']
            del kwargs['sep']
        if ""quotechar"" in kwargs:
            quote_char = kwargs['quotechar']
            del kwargs['quotechar']
        if ""doublequote"" in kwargs:
            double_quote = kwargs['doublequote']
            del kwargs['doublequote']
        if ""lineterminator"" in kwargs:
            line_terminator = kwargs['lineterminator']
            del kwargs['lineterminator']
        if len(kwargs) > 0:
            raise TypeError(""Unexpected keyword arguments "" + str(list(kwargs.keys())))

        write_csv_options = {}
        write_csv_options['delimiter'] = delimiter
        write_csv_options['escape_char'] = escape_char
        write_csv_options['double_quote'] = double_quote
        write_csv_options['quote_char'] = quote_char
        if quote_level == csv.QUOTE_MINIMAL:
            write_csv_options['quote_level'] = 0
        elif quote_level == csv.QUOTE_ALL:
            write_csv_options['quote_level'] = 1
        elif quote_level == csv.QUOTE_NONNUMERIC:
            write_csv_options['quote_level'] = 2
        elif quote_level == csv.QUOTE_NONE:
            write_csv_options['quote_level'] = 3
        write_csv_options['header'] = header
        write_csv_options['line_terminator'] = line_terminator
        write_csv_options['na_value'] = na_rep
        write_csv_options['file_header'] = file_header
        write_csv_options['file_footer'] = file_footer
        write_csv_options['line_prefix'] = line_prefix

        # undocumented option. Disables line prefix on the first value line
        write_csv_options['_no_prefix_on_first_value'] = _no_prefix_on_first_value

        url = _make_internal_url(filename)
        self.__proxy__.save_as_csv(url, write_csv_options)"
interpreter_versions;"def interpreter_versions(self):
        """"""Python and IPython versions used by clients""""""
        if CONF.get('main_interpreter', 'default'):
            from IPython.core import release
            versions = dict(
                python_version = sys.version.split(""\n"")[0].strip(),
                ipython_version = release.version
            )
        else:
            import subprocess
            versions = {}
            pyexec = CONF.get('main_interpreter', 'executable')
            py_cmd = ""%s -c 'import sys; print(sys.version.split(\""\\n\"")[0])'"" % \
                     pyexec
            ipy_cmd = ""%s -c 'import IPython.core.release as r; print(r.version)'"" \
                      % pyexec
            for cmd in [py_cmd, ipy_cmd]:
                try:
                    proc = programs.run_shell_command(cmd)
                    output, _err = proc.communicate()
                except subprocess.CalledProcessError:
                    output = ''
                output = output.decode().split('\n')[0].strip()
                if 'IPython' in cmd:
                    versions['ipython_version'] = output
                else:
                    versions['python_version'] = output

        return versions"
configuration_import;"def configuration_import(config_file, rules=None, file_format='xml', **kwargs):
    '''
    .. versionadded:: 2017.7

    Imports Zabbix configuration specified in file to Zabbix server.

    :param config_file: File with Zabbix config (local or remote)
    :param rules: Optional - Rules that have to be different from default (defaults are the same as in Zabbix web UI.)
    :param file_format: Config file format (default: xml)
    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)
    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)
    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)

    CLI Example:

    .. code-block:: bash

        salt '*' zabbix.configuration_import salt://zabbix/config/zabbix_templates.xml \
        ""{'screens': {'createMissing': True, 'updateExisting': True}}""
    '''
    if rules is None:
        rules = {}
    default_rules = {'applications': {'createMissing': True, 'updateExisting': False, 'deleteMissing': False},
                     'discoveryRules': {'createMissing': True, 'updateExisting': True, 'deleteMissing': False},
                     'graphs': {'createMissing': True, 'updateExisting': True, 'deleteMissing': False},
                     'groups': {'createMissing': True},
                     'hosts': {'createMissing': False, 'updateExisting': False},
                     'images': {'createMissing': False, 'updateExisting': False},
                     'items': {'createMissing': True, 'updateExisting': True, 'deleteMissing': False},
                     'maps': {'createMissing': False, 'updateExisting': False},
                     'screens': {'createMissing': False, 'updateExisting': False},
                     'templateLinkage': {'createMissing': True},
                     'templates': {'createMissing': True, 'updateExisting': True},
                     'templateScreens': {'createMissing': True, 'updateExisting': True, 'deleteMissing': False},
                     'triggers': {'createMissing': True, 'updateExisting': True, 'deleteMissing': False},
                     'valueMaps': {'createMissing': True, 'updateExisting': False}}
    new_rules = dict(default_rules)

    if rules:
        for rule in rules:
            if rule in new_rules:
                new_rules[rule].update(rules[rule])
            else:
                new_rules[rule] = rules[rule]
    if 'salt://' in config_file:
        tmpfile = salt.utils.files.mkstemp()
        cfile = __salt__['cp.get_file'](config_file, tmpfile)
        if not cfile or os.path.getsize(cfile) == 0:
            return {'name': config_file, 'result': False, 'message': 'Failed to fetch config file.'}
    else:
        cfile = config_file
        if not os.path.isfile(cfile):
            return {'name': config_file, 'result': False, 'message': 'Invalid file path.'}

    with salt.utils.files.fopen(cfile, mode='r') as fp_:
        xml = fp_.read()

    if 'salt://' in config_file:
        salt.utils.files.safe_rm(cfile)

    params = {'format': file_format,
              'rules': new_rules,
              'source': xml}
    log.info('CONFIGURATION IMPORT: rules: %s', six.text_type(params['rules']))
    try:
        run_query('configuration.import', params, **kwargs)
        return {'name': config_file, 'result': True, 'message': 'Zabbix API ""configuration.import"" method '
                                                                'called successfully.'}
    except SaltException as exc:
        return {'name': config_file, 'result': False, 'message': six.text_type(exc)}"
plasma_get;"def plasma_get(object_id):
    """"""Get an object directly from plasma without going through object table.

    Precondition: plasma_prefetch(object_id) has been called before.
    """"""
    client = ray.worker.global_worker.plasma_client
    plasma_id = ray.pyarrow.plasma.ObjectID(object_id)
    while not client.contains(plasma_id):
        pass
    return client.get(plasma_id)"
get_output_info_dict;"def get_output_info_dict(self, signature=None):
    """"""Describes the outputs provided by a signature.

    Args:
      signature: A string with the signature to get ouputs information for.
        If None, the default signature is used if defined.

    Returns:
      The result of ModuleSpec.get_output_info_dict() for the given signature,
      and the graph variant selected by `tags` when this Module was initialized.

    Raises:
      KeyError: if there is no such signature.
    """"""
    return self._spec.get_output_info_dict(signature=signature, tags=self._tags)"
set_collapsed;"def set_collapsed(block, val):
        """"""
        Sets the fold trigger state (collapsed or expanded).

        :param block: The block to modify
        :param val: The new trigger state (True=collapsed, False=expanded)
        """"""
        if block is None:
            return
        state = block.userState()
        if state == -1:
            state = 0
        state &= 0x77FFFFFF
        state |= int(val) << 27
        block.setUserState(state)"
extract_numerics_alert;"def extract_numerics_alert(event):
  """"""Determines whether a health pill event contains bad values.

  A bad value is one of NaN, -Inf, or +Inf.

  Args:
    event: (`Event`) A `tensorflow.Event` proto from `DebugNumericSummary`
      ops.

  Returns:
    An instance of `NumericsAlert`, if bad values are found.
    `None`, if no bad values are found.

  Raises:
    ValueError: if the event does not have the expected tag prefix or the
      debug op name is not the expected debug op name suffix.
  """"""
  value = event.summary.value[0]
  debugger_plugin_metadata_content = None
  if value.HasField(""metadata""):
    plugin_data = value.metadata.plugin_data
    if plugin_data.plugin_name == constants.DEBUGGER_PLUGIN_NAME:
      debugger_plugin_metadata_content = plugin_data.content

  if not debugger_plugin_metadata_content:
    raise ValueError(""Event proto input lacks debugger plugin SummaryMetadata."")

  debugger_plugin_metadata_content = tf.compat.as_text(
      debugger_plugin_metadata_content)
  try:
    content_object = json.loads(debugger_plugin_metadata_content)
    device_name = content_object[""device""]
  except (KeyError, ValueError) as e:
    raise ValueError(""Could not determine device from JSON string %r, %r"" %
                     (debugger_plugin_metadata_content, e))

  debug_op_suffix = "":DebugNumericSummary""
  if not value.node_name.endswith(debug_op_suffix):
    raise ValueError(
        ""Event proto input does not have the expected debug op suffix %s"" %
        debug_op_suffix)
  tensor_name = value.node_name[:-len(debug_op_suffix)]

  elements = tf_debug.load_tensor_from_event(event)
  nan_count = elements[constants.NAN_NUMERIC_SUMMARY_OP_INDEX]
  neg_inf_count = elements[constants.NEG_INF_NUMERIC_SUMMARY_OP_INDEX]
  pos_inf_count = elements[constants.POS_INF_NUMERIC_SUMMARY_OP_INDEX]
  if nan_count > 0 or neg_inf_count > 0 or pos_inf_count > 0:
    return NumericsAlert(
        device_name, tensor_name, event.wall_time, nan_count, neg_inf_count,
        pos_inf_count)
  return None"
convert_multinomial;"def convert_multinomial(node, **kwargs):
    """"""Map MXNet's multinomial operator attributes to onnx's
    Multinomial operator and return the created node.
    """"""
    name, input_nodes, attrs = get_inputs(node, kwargs)
    dtype = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(attrs.get(""dtype"", 'int32'))]
    sample_size = convert_string_to_list(attrs.get(""shape"", '1'))
    if len(sample_size) < 2:
        sample_size = sample_size[-1]
    else:
        raise AttributeError(""ONNX currently supports integer sample_size only"")
    node = onnx.helper.make_node(
        ""Multinomial"",
        input_nodes,
        [name],
        dtype=dtype,
        sample_size=sample_size,
        name=name,
    )
    return [node]"
run_selection;"def run_selection(self):
        """"""
        Run selected text or current line in console.

        If some text is selected, then execute that text in console.

        If no text is selected, then execute current line, unless current line
        is empty. Then, advance cursor to next line. If cursor is on last line
        and that line is not empty, then add a new blank line and move the
        cursor there. If cursor is on last line and that line is empty, then do
        not move cursor.
        """"""
        text = self.get_current_editor().get_selection_as_executable_code()
        if text:
            self.exec_in_extconsole.emit(text.rstrip(), self.focus_to_editor)
            return
        editor = self.get_current_editor()
        line = editor.get_current_line()
        text = line.lstrip()
        if text:
            self.exec_in_extconsole.emit(text, self.focus_to_editor)
        if editor.is_cursor_on_last_line() and text:
            editor.append(editor.get_line_separator())
        editor.move_cursor_to_next('line', 'down')"
maybe_thenable;"def maybe_thenable(obj, on_resolve):
    """"""
    Execute a on_resolve function once the thenable is resolved,
    returning the same type of object inputed.
    If the object is not thenable, it should return on_resolve(obj)
    """"""
    if isawaitable(obj) and not isinstance(obj, Promise):
        return await_and_execute(obj, on_resolve)

    if is_thenable(obj):
        return Promise.resolve(obj).then(on_resolve)

    # If it's not awaitable not a Promise, return
    # the function executed over the object
    return on_resolve(obj)"
terminate_nodes;"def terminate_nodes(self, node_ids):
        """"""Terminates a set of nodes. May be overridden with a batch method.""""""
        for node_id in node_ids:
            logger.info(""NodeProvider: ""
                        ""{}: Terminating node"".format(node_id))
            self.terminate_node(node_id)"
create_new_client;"def create_new_client(self, give_focus=True, filename='', is_cython=False,
                          is_pylab=False, is_sympy=False, given_name=None):
        """"""Create a new client""""""
        self.master_clients += 1
        client_id = dict(int_id=to_text_string(self.master_clients),
                         str_id='A')
        cf = self._new_connection_file()
        show_elapsed_time = self.get_option('show_elapsed_time')
        reset_warning = self.get_option('show_reset_namespace_warning')
        ask_before_restart = self.get_option('ask_before_restart')
        client = ClientWidget(self, id_=client_id,
                              history_filename=get_conf_path('history.py'),
                              config_options=self.config_options(),
                              additional_options=self.additional_options(
                                      is_pylab=is_pylab,
                                      is_sympy=is_sympy),
                              interpreter_versions=self.interpreter_versions(),
                              connection_file=cf,
                              menu_actions=self.menu_actions,
                              options_button=self.options_button,
                              show_elapsed_time=show_elapsed_time,
                              reset_warning=reset_warning,
                              given_name=given_name,
                              ask_before_restart=ask_before_restart,
                              css_path=self.css_path)

        # Change stderr_dir if requested
        if self.test_dir is not None:
            client.stderr_dir = self.test_dir

        self.add_tab(client, name=client.get_name(), filename=filename)

        if cf is None:
            error_msg = self.permission_error_msg.format(jupyter_runtime_dir())
            client.show_kernel_error(error_msg)
            return

        # Check if ipykernel is present in the external interpreter.
        # Else we won't be able to create a client
        if not CONF.get('main_interpreter', 'default'):
            pyexec = CONF.get('main_interpreter', 'executable')
            has_spyder_kernels = programs.is_module_installed(
                                                        'spyder_kernels',
                                                        interpreter=pyexec,
                                                        version='>=1.0.0')
            if not has_spyder_kernels:
                client.show_kernel_error(
                        _(""Your Python environment or installation doesn't ""
                          ""have the <tt>spyder-kernels</tt> module or the ""
                          ""right version of it installed. ""
                          ""Without this module is not possible for ""
                          ""Spyder to create a console for you.<br><br>""
                          ""You can install it by running in a system terminal:""
                          ""<br><br>""
                          ""<tt>conda install spyder-kernels</tt>""
                          ""<br><br>or<br><br>""
                          ""<tt>pip install spyder-kernels</tt>""))
                return

        self.connect_client_to_kernel(client, is_cython=is_cython,
                                      is_pylab=is_pylab, is_sympy=is_sympy)
        if client.shellwidget.kernel_manager is None:
            return
        self.register_client(client)"
report_error_event;"def report_error_event(
        self,
        project_name,
        event,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Report an individual error event.

        Example:
            >>> from google.cloud import errorreporting_v1beta1
            >>>
            >>> client = errorreporting_v1beta1.ReportErrorsServiceClient()
            >>>
            >>> project_name = client.project_path('[PROJECT]')
            >>>
            >>> # TODO: Initialize `event`:
            >>> event = {}
            >>>
            >>> response = client.report_error_event(project_name, event)

        Args:
            project_name (str): [Required] The resource name of the Google Cloud Platform project.
                Written as ``projects/`` plus the `Google Cloud Platform project
                ID <https://support.google.com/cloud/answer/6158840>`__. Example:
                ``projects/my-project-123``.
            event (Union[dict, ~google.cloud.errorreporting_v1beta1.types.ReportedErrorEvent]): [Required] The error event to be reported.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.errorreporting_v1beta1.types.ReportedErrorEvent`
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.errorreporting_v1beta1.types.ReportErrorEventResponse` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""report_error_event"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""report_error_event""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.report_error_event,
                default_retry=self._method_configs[""ReportErrorEvent""].retry,
                default_timeout=self._method_configs[""ReportErrorEvent""].timeout,
                client_info=self._client_info,
            )

        request = report_errors_service_pb2.ReportErrorEventRequest(
            project_name=project_name, event=event
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""project_name"", project_name)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""report_error_event""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
show_session;"def show_session(session_id=None, url='default', session=None, browser=None, new=""tab"", controller=None):
        ''' Open a browser displaying a session document.

        If you have a session from ``pull_session()`` or ``push_session`` you
        can ``show_session(session=mysession)``. If you don't need to open a
        connection to the server yourself, you can show a new session in a
        browser by providing just the ``url``.

        Args:
            session_id (string, optional) :
               The name of the session, None to autogenerate a random one (default: None)

            url : (str, optional): The URL to a Bokeh application on a Bokeh server
                can also be `""default""` which will connect to the default app URL

            session (ClientSession, optional) : session to get session ID and server URL from
                If you specify this, you don't need to specify session_id and url

            browser (str, optional) : browser to show with (default: None)
                For systems that support it, the **browser** argument allows
                specifying which browser to display in, e.g. ""safari"", ""firefox"",
                ""opera"", ""windows-default"" (see the ``webbrowser`` module
                documentation in the standard lib for more details).

            new (str, optional) : new file output mode (default: ""tab"")
                For file-based output, opens or raises the browser window
                showing the current output file.  If **new** is 'tab', then
                opens a new tab. If **new** is 'window', then opens a new window.

        '''
        if session is not None:
            server_url = server_url_for_websocket_url(session._connection.url)
            session_id = session.id
        else:
            coords = _SessionCoordinates(session_id=session_id, url=url)
            server_url = coords.url
            session_id = coords.session_id

        if controller is None:
            from bokeh.util.browser import get_browser_controller
            controller = get_browser_controller(browser=browser)

        controller.open(server_url + ""?bokeh-session-id="" + quote_plus(session_id),
                        new=NEW_PARAM[new])"
expand_composites;"def expand_composites (properties):
    """""" Expand all composite properties in the set so that all components
        are explicitly expressed.
    """"""
    if __debug__:
        from .property import Property
        assert is_iterable_typed(properties, Property)
    explicit_features = set(p.feature for p in properties)

    result = []

    # now expand composite features
    for p in properties:
        expanded = expand_composite(p)

        for x in expanded:
            if not x in result:
                f = x.feature

                if f.free:
                    result.append (x)
                elif not x in properties:  # x is the result of expansion
                    if not f in explicit_features:  # not explicitly-specified
                        if any(r.feature == f for r in result):
                            raise FeatureConflict(
                                ""expansions of composite features result in ""
                                ""conflicting values for '%s'\nvalues: '%s'\none contributing composite property was '%s'"" %
                                (f.name, [r.value for r in result if r.feature == f] + [x.value], p))
                        else:
                            result.append (x)
                elif any(r.feature == f for r in result):
                    raise FeatureConflict (""explicitly-specified values of non-free feature '%s' conflict\n""
                    ""existing values: '%s'\nvalue from expanding '%s': '%s'"" % (f,
                    [r.value for r in result if r.feature == f], p, x.value))
                else:
                    result.append (x)

    return result"
cumulative_mean;"def cumulative_mean(self):
        """"""
        Return the cumulative mean of the elements in the SArray.

        Returns an SArray where each element in the output corresponds to the
        mean value of all the elements preceding and including it. The SArray
        is expected to be of numeric type (int, float), or a numeric vector
        type.

        Returns
        -------
        out : Sarray[float, array.array]

        Notes
        -----
         - Missing values are ignored while performing the cumulative
           aggregate operation.
         - For SArray's of type array.array, all entries are expected to
           be of the same size.

        Examples
        --------
        >>> sa = SArray([1, 2, 3, 4, 5])
        >>> sa.cumulative_mean()
        dtype: float
        rows: 3
        [1, 1.5, 2, 2.5, 3]
        """"""
        from .. import extensions
        agg_op = ""__builtin__cum_avg__""
        return SArray(_proxy = self.__proxy__.builtin_cumulative_aggregate(agg_op))"
map_providers;"def map_providers(self, query='list_nodes', cached=False):
        '''
        Return a mapping of what named VMs are running on what VM providers
        based on what providers are defined in the configuration and VMs
        '''
        if cached is True and query in self.__cached_provider_queries:
            return self.__cached_provider_queries[query]

        pmap = {}
        for alias, drivers in six.iteritems(self.opts['providers']):
            for driver, details in six.iteritems(drivers):
                fun = '{0}.{1}'.format(driver, query)
                if fun not in self.clouds:
                    log.error('Public cloud provider %s is not available', driver)
                    continue
                if alias not in pmap:
                    pmap[alias] = {}

                try:
                    with salt.utils.context.func_globals_inject(
                        self.clouds[fun],
                        __active_provider_name__=':'.join([alias, driver])
                    ):
                        pmap[alias][driver] = self.clouds[fun]()
                except Exception as err:
                    log.debug(
                        'Failed to execute \'%s()\' while querying for '
                        'running nodes: %s', fun, err,
                        exc_info_on_loglevel=logging.DEBUG
                    )
                    # Failed to communicate with the provider, don't list any
                    # nodes
                    pmap[alias][driver] = []
        self.__cached_provider_queries[query] = pmap
        return pmap"
buy_holding_pnl;"def buy_holding_pnl(self):
        """"""
        [float] 买方向当日持仓盈亏
        """"""
        return (self.last_price - self.buy_avg_holding_price) * self.buy_quantity * self.contract_multiplier"
table_admin_client;"def table_admin_client(self):
        """"""Getter for the gRPC stub used for the Table Admin API.

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_table_admin_client]
            :end-before: [END bigtable_table_admin_client]

        :rtype: :class:`.bigtable_admin_pb2.BigtableTableAdmin`
        :returns: A BigtableTableAdmin instance.
        :raises: :class:`ValueError <exceptions.ValueError>` if the current
                 client is not an admin client or if it has not been
                 :meth:`start`-ed.
        """"""
        if self._table_admin_client is None:
            if not self._admin:
                raise ValueError(""Client is not an admin client."")
            self._table_admin_client = _create_gapic_client(
                bigtable_admin_v2.BigtableTableAdminClient
            )(self)
        return self._table_admin_client"
load_task;"def load_task(module, task_name, params_str):
    """"""
    Imports task dynamically given a module and a task name.
    """"""
    if module is not None:
        __import__(module)
    task_cls = Register.get_task_cls(task_name)
    return task_cls.from_str_params(params_str)"
restore_scrollbar_position;"def restore_scrollbar_position(self):
        """"""Restoring scrollbar position after main window is visible""""""
        scrollbar_pos = self.get_option('scrollbar_position', None)
        if scrollbar_pos is not None:
            self.explorer.treewidget.set_scrollbar_position(scrollbar_pos)"
connect_post_namespaced_service_proxy_with_path;"def connect_post_namespaced_service_proxy_with_path(self, name, namespace, path, **kwargs):
        """"""
        connect POST requests to proxy of Service
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.connect_post_namespaced_service_proxy_with_path(name, namespace, path, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ServiceProxyOptions (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str path: path to the resource (required)
        :param str path2: Path is the part of URLs that include service endpoints, suffixes, and parameters to use for the current proxy request to service. For example, the whole request URL is http://localhost/api/v1/namespaces/kube-system/services/elasticsearch-logging/_search?q=user:kimchy. Path is _search?q=user:kimchy.
        :return: str
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.connect_post_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
        else:
            (data) = self.connect_post_namespaced_service_proxy_with_path_with_http_info(name, namespace, path, **kwargs)
            return data"
delete_website;"def delete_website(Bucket,
           region=None, key=None, keyid=None, profile=None):
    '''
    Remove the website configuration from the given bucket

    Returns {deleted: true} if website configuration was deleted and returns
    {deleted: False} if website configuration was not deleted.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_s3_bucket.delete_website my_bucket

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        conn.delete_bucket_website(Bucket=Bucket)
        return {'deleted': True, 'name': Bucket}
    except ClientError as e:
        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"
validate_column_specs;"def validate_column_specs(events, columns):
    """"""
    Verify that the columns of ``events`` can be used by a
    EarningsEstimatesLoader to serve the BoundColumns described by
    `columns`.
    """"""
    required = required_estimates_fields(columns)
    received = set(events.columns)
    missing = required - received
    if missing:
        raise ValueError(
            ""EarningsEstimatesLoader missing required columns {missing}.\n""
            ""Got Columns: {received}\n""
            ""Expected Columns: {required}"".format(
                missing=sorted(missing),
                received=sorted(received),
                required=sorted(required),
            )
        )"
content_disposition_header;"def content_disposition_header(disptype: str,
                               quote_fields: bool=True,
                               **params: str) -> str:
    """"""Sets ``Content-Disposition`` header.

    disptype is a disposition type: inline, attachment, form-data.
    Should be valid extension token (see RFC 2183)

    params is a dict with disposition params.
    """"""
    if not disptype or not (TOKEN > set(disptype)):
        raise ValueError('bad content disposition type {!r}'
                         ''.format(disptype))

    value = disptype
    if params:
        lparams = []
        for key, val in params.items():
            if not key or not (TOKEN > set(key)):
                raise ValueError('bad content disposition parameter'
                                 ' {!r}={!r}'.format(key, val))
            qval = quote(val, '') if quote_fields else val
            lparams.append((key, '""%s""' % qval))
            if key == 'filename':
                lparams.append(('filename*', ""utf-8''"" + qval))
        sparams = '; '.join('='.join(pair) for pair in lparams)
        value = '; '.join((value, sparams))
    return value"
set_sample_weight;"def set_sample_weight(pipeline_steps, sample_weight=None):
    """"""Recursively iterates through all objects in the pipeline and sets sample weight.

    Parameters
    ----------
    pipeline_steps: array-like
        List of (str, obj) tuples from a scikit-learn pipeline or related object
    sample_weight: array-like
        List of sample weight
    Returns
    -------
    sample_weight_dict:
        A dictionary of sample_weight

    """"""
    sample_weight_dict = {}
    if not isinstance(sample_weight, type(None)):
        for (pname, obj) in pipeline_steps:
            if inspect.getargspec(obj.fit).args.count('sample_weight'):
                step_sw = pname + '__sample_weight'
                sample_weight_dict[step_sw] = sample_weight

    if sample_weight_dict:
        return sample_weight_dict
    else:
        return None"
policy_backward;"def policy_backward(eph, epx, epdlogp, model):
    """"""backward pass. (eph is array of intermediate hidden states)""""""
    dW2 = np.dot(eph.T, epdlogp).ravel()
    dh = np.outer(epdlogp, model[""W2""])
    # Backprop relu.
    dh[eph <= 0] = 0
    dW1 = np.dot(dh.T, epx)
    return {""W1"": dW1, ""W2"": dW2}"
backport_makefile;"def backport_makefile(self, mode=""r"", buffering=None, encoding=None,
                      errors=None, newline=None):
    """"""
    Backport of ``socket.makefile`` from Python 3.5.
    """"""
    if not set(mode) <= {""r"", ""w"", ""b""}:
        raise ValueError(
            ""invalid mode %r (only r, w, b allowed)"" % (mode,)
        )
    writing = ""w"" in mode
    reading = ""r"" in mode or not writing
    assert reading or writing
    binary = ""b"" in mode
    rawmode = """"
    if reading:
        rawmode += ""r""
    if writing:
        rawmode += ""w""
    raw = SocketIO(self, rawmode)
    self._makefile_refs += 1
    if buffering is None:
        buffering = -1
    if buffering < 0:
        buffering = io.DEFAULT_BUFFER_SIZE
    if buffering == 0:
        if not binary:
            raise ValueError(""unbuffered streams must be binary"")
        return raw
    if reading and writing:
        buffer = io.BufferedRWPair(raw, raw, buffering)
    elif reading:
        buffer = io.BufferedReader(raw, buffering)
    else:
        assert writing
        buffer = io.BufferedWriter(raw, buffering)
    if binary:
        return buffer
    text = io.TextIOWrapper(buffer, encoding, errors, newline)
    text.mode = mode
    return text"
add_alternative;"def add_alternative (self, target_instance):
        """""" Add new target alternative.
        """"""
        assert isinstance(target_instance, AbstractTarget)
        if self.built_main_targets_:
            raise IllegalOperation (""add-alternative called when main targets are already created for project '%s'"" % self.full_name ())

        self.alternatives_.append (target_instance)"
get_profiles;"def get_profiles(config):
    '''
    Get available profiles.

    :return:
    '''
    profiles = []
    for profile_name in os.listdir(os.path.join(os.path.dirname(__file__), 'profiles')):
        if profile_name.endswith('.yml'):
            profiles.append(profile_name.split('.')[0])

    return sorted(profiles)"
cursor_save_attrs;"def cursor_save_attrs (self): # <ESC>7
        '''Save current cursor position.'''

        self.cur_saved_r = self.cur_r
        self.cur_saved_c = self.cur_c"
sample_prior;"def sample_prior(batch_size):
    cat, _ = get_distributions(DIST_PRIOR_PARAM[:NUM_CLASS], DIST_PRIOR_PARAM[NUM_CLASS:])
    sample_cat = tf.one_hot(cat.sample(batch_size), NUM_CLASS)

    """"""
    OpenAI official code actually models the ""uniform"" latent code as
    a Gaussian distribution, but obtain the samples from a uniform distribution.
    """"""
    sample_uni = tf.random_uniform([batch_size, NUM_UNIFORM], -1, 1)
    samples = tf.concat([sample_cat, sample_uni], axis=1)
    return samples"
current_changed;"def current_changed(self, index):
        """"""Stack index has changed""""""
#        count = self.get_stack_count()
#        for btn in (self.filelist_btn, self.previous_btn, self.next_btn):
#            btn.setEnabled(count > 1)

        editor = self.get_current_editor()
        if editor.lsp_ready and not editor.document_opened:
            editor.document_did_open()
        if index != -1:
            editor.setFocus()
            logger.debug(""Set focus to: %s"" % editor.filename)
        else:
            self.reset_statusbar.emit()
        self.opened_files_list_changed.emit()

        self.stack_history.refresh()
        self.stack_history.remove_and_append(index)

        # Needed to avoid an error generated after moving/renaming
        # files outside Spyder while in debug mode.
        # See issue 8749.
        try:
            logger.debug(""Current changed: %d - %s"" %
                         (index, self.data[index].editor.filename))
        except IndexError:
            pass

        self.update_plugin_title.emit()
        if editor is not None:
            # Needed in order to handle the close of files open in a directory
            # that has been renamed. See issue 5157
            try:
                self.current_file_changed.emit(self.data[index].filename,
                                               editor.get_position('cursor'))
            except IndexError:
                pass"
log_response;"def log_response(self, response):
        """"""
        Helper method provided to enable the logging of responses in case if
        the :attr:`HttpProtocol.access_log` is enabled.

        :param response: Response generated for the current request

        :type response: :class:`sanic.response.HTTPResponse` or
            :class:`sanic.response.StreamingHTTPResponse`

        :return: None
        """"""
        if self.access_log:
            extra = {""status"": getattr(response, ""status"", 0)}

            if isinstance(response, HTTPResponse):
                extra[""byte""] = len(response.body)
            else:
                extra[""byte""] = -1

            extra[""host""] = ""UNKNOWN""
            if self.request is not None:
                if self.request.ip:
                    extra[""host""] = ""{0}:{1}"".format(
                        self.request.ip, self.request.port
                    )

                extra[""request""] = ""{0} {1}"".format(
                    self.request.method, self.request.url
                )
            else:
                extra[""request""] = ""nil""

            access_logger.info("""", extra=extra)"
get_font;"def get_font(section='appearance', option='font', font_size_delta=0):
    """"""Get console font properties depending on OS and user options""""""
    font = FONT_CACHE.get((section, option))

    if font is None:
        families = CONF.get(section, option+""/family"", None)

        if families is None:
            return QFont()

        family = get_family(families)
        weight = QFont.Normal
        italic = CONF.get(section, option+'/italic', False)

        if CONF.get(section, option+'/bold', False):
            weight = QFont.Bold

        size = CONF.get(section, option+'/size', 9) + font_size_delta
        font = QFont(family, size, weight)
        font.setItalic(italic)
        FONT_CACHE[(section, option)] = font

    size = CONF.get(section, option+'/size', 9) + font_size_delta
    font.setPointSize(size)
    return font"
set_name_filters;"def set_name_filters(self, name_filters):
        """"""Set name filters""""""
        self.name_filters = name_filters
        self.fsmodel.setNameFilters(name_filters)"
remove_config;"def remove_config(reset=False):
    '''
    Remove the current DSC Configuration. Removes current, pending, and previous
    dsc configurations.

    .. versionadded:: 2017.7.5

    Args:
        reset (bool):
            Attempts to reset the DSC configuration by removing the following
            from ``C:\\Windows\\System32\\Configuration``:

            - File: DSCStatusHistory.mof
            - File: DSCEngineCache.mof
            - Dir: ConfigurationStatus

            Default is False

            .. warning::
                ``remove_config`` may fail to reset the DSC environment if any
                of the files in the ``ConfigurationStatus`` directory. If you
                wait a few minutes and run again, it may complete successfully.

    Returns:
        bool: True if successful

    Raises:
        CommandExecutionError: On failure

    CLI Example:

    .. code-block:: bash

        salt '*' dsc.remove_config True
    '''
    # Stopping a running config (not likely to occur)
    cmd = 'Stop-DscConfiguration'
    log.info('DSC: Stopping Running Configuration')
    try:
        _pshell(cmd)
    except CommandExecutionError as exc:
        if exc.info['retcode'] != 0:
            raise CommandExecutionError('Failed to Stop DSC Configuration',
                                        info=exc.info)
        log.info('DSC: %s', exc.info['stdout'])

    # Remove configuration files
    cmd = 'Remove-DscConfigurationDocument -Stage Current, Pending, Previous ' \
          '-Force'
    log.info('DSC: Removing Configuration')
    try:
        _pshell(cmd)
    except CommandExecutionError as exc:
        if exc.info['retcode'] != 0:
            raise CommandExecutionError('Failed to remove DSC Configuration',
                                        info=exc.info)
        log.info('DSC: %s', exc.info['stdout'])

    if not reset:
        return True

    def _remove_fs_obj(path):
        if os.path.exists(path):
            log.info('DSC: Removing %s', path)
            if not __salt__['file.remove'](path):
                error = 'Failed to remove {0}'.format(path)
                log.error('DSC: %s', error)
                raise CommandExecutionError(error)

    dsc_config_dir = '{0}\\System32\\Configuration' \
                     ''.format(os.getenv('SystemRoot', 'C:\\Windows'))

    # Remove History
    _remove_fs_obj('{0}\\DSCStatusHistory.mof'.format(dsc_config_dir))

    # Remove Engine Cache
    _remove_fs_obj('{0}\\DSCEngineCache.mof'.format(dsc_config_dir))

    # Remove Status Directory
    _remove_fs_obj('{0}\\ConfigurationStatus'.format(dsc_config_dir))

    return True"
generate_parameters;"def generate_parameters(self, parameter_id):
        """"""Returns a dict of trial (hyper-)parameters, as a serializable object.

        Parameters
        ----------
        parameter_id : int
        """"""
        self.count +=1
        if self.count>len(self.values)-1:
            raise nni.NoMoreTrialError('no more parameters now.')
        return self.values[self.count]"
resolve_command;"def resolve_command(self, ctx, args):
        """"""
        Overrides clicks ``resolve_command`` method
        and appends *Did you mean ...* suggestions
        to the raised exception message.
        """"""
        original_cmd_name = click.utils.make_str(args[0])

        try:
            return super(DYMMixin, self).resolve_command(ctx, args)
        except click.exceptions.UsageError as error:
            error_msg = str(error)
            matches = difflib.get_close_matches(original_cmd_name,
                                                self.list_commands(ctx), self.max_suggestions, self.cutoff)
            if matches:
                error_msg += '\n\nDid you mean one of these?\n    %s' % '\n    '.join(matches)  # pylint: disable=line-too-long

            raise click.exceptions.UsageError(error_msg, error.ctx)"
get_repository;"def get_repository(self, repository=None, params=None):
        """"""
        Return information about registered repositories.
        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_

        :arg repository: A comma-separated list of repository names
        :arg local: Return local information, do not retrieve the state from
            master node (default: false)
        :arg master_timeout: Explicit operation timeout for connection to master
            node
        """"""
        return self.transport.perform_request('GET', _make_path('_snapshot',
            repository), params=params)"
model_evaluation_path;"def model_evaluation_path(cls, project, location, model, model_evaluation):
        """"""Return a fully-qualified model_evaluation string.""""""
        return google.api_core.path_template.expand(
            ""projects/{project}/locations/{location}/models/{model}/modelEvaluations/{model_evaluation}"",
            project=project,
            location=location,
            model=model,
            model_evaluation=model_evaluation,
        )"
create_sink;"def create_sink(
        self,
        parent,
        sink,
        unique_writer_identity=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Creates a sink that exports specified log entries to a destination. The
        export of newly-ingested log entries begins immediately, unless the
        sink's ``writer_identity`` is not permitted to write to the destination.
        A sink can export log entries only from the resource owning the sink.

        Example:
            >>> from google.cloud import logging_v2
            >>>
            >>> client = logging_v2.ConfigServiceV2Client()
            >>>
            >>> parent = client.project_path('[PROJECT]')
            >>>
            >>> # TODO: Initialize `sink`:
            >>> sink = {}
            >>>
            >>> response = client.create_sink(parent, sink)

        Args:
            parent (str): Required. The resource in which to create the sink:

                ::

                     ""projects/[PROJECT_ID]""
                     ""organizations/[ORGANIZATION_ID]""
                     ""billingAccounts/[BILLING_ACCOUNT_ID]""
                     ""folders/[FOLDER_ID]""

                Examples: ``""projects/my-logging-project""``,
                ``""organizations/123456789""``.
            sink (Union[dict, ~google.cloud.logging_v2.types.LogSink]): Required. The new sink, whose ``name`` parameter is a sink identifier
                that is not already in use.

                If a dict is provided, it must be of the same form as the protobuf
                message :class:`~google.cloud.logging_v2.types.LogSink`
            unique_writer_identity (bool): Optional. Determines the kind of IAM identity returned as
                ``writer_identity`` in the new sink. If this value is omitted or set to
                false, and if the sink's parent is a project, then the value returned as
                ``writer_identity`` is the same group or service account used by Logging
                before the addition of writer identities to this API. The sink's
                destination must be in the same project as the sink itself.

                If this field is set to true, or if the sink is owned by a non-project
                resource such as an organization, then the value of ``writer_identity``
                will be a unique service account used only for exports from the new
                sink. For more information, see ``writer_identity`` in ``LogSink``.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.logging_v2.types.LogSink` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""create_sink"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""create_sink""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.create_sink,
                default_retry=self._method_configs[""CreateSink""].retry,
                default_timeout=self._method_configs[""CreateSink""].timeout,
                client_info=self._client_info,
            )

        request = logging_config_pb2.CreateSinkRequest(
            parent=parent, sink=sink, unique_writer_identity=unique_writer_identity
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""parent"", parent)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        return self._inner_api_calls[""create_sink""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )"
proxy_napalm_wrap;"def proxy_napalm_wrap(func):
    '''
    This decorator is used to make the execution module functions
    available outside a proxy minion, or when running inside a proxy
    minion. If we are running in a proxy, retrieve the connection details
    from the __proxy__ injected variable.  If we are not, then
    use the connection information from the opts.
    :param func:
    :return:
    '''
    @wraps(func)
    def func_wrapper(*args, **kwargs):
        wrapped_global_namespace = func.__globals__
        # get __opts__ and __proxy__ from func_globals
        proxy = wrapped_global_namespace.get('__proxy__')
        opts = copy.deepcopy(wrapped_global_namespace.get('__opts__'))
        # in any case, will inject the `napalm_device` global
        # the execution modules will make use of this variable from now on
        # previously they were accessing the device properties through the __proxy__ object
        always_alive = opts.get('proxy', {}).get('always_alive', True)
        # force_reconnect is a magic keyword arg that allows one to establish
        # a separate connection to the network device running under an always
        # alive Proxy Minion, using new credentials (overriding the ones
        # configured in the opts / pillar.
        force_reconnect = kwargs.get('force_reconnect', False)
        if force_reconnect:
            log.debug('Usage of reconnect force detected')
            log.debug('Opts before merging')
            log.debug(opts['proxy'])
            opts['proxy'].update(**kwargs)
            log.debug('Opts after merging')
            log.debug(opts['proxy'])
        if is_proxy(opts) and always_alive:
            # if it is running in a NAPALM Proxy and it's using the default
            # always alive behaviour, will get the cached copy of the network
            # device object which should preserve the connection.
            if force_reconnect:
                wrapped_global_namespace['napalm_device'] = get_device(opts)
            else:
                wrapped_global_namespace['napalm_device'] = proxy['napalm.get_device']()
        elif is_proxy(opts) and not always_alive:
            # if still proxy, but the user does not want the SSH session always alive
            # get a new device instance
            # which establishes a new connection
            # which is closed just before the call() function defined above returns
            if 'inherit_napalm_device' not in kwargs or ('inherit_napalm_device' in kwargs and
                                                         not kwargs['inherit_napalm_device']):
                # try to open a new connection
                # but only if the function does not inherit the napalm driver
                # for configuration management this is very important,
                # in order to make sure we are editing the same session.
                try:
                    wrapped_global_namespace['napalm_device'] = get_device(opts)
                except napalm_base.exceptions.ConnectionException as nce:
                    log.error(nce)
                    return '{base_msg}. See log for details.'.format(
                        base_msg=six.text_type(nce.msg)
                    )
            else:
                # in case the `inherit_napalm_device` is set
                # and it also has a non-empty value,
                # the global var `napalm_device` will be overridden.
                # this is extremely important for configuration-related features
                # as all actions must be issued within the same configuration session
                # otherwise we risk to open multiple sessions
                wrapped_global_namespace['napalm_device'] = kwargs['inherit_napalm_device']
        else:
            # if not a NAPLAM proxy
            # thus it is running on a regular minion, directly on the network device
            # or another flavour of Minion from where we can invoke arbitrary
            # NAPALM commands
            # get __salt__ from func_globals
            log.debug('Not running in a NAPALM Proxy Minion')
            _salt_obj = wrapped_global_namespace.get('__salt__')
            napalm_opts = _salt_obj['config.get']('napalm', {})
            napalm_inventory = _salt_obj['config.get']('napalm_inventory', {})
            log.debug('NAPALM opts found in the Minion config')
            log.debug(napalm_opts)
            clean_kwargs = salt.utils.args.clean_kwargs(**kwargs)
            napalm_opts.update(clean_kwargs)  # no need for deeper merge
            log.debug('Merging the found opts with the CLI args')
            log.debug(napalm_opts)
            host = napalm_opts.get('host') or napalm_opts.get('hostname') or\
                   napalm_opts.get('fqdn') or napalm_opts.get('ip')
            if host and napalm_inventory and isinstance(napalm_inventory, dict) and\
               host in napalm_inventory:
                inventory_opts = napalm_inventory[host]
                log.debug('Found %s in the NAPALM inventory:', host)
                log.debug(inventory_opts)
                napalm_opts.update(inventory_opts)
                log.debug('Merging the config for %s with the details found in the napalm inventory:', host)
                log.debug(napalm_opts)
            opts = copy.deepcopy(opts)  # make sure we don't override the original
            # opts, but just inject the CLI args from the kwargs to into the
            # object manipulated by ``get_device_opts`` to extract the
            # connection details, then use then to establish the connection.
            opts['napalm'] = napalm_opts
            if 'inherit_napalm_device' not in kwargs or ('inherit_napalm_device' in kwargs and
                                                         not kwargs['inherit_napalm_device']):
                # try to open a new connection
                # but only if the function does not inherit the napalm driver
                # for configuration management this is very important,
                # in order to make sure we are editing the same session.
                try:
                    wrapped_global_namespace['napalm_device'] = get_device(opts, salt_obj=_salt_obj)
                except napalm_base.exceptions.ConnectionException as nce:
                    log.error(nce)
                    return '{base_msg}. See log for details.'.format(
                        base_msg=six.text_type(nce.msg)
                    )
            else:
                # in case the `inherit_napalm_device` is set
                # and it also has a non-empty value,
                # the global var `napalm_device` will be overridden.
                # this is extremely important for configuration-related features
                # as all actions must be issued within the same configuration session
                # otherwise we risk to open multiple sessions
                wrapped_global_namespace['napalm_device'] = kwargs['inherit_napalm_device']
        if not_always_alive(opts):
            # inject the __opts__ only when not always alive
            # otherwise, we don't want to overload the always-alive proxies
            wrapped_global_namespace['napalm_device']['__opts__'] = opts
        ret = func(*args, **kwargs)
        if force_reconnect:
            log.debug('That was a forced reconnect, gracefully clearing up')
            device = wrapped_global_namespace['napalm_device']
            closing = call(device, 'close', __retry=False)
        return ret
    return func_wrapper"
delete_namespaced_config_map;"def delete_namespaced_config_map(self, name, namespace, **kwargs):
        """"""
        delete a ConfigMap
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_namespaced_config_map(name, namespace, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ConfigMap (required)
        :param str namespace: object name and auth scope, such as for teams and projects (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param V1DeleteOptions body:
        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed
        :param int grace_period_seconds: The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.
        :param bool orphan_dependents: Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \""orphan\"" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.
        :param str propagation_policy: Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_namespaced_config_map_with_http_info(name, namespace, **kwargs)
        else:
            (data) = self.delete_namespaced_config_map_with_http_info(name, namespace, **kwargs)
            return data"
run_and_track_hadoop_job;"def run_and_track_hadoop_job(arglist, tracking_url_callback=None, env=None):
    """"""
    Runs the job by invoking the command from the given arglist.
    Finds tracking urls from the output and attempts to fetch errors using those urls if the job fails.
    Throws HadoopJobError with information about the error
    (including stdout and stderr from the process)
    on failure and returns normally otherwise.

    :param arglist:
    :param tracking_url_callback:
    :param env:
    :return:
    """"""
    logger.info('%s', subprocess.list2cmdline(arglist))

    def write_luigi_history(arglist, history):
        """"""
        Writes history to a file in the job's output directory in JSON format.
        Currently just for tracking the job ID in a configuration where
        no history is stored in the output directory by Hadoop.
        """"""
        history_filename = configuration.get_config().get('core', 'history-filename', '')
        if history_filename and '-output' in arglist:
            output_dir = arglist[arglist.index('-output') + 1]
            f = luigi.contrib.hdfs.HdfsTarget(os.path.join(output_dir, history_filename)).open('w')
            f.write(json.dumps(history))
            f.close()

    def track_process(arglist, tracking_url_callback, env=None):
        # Dump stdout to a temp file, poll stderr and log it
        temp_stdout = tempfile.TemporaryFile('w+t')
        proc = subprocess.Popen(arglist, stdout=temp_stdout, stderr=subprocess.PIPE, env=env, close_fds=True, universal_newlines=True)

        # We parse the output to try to find the tracking URL.
        # This URL is useful for fetching the logs of the job.
        tracking_url = None
        job_id = None
        application_id = None
        err_lines = []

        with HadoopRunContext() as hadoop_context:
            while proc.poll() is None:
                err_line = proc.stderr.readline()
                err_lines.append(err_line)
                err_line = err_line.strip()
                if err_line:
                    logger.info('%s', err_line)
                err_line = err_line.lower()
                tracking_url_match = TRACKING_RE.search(err_line)
                if tracking_url_match:
                    tracking_url = tracking_url_match.group('url')
                    try:
                        tracking_url_callback(tracking_url)
                    except Exception as e:
                        logger.error(""Error in tracking_url_callback, disabling! %s"", e)

                        def tracking_url_callback(x):
                            return None
                if err_line.find('running job') != -1:
                    # hadoop jar output
                    job_id = err_line.split('running job: ')[-1]
                if err_line.find('submitted hadoop job:') != -1:
                    # scalding output
                    job_id = err_line.split('submitted hadoop job: ')[-1]
                if err_line.find('submitted application ') != -1:
                    application_id = err_line.split('submitted application ')[-1]
                hadoop_context.job_id = job_id
                hadoop_context.application_id = application_id

        # Read the rest + stdout
        err = ''.join(err_lines + [an_err_line for an_err_line in proc.stderr])
        temp_stdout.seek(0)
        out = ''.join(temp_stdout.readlines())

        if proc.returncode == 0:
            write_luigi_history(arglist, {'job_id': job_id})
            return (out, err)

        # Try to fetch error logs if possible
        message = 'Streaming job failed with exit code %d. ' % proc.returncode
        if not tracking_url:
            raise HadoopJobError(message + 'Also, no tracking url found.', out, err)

        try:
            task_failures = fetch_task_failures(tracking_url)
        except Exception as e:
            raise HadoopJobError(message + 'Additionally, an error occurred when fetching data from %s: %s' %
                                 (tracking_url, e), out, err)

        if not task_failures:
            raise HadoopJobError(message + 'Also, could not fetch output from tasks.', out, err)
        else:
            raise HadoopJobError(message + 'Output from tasks below:\n%s' % task_failures, out, err)

    if tracking_url_callback is None:
        def tracking_url_callback(x): return None

    return track_process(arglist, tracking_url_callback, env)"
get_items;"def get_items(self):
        """"""Return items (excluding top level items)""""""
        itemlist = []
        def add_to_itemlist(item):
            for index in range(item.childCount()):
                citem = item.child(index)
                itemlist.append(citem)
                add_to_itemlist(citem)
        for tlitem in self.get_top_level_items():
            add_to_itemlist(tlitem)
        return itemlist"
handle_publish;"def handle_publish(self, package, _):
        '''
        Get something from epull, publish it out epub, and return the package (or None)
        '''
        try:
            self.publisher.publish(package)
            return package
        # Add an extra fallback in case a forked process leeks through
        except Exception:
            log.critical('Unexpected error while polling master events',
                         exc_info=True)
            return None"
name_conversion;"def name_conversion(caffe_layer_name):
    """""" Convert a caffe parameter name to a tensorflow parameter name as
        defined in the above model """"""
    # beginning & end mapping
    NAME_MAP = {'bn_conv1/beta': 'conv0/bn/beta',
                'bn_conv1/gamma': 'conv0/bn/gamma',
                'bn_conv1/mean/EMA': 'conv0/bn/mean/EMA',
                'bn_conv1/variance/EMA': 'conv0/bn/variance/EMA',
                'conv1/W': 'conv0/W', 'conv1/b': 'conv0/b',
                'fc1000/W': 'linear/W', 'fc1000/b': 'linear/b'}
    if caffe_layer_name in NAME_MAP:
        return NAME_MAP[caffe_layer_name]

    s = re.search('([a-z]+)([0-9]+)([a-z]+)_', caffe_layer_name)
    if s is None:
        s = re.search('([a-z]+)([0-9]+)([a-z]+)([0-9]+)_', caffe_layer_name)
        layer_block_part1 = s.group(3)
        layer_block_part2 = s.group(4)
        assert layer_block_part1 in ['a', 'b']
        layer_block = 0 if layer_block_part1 == 'a' else int(layer_block_part2)
    else:
        layer_block = ord(s.group(3)) - ord('a')
    layer_type = s.group(1)
    layer_group = s.group(2)

    layer_branch = int(re.search('_branch([0-9])', caffe_layer_name).group(1))
    assert layer_branch in [1, 2]
    if layer_branch == 2:
        layer_id = re.search('_branch[0-9]([a-z])/', caffe_layer_name).group(1)
        layer_id = ord(layer_id) - ord('a') + 1

    TYPE_DICT = {'res': 'conv{}', 'bn': 'conv{}/bn'}
    layer_type = TYPE_DICT[layer_type].format(layer_id if layer_branch == 2 else 'shortcut')

    tf_name = caffe_layer_name[caffe_layer_name.index('/'):]
    tf_name = 'group{}/block{}/{}'.format(
        int(layer_group) - 2, layer_block, layer_type) + tf_name
    return tf_name"
postprocess_keyevent;"def postprocess_keyevent(self, event):
        """"""Process keypress event""""""
        ShellBaseWidget.postprocess_keyevent(self, event)
        if QToolTip.isVisible():
            _event, _text, key, _ctrl, _shift = restore_keyevent(event)
            self.hide_tooltip_if_necessary(key)"
get_spontaneous_environment;"def get_spontaneous_environment(*args):
    """"""Return a new spontaneous environment.  A spontaneous environment is an
    unnamed and unaccessible (in theory) environment that is used for
    templates generated from a string and not from the file system.
    """"""
    try:
        env = _spontaneous_environments.get(args)
    except TypeError:
        return Environment(*args)
    if env is not None:
        return env
    _spontaneous_environments[args] = env = Environment(*args)
    env.shared = True
    return env"
save_all;"def save_all(self):
        """"""Save all opened files.

        Iterate through self.data and call save() on any modified files.
        """"""
        for index in range(self.get_stack_count()):
            if self.data[index].editor.document().isModified():
                self.save(index)"
user_create;"def user_create(name, passwd, user=None, password=None, host=None, port=None,
                database='admin', authdb=None, roles=None):
    '''
    Create a MongoDB user

    CLI Example:

    .. code-block:: bash

        salt '*' mongodb.user_create <user_name> <user_password> <roles> <user> <password> <host> <port> <database>
    '''
    conn = _connect(user, password, host, port, authdb=authdb)
    if not conn:
        return 'Failed to connect to mongo database'

    if not roles:
        roles = []

    try:
        log.info('Creating user %s', name)
        mdb = pymongo.database.Database(conn, database)
        mdb.add_user(name, passwd, roles=roles)
    except pymongo.errors.PyMongoError as err:
        log.error('Creating database %s failed with error: %s', name, err)
        return six.text_type(err)
    return True"
force_close;"def force_close(self) -> None:
        """"""Force close connection""""""
        self._force_close = True
        if self._waiter:
            self._waiter.cancel()
        if self.transport is not None:
            self.transport.close()
            self.transport = None"
get_pending_update;"def get_pending_update():
    '''
    Determine whether there are pending updates that require a reboot.

    .. versionadded:: 2016.11.0

    Returns:
        bool: ``True`` if there are pending updates, otherwise ``False``

    CLI Example:

    .. code-block:: bash

        salt '*' system.get_pending_update
    '''
    key = r'SOFTWARE\Microsoft\Windows\CurrentVersion\WindowsUpdate\Auto Update\RebootRequired'

    # So long as the registry key exists, a reboot is pending.
    if __utils__['reg.key_exists']('HKLM', key):
        log.debug('Key exists: %s', key)
        return True
    else:
        log.debug('Key does not exist: %s', key)

    return False"
get_global_name_value;"def get_global_name_value(self):
        """"""Returns zipped name and value pairs for global results.

        Returns
        -------
        list of tuples
            A (name, value) tuple list.
        """"""
        if self._has_global_stats:
            name, value = self.get_global()
            if not isinstance(name, list):
                name = [name]
            if not isinstance(value, list):
                value = [value]
            return list(zip(name, value))
        else:
            return self.get_name_value()"
start_after;"def start_after(self, document_fields):
        """"""Start query after a cursor with this collection as parent.

        See
        :meth:`~.firestore_v1beta1.query.Query.start_after` for
        more information on this method.

        Args:
            document_fields (Union[~.firestore_v1beta1.\
                document.DocumentSnapshot, dict, list, tuple]): a document
                snapshot or a dictionary/list/tuple of fields representing a
                query results cursor. A cursor is a collection of values that
                represent a position in a query result set.

        Returns:
            ~.firestore_v1beta1.query.Query: A query with cursor.
        """"""
        query = query_mod.Query(self)
        return query.start_after(document_fields)"
split_datetime;"def split_datetime(self, column_name_prefix = ""X"", limit=None, timezone=False):
        """"""
        Splits an SArray of datetime type to multiple columns, return a
        new SFrame that contains expanded columns. A SArray of datetime will be
        split by default into an SFrame of 6 columns, one for each
        year/month/day/hour/minute/second element.

        **Column Naming**

        When splitting a SArray of datetime type, new columns are named:
        prefix.year, prefix.month, etc. The prefix is set by the parameter
        ""column_name_prefix"" and defaults to 'X'. If column_name_prefix is
        None or empty, then no prefix is used.

        **Timezone Column**
        If timezone parameter is True, then timezone information is represented
        as one additional column which is a float shows the offset from
        GMT(0.0) or from UTC.


        Parameters
        ----------
        column_name_prefix: str, optional
            If provided, expanded column names would start with the given prefix.
            Defaults to ""X"".

        limit: list[str], optional
            Limits the set of datetime elements to expand.
            Possible values are 'year','month','day','hour','minute','second',
            'weekday', 'isoweekday', 'tmweekday', and 'us'.
            If not provided, only ['year','month','day','hour','minute','second']
            are expanded.

            - 'year': The year number
            - 'month': A value between 1 and 12 where 1 is January.
            - 'day': Day of the months. Begins at 1.
            - 'hour': Hours since midnight.
            - 'minute': Minutes after the hour.
            - 'second': Seconds after the minute.
            - 'us': Microseconds after the second. Between 0 and 999,999.
            - 'weekday': A value between 0 and 6 where 0 is Monday.
            - 'isoweekday': A value between 1 and 7 where 1 is Monday.
            - 'tmweekday': A value between 0 and 7 where 0 is Sunday

        timezone: bool, optional
            A boolean parameter that determines whether to show timezone column or not.
            Defaults to False.

        Returns
        -------
        out : SFrame
            A new SFrame that contains all expanded columns

        Examples
        --------
        To expand only day and year elements of a datetime SArray

         >>> sa = SArray(
            [datetime(2011, 1, 21, 7, 7, 21, tzinfo=GMT(0)),
             datetime(2010, 2, 5, 7, 8, 21, tzinfo=GMT(4.5)])

         >>> sa.split_datetime(column_name_prefix=None,limit=['day','year'])
            Columns:
                day   int
                year  int
            Rows: 2
            Data:
            +-------+--------+
            |  day  |  year  |
            +-------+--------+
            |   21  |  2011  |
            |   5   |  2010  |
            +-------+--------+
            [2 rows x 2 columns]


        To expand only year and timezone elements of a datetime SArray
        with timezone column represented as a string. Columns are named with prefix:
        'Y.column_name'.

        >>> sa.split_datetime(column_name_prefix=""Y"",limit=['year'],timezone=True)
            Columns:
                Y.year  int
                Y.timezone float
            Rows: 2
            Data:
            +----------+---------+
            |  Y.year  | Y.timezone |
            +----------+---------+
            |    2011  |  0.0    |
            |    2010  |  4.5    |
            +----------+---------+
            [2 rows x 2 columns]
        """"""
        from .sframe import SFrame as _SFrame

        if self.dtype != datetime.datetime:
            raise TypeError(""Only column of datetime type is supported."")

        if column_name_prefix is None:
            column_name_prefix = """"
        if six.PY2 and type(column_name_prefix) == unicode:
            column_name_prefix = column_name_prefix.encode('utf-8')
        if type(column_name_prefix) != str:
            raise TypeError(""'column_name_prefix' must be a string"")

        # convert limit to column_keys
        if limit is not None:
            if not _is_non_string_iterable(limit):
                raise TypeError(""'limit' must be a list"")

            name_types = set([type(i) for i in limit])
            if (len(name_types) != 1):
                raise TypeError(""'limit' contains values that are different types"")

            if (name_types.pop() != str):
                raise TypeError(""'limit' must contain string values."")

            if len(set(limit)) != len(limit):
                raise ValueError(""'limit' contains duplicate values"")

        column_types = []

        if(limit is None):
            limit = ['year','month','day','hour','minute','second']

        column_types = [int] * len(limit)

        if(timezone == True):
            limit += ['timezone']
            column_types += [float]


        with cython_context():
           return _SFrame(_proxy=self.__proxy__.expand(column_name_prefix, limit, column_types))"
generate_blob_names;"def generate_blob_names(self):
        """"""
        Generate blob names for each one of the edge.  At this time, Keras does not
        support ""fork"" operation (a layer with more than 1 blob output). So we just
        use names of the src layer to identify a blob.  We also assume all neural
        networks are singly-connected graphs - which should be the case.
        """"""
        # generate blob names that represent edges in blob_name_map
        # because of the InputLayers, input blobs are also generated.

        # Generate each layer's input / output blob names
        for layer in self.layer_list:
            keras_layer = self.keras_layer_map[layer]
            # no need to generate InputLayers' blobs
            if not isinstance(keras_layer, _keras.engine.topology.InputLayer):
                # layer's input blob names depend on predecessors
                preds = self.get_predecessors(layer)
                for pred in preds:
                    blob_name = pred + '_output'
                    _insert_to_dict(self.layers_inputs, layer, blob_name)
                # layer's output blob is just named after itself
                blob_name = layer + '_output'
                _insert_to_dict(self.layers_outputs, layer, blob_name)"
iam_configuration;"def iam_configuration(self):
        """"""Retrieve IAM configuration for this bucket.

        :rtype: :class:`IAMConfiguration`
        :returns: an instance for managing the bucket's IAM configuration.
        """"""
        info = self._properties.get(""iamConfiguration"", {})
        return IAMConfiguration.from_api_repr(info, self)"
parse_response_start_line;"def parse_response_start_line(line: str) -> ResponseStartLine:
    """"""Returns a (version, code, reason) tuple for an HTTP 1.x response line.

    The response is a `collections.namedtuple`.

    >>> parse_response_start_line(""HTTP/1.1 200 OK"")
    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')
    """"""
    line = native_str(line)
    match = re.match(""(HTTP/1.[0-9]) ([0-9]+) ([^\r]*)"", line)
    if not match:
        raise HTTPInputError(""Error parsing response start line"")
    return ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))"
load_states;"def load_states(self, fname):
        """"""Loads trainer states (e.g. optimizer, momentum) from a file.

        Parameters
        ----------
        fname : str
            Path to input states file.

        Note
        ----
        `optimizer.param_dict`, which contains Parameter information (such as
        `lr_mult` and `wd_mult`) will not be loaded from the file, but rather set
        based on current Trainer's parameters.
        """"""
        if not self._kv_initialized:
            self._init_kvstore()
        if self._params_to_init:
            self._init_params()

        if self._update_on_kvstore:
            self._kvstore.load_optimizer_states(fname)
            self._optimizer = self._kvstore._updater.optimizer
        else:
            with open(fname, 'rb') as f:
                states = f.read()
            for updater in self._updaters:
                updater.set_states(states)
                updater.optimizer = self._updaters[0].optimizer
            self._optimizer = self._updaters[0].optimizer
        param_dict = {i: param for i, param in enumerate(self._params)}
        self._optimizer.param_dict = param_dict"
get_requirement_info;"def get_requirement_info(dist):
    # type: (Distribution) -> RequirementInfo
    """"""
    Compute and return values (req, editable, comments) for use in
    FrozenRequirement.from_dist().
    """"""
    if not dist_is_editable(dist):
        return (None, False, [])

    location = os.path.normcase(os.path.abspath(dist.location))

    from pipenv.patched.notpip._internal.vcs import vcs, RemoteNotFoundError
    vc_type = vcs.get_backend_type(location)

    if not vc_type:
        req = dist.as_requirement()
        logger.debug(
            'No VCS found for editable requirement {!r} in: {!r}', req,
            location,
        )
        comments = [
            '# Editable install with no version control ({})'.format(req)
        ]
        return (location, True, comments)

    try:
        req = vc_type.get_src_requirement(location, dist.project_name)
    except RemoteNotFoundError:
        req = dist.as_requirement()
        comments = [
            '# Editable {} install with no remote ({})'.format(
                vc_type.__name__, req,
            )
        ]
        return (location, True, comments)

    except BadCommand:
        logger.warning(
            'cannot determine version of editable source in %s '
            '(%s command not found in path)',
            location,
            vc_type.name,
        )
        return (None, True, [])

    except InstallationError as exc:
        logger.warning(
            ""Error when trying to get requirement for VCS system %s, ""
            ""falling back to uneditable format"", exc
        )
    else:
        if req is not None:
            return (req, True, [])

    logger.warning(
        'Could not determine repository location of %s', location
    )
    comments = ['## !! Could not determine repository location']

    return (None, False, comments)"
key_pair_name;"def key_pair_name(i, region, project_id, ssh_user):
    """"""Returns the ith default gcp_key_pair_name.""""""
    key_name = ""{}_gcp_{}_{}_{}"".format(RAY, region, project_id, ssh_user, i)
    return key_name"
load_bookmarks_without_file;"def load_bookmarks_without_file(filename):
    """"""Load all bookmarks but those from a specific file.""""""
    bookmarks = _load_all_bookmarks()
    return {k: v for k, v in bookmarks.items() if v[0] != filename}"
build_dataset;"def build_dataset(instruction_dicts,
                  dataset_from_file_fn,
                  shuffle_files=False,
                  parallel_reads=64):
  """"""Constructs a `tf.data.Dataset` from TFRecord files.

  Args:
    instruction_dicts: `list` of {'filepath':, 'mask':, 'offset_mask':}
      containing the information about which files and which examples to use.
      The boolean mask will be repeated and zipped with the examples from
      filepath.
    dataset_from_file_fn: function returning a `tf.data.Dataset` given a
      filename.
    shuffle_files: `bool`, Whether to shuffle the input filenames.
    parallel_reads: `int`, how many files to read in parallel.

  Returns:
    `tf.data.Dataset`
  """"""

  # First case: All examples are taken (No value skipped)
  if _no_examples_skipped(instruction_dicts):
    # Only use the filenames as instruction
    instruction_ds = tf.data.Dataset.from_tensor_slices([
        d[""filepath""] for d in instruction_dicts
    ])
    build_ds_from_instruction = dataset_from_file_fn
  # Second case: Use the instructions to read the examples
  else:
    instruction_ds = _build_instruction_ds(instruction_dicts)
    build_ds_from_instruction = functools.partial(
        _build_ds_from_instruction,
        ds_from_file_fn=dataset_from_file_fn,
    )

  # If shuffle is True, we shuffle the instructions/shards
  if shuffle_files:
    instruction_ds = instruction_ds.shuffle(len(instruction_dicts))

  # Use interleave to parallel read files and decode records
  ds = instruction_ds.interleave(
      build_ds_from_instruction,
      cycle_length=parallel_reads,
      num_parallel_calls=tf.data.experimental.AUTOTUNE)
  return ds"
get_prediction_path;"def get_prediction_path(self, node_id, missing_id = []):
        """"""
        Return the prediction path from this node to the parent node.

        Parameters
        ----------
        node_id    : id of the node to get the prediction path.
        missing_id : Additional info that contains nodes with missing features.

        Returns
        -------
        list: The list of decisions (top to bottom) from the root to this node.

        Examples
        --------
        .. sourcecode:: python

            >>> tree.get_prediction_score(5)  # Any node
             [{'child_id': 2,
               'feature': 'Quantity_features_90',
               'index': 'sum_timegaplast_gap',
               'node_id': 0,
               'sign': '>',
               'value': 53.5},
              {'child_id': 5,
               'feature': 'Quantity_features_90',
               'index': 'sum_sum',
               'node_id': 2,
               'sign': '<=',
               'value': 146.5}]
        """"""
        _raise_error_if_not_of_type(node_id, [int,long], ""node_id"")
        _numeric_param_check_range(""node_id"", node_id, 0, self.num_nodes - 1)

        def _deduplicate_path(path):
            s_nodes = {} # super_nodes
            s_path = []  # paths of super nodes.

            for node in path:
                feature = node['feature']
                index = node['index']
                if (feature, index) not in s_nodes:
                    s_nodes[feature, index] = node
                    s_path.append(node)
                else:
                    s_node = s_nodes[feature, index]
                    s_sign = s_node['sign']
                    sign = node['sign']
                    value = node['value']

                    # Supernode has no range.
                    if s_sign == ""<"":
                        if sign == "">="":
                            s_node[""value""] = [value, s_node[""value""]]
                            s_node[""sign""] = ""in""
                        elif sign == ""<"":
                            s_node[""value""] = value
                    elif s_sign == "">="":
                        if sign == "">="":
                            s_node[""value""] = value
                        elif sign == ""<"":
                            s_node[""value""] = [s_node[""value""], value]
                            s_node[""sign""] = ""in""

                    # Supernode has a range.
                    elif s_sign == ""in"":
                        if sign == "">="":
                            s_node[""value""][0] = value
                        elif sign == ""<"":
                            s_node[""value""][1] = value

            # Return super node path.
            return s_path

        path = []
        node = self.nodes[node_id]
        while node.parent is not None:
            parent = node.parent
            is_missing = node.node_id in missing_id
            path.insert(0, parent.get_decision(node, is_missing))
            node = node.parent
        return _deduplicate_path(path)"
set_monitor_callback;"def set_monitor_callback(self, callback, monitor_all=False):
        """"""Install callback for monitor.

        Parameters
        ----------
        callback : function
            Takes a string and an NDArrayHandle.
        monitor_all : bool, default False
            If true, monitor both input and output, otherwise monitor output only.

        Examples
        --------
        >>> def mon_callback(*args, **kwargs):
        >>>     print(""Do your stuff here."")
        >>>
        >>> texe.set_monitor_callback(mon_callback)
        """"""
        cb_type = ctypes.CFUNCTYPE(None, ctypes.c_char_p, NDArrayHandle, ctypes.c_void_p)
        self._monitor_callback = cb_type(_monitor_callback_wrapper(callback))
        check_call(_LIB.MXExecutorSetMonitorCallbackEX(
            self.handle,
            self._monitor_callback,
            None,
            ctypes.c_int(monitor_all)))"
clear_all_breakpoints;"def clear_all_breakpoints(self):
        """"""Clear breakpoints in all files""""""
        self.switch_to_plugin()
        clear_all_breakpoints()
        self.breakpoints_saved.emit()
        editorstack = self.get_current_editorstack()
        if editorstack is not None:
            for data in editorstack.data:
                data.editor.debugger.clear_breakpoints()
        self.refresh_plugin()"
output_file;"def output_file(filename, title=""Bokeh Plot"", mode=""cdn"", root_dir=None):
    '''Configure the default output state to generate output saved
    to a file when :func:`show` is called.

    Does not change the current ``Document`` from ``curdoc()``. File and notebook
    output may be active at the same time, so e.g., this does not clear the
    effects of ``output_notebook()``.

    Args:
        filename (str) : a filename for saving the HTML document

        title (str, optional) : a title for the HTML document (default: ""Bokeh Plot"")

        mode (str, optional) : how to include BokehJS (default: ``'cdn'``)
            One of: ``'inline'``, ``'cdn'``, ``'relative(-dev)'`` or
            ``'absolute(-dev)'``. See :class:`bokeh.resources.Resources` for more details.

        root_dir (str, optional) : root directory to use for 'absolute' resources. (default: None)
            This value is ignored for other resource types, e.g. ``INLINE`` or
            ``CDN``.

    Returns:
        None

    .. note::
        Generally, this should be called at the beginning of an interactive
        session or the top of a script.

    .. warning::
        This output file will be overwritten on every save, e.g., each time
        show() or save() is invoked.

    '''
    curstate().output_file(
        filename,
        title=title,
        mode=mode,
        root_dir=root_dir
    )"
split_delimited_symbol;"def split_delimited_symbol(symbol):
    """"""
    Takes in a symbol that may be delimited and splits it in to a company
    symbol and share class symbol. Also returns the fuzzy symbol, which is the
    symbol without any fuzzy characters at all.

    Parameters
    ----------
    symbol : str
        The possibly-delimited symbol to be split

    Returns
    -------
    company_symbol : str
        The company part of the symbol.
    share_class_symbol : str
        The share class part of a symbol.
    """"""
    # return blank strings for any bad fuzzy symbols, like NaN or None
    if symbol in _delimited_symbol_default_triggers:
        return '', ''

    symbol = symbol.upper()

    split_list = re.split(
        pattern=_delimited_symbol_delimiters_regex,
        string=symbol,
        maxsplit=1,
    )

    # Break the list up in to its two components, the company symbol and the
    # share class symbol
    company_symbol = split_list[0]
    if len(split_list) > 1:
        share_class_symbol = split_list[1]
    else:
        share_class_symbol = ''

    return company_symbol, share_class_symbol"
init_optimizer;"def init_optimizer(self, kvstore='local', optimizer='sgd',
                       optimizer_params=(('learning_rate', 0.01),), force_init=False):
        """"""Installs and initializes optimizers.

        Parameters
        ----------
        kvstore : str or KVStore
            Default `'local'`.
        optimizer : str or Optimizer
            Default `'sgd'`
        optimizer_params : dict
            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
            just to avoid pylint warning of dangerous default values.
        force_init : bool
            Default ``False``, indicating whether we should force re-initializing the
            optimizer in the case an optimizer is already installed.
        """"""
        assert self.binded and self.params_initialized

        if self.optimizer_initialized and not force_init:
            self.logger.warning('optimizer already initialized, ignoring...')
            return

        if self._params_dirty:
            self._sync_params_from_devices()

        (kvstore, update_on_kvstore) = \
                _create_kvstore(kvstore, len(self._context), self._arg_params)

        batch_size = self._exec_group.batch_size
        if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:
            batch_size *= kvstore.num_workers
        rescale_grad = 1.0/batch_size

        idx2name = {}
        if update_on_kvstore:
            idx2name.update(enumerate(self._exec_group.param_names))
        else:
            for k in range(len(self._context)):
                idx2name.update({i*len(self._context)+k: n
                                 for i, n in enumerate(self._exec_group.param_names)})
        if isinstance(optimizer, str):
            optimizer_params = dict(optimizer_params)
            if 'rescale_grad' not in optimizer_params:
                optimizer_params['rescale_grad'] = rescale_grad
            optimizer = opt.create(optimizer,
                                   sym=self.symbol, param_idx2name=idx2name,
                                   **optimizer_params)
        else:
            assert isinstance(optimizer, opt.Optimizer)
            if optimizer.rescale_grad != rescale_grad:
                #pylint: disable=no-member
                warnings.warn(
                    ""Optimizer created manually outside Module but rescale_grad "" +
                    ""is not normalized to 1.0/batch_size/num_workers (%s vs. %s). ""%(
                        optimizer.rescale_grad, rescale_grad) +
                    ""Is this intended?"", stacklevel=2)
            if not optimizer.idx2name:
                optimizer.idx2name = idx2name.copy()

        self._optimizer = optimizer
        self._kvstore = kvstore
        self._update_on_kvstore = update_on_kvstore
        self._updater = None

        if kvstore:
            if self._compression_params:
                kvstore.set_gradient_compression(self._compression_params)
            if update_on_kvstore:
                kvstore.set_optimizer(self._optimizer)
            # copy initialized local parameters to kvstore
            _initialize_kvstore(kvstore=kvstore,
                                param_arrays=self._exec_group.param_arrays,
                                arg_params=self._arg_params,
                                param_names=self._param_names,
                                update_on_kvstore=update_on_kvstore)

        if not update_on_kvstore:
            self._updater = opt.get_updater(optimizer)

        self.optimizer_initialized = True

        if self._preload_opt_states is not None:
            self.load_optimizer_states(self._preload_opt_states)
            self._preload_opt_states = None"
predict_topk;"def predict_topk(self, dataset, output_type='probability', k=3, verbose=True, batch_size=64):
        """"""
        Return top-k predictions for the ``dataset``.
        Predictions are returned as an SFrame with three columns: `id`,
        `class`, and `probability` or `rank` depending on the ``output_type``
        parameter.

        Parameters
        ----------
        dataset : SFrame | SArray | dict
            The audio data to be classified.
            If dataset is an SFrame, it must have a column with the same name as
            the feature used for model training, but does not require a target
            column. Additional columns are ignored.

        output_type : {'probability', 'rank'}, optional
            Choose the return type of the prediction:
            - `probability`: Probability associated with each label in the prediction.
            - `rank`       : Rank associated with each label in the prediction.

        k : int, optional
            Number of classes to return for each input example.

        verbose : bool, optional
            If True, prints progress updates and model details.

        batch_size : int, optional
            If you are getting memory errors, try decreasing this value. If you
            have a powerful computer, increasing this value may improve performance.

        Returns
        -------
        out : SFrame
            An SFrame with model predictions.

        See Also
        --------
        predict, classify, evaluate

        Examples
        --------
        >>> pred = m.predict_topk(validation_data, k=3)
        >>> pred
        +------+-------+-------------------+
        |  id  | class |    probability    |
        +------+-------+-------------------+
        |  0   |   4   |   0.995623886585  |
        |  0   |   9   |  0.0038311756216  |
        |  0   |   7   | 0.000301006948575 |
        |  1   |   1   |   0.928708016872  |
        |  1   |   3   |  0.0440889261663  |
        |  1   |   2   |  0.0176190119237  |
        |  2   |   3   |   0.996967732906  |
        |  2   |   2   |  0.00151345680933 |
        |  2   |   7   | 0.000637513934635 |
        |  3   |   1   |   0.998070061207  |
        | ...  |  ...  |        ...        |
        +------+-------+-------------------+
        """"""
        prob_vector = self.predict(dataset, output_type='probability_vector',
                                   verbose=verbose, batch_size=64)
        id_to_label = self._id_to_class_label

        if output_type == 'probability':
            results = prob_vector.apply(lambda p: [
                {'class': id_to_label[i], 'probability': p[i]}
                for i in reversed(_np.argsort(p)[-k:])]
            )
        else:
            assert(output_type == 'rank')
            results = prob_vector.apply(lambda p: [
                {'class': id_to_label[i], 'rank': rank}
                for rank, i in enumerate(reversed(_np.argsort(p)[-k:]))]
            )

        results = _tc.SFrame({'X': results})
        results = results.add_row_number()
        results = results.stack('X', new_column_name='X')
        results = results.unpack('X', column_name_prefix='')
        return results"
load_defense_output;"def load_defense_output(filename):
  """"""Loads output of defense from given file.""""""
  result = {}
  with open(filename) as f:
    for row in csv.reader(f):
      try:
        image_filename = row[0]
        if image_filename.endswith('.png') or image_filename.endswith('.jpg'):
          image_filename = image_filename[:image_filename.rfind('.')]
        label = int(row[1])
      except (IndexError, ValueError):
        continue
      result[image_filename] = label
  return result"
update_cluster;"def update_cluster(
        self,
        name,
        serve_nodes,
        location=None,
        state=None,
        default_storage_type=None,
        retry=google.api_core.gapic_v1.method.DEFAULT,
        timeout=google.api_core.gapic_v1.method.DEFAULT,
        metadata=None,
    ):
        """"""
        Updates a cluster within an instance.

        Example:
            >>> from google.cloud import bigtable_admin_v2
            >>>
            >>> client = bigtable_admin_v2.BigtableInstanceAdminClient()
            >>>
            >>> name = client.cluster_path('[PROJECT]', '[INSTANCE]', '[CLUSTER]')
            >>>
            >>> # TODO: Initialize `serve_nodes`:
            >>> serve_nodes = 0
            >>>
            >>> response = client.update_cluster(name, serve_nodes)
            >>>
            >>> def callback(operation_future):
            ...     # Handle result.
            ...     result = operation_future.result()
            >>>
            >>> response.add_done_callback(callback)
            >>>
            >>> # Handle metadata.
            >>> metadata = response.metadata()

        Args:
            name (str): (``OutputOnly``) The unique name of the cluster. Values are of the form
                ``projects/<project>/instances/<instance>/clusters/[a-z][-a-z0-9]*``.
            serve_nodes (int): The number of nodes allocated to this cluster. More nodes enable higher
                throughput and more consistent performance.
            location (str): (``CreationOnly``) The location where this cluster's nodes and storage
                reside. For best performance, clients should be located as close as
                possible to this cluster. Currently only zones are supported, so values
                should be of the form ``projects/<project>/locations/<zone>``.
            state (~google.cloud.bigtable_admin_v2.types.State): (``OutputOnly``) The current state of the cluster.
            default_storage_type (~google.cloud.bigtable_admin_v2.types.StorageType): (``CreationOnly``) The type of storage used by this cluster to serve its
                parent instance's tables, unless explicitly overridden.
            retry (Optional[google.api_core.retry.Retry]):  A retry object used
                to retry requests. If ``None`` is specified, requests will not
                be retried.
            timeout (Optional[float]): The amount of time, in seconds, to wait
                for the request to complete. Note that if ``retry`` is
                specified, the timeout applies to each individual attempt.
            metadata (Optional[Sequence[Tuple[str, str]]]): Additional metadata
                that is provided to the method.

        Returns:
            A :class:`~google.cloud.bigtable_admin_v2.types._OperationFuture` instance.

        Raises:
            google.api_core.exceptions.GoogleAPICallError: If the request
                    failed for any reason.
            google.api_core.exceptions.RetryError: If the request failed due
                    to a retryable error and retry attempts failed.
            ValueError: If the parameters are invalid.
        """"""
        # Wrap the transport method to add retry and timeout logic.
        if ""update_cluster"" not in self._inner_api_calls:
            self._inner_api_calls[
                ""update_cluster""
            ] = google.api_core.gapic_v1.method.wrap_method(
                self.transport.update_cluster,
                default_retry=self._method_configs[""UpdateCluster""].retry,
                default_timeout=self._method_configs[""UpdateCluster""].timeout,
                client_info=self._client_info,
            )

        request = instance_pb2.Cluster(
            name=name,
            serve_nodes=serve_nodes,
            location=location,
            state=state,
            default_storage_type=default_storage_type,
        )
        if metadata is None:
            metadata = []
        metadata = list(metadata)
        try:
            routing_header = [(""name"", name)]
        except AttributeError:
            pass
        else:
            routing_metadata = google.api_core.gapic_v1.routing_header.to_grpc_metadata(
                routing_header
            )
            metadata.append(routing_metadata)

        operation = self._inner_api_calls[""update_cluster""](
            request, retry=retry, timeout=timeout, metadata=metadata
        )
        return google.api_core.operation.from_gapic(
            operation,
            self.transport._operations_client,
            instance_pb2.Cluster,
            metadata_type=bigtable_instance_admin_pb2.UpdateClusterMetadata,
        )"
pack_range;"def pack_range(key, packing, grad_vars, rng):
    """"""Form the concatenation of a specified range of gradient tensors.

  Args:
    key: Value under which to store meta-data in packing that will be used
      later to restore the grad_var list structure.
    packing: Dict holding data describing packed ranges of small tensors.
    grad_vars: List of (grad, var) pairs for one tower.
    rng: A pair of integers giving the first, last indices of a consecutive
      range of tensors to be packed.

  Returns:
    A tensor that is the concatenation of all the specified small tensors.
  """"""
    to_pack = grad_vars[rng[0]:rng[1] + 1]
    members = []
    variables = []
    restore_shapes = []
    with tf.name_scope(""pack""):
        for g, v in to_pack:
            variables.append(v)
            restore_shapes.append(g.shape)
            with tf.device(g.device):
                members.append(tf.reshape(g, [-1]))
        packing[key] = GradPackTuple(
            indices=range(rng[0], rng[1] + 1),
            vars=variables,
            shapes=restore_shapes)
        with tf.device(members[0].device):
            return tf.concat(members, 0)"
list_nodes_min;"def list_nodes_min(call=None):
    '''
    Return a list of the instances that are on the provider. Only a list of
    instances names, and their state, is returned.

    CLI Examples:

    .. code-block:: bash

        salt-cloud -f list_nodes_min my-qingcloud
    '''
    if call != 'function':
        raise SaltCloudSystemExit(
            'The list_nodes_min function must be called with -f or --function.'
        )

    nodes = list_nodes_full()

    result = {}
    for instance_id, full_node in nodes.items():
        result[instance_id] = {
            'name': full_node['instance_name'],
            'status': full_node['status'],
        }

    return result"
check_etag_header;"def check_etag_header(self) -> bool:
        """"""Checks the ``Etag`` header against requests's ``If-None-Match``.

        Returns ``True`` if the request's Etag matches and a 304 should be
        returned. For example::

            self.set_etag_header()
            if self.check_etag_header():
                self.set_status(304)
                return

        This method is called automatically when the request is finished,
        but may be called earlier for applications that override
        `compute_etag` and want to do an early check for ``If-None-Match``
        before completing the request.  The ``Etag`` header should be set
        (perhaps with `set_etag_header`) before calling this method.
        """"""
        computed_etag = utf8(self._headers.get(""Etag"", """"))
        # Find all weak and strong etag values from If-None-Match header
        # because RFC 7232 allows multiple etag values in a single header.
        etags = re.findall(
            br'\*|(?:W/)?""[^""]*""', utf8(self.request.headers.get(""If-None-Match"", """"))
        )
        if not computed_etag or not etags:
            return False

        match = False
        if etags[0] == b""*"":
            match = True
        else:
            # Use a weak comparison when comparing entity-tags.
            def val(x: bytes) -> bytes:
                return x[2:] if x.startswith(b""W/"") else x

            for etag in etags:
                if val(etag) == val(computed_etag):
                    match = True
                    break
        return match"
read_rows;"def read_rows(
        self,
        start_key=None,
        end_key=None,
        limit=None,
        filter_=None,
        end_inclusive=False,
        row_set=None,
        retry=DEFAULT_RETRY_READ_ROWS,
    ):
        """"""Read rows from this table.

        For example:

        .. literalinclude:: snippets_table.py
            :start-after: [START bigtable_read_rows]
            :end-before: [END bigtable_read_rows]

        :type start_key: bytes
        :param start_key: (Optional) The beginning of a range of row keys to
                          read from. The range will include ``start_key``. If
                          left empty, will be interpreted as the empty string.

        :type end_key: bytes
        :param end_key: (Optional) The end of a range of row keys to read from.
                        The range will not include ``end_key``. If left empty,
                        will be interpreted as an infinite string.

        :type limit: int
        :param limit: (Optional) The read will terminate after committing to N
                      rows' worth of results. The default (zero) is to return
                      all results.

        :type filter_: :class:`.RowFilter`
        :param filter_: (Optional) The filter to apply to the contents of the
                        specified row(s). If unset, reads every column in
                        each row.

        :type end_inclusive: bool
        :param end_inclusive: (Optional) Whether the ``end_key`` should be
                      considered inclusive. The default is False (exclusive).

        :type row_set: :class:`row_set.RowSet`
        :param row_set: (Optional) The row set containing multiple row keys and
                        row_ranges.

        :type retry: :class:`~google.api_core.retry.Retry`
        :param retry:
            (Optional) Retry delay and deadline arguments. To override, the
            default value :attr:`DEFAULT_RETRY_READ_ROWS` can be used and
            modified with the :meth:`~google.api_core.retry.Retry.with_delay`
            method or the :meth:`~google.api_core.retry.Retry.with_deadline`
            method.

        :rtype: :class:`.PartialRowsData`
        :returns: A :class:`.PartialRowsData` a generator for consuming
                  the streamed results.
        """"""
        request_pb = _create_row_request(
            self.name,
            start_key=start_key,
            end_key=end_key,
            filter_=filter_,
            limit=limit,
            end_inclusive=end_inclusive,
            app_profile_id=self._app_profile_id,
            row_set=row_set,
        )
        data_client = self._instance._client.table_data_client
        return PartialRowsData(data_client.transport.read_rows, request_pb, retry)"
module_install;"def module_install(name):
    '''
    Installs custom SELinux module from given file

    name
        Path to file with module to install

    .. versionadded:: 2016.11.6
    '''
    ret = {'name': name,
           'result': True,
           'comment': '',
           'changes': {}}
    if __salt__['selinux.install_semod'](name):
        ret['comment'] = 'Module {0} has been installed'.format(name)
        return ret
    ret['result'] = False
    ret['comment'] = 'Failed to install module {0}'.format(name)
    return ret"
parse_line;"def parse_line(self, line: str) -> None:
        """"""Updates the dictionary with a single header line.

        >>> h = HTTPHeaders()
        >>> h.parse_line(""Content-Type: text/html"")
        >>> h.get('content-type')
        'text/html'
        """"""
        if line[0].isspace():
            # continuation of a multi-line header
            if self._last_key is None:
                raise HTTPInputError(""first header line cannot start with whitespace"")
            new_part = "" "" + line.lstrip()
            self._as_list[self._last_key][-1] += new_part
            self._dict[self._last_key] += new_part
        else:
            try:
                name, value = line.split("":"", 1)
            except ValueError:
                raise HTTPInputError(""no colon in header line"")
            self.add(name, value.strip())"
get_word_embs;"def get_word_embs(self, word_dims):
        """"""Get randomly initialized embeddings when pre-trained embeddings are used, otherwise zero vectors

        Parameters
        ----------
        word_dims : int
            word vector size
        Returns
        -------
        numpy.ndarray
            T x C numpy NDArray
        """"""
        if self._pret_embeddings is not None:
            return np.random.randn(self.words_in_train, word_dims).astype(np.float32)
        return np.zeros((self.words_in_train, word_dims), dtype=np.float32)"
unlock_weixin_callback_example;"def unlock_weixin_callback_example(url, req, resp, img, identify_image_callback):
    """"""手动打码解锁

    Parameters
    ----------
    url : str or unicode
        验证码页面 之前的 url
    req : requests.sessions.Session
        requests.Session() 供调用解锁
    resp : requests.models.Response
        requests 访问页面返回的，已经跳转了
    img : bytes
        验证码图片二进制数据
    identify_image_callback : callable
        处理验证码函数，输入验证码二进制数据，输出文字，参见 identify_image_callback_example

    Returns
    -------
    dict
        {
            'ret': '',
            'errmsg': '',
            'cookie_count': '',
        }
    """"""
    # no use resp

    unlock_url = 'https://mp.weixin.qq.com/mp/verifycode'
    data = {
        'cert': time.time() * 1000,
        'input': identify_image_callback(img)
    }
    headers = {
        'Host': 'mp.weixin.qq.com',
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
        'Referer': url
    }
    r_unlock = req.post(unlock_url, data, headers=headers)
    if not r_unlock.ok:
        raise WechatSogouVcodeOcrException(
            'unlock[{}] failed: {}[{}]'.format(unlock_url, r_unlock.text, r_unlock.status_code))

    return r_unlock.json()"
set_config_value;"def set_config_value(self, name, value, quiet=False):
        """"""a client helper function to set a configuration value, meaning
           reading in the configuration file (if it exists), saving a new
           config value, and then writing back

           Parameters
           ==========
           name: the name of the value to set (key in dictionary)
           value: the value to set at the key
           quiet: disable verbose output if True (default is False)
        """"""

        config_data = self._read_config_file()

        if value is not None:

            # Update the config file with the value
            config_data[name] = value

            # Update the instance with the value
            self.config_values[name] = value

            # If defined by client, set and save!
            self._write_config_file(config_data)

            if not quiet:
                self.print_config_value(name, separator=' is now set to: ')"
prepare_outputs;"def prepare_outputs(self, job):
        """"""
        Called before job is started.

        If output is a `FileSystemTarget`, create parent directories so the hive command won't fail
        """"""
        outputs = flatten(job.output())
        for o in outputs:
            if isinstance(o, FileSystemTarget):
                parent_dir = os.path.dirname(o.path)
                if parent_dir and not o.fs.exists(parent_dir):
                    logger.info(""Creating parent directory %r"", parent_dir)
                    try:
                        # there is a possible race condition
                        # which needs to be handled here
                        o.fs.mkdir(parent_dir)
                    except FileAlreadyExists:
                        pass"
get_default_handler;"def get_default_handler(self, **kw):
        """"""Return the default logging handler based on the local environment.

        :type kw: dict
        :param kw: keyword args passed to handler constructor

        :rtype: :class:`logging.Handler`
        :returns: The default log handler based on the environment
        """"""
        gke_cluster_name = retrieve_metadata_server(_GKE_CLUSTER_NAME)

        if (
            _APPENGINE_FLEXIBLE_ENV_VM in os.environ
            or _APPENGINE_INSTANCE_ID in os.environ
        ):
            return AppEngineHandler(self, **kw)
        elif gke_cluster_name is not None:
            return ContainerEngineHandler(**kw)
        else:
            return CloudLoggingHandler(self, **kw)"
compute_results;"def compute_results(self):
    """"""Computes results (scores, stats, etc...) of competition evaluation.

    Results are saved into output directory (self.results_dir).
    Also this method saves all intermediate data into output directory as well,
    so it can resume computation if it was interrupted for some reason.
    This is useful because computatin of resuls could take many minutes.
    """"""
    # read all data
    logging.info('Reading data from datastore')
    dataset_meta = self._read_dataset_metadata()
    self.submissions.init_from_datastore()
    self.dataset_batches.init_from_datastore()
    self.adv_batches.init_from_datastore()
    self.attack_work.read_all_from_datastore()

    if os.path.exists(os.path.join(self.results_dir, 'defense_work.dump')):
      with open(os.path.join(self.results_dir, 'defense_work.dump')) as f:
        self.defense_work.deserialize(f)
    else:
      self.defense_work.read_all_from_datastore()
      with open(os.path.join(self.results_dir, 'defense_work.dump'), 'w') as f:
        self.defense_work.serialize(f)

    if os.path.exists(os.path.join(self.results_dir, 'class_batches.dump')):
      with open(os.path.join(self.results_dir, 'class_batches.dump')) as f:
        self.class_batches.deserialize(f)
    else:
      self.class_batches.init_from_datastore()
      with open(os.path.join(self.results_dir, 'class_batches.dump'), 'w') as f:
        self.class_batches.serialize(f)

    # process data
    logging.info('Processing classification results')
    count_adv_images = self.adv_batches.count_generated_adv_examples()
    intermediate_files = ['acc_matrix.dump', 'error_matrix.dump',
                          'hit_tc_matrix.dump', 'classified_images_count.dump']
    if all([os.path.exists(os.path.join(self.results_dir, fname))
            for fname in intermediate_files]):
      with open(os.path.join(self.results_dir, 'acc_matrix.dump')) as f:
        acc_matrix = pickle.load(f)
      with open(os.path.join(self.results_dir, 'error_matrix.dump')) as f:
        error_matrix = pickle.load(f)
      with open(os.path.join(self.results_dir, 'hit_tc_matrix.dump')) as f:
        hit_tc_matrix = pickle.load(f)
      with open(os.path.join(self.results_dir,
                             'classified_images_count.dump')) as f:
        classified_images_count = pickle.load(f)
    else:
      acc_matrix, error_matrix, hit_tc_matrix, classified_images_count = (
          self.class_batches.compute_classification_results(
              self.adv_batches,
              self.dataset_batches,
              dataset_meta,
              self.defense_work))
      with open(os.path.join(self.results_dir, 'acc_matrix.dump'), 'w') as f:
        pickle.dump(acc_matrix, f)
      with open(os.path.join(self.results_dir, 'error_matrix.dump'), 'w') as f:
        pickle.dump(error_matrix, f)
      with open(os.path.join(self.results_dir, 'hit_tc_matrix.dump'), 'w') as f:
        pickle.dump(hit_tc_matrix, f)
      with open(os.path.join(self.results_dir,
                             'classified_images_count.dump'), 'w') as f:
        pickle.dump(classified_images_count, f)

    # compute attacks and defenses which will be used for scoring
    logging.info('Computing attacks and defenses which are used for scoring')
    expected_num_adv_images = self.dataset_batches.count_num_images()
    attacks_to_use = [k for k, v in iteritems(count_adv_images)
                      if ((v == expected_num_adv_images)
                          and (k not in self.blacklisted_submissions))]

    total_num_adversarial = sum(itervalues(count_adv_images))
    defenses_to_use = [k for k, v in iteritems(classified_images_count)
                       if ((v == total_num_adversarial)
                           and (k not in self.blacklisted_submissions))]

    logging.info('Expected number of adversarial images: %d',
                 expected_num_adv_images)
    logging.info('Number of attacks to use to score defenses: %d',
                 len(attacks_to_use))
    logging.info('Expected number of classification predictions: %d',
                 total_num_adversarial)
    logging.info('Number of defenses to use to score attacks: %d',
                 len(defenses_to_use))

    save_dict_to_file(os.path.join(self.results_dir, 'count_adv_images.csv'),
                      count_adv_images)
    save_dict_to_file(os.path.join(self.results_dir,
                                   'classified_images_count.csv'),
                      classified_images_count)

    # compute scores
    logging.info('Computing scores')
    attack_scores = defaultdict(lambda: 0)
    targeted_attack_scores = defaultdict(lambda: 0)
    defense_scores = defaultdict(lambda: 0)
    for defense_id in acc_matrix.dim0:
      for attack_id in acc_matrix.dim1:
        if attack_id in attacks_to_use:
          defense_scores[defense_id] += acc_matrix[defense_id, attack_id]
        if defense_id in defenses_to_use:
          if attack_id in self.submissions.targeted_attacks:
            targeted_attack_scores[attack_id] += (
                hit_tc_matrix[defense_id, attack_id])
          else:
            attack_scores[attack_id] += error_matrix[defense_id, attack_id]
    # negate results of blacklisted submissions
    for s_id in self.blacklisted_submissions:
      if s_id in defense_scores:
        defense_scores[s_id] = -defense_scores[s_id]
      if s_id in attack_scores:
        attack_scores[s_id] = -attack_scores[s_id]
      if s_id in targeted_attack_scores:
        targeted_attack_scores[s_id] = -targeted_attack_scores[s_id]
    # save results
    logging.info('Saving results')
    all_attack_stats = self.attack_work.compute_work_statistics()
    nontargeted_attack_stats = {k: v for k, v in iteritems(all_attack_stats)
                                if k in self.submissions.attacks}
    targeted_attack_stats = {k: v for k, v in iteritems(all_attack_stats)
                             if k in self.submissions.targeted_attacks}
    defense_stats = self.defense_work.compute_work_statistics()
    self._save_work_results(
        nontargeted_attack_stats, attack_scores, count_adv_images,
        os.path.join(self.results_dir, 'attack_results.csv'))
    self._save_work_results(
        targeted_attack_stats, targeted_attack_scores, count_adv_images,
        os.path.join(self.results_dir, 'targeted_attack_results.csv'))
    self._save_work_results(
        defense_stats, defense_scores,
        classified_images_count,
        os.path.join(self.results_dir, 'defense_results.csv'))

    self._save_sorted_results(
        nontargeted_attack_stats, attack_scores, count_adv_images,
        os.path.join(self.results_dir, 'sorted_attack_results.csv'))
    self._save_sorted_results(
        targeted_attack_stats, targeted_attack_scores, count_adv_images,
        os.path.join(self.results_dir, 'sorted_target_attack_results.csv'))
    self._save_sorted_results(
        defense_stats, defense_scores, classified_images_count,
        os.path.join(self.results_dir, 'sorted_defense_results.csv'))

    defense_id_to_name = {k: self.submissions.get_external_id(k)
                          for k in iterkeys(self.submissions.defenses)}
    attack_id_to_name = {k: self.submissions.get_external_id(k)
                         for k in self.submissions.get_all_attack_ids()}
    acc_matrix.save_to_file(
        os.path.join(self.results_dir, 'accuracy_matrix.csv'),
        remap_dim0=defense_id_to_name, remap_dim1=attack_id_to_name)
    error_matrix.save_to_file(
        os.path.join(self.results_dir, 'error_matrix.csv'),
        remap_dim0=defense_id_to_name, remap_dim1=attack_id_to_name)
    hit_tc_matrix.save_to_file(
        os.path.join(self.results_dir, 'hit_target_class_matrix.csv'),
        remap_dim0=defense_id_to_name, remap_dim1=attack_id_to_name)

    save_dict_to_file(os.path.join(self.results_dir, 'defense_id_to_name.csv'),
                      defense_id_to_name)
    save_dict_to_file(os.path.join(self.results_dir, 'attack_id_to_name.csv'),
                      attack_id_to_name)"
single_run_max_confidence_recipe;"def single_run_max_confidence_recipe(sess, model, x, y, nb_classes, eps,
                                     clip_min, clip_max, eps_iter, nb_iter,
                                     report_path,
                                     batch_size=BATCH_SIZE,
                                     eps_iter_small=None):
  """"""A reasonable attack bundling recipe for a max norm threat model and
  a defender that uses confidence thresholding. This recipe uses both
  uniform noise and randomly-initialized PGD targeted attacks.

  References:
  https://openreview.net/forum?id=H1g0piA9tQ

  This version runs each attack (noise, targeted PGD for each class with
  nb_iter iterations, target PGD for each class with 25X more iterations)
  just once and then stops. See `basic_max_confidence_recipe` for a version
  that runs indefinitely.

  :param sess: tf.Session
  :param model: cleverhans.model.Model
  :param x: numpy array containing clean example inputs to attack
  :param y: numpy array containing true labels
  :param nb_classes: int, number of classes
  :param eps: float, maximum size of perturbation (measured by max norm)
  :param eps_iter: float, step size for one version of PGD attacks
    (will also run another version with eps_iter_small step size)
  :param nb_iter: int, number of iterations for the cheaper PGD attacks
    (will also run another version with 25X more iterations)
  :param report_path: str, the path that the report will be saved to.
  :param batch_size: int, the total number of examples to run simultaneously
  :param eps_iter_small: optional, float.
    The second version of the PGD attack is run with 25 * nb_iter iterations
    and eps_iter_small step size. If eps_iter_small is not specified it is
    set to eps_iter / 25.
  """"""
  noise_attack = Noise(model, sess)
  pgd_attack = ProjectedGradientDescent(model, sess)
  threat_params = {""eps"": eps, ""clip_min"": clip_min, ""clip_max"": clip_max}
  noise_attack_config = AttackConfig(noise_attack, threat_params, ""noise"")
  attack_configs = [noise_attack_config]
  pgd_attack_configs = []
  pgd_params = copy.copy(threat_params)
  pgd_params[""eps_iter""] = eps_iter
  pgd_params[""nb_iter""] = nb_iter
  assert batch_size % num_devices == 0
  dev_batch_size = batch_size // num_devices
  ones = tf.ones(dev_batch_size, tf.int32)
  expensive_pgd = []
  if eps_iter_small is None:
    eps_iter_small = eps_iter / 25.
  for cls in range(nb_classes):
    cls_params = copy.copy(pgd_params)
    cls_params['y_target'] = tf.to_float(tf.one_hot(ones * cls, nb_classes))
    cls_attack_config = AttackConfig(pgd_attack, cls_params, ""pgd_"" + str(cls))
    pgd_attack_configs.append(cls_attack_config)
    expensive_params = copy.copy(cls_params)
    expensive_params[""eps_iter""] = eps_iter_small
    expensive_params[""nb_iter""] *= 25.
    expensive_config = AttackConfig(
        pgd_attack, expensive_params, ""expensive_pgd_"" + str(cls))
    expensive_pgd.append(expensive_config)
  attack_configs = [noise_attack_config] + pgd_attack_configs + expensive_pgd
  new_work_goal = {config: 1 for config in attack_configs}
  goals = [MaxConfidence(t=1., new_work_goal=new_work_goal)]
  bundle_attacks(sess, model, x, y, attack_configs, goals, report_path, attack_batch_size=batch_size,
                 eval_batch_size=batch_size)"
export_png;"def export_png(obj, filename=None, height=None, width=None, webdriver=None, timeout=5):
    ''' Export the ``LayoutDOM`` object or document as a PNG.

    If the filename is not given, it is derived from the script name (e.g.
    ``/foo/myplot.py`` will create ``/foo/myplot.png``)

    Args:
        obj (LayoutDOM or Document) : a Layout (Row/Column), Plot or Widget
            object or Document to export.

        filename (str, optional) : filename to save document under (default: None)
            If None, infer from the filename.

        height (int) : the desired height of the exported layout obj only if
            it's a Plot instance. Otherwise the height kwarg is ignored.

        width (int) : the desired width of the exported layout obj only if
            it's a Plot instance. Otherwise the width kwarg is ignored.

        webdriver (selenium.webdriver) : a selenium webdriver instance to use
            to export the image.

        timeout (int) : the maximum amount of time (in seconds) to wait for
            Bokeh to initialize (default: 5) (Added in 1.1.1).

    Returns:
        filename (str) : the filename where the static file is saved.

    If you would like to access an Image object directly, rather than save a
    file to disk, use the lower-level :func:`~bokeh.io.export.get_screenshot_as_png`
    function.

    .. warning::
        Responsive sizing_modes may generate layouts with unexpected size and
        aspect ratios. It is recommended to use the default ``fixed`` sizing mode.

    '''

    image = get_screenshot_as_png(obj, height=height, width=width, driver=webdriver, timeout=timeout)

    if filename is None:
        filename = default_filename(""png"")

    if image.width == 0 or image.height == 0:
        raise ValueError(""unable to save an empty image"")

    image.save(filename)

    return abspath(filename)"
check_managed;"def check_managed(
        name,
        source,
        source_hash,
        source_hash_name,
        user,
        group,
        mode,
        attrs,
        template,
        context,
        defaults,
        saltenv,
        contents=None,
        skip_verify=False,
        seuser=None,
        serole=None,
        setype=None,
        serange=None,
        **kwargs):
    '''
    Check to see what changes need to be made for a file

    CLI Example:

    .. code-block:: bash

        salt '*' file.check_managed /etc/httpd/conf.d/httpd.conf salt://http/httpd.conf '{hash_type: 'md5', 'hsum': <md5sum>}' root, root, '755' jinja True None None base
    '''
    # If the source is a list then find which file exists
    source, source_hash = source_list(source,           # pylint: disable=W0633
                                      source_hash,
                                      saltenv)

    sfn = ''
    source_sum = None

    if contents is None:
        # Gather the source file from the server
        sfn, source_sum, comments = get_managed(
            name,
            template,
            source,
            source_hash,
            source_hash_name,
            user,
            group,
            mode,
            attrs,
            saltenv,
            context,
            defaults,
            skip_verify,
            **kwargs)
        if comments:
            __clean_tmp(sfn)
            return False, comments
    changes = check_file_meta(name, sfn, source, source_sum, user,
                              group, mode, attrs, saltenv, contents,
                              seuser=seuser, serole=serole, setype=setype,
                              serange=serange)
    # Ignore permission for files written temporary directories
    # Files in any path will still be set correctly using get_managed()
    if name.startswith(tempfile.gettempdir()):
        for key in ['user', 'group', 'mode']:
            changes.pop(key, None)
    __clean_tmp(sfn)
    if changes:
        log.info(changes)
        comments = ['The following values are set to be changed:\n']
        comments.extend('{0}: {1}\n'.format(key, val)
                        for key, val in six.iteritems(changes))
        return None, ''.join(comments)
    return True, 'The file {0} is in the correct state'.format(name)"
image_update;"def image_update(id=None, name=None, profile=None, **kwargs):  # pylint: disable=C0103
    '''
    Update properties of given image.
    Known to work for:
    - min_ram (in MB)
    - protected (bool)
    - visibility ('public' or 'private')

    CLI Example:

    .. code-block:: bash

        salt '*' glance.image_update id=c2eb2eb0-53e1-4a80-b990-8ec887eae7df
        salt '*' glance.image_update name=f16-jeos
    '''
    if id:
        image = image_show(id=id, profile=profile)
        if 'result' in image and not image['result']:
            return image
        elif len(image) == 1:
            image = image.values()[0]
    elif name:
        img_list = image_list(name=name, profile=profile)
        if img_list is dict and 'result' in img_list:
            return img_list
        elif not img_list:
            return {
                'result': False,
                'comment':
                    'No image with name \'{0}\' '
                    'found.'.format(name)
                }
        elif len(img_list) == 1:
            try:
                image = img_list[0]
            except KeyError:
                image = img_list[name]
    else:
        raise SaltInvocationError
    log.debug('Found image:\n%s', image)
    to_update = {}
    for key, value in kwargs.items():
        if key.startswith('_'):
            continue
        if key not in image or image[key] != value:
            log.debug('add <%s=%s> to to_update', key, value)
            to_update[key] = value
    g_client = _auth(profile)
    updated = g_client.images.update(image['id'], **to_update)
    return updated"
bbox_transform;"def bbox_transform(ex_rois, gt_rois, box_stds):
    """"""
    compute bounding box regression targets from ex_rois to gt_rois
    :param ex_rois: [N, 4]
    :param gt_rois: [N, 4]
    :return: [N, 4]
    """"""
    assert ex_rois.shape[0] == gt_rois.shape[0], 'inconsistent rois number'

    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0
    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0
    ex_ctr_x = ex_rois[:, 0] + 0.5 * (ex_widths - 1.0)
    ex_ctr_y = ex_rois[:, 1] + 0.5 * (ex_heights - 1.0)

    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0
    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0
    gt_ctr_x = gt_rois[:, 0] + 0.5 * (gt_widths - 1.0)
    gt_ctr_y = gt_rois[:, 1] + 0.5 * (gt_heights - 1.0)

    targets_dx = (gt_ctr_x - ex_ctr_x) / (ex_widths + 1e-14) / box_stds[0]
    targets_dy = (gt_ctr_y - ex_ctr_y) / (ex_heights + 1e-14) / box_stds[1]
    targets_dw = np.log(gt_widths / ex_widths) / box_stds[2]
    targets_dh = np.log(gt_heights / ex_heights) / box_stds[3]

    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()
    return targets"
parse_assessor_content;"def parse_assessor_content(experiment_config):
    '''Validate whether assessor in experiment_config is valid'''
    if experiment_config.get('assessor'):
        if experiment_config['assessor'].get('builtinAssessorName'):
            experiment_config['assessor']['className'] = experiment_config['assessor']['builtinAssessorName']
        else:
            validate_customized_file(experiment_config, 'assessor')"
report_missing_dependencies;"def report_missing_dependencies(self):
        """"""Show a QMessageBox with a list of missing hard dependencies""""""
        missing_deps = dependencies.missing_dependencies()
        if missing_deps:
            QMessageBox.critical(self, _('Error'),
                _(""<b>You have missing dependencies!</b>""
                  ""<br><br><tt>%s</tt><br><br>""
                  ""<b>Please install them to avoid this message.</b>""
                  ""<br><br>""
                  ""<i>Note</i>: Spyder could work without some of these ""
                  ""dependencies, however to have a smooth experience when ""
                  ""using Spyder we <i>strongly</i> recommend you to install ""
                  ""all the listed missing dependencies.<br><br>""
                  ""Failing to install these dependencies might result in bugs. ""
                  ""Please be sure that any found bugs are not the direct ""
                  ""result of missing dependencies, prior to reporting a new ""
                  ""issue.""
                  ) % missing_deps, QMessageBox.Ok)"
load_data;"def load_data(self):
        """"""Fill ListWidget with the tabs texts.

        Add elements in inverse order of stack_history.
        """"""

        for index in reversed(self.stack_history):
            text = self.tabs.tabText(index)
            text = text.replace('&', '')
            item = QListWidgetItem(ima.icon('TextFileIcon'), text)
            self.addItem(item)"
simplex_projection;"def simplex_projection(v, b=1):
    r""""""Projection vectors to the simplex domain

    Implemented according to the paper: Efficient projections onto the
    l1-ball for learning in high dimensions, John Duchi, et al. ICML 2008.
    Implementation Time: 2011 June 17 by Bin@libin AT pmail.ntu.edu.sg
    Optimization Problem: min_{w}\| w - v \|_{2}^{2}
    s.t. sum_{i=1}^{m}=z, w_{i}\geq 0

    Input: A vector v \in R^{m}, and a scalar z > 0 (default=1)
    Output: Projection vector w

    :Example:
    >>> proj = simplex_projection([.4 ,.3, -.4, .5])
    >>> proj  # doctest: +NORMALIZE_WHITESPACE
    array([ 0.33333333, 0.23333333, 0. , 0.43333333])
    >>> print(proj.sum())
    1.0

    Original matlab implementation: John Duchi (jduchi@cs.berkeley.edu)
    Python-port: Copyright 2013 by Thomas Wiecki (thomas.wiecki@gmail.com).
    """"""

    v = np.asarray(v)
    p = len(v)

    # Sort v into u in descending order
    v = (v > 0) * v
    u = np.sort(v)[::-1]
    sv = np.cumsum(u)

    rho = np.where(u > (sv - b) / np.arange(1, p + 1))[0][-1]
    theta = np.max([0, (sv[rho] - b) / (rho + 1)])
    w = (v - theta)
    w[w < 0] = 0
    return w"
find_requirements;"def find_requirements(max_depth=3):
    """"""Returns the path of a Pipfile in parent directories.""""""
    i = 0
    for c, d, f in walk_up(os.getcwd()):
        i += 1
        if i < max_depth:
            if ""requirements.txt"":
                r = os.path.join(c, ""requirements.txt"")
                if os.path.isfile(r):
                    return r

    raise RuntimeError(""No requirements.txt found!"")"
remove_diskgroup;"def remove_diskgroup(service_instance, host_ref, diskgroup, hostname=None,
                     host_vsan_system=None, erase_disk_partitions=False,
                     data_accessibility=True):
    '''
    Removes a disk group.

    service_instance
        Service instance to the host or vCenter

    host_ref
        Reference to the ESXi host

    diskgroup
        The vsan.HostDiskMapping object representing the host's diskgroup from
        where the capacity needs to be removed

    hostname
        Name of ESXi host. Default value is None.

    host_vsan_system
        ESXi host's VSAN system. Default value is None.

    data_accessibility
        Specifies whether to ensure data accessibility. Default value is True.
    '''
    if not hostname:
        hostname = salt.utils.vmware.get_managed_object_name(host_ref)
    cache_disk_id = diskgroup.ssd.canonicalName
    log.debug('Removing disk group with cache disk \'%s\' on '
              'host \'%s\'', cache_disk_id, hostname)
    if not host_vsan_system:
        host_vsan_system = get_host_vsan_system(
            service_instance, host_ref, hostname)
    # Set to evacuate all data before removing the disks
    maint_spec = vim.HostMaintenanceSpec()
    maint_spec.vsanMode = vim.VsanHostDecommissionMode()
    object_action = vim.VsanHostDecommissionModeObjectAction
    if data_accessibility:
        maint_spec.vsanMode.objectAction = \
                object_action.ensureObjectAccessibility
    else:
        maint_spec.vsanMode.objectAction = object_action.noAction
    try:
        task = host_vsan_system.RemoveDiskMapping_Task(
            mapping=[diskgroup], maintenanceSpec=maint_spec)
    except vim.fault.NoPermission as exc:
        log.exception(exc)
        raise VMwareApiError('Not enough permissions. Required privilege: '
                             '{0}'.format(exc.privilegeId))
    except vim.fault.VimFault as exc:
        log.exception(exc)
        raise VMwareApiError(exc.msg)
    except vmodl.RuntimeFault as exc:
        log.exception(exc)
        raise VMwareRuntimeError(exc.msg)
    salt.utils.vmware.wait_for_task(task, hostname, 'remove_diskgroup')
    log.debug('Removed disk group with cache disk \'%s\' on host \'%s\'',
              cache_disk_id, hostname)
    return True"
format_system_message;"def format_system_message(errno):
    """"""
    Call FormatMessage with a system error number to retrieve
    the descriptive error message.
    """"""
    # first some flags used by FormatMessageW
    ALLOCATE_BUFFER = 0x100
    FROM_SYSTEM = 0x1000

    # Let FormatMessageW allocate the buffer (we'll free it below)
    # Also, let it know we want a system error message.
    flags = ALLOCATE_BUFFER | FROM_SYSTEM
    source = None
    message_id = errno
    language_id = 0
    result_buffer = ctypes.wintypes.LPWSTR()
    buffer_size = 0
    arguments = None
    bytes = ctypes.windll.kernel32.FormatMessageW(
        flags,
        source,
        message_id,
        language_id,
        ctypes.byref(result_buffer),
        buffer_size,
        arguments,
    )
    # note the following will cause an infinite loop if GetLastError
    #  repeatedly returns an error that cannot be formatted, although
    #  this should not happen.
    handle_nonzero_success(bytes)
    message = result_buffer.value
    ctypes.windll.kernel32.LocalFree(result_buffer)
    return message"
format_for_columns;"def format_for_columns(pkgs, options):
    """"""
    Convert the package data into something usable
    by output_package_listing_columns.
    """"""
    running_outdated = options.outdated
    # Adjust the header for the `pip list --outdated` case.
    if running_outdated:
        header = [""Package"", ""Version"", ""Latest"", ""Type""]
    else:
        header = [""Package"", ""Version""]

    data = []
    if options.verbose >= 1 or any(dist_is_editable(x) for x in pkgs):
        header.append(""Location"")
    if options.verbose >= 1:
        header.append(""Installer"")

    for proj in pkgs:
        # if we're working on the 'outdated' list, separate out the
        # latest_version and type
        row = [proj.project_name, proj.version]

        if running_outdated:
            row.append(proj.latest_version)
            row.append(proj.latest_filetype)

        if options.verbose >= 1 or dist_is_editable(proj):
            row.append(proj.location)
        if options.verbose >= 1:
            row.append(get_installer(proj))

        data.append(row)

    return data, header"
new_value;"def new_value(self, name, value):
        """"""Create new value in data""""""
        try:
            # We need to enclose values in a list to be able to send
            # them to the kernel in Python 2
            svalue = [cloudpickle.dumps(value, protocol=PICKLE_PROTOCOL)]

            # Needed to prevent memory leaks. See issue 7158
            if len(svalue) < MAX_SERIALIZED_LENGHT:
                self.shellwidget.set_value(name, svalue)
            else:
                QMessageBox.warning(self, _(""Warning""),
                                    _(""The object you are trying to modify is ""
                                      ""too big to be sent back to the kernel. ""
                                      ""Therefore, your modifications won't ""
                                      ""take place.""))
        except TypeError as e:
            QMessageBox.critical(self, _(""Error""),
                                 ""TypeError: %s"" % to_text_string(e))
        self.shellwidget.refresh_namespacebrowser()"
get_domain_name;"def get_domain_name(self, domain_name, route53=True):
        """"""
        Scan our hosted zones for the record of a given name.

        Returns the record entry, else None.

        """"""
        # Make sure api gateway domain is present
        try:
            self.apigateway_client.get_domain_name(domainName=domain_name)
        except Exception:
            return None

        if not route53:
            return True

        try:
            zones = self.get_all_zones()
            for zone in zones['HostedZones']:
                records = self.route53.list_resource_record_sets(HostedZoneId=zone['Id'])
                for record in records['ResourceRecordSets']:
                    if record['Type'] in ('CNAME', 'A') and record['Name'][:-1] == domain_name:
                        return record

        except Exception as e:
            return None

        ##
        # Old, automatic logic.
        # If re-introduced, should be moved to a new function.
        # Related ticket: https://github.com/Miserlou/Zappa/pull/458
        ##

        # We may be in a position where Route53 doesn't have a domain, but the API Gateway does.
        # We need to delete this before we can create the new Route53.
        # try:
        #     api_gateway_domain = self.apigateway_client.get_domain_name(domainName=domain_name)
        #     self.apigateway_client.delete_domain_name(domainName=domain_name)
        # except Exception:
        #     pass

        return None"
get_external_paths;"def get_external_paths(self):
        """"""Returns a list of the external paths listed in the combobox.""""""
        return [to_text_string(self.itemText(i))
                for i in range(EXTERNAL_PATHS, self.count())]"
mediatype_get;"def mediatype_get(name=None, mediatypeids=None, **kwargs):
    '''
    Retrieve mediatypes according to the given parameters.

    Args:
        name:         Name or description of the mediatype
        mediatypeids: ids of the mediatypes

        optional kwargs:
                _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)
                _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)
                _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)

                all optional mediatype.get parameters: keyword argument names depends on your zabbix version, see:

                https://www.zabbix.com/documentation/2.2/manual/api/reference/mediatype/get

    Returns:
        Array with mediatype details, False if no mediatype found or on failure.

    CLI Example:
    .. code-block:: bash

        salt '*' zabbix.mediatype_get name='Email'
        salt '*' zabbix.mediatype_get mediatypeids=""['1', '2', '3']""
    '''
    conn_args = _login(**kwargs)
    ret = {}
    try:
        if conn_args:
            method = 'mediatype.get'
            params = {""output"": ""extend"", ""filter"": {}}
            if name:
                params['filter'].setdefault('description', name)
            if mediatypeids:
                params.setdefault('mediatypeids', mediatypeids)
            params = _params_extend(params, **kwargs)
            ret = _query(method, params, conn_args['url'], conn_args['auth'])
            return ret['result'] if ret['result'] else False
        else:
            raise KeyError
    except KeyError:
        return ret"
set_flushing_policy;"def set_flushing_policy(flushing_policy):
    """"""Serialize this policy for Monitor to pick up.""""""
    if ""RAY_USE_NEW_GCS"" not in os.environ:
        raise Exception(
            ""set_flushing_policy() is only available when environment ""
            ""variable RAY_USE_NEW_GCS is present at both compile and run time.""
        )
    ray.worker.global_worker.check_connected()
    redis_client = ray.worker.global_worker.redis_client

    serialized = pickle.dumps(flushing_policy)
    redis_client.set(""gcs_flushing_policy"", serialized)"
accept_all;"def accept_all(self):
        '''
        Accept all keys in pre
        '''
        keys = self.list_keys()
        for key in keys[self.PEND]:
            try:
                shutil.move(
                        os.path.join(
                            self.opts['pki_dir'],
                            self.PEND,
                            key),
                        os.path.join(
                            self.opts['pki_dir'],
                            self.ACC,
                            key)
                        )
                eload = {'result': True,
                         'act': 'accept',
                         'id': key}
                self.event.fire_event(eload,
                                      salt.utils.event.tagify(prefix='key'))
            except (IOError, OSError):
                pass
        return self.list_keys()"
move_tab;"def move_tab(self, index_from, index_to):
        """"""
        Move tab.

        (tabs themselves have already been moved by the history.tabwidget)
        """"""
        filename = self.filenames.pop(index_from)
        editor = self.editors.pop(index_from)

        self.filenames.insert(index_to, filename)
        self.editors.insert(index_to, editor)"
enable_argscope_for_module;"def enable_argscope_for_module(module, log_shape=True):
    """"""
    Overwrite all functions of a given module to support argscope.
    Note that this function monkey-patches the module and therefore could
    have unexpected consequences.
    It has been only tested to work well with ``tf.layers`` module.

    Example:

        .. code-block:: python

            import tensorflow as tf
            enable_argscope_for_module(tf.layers)

    Args:
        log_shape (bool): print input/output shapes of each function.
    """"""
    if is_tfv2() and module == tf.layers:
        module = tf.compat.v1.layers
    for name, obj in getmembers(module):
        if isfunction(obj):
            setattr(module, name, enable_argscope_for_function(obj,
                    log_shape=log_shape))"
list_security_groups;"def list_security_groups(call=None):
    '''
    Lists all security groups available to the user and the user's groups.

    .. versionadded:: 2016.3.0

    CLI Example:

    .. code-block:: bash

        salt-cloud -f list_security_groups opennebula
    '''
    if call == 'action':
        raise SaltCloudSystemExit(
            'The list_security_groups function must be called with -f or --function.'
        )

    server, user, password = _get_xml_rpc()
    auth = ':'.join([user, password])
    secgroup_pool = server.one.secgrouppool.info(auth, -2, -1, -1)[1]

    groups = {}
    for group in _get_xml(secgroup_pool):
        groups[group.find('NAME').text] = _xml_to_dict(group)

    return groups"
register_middleware;"def register_middleware(self, middleware, attach_to=""request""):
        """"""
        Register an application level middleware that will be attached
        to all the API URLs registered under this application.

        This method is internally invoked by the :func:`middleware`
        decorator provided at the app level.

        :param middleware: Callback method to be attached to the
            middleware
        :param attach_to: The state at which the middleware needs to be
            invoked in the lifecycle of an *HTTP Request*.
            **request** - Invoke before the request is processed
            **response** - Invoke before the response is returned back
        :return: decorated method
        """"""
        if attach_to == ""request"":
            if middleware not in self.request_middleware:
                self.request_middleware.append(middleware)
        if attach_to == ""response"":
            if middleware not in self.response_middleware:
                self.response_middleware.appendleft(middleware)
        return middleware"
render_fields;"def render_fields(dictionary,
                  *fields,
                  **opts):
    '''
    This function works similarly to
    :mod:`render_field <salt.modules.napalm_formula.render_field>` but for a
    list of fields from the same dictionary, rendering, indenting and
    distributing them on separate lines.

    dictionary
        The dictionary to traverse.

    fields
        A list of field names or paths in the dictionary.

    indent: ``0``
        The indentation to use, prepended to the rendered field.

    separator: ``\\n``
        The separator to use between fields.

    CLI Example:

    .. code-block:: bash

        salt '*' napalm_formula.render_fields ""{'mtu': 68, 'description': 'Interface description'}"" mtu description

    Jinja usage example:

    .. code-block:: jinja

        {%- set config={'mtu': 68, 'description': 'Interface description'} %}
        {{ salt.napalm_formula.render_fields(config, 'mtu', 'description', quotes=True) }}

    The Jinja example above would generate the following configuration:

    .. code-block:: text

        mtu ""68""
        description ""Interface description""
    '''
    results = []
    for field in fields:
        res = render_field(dictionary, field, **opts)
        if res:
            results.append(res)
    if 'indent' not in opts:
        opts['indent'] = 0
    if 'separator' not in opts:
        opts['separator'] = '\n{ind}'.format(ind=' '*opts['indent'])
    return opts['separator'].join(results)"
bias_variable;"def bias_variable(shape):
    """"""bias_variable generates a bias variable of a given shape.""""""
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)"
make_rotating_equity_info;"def make_rotating_equity_info(num_assets,
                              first_start,
                              frequency,
                              periods_between_starts,
                              asset_lifetime,
                              exchange='TEST'):
    """"""
    Create a DataFrame representing lifetimes of assets that are constantly
    rotating in and out of existence.

    Parameters
    ----------
    num_assets : int
        How many assets to create.
    first_start : pd.Timestamp
        The start date for the first asset.
    frequency : str or pd.tseries.offsets.Offset (e.g. trading_day)
        Frequency used to interpret next two arguments.
    periods_between_starts : int
        Create a new asset every `frequency` * `periods_between_new`
    asset_lifetime : int
        Each asset exists for `frequency` * `asset_lifetime` days.
    exchange : str, optional
        The exchange name.

    Returns
    -------
    info : pd.DataFrame
        DataFrame representing newly-created assets.
    """"""
    return pd.DataFrame(
        {
            'symbol': [chr(ord('A') + i) for i in range(num_assets)],
            # Start a new asset every `periods_between_starts` days.
            'start_date': pd.date_range(
                first_start,
                freq=(periods_between_starts * frequency),
                periods=num_assets,
            ),
            # Each asset lasts for `asset_lifetime` days.
            'end_date': pd.date_range(
                first_start + (asset_lifetime * frequency),
                freq=(periods_between_starts * frequency),
                periods=num_assets,
            ),
            'exchange': exchange,
        },
        index=range(num_assets),
    )"
mnist_tutorial;"def mnist_tutorial(train_start=0, train_end=60000, test_start=0,
                   test_end=10000, nb_epochs=NB_EPOCHS, batch_size=BATCH_SIZE,
                   learning_rate=LEARNING_RATE,
                   clean_train=True,
                   testing=False,
                   backprop_through_attack=False,
                   nb_filters=NB_FILTERS, num_threads=None,
                   attack_string=None):
  """"""
  MNIST cleverhans tutorial
  :param train_start: index of first training set example.
  :param train_end: index of last training set example.
  :param test_start: index of first test set example.
  :param test_end: index of last test set example.
  :param nb_epochs: number of epochs to train model.
  :param batch_size: size of training batches.
  :param learning_rate: learning rate for training.
  :param clean_train: perform normal training on clean examples only
                      before performing adversarial training.
  :param testing: if true, complete an AccuracyReport for unit tests
                  to verify that performance is adequate.
  :param backprop_through_attack: If True, backprop through adversarial
                                  example construction process during
                                  adversarial training.
  :param nb_filters: number of filters in the CNN used for training.
  :param num_threads: number of threads used for running the process.
  :param attack_string: attack name for crafting adversarial attacks and
                          adversarial training, in string format.
  :return: an AccuracyReport object
  """"""

  # Object used to keep track of (and return) key accuracies
  report = AccuracyReport()

  # Set TF random seed to improve reproducibility
  tf.set_random_seed(1234)

  # Set logging level to see debug information
  set_log_level(logging.DEBUG)

  # Get MNIST test data
  mnist = MNIST(train_start=train_start, train_end=train_end,
                test_start=test_start, test_end=test_end)
  X_train, Y_train = mnist.get_set('train')
  X_test, Y_test = mnist.get_set('test')

  # Use label smoothing
  assert Y_train.shape[1] == 10
  label_smooth = .1
  Y_train = Y_train.clip(label_smooth / 9., 1. - label_smooth)

  # Train an MNIST model
  train_params = {
      'nb_epochs': nb_epochs,
      'batch_size': batch_size,
      'learning_rate': learning_rate
  }

  # Initialize the attack object
  attack_class = attack_selection(attack_string)
  attack_params = {'eps': 0.3, 'clip_min': 0.,
                   'clip_max': 1.}

  rng = np.random.RandomState([2018, 6, 18])
  if clean_train:
    model = ModelBasicCNNTFE(nb_filters=nb_filters)

    def evaluate_clean():
      """"""Evaluate the accuracy of the MNIST model on legitimate test
      examples
      """"""
      eval_params = {'batch_size': batch_size}
      acc = model_eval(model, X_test, Y_test, args=eval_params)
      report.clean_train_clean_eval = acc
      assert X_test.shape[0] == test_end - test_start, X_test.shape
      print('Test accuracy on legitimate examples: %0.4f' % acc)

    train(model, X_train, Y_train, evaluate=evaluate_clean,
          args=train_params, rng=rng, var_list=model.get_params())

    if testing:
      # Calculate training error
      eval_params = {'batch_size': batch_size}
      acc = model_eval(model, X_train, Y_train, args=eval_params)
      report.train_clean_train_clean_eval = acc

    # Evaluate the accuracy of the MNIST model on adversarial examples
    eval_par = {'batch_size': batch_size}
    attack = attack_class(model)
    acc = model_eval(
        model, X_test, Y_test, args=eval_par,
        attack=attack, attack_args=attack_params)
    print('Test accuracy on adversarial examples: %0.4f\n' % acc)
    report.clean_train_adv_eval = acc

    # Calculate training error
    if testing:
      eval_par = {'batch_size': batch_size}
      acc = model_eval(
          model, X_train, Y_train, args=eval_par,
          attack=attack, attack_args=attack_params)
      print('Train accuracy on adversarial examples: %0.4f\n' % acc)
      report.train_clean_train_adv_eval = acc

    attack = None
    print(""Repeating the process, using adversarial training"")

  model_adv_train = ModelBasicCNNTFE(nb_filters=nb_filters)
  attack = attack_class(model_adv_train)

  def evaluate_adv():
    # Accuracy of adversarially trained model on legitimate test inputs
    eval_params = {'batch_size': batch_size}
    accuracy = model_eval(
        model_adv_train, X_test, Y_test,
        args=eval_params)
    print('Test accuracy on legitimate examples: %0.4f' % accuracy)
    report.adv_train_clean_eval = accuracy
    # Accuracy of the adversarially trained model on adversarial examples
    accuracy = model_eval(
        model_adv_train, X_test, Y_test,
        args=eval_params, attack=attack,
        attack_args=attack_params)
    print('Test accuracy on adversarial examples: %0.4f' % accuracy)
    report.adv_train_adv_eval = accuracy

  # Perform and evaluate adversarial training
  train(model_adv_train, X_train, Y_train, evaluate=evaluate_adv,
        args=train_params, rng=rng,
        var_list=model_adv_train.get_params(),
        attack=attack, attack_args=attack_params)

  # Calculate training errors
  if testing:
    eval_params = {'batch_size': batch_size}
    accuracy = model_eval(
        model_adv_train, X_train, Y_train, args=eval_params,
        attack=None, attack_args=None)
    report.train_adv_train_clean_eval = accuracy
    accuracy = model_eval(
        model_adv_train, X_train, Y_train, args=eval_params,
        attack=attack, attack_args=attack_params)
    report.train_adv_train_adv_eval = accuracy
  return report"
get_terminal_size;"def get_terminal_size(fallback=(80, 24)):
    """"""Get the size of the terminal window.

    For each of the two dimensions, the environment variable, COLUMNS
    and LINES respectively, is checked. If the variable is defined and
    the value is a positive integer, it is used.

    When COLUMNS or LINES is not defined, which is the common case,
    the terminal connected to sys.__stdout__ is queried
    by invoking os.get_terminal_size.

    If the terminal size cannot be successfully queried, either because
    the system doesn't support querying, or because we are not
    connected to a terminal, the value given in fallback parameter
    is used. Fallback defaults to (80, 24) which is the default
    size used by many terminal emulators.

    The value returned is a named tuple of type os.terminal_size.
    """"""
    # Try the environment first
    try:
        columns = int(os.environ[""COLUMNS""])
    except (KeyError, ValueError):
        columns = 0

    try:
        lines = int(os.environ[""LINES""])
    except (KeyError, ValueError):
        lines = 0

    # Only query if necessary
    if columns <= 0 or lines <= 0:
        try:
            size = _get_terminal_size(sys.__stdout__.fileno())
        except (NameError, OSError):
            size = terminal_size(*fallback)

        if columns <= 0:
            columns = size.columns
        if lines <= 0:
            lines = size.lines

    return terminal_size(columns, lines)"
authorize_url;"def authorize_url(self, state=None):
        '''
        Generate authorize_url.

        >>> GitHub(client_id='3ebf94c5776d565bcf75').authorize_url()
        'https://github.com/login/oauth/authorize?client_id=3ebf94c5776d565bcf75'
        '''
        if not self._client_id:
            raise ApiAuthError('No client id.')
        kw = dict(client_id=self._client_id)
        if self._redirect_uri:
            kw['redirect_uri'] = self._redirect_uri
        if self._scope:
            kw['scope'] = self._scope
        if state:
            kw['state'] = state
        return 'https://github.com/login/oauth/authorize?%s' % _encode_params(kw)"
convert_deconvolution;"def convert_deconvolution(node, **kwargs):
    """"""Map MXNet's deconvolution operator attributes to onnx's ConvTranspose operator
    and return the created node.
    """"""
    name, inputs, attrs = get_inputs(node, kwargs)

    kernel_dims = list(parse_helper(attrs, ""kernel""))
    stride_dims = list(parse_helper(attrs, ""stride"", [1, 1]))
    pad_dims = list(parse_helper(attrs, ""pad"", [0, 0]))
    num_group = int(attrs.get(""num_group"", 1))
    dilations = list(parse_helper(attrs, ""dilate"", [1, 1]))
    adj_dims = list(parse_helper(attrs, ""adj"", [0, 0]))

    pad_dims = pad_dims + pad_dims

    deconv_node = onnx.helper.make_node(
        ""ConvTranspose"",
        inputs=inputs,
        outputs=[name],
        kernel_shape=kernel_dims,
        strides=stride_dims,
        dilations=dilations,
        output_padding=adj_dims,
        pads=pad_dims,
        group=num_group,
        name=name
    )

    return [deconv_node]"
set_umr_namelist;"def set_umr_namelist(self):
        """"""Set UMR excluded modules name list""""""
        arguments, valid = QInputDialog.getText(self, _('UMR'),
                                  _(""Set the list of excluded modules as ""
                                    ""this: <i>numpy, scipy</i>""),
                                  QLineEdit.Normal,
                                  "", "".join(self.get_option('umr/namelist')))
        if valid:
            arguments = to_text_string(arguments)
            if arguments:
                namelist = arguments.replace(' ', '').split(',')
                fixed_namelist = []
                non_ascii_namelist = []
                for module_name in namelist:
                    if PY2:
                        if all(ord(c) < 128 for c in module_name):
                            if programs.is_module_installed(module_name):
                                fixed_namelist.append(module_name)
                        else:
                            QMessageBox.warning(self, _('Warning'),
                            _(""You are working with Python 2, this means that ""
                              ""you can not import a module that contains non-""
                              ""ascii characters.""), QMessageBox.Ok)
                            non_ascii_namelist.append(module_name)
                    elif programs.is_module_installed(module_name):
                        fixed_namelist.append(module_name)
                invalid = "", "".join(set(namelist)-set(fixed_namelist)-
                                    set(non_ascii_namelist))
                if invalid:
                    QMessageBox.warning(self, _('UMR'),
                                        _(""The following modules are not ""
                                          ""installed on your machine:\n%s""
                                          ) % invalid, QMessageBox.Ok)
                QMessageBox.information(self, _('UMR'),
                                    _(""Please note that these changes will ""
                                      ""be applied only to new Python/IPython ""
                                      ""consoles""), QMessageBox.Ok)
            else:
                fixed_namelist = []
            self.set_option('umr/namelist', fixed_namelist)"
save_figure_tofile;"def save_figure_tofile(fig, fmt, fname):
    """"""Save fig to fname in the format specified by fmt.""""""
    root, ext = osp.splitext(fname)
    if ext == '.png' and fmt == 'image/svg+xml':
        qimg = svg_to_image(fig)
        qimg.save(fname)
    else:
        if fmt == 'image/svg+xml' and is_unicode(fig):
            fig = fig.encode('utf-8')

        with open(fname, 'wb') as f:
            f.write(fig)"
list_stateful_set_for_all_namespaces;"def list_stateful_set_for_all_namespaces(self, **kwargs):
        """"""
        list or watch objects of kind StatefulSet
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.list_stateful_set_for_all_namespaces(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str pretty: If 'true', then the output is pretty printed.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1StatefulSetList
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.list_stateful_set_for_all_namespaces_with_http_info(**kwargs)
        else:
            (data) = self.list_stateful_set_for_all_namespaces_with_http_info(**kwargs)
            return data"
export_coreml;"def export_coreml(self, filename):
        """"""
        Export the model in Core ML format.

        Parameters
        ----------
        filename: str
          A valid filename where the model can be saved.

        Examples
        --------
        >>> model.export_coreml(""MyModel.mlmodel"")
        """"""
        import coremltools as _cmt
        import mxnet as _mx
        from ._mx_model_architecture import _net_params

        prob_name = self.target + 'Probability'
        label_name = self.target

        input_features = [
            ('features', _cmt.models.datatypes.Array(*(1, self.prediction_window, self.num_features)))
        ]
        output_features = [
            (prob_name, _cmt.models.datatypes.Array(*(self.num_classes,)))
        ]

        model_params = self._pred_model.get_params()
        weights = {k: v.asnumpy() for k, v in model_params[0].items()}
        weights = _mx.rnn.LSTMCell(num_hidden=_net_params['lstm_h']).unpack_weights(weights)
        moving_weights = {k: v.asnumpy() for k, v in model_params[1].items()}

        builder = _cmt.models.neural_network.NeuralNetworkBuilder(
            input_features,
            output_features,
            mode='classifier'
        )

        # Conv
        # (1,1,W,C) -> (1,C,1,W)
        builder.add_permute(name='permute_layer', dim=(0, 3, 1, 2),
                            input_name='features', output_name='conv_in')
        W = _np.expand_dims(weights['conv_weight'], axis=0).transpose((2, 3, 1, 0))
        builder.add_convolution(name='conv_layer',
                                kernel_channels=self.num_features,
                                output_channels=_net_params['conv_h'],
                                height=1, width=self.prediction_window,
                                stride_height=1, stride_width=self.prediction_window,
                                border_mode='valid', groups=1,
                                W=W, b=weights['conv_bias'], has_bias=True,
                                input_name='conv_in', output_name='relu0_in')
        builder.add_activation(name='relu_layer0', non_linearity='RELU',
                               input_name='relu0_in', output_name='lstm_in')

        # LSTM
        builder.add_optionals([('lstm_h_in', _net_params['lstm_h']),
                               ('lstm_c_in', _net_params['lstm_h'])],
                              [('lstm_h_out', _net_params['lstm_h']),
                               ('lstm_c_out', _net_params['lstm_h'])])

        W_x = [weights['lstm_i2h_i_weight'], weights['lstm_i2h_f_weight'],
               weights['lstm_i2h_o_weight'], weights['lstm_i2h_c_weight']]
        W_h = [weights['lstm_h2h_i_weight'], weights['lstm_h2h_f_weight'],
               weights['lstm_h2h_o_weight'], weights['lstm_h2h_c_weight']]
        bias = [weights['lstm_h2h_i_bias'], weights['lstm_h2h_f_bias'],
                weights['lstm_h2h_o_bias'], weights['lstm_h2h_c_bias']]

        builder.add_unilstm(name='lstm_layer',
                            W_h=W_h, W_x=W_x, b=bias,
                            input_size=_net_params['conv_h'],
                            hidden_size=_net_params['lstm_h'],
                            input_names=['lstm_in', 'lstm_h_in', 'lstm_c_in'],
                            output_names=['dense0_in', 'lstm_h_out', 'lstm_c_out'],
                            inner_activation='SIGMOID')

        # Dense
        builder.add_inner_product(name='dense_layer',
                                  W=weights['dense0_weight'], b=weights['dense0_bias'],
                                  input_channels=_net_params['lstm_h'],
                                  output_channels=_net_params['dense_h'],
                                  has_bias=True,
                                  input_name='dense0_in',
                                  output_name='bn_in')

        builder.add_batchnorm(name='bn_layer',
                              channels=_net_params['dense_h'],
                              gamma=weights['bn_gamma'], beta=weights['bn_beta'],
                              mean=moving_weights['bn_moving_mean'],
                              variance=moving_weights['bn_moving_var'],
                              input_name='bn_in', output_name='relu1_in',
                              epsilon=0.001)
        builder.add_activation(name='relu_layer1', non_linearity='RELU',
                               input_name='relu1_in', output_name='dense1_in')

        # Softmax
        builder.add_inner_product(name='dense_layer1',
                                  W=weights['dense1_weight'], b=weights['dense1_bias'],
                                  has_bias=True,
                                  input_channels=_net_params['dense_h'],
                                  output_channels=self.num_classes,
                                  input_name='dense1_in', output_name='softmax_in')

        builder.add_softmax(name=prob_name,
                            input_name='softmax_in',
                            output_name=prob_name)


        labels = list(map(str, sorted(self._target_id_map.keys())))
        builder.set_class_labels(labels)
        mlmodel = _cmt.models.MLModel(builder.spec)
        model_type = 'activity classifier'
        mlmodel.short_description = _coreml_utils._mlmodel_short_description(model_type)
        # Add useful information to the mlmodel
        features_str = ', '.join(self.features)
        mlmodel.input_description['features'] = u'Window \xd7 [%s]' % features_str
        mlmodel.input_description['lstm_h_in'] = 'LSTM hidden state input'
        mlmodel.input_description['lstm_c_in'] = 'LSTM cell state input'
        mlmodel.output_description[prob_name] = 'Activity prediction probabilities'
        mlmodel.output_description['classLabel'] = 'Class label of top prediction'
        mlmodel.output_description['lstm_h_out'] = 'LSTM hidden state output'
        mlmodel.output_description['lstm_c_out'] = 'LSTM cell state output'
        _coreml_utils._set_model_metadata(mlmodel, self.__class__.__name__, {
                'prediction_window': str(self.prediction_window),
                'session_id': self.session_id,
                'target': self.target,
                'features': ','.join(self.features),
                'max_iterations': str(self.max_iterations),
            }, version=ActivityClassifier._PYTHON_ACTIVITY_CLASSIFIER_VERSION)
        spec = mlmodel.get_spec()
        _cmt.models.utils.rename_feature(spec, 'classLabel', label_name)
        _cmt.models.utils.rename_feature(spec, 'lstm_h_in', 'hiddenIn')
        _cmt.models.utils.rename_feature(spec, 'lstm_c_in', 'cellIn')
        _cmt.models.utils.rename_feature(spec, 'lstm_h_out', 'hiddenOut')
        _cmt.models.utils.rename_feature(spec, 'lstm_c_out', 'cellOut')
        _cmt.utils.save_spec(spec, filename)"
register_unpack_format;"def register_unpack_format(name, extensions, function, extra_args=None,
                           description=''):
    """"""Registers an unpack format.

    `name` is the name of the format. `extensions` is a list of extensions
    corresponding to the format.

    `function` is the callable that will be
    used to unpack archives. The callable will receive archives to unpack.
    If it's unable to handle an archive, it needs to raise a ReadError
    exception.

    If provided, `extra_args` is a sequence of
    (name, value) tuples that will be passed as arguments to the callable.
    description can be provided to describe the format, and will be returned
    by the get_unpack_formats() function.
    """"""
    if extra_args is None:
        extra_args = []
    _check_unpack_options(extensions, function, extra_args)
    _UNPACK_FORMATS[name] = extensions, function, extra_args, description"
create_data_table;"def create_data_table(self):
        """"""Create the QTableView that will hold the data model.""""""
        self.dataTable = DataFrameView(self, self.dataModel,
                                       self.table_header.horizontalHeader(),
                                       self.hscroll, self.vscroll)
        self.dataTable.verticalHeader().hide()
        self.dataTable.horizontalHeader().hide()
        self.dataTable.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.dataTable.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        self.dataTable.setHorizontalScrollMode(QTableView.ScrollPerPixel)
        self.dataTable.setVerticalScrollMode(QTableView.ScrollPerPixel)
        self.dataTable.setFrameStyle(QFrame.Plain)
        self.dataTable.setItemDelegate(QItemDelegate())
        self.layout.addWidget(self.dataTable, 1, 1)
        self.setFocusProxy(self.dataTable)
        self.dataTable.sig_sort_by_column.connect(self._sort_update)
        self.dataTable.sig_fetch_more_columns.connect(self._fetch_more_columns)
        self.dataTable.sig_fetch_more_rows.connect(self._fetch_more_rows)"
read_text_file;"def read_text_file(filename):
    # type: (str) -> str
    """"""Return the contents of *filename*.

    Try to decode the file contents with utf-8, the preferred system encoding
    (e.g., cp1252 on some Windows machines), and latin1, in that order.
    Decoding a byte string with latin1 will never raise an error. In the worst
    case, the returned string will contain some garbage characters.

    """"""
    with open(filename, 'rb') as fp:
        data = fp.read()

    encodings = ['utf-8', locale.getpreferredencoding(False), 'latin1']
    for enc in encodings:
        try:
            # https://github.com/python/mypy/issues/1174
            data = data.decode(enc)  # type: ignore
        except UnicodeDecodeError:
            continue
        break

    assert not isinstance(data, bytes)  # Latin1 should have worked.
    return data"
aggregate_gradients_using_copy_with_device_selection;"def aggregate_gradients_using_copy_with_device_selection(
        tower_grads, avail_devices, use_mean=True, check_inf_nan=False):
    """"""Aggregate gradients, controlling device for the aggregation.

  Args:
    tower_grads: List of lists of (gradient, variable) tuples. The outer list
      is over towers. The inner list is over individual gradients.
    use_mean: if True, mean is taken, else sum of gradients is taken.
    check_inf_nan: If true, check grads for nans and infs.

  Returns:
    The tuple ([(average_gradient, variable),], has_nan_or_inf) where the
      gradient has been averaged across all towers. The variable is chosen from
      the first tower. The has_nan_or_inf indicates the grads has nan or inf.
  """"""
    agg_grads = []
    has_nan_or_inf_list = []
    for i, single_grads in enumerate(zip(*tower_grads)):
        with tf.device(avail_devices[i % len(avail_devices)]):
            grad_and_var, has_nan_or_inf = aggregate_single_gradient(
                single_grads, use_mean, check_inf_nan)
            agg_grads.append(grad_and_var)
            has_nan_or_inf_list.append(has_nan_or_inf)
    return agg_grads"
histograms_route;"def histograms_route(self, request):
    """"""Given a tag and single run, return array of histogram values.""""""
    tag = request.args.get('tag')
    run = request.args.get('run')
    try:
      (body, mime_type) = self.histograms_impl(
          tag, run, downsample_to=self.SAMPLE_SIZE)
      code = 200
    except ValueError as e:
      (body, mime_type) = (str(e), 'text/plain')
      code = 400
    return http_util.Respond(request, body, mime_type, code=code)"
get_multilevel_rpn_anchor_input;"def get_multilevel_rpn_anchor_input(im, boxes, is_crowd):
    """"""
    Args:
        im: an image
        boxes: nx4, floatbox, gt. shoudn't be changed
        is_crowd: n,

    Returns:
        [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level.
        Each tuple contains the anchor labels and target boxes for each pixel in the featuremap.

        fm_labels: fHxfWx NUM_ANCHOR_RATIOS
        fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4
    """"""
    boxes = boxes.copy()
    anchors_per_level = get_all_anchors_fpn()
    flatten_anchors_per_level = [k.reshape((-1, 4)) for k in anchors_per_level]
    all_anchors_flatten = np.concatenate(flatten_anchors_per_level, axis=0)

    inside_ind, inside_anchors = filter_boxes_inside_shape(all_anchors_flatten, im.shape[:2])
    anchor_labels, anchor_gt_boxes = get_anchor_labels(inside_anchors, boxes[is_crowd == 0], boxes[is_crowd == 1])

    # map back to all_anchors, then split to each level
    num_all_anchors = all_anchors_flatten.shape[0]
    all_labels = -np.ones((num_all_anchors, ), dtype='int32')
    all_labels[inside_ind] = anchor_labels
    all_boxes = np.zeros((num_all_anchors, 4), dtype='float32')
    all_boxes[inside_ind] = anchor_gt_boxes

    start = 0
    multilevel_inputs = []
    for level_anchor in anchors_per_level:
        assert level_anchor.shape[2] == len(cfg.RPN.ANCHOR_RATIOS)
        anchor_shape = level_anchor.shape[:3]   # fHxfWxNUM_ANCHOR_RATIOS
        num_anchor_this_level = np.prod(anchor_shape)
        end = start + num_anchor_this_level
        multilevel_inputs.append(
            (all_labels[start: end].reshape(anchor_shape),
             all_boxes[start: end, :].reshape(anchor_shape + (4,))
             ))
        start = end
    assert end == num_all_anchors, ""{} != {}"".format(end, num_all_anchors)
    return multilevel_inputs"
enforce_epsilon_and_compute_hash;"def enforce_epsilon_and_compute_hash(dataset_batch_dir, adv_dir, output_dir,
                                     epsilon):
  """"""Enforces size of perturbation on images, and compute hashes for all images.

  Args:
    dataset_batch_dir: directory with the images of specific dataset batch
    adv_dir: directory with generated adversarial images
    output_dir: directory where to copy result
    epsilon: size of perturbation

  Returns:
    dictionary with mapping form image ID to hash.
  """"""
  dataset_images = [f for f in os.listdir(dataset_batch_dir)
                    if f.endswith('.png')]
  image_hashes = {}
  resize_warning = False
  for img_name in dataset_images:
    if not os.path.exists(os.path.join(adv_dir, img_name)):
      logging.warning('Image %s not found in the output', img_name)
      continue
    image = np.array(
        Image.open(os.path.join(dataset_batch_dir, img_name)).convert('RGB'))
    image = image.astype('int32')
    image_max_clip = np.clip(image + epsilon, 0, 255).astype('uint8')
    image_min_clip = np.clip(image - epsilon, 0, 255).astype('uint8')
    # load and resize adversarial image if needed
    adv_image = Image.open(os.path.join(adv_dir, img_name)).convert('RGB')
    # Image.size is reversed compared to np.array.shape
    if adv_image.size[::-1] != image.shape[:2]:
      resize_warning = True
      adv_image = adv_image.resize((image.shape[1], image.shape[0]),
                                   Image.BICUBIC)
    adv_image = np.array(adv_image)
    clipped_adv_image = np.clip(adv_image,
                                image_min_clip,
                                image_max_clip)
    Image.fromarray(clipped_adv_image).save(os.path.join(output_dir, img_name))
    # compute hash
    image_hashes[img_name[:-4]] = hashlib.sha1(
        clipped_adv_image.view(np.uint8)).hexdigest()
  if resize_warning:
    logging.warning('One or more adversarial images had incorrect size')
  return image_hashes"
handle_violation;"def handle_violation(self, asset, amount, datetime, metadata=None):
        """"""
        Handle a TradingControlViolation, either by raising or logging and
        error with information about the failure.

        If dynamic information should be displayed as well, pass it in via
        `metadata`.
        """"""
        constraint = self._constraint_msg(metadata)

        if self.on_error == 'fail':
            raise TradingControlViolation(
                asset=asset,
                amount=amount,
                datetime=datetime,
                constraint=constraint)
        elif self.on_error == 'log':
            log.error(""Order for {amount} shares of {asset} at {dt} ""
                      ""violates trading constraint {constraint}"",
                      amount=amount, asset=asset, dt=datetime,
                      constraint=constraint)"
parse_field_path;"def parse_field_path(api_repr):
    """"""Parse a **field path** from into a list of nested field names.

    See :func:`field_path` for more on **field paths**.

    Args:
        api_repr (str):
            The unique Firestore api representation which consists of
            either simple or UTF-8 field names. It cannot exceed
            1500 bytes, and cannot be empty. Simple field names match
            ``'^[_a-zA-Z][_a-zA-Z0-9]*$'``. All other field names are
            escaped by surrounding them with backticks.

    Returns:
        List[str, ...]: The list of field names in the field path.
    """"""
    # code dredged back up from
    # https://github.com/googleapis/google-cloud-python/pull/5109/files
    field_names = []
    for field_name in split_field_path(api_repr):
        # non-simple field name
        if field_name[0] == ""`"" and field_name[-1] == ""`"":
            field_name = field_name[1:-1]
            field_name = field_name.replace(_ESCAPED_BACKTICK, _BACKTICK)
            field_name = field_name.replace(_ESCAPED_BACKSLASH, _BACKSLASH)
        field_names.append(field_name)
    return field_names"
create_file_manage_actions;"def create_file_manage_actions(self, fnames):
        """"""Return file management actions""""""
        only_files = all([osp.isfile(_fn) for _fn in fnames])
        only_modules = all([osp.splitext(_fn)[1] in ('.py', '.pyw', '.ipy')
                            for _fn in fnames])
        only_notebooks = all([osp.splitext(_fn)[1] == '.ipynb'
                              for _fn in fnames])
        only_valid = all([encoding.is_text_file(_fn) for _fn in fnames])
        run_action = create_action(self, _(""Run""), icon=ima.icon('run'),
                                   triggered=self.run)
        edit_action = create_action(self, _(""Edit""), icon=ima.icon('edit'),
                                    triggered=self.clicked)
        move_action = create_action(self, _(""Move...""),
                                    icon=""move.png"",
                                    triggered=self.move)
        delete_action = create_action(self, _(""Delete...""),
                                      icon=ima.icon('editdelete'),
                                      triggered=self.delete)
        rename_action = create_action(self, _(""Rename...""),
                                      icon=ima.icon('rename'),
                                      triggered=self.rename)
        open_external_action = create_action(self, _(""Open With OS""), 
                                             triggered=self.open_external)
        ipynb_convert_action = create_action(self, _(""Convert to Python script""),
                                             icon=ima.icon('python'),
                                             triggered=self.convert_notebooks)
        copy_file_clipboard_action = (
            create_action(self, _(""Copy""),
                          QKeySequence(get_shortcut('explorer', 'copy file')),
                          icon=ima.icon('editcopy'),
                          triggered=self.copy_file_clipboard))
        save_file_clipboard_action = (
            create_action(self, _(""Paste""),
                          QKeySequence(get_shortcut('explorer', 'paste file')),
                          icon=ima.icon('editpaste'),
                          triggered=self.save_file_clipboard))
        copy_absolute_path_action = (
            create_action(self, _(""Copy Absolute Path""), QKeySequence(
                get_shortcut('explorer', 'copy absolute path')),
                          triggered=self.copy_absolute_path))
        copy_relative_path_action = (
            create_action(self, _(""Copy Relative Path""), QKeySequence(
                get_shortcut('explorer', 'copy relative path')),
                          triggered=self.copy_relative_path))
        
        actions = []
        if only_modules:
            actions.append(run_action)
        if only_valid and only_files:
            actions.append(edit_action)
        
        if sys.platform == 'darwin':
            text=_(""Show in Finder"")
        else:
            text=_(""Show in Folder"")
        external_fileexp_action = create_action(self, text, 
                                triggered=self.show_in_external_file_explorer)        
        actions += [delete_action, rename_action]
        basedir = fixpath(osp.dirname(fnames[0]))
        if all([fixpath(osp.dirname(_fn)) == basedir for _fn in fnames]):
            actions.append(move_action)
        actions += [None]
        actions += [copy_file_clipboard_action, save_file_clipboard_action,
                    copy_absolute_path_action, copy_relative_path_action]
        if not QApplication.clipboard().mimeData().hasUrls():
            save_file_clipboard_action.setDisabled(True)
        actions += [None]
        if only_files:
            actions.append(open_external_action)
        actions.append(external_fileexp_action)
        actions.append([None])
        if only_notebooks and nbexporter is not None:
            actions.append(ipynb_convert_action)

        # VCS support is quite limited for now, so we are enabling the VCS
        # related actions only when a single file/folder is selected:
        dirname = fnames[0] if osp.isdir(fnames[0]) else osp.dirname(fnames[0])
        if len(fnames) == 1 and vcs.is_vcs_repository(dirname):
            commit_slot = lambda : self.vcs_command([dirname], 'commit')
            browse_slot = lambda : self.vcs_command([dirname], 'browse')
            vcs_ci = create_action(self, _(""Commit""),
                                   icon=ima.icon('vcs_commit'),
                                   triggered=commit_slot)
            vcs_log = create_action(self, _(""Browse repository""),
                                    icon=ima.icon('vcs_browse'),
                                    triggered=browse_slot)
            actions += [None, vcs_ci, vcs_log]

        return actions"
insert_text;"def insert_text(self, text):
        """"""Reimplement TextEditBaseWidget method""""""
        # Eventually this maybe should wrap to insert_text_to if
        # backspace-handling is required
        self.textCursor().insertText(text, self.default_style.format)"
zeroed_observation;"def zeroed_observation(observation):
    """"""Return an array of zeros with same shape as given observation

    # Argument
        observation (list): List of observation
    
    # Return
        A np.ndarray of zeros with observation.shape
    """"""
    if hasattr(observation, 'shape'):
        return np.zeros(observation.shape)
    elif hasattr(observation, '__iter__'):
        out = []
        for x in observation:
            out.append(zeroed_observation(x))
        return out
    else:
        return 0."
get_event_name;"def get_event_name(lambda_name, name):
        """"""
        Returns an AWS-valid Lambda event name.

        """"""
        return '{prefix:.{width}}-{postfix}'.format(prefix=lambda_name, width=max(0, 63 - len(name)), postfix=name)[:64]"
validate_absolute_path;"def validate_absolute_path(self, root: str, absolute_path: str) -> Optional[str]:
        """"""Validate and return the absolute path.

        ``root`` is the configured path for the `StaticFileHandler`,
        and ``path`` is the result of `get_absolute_path`

        This is an instance method called during request processing,
        so it may raise `HTTPError` or use methods like
        `RequestHandler.redirect` (return None after redirecting to
        halt further processing).  This is where 404 errors for missing files
        are generated.

        This method may modify the path before returning it, but note that
        any such modifications will not be understood by `make_static_url`.

        In instance methods, this method's result is available as
        ``self.absolute_path``.

        .. versionadded:: 3.1
        """"""
        # os.path.abspath strips a trailing /.
        # We must add it back to `root` so that we only match files
        # in a directory named `root` instead of files starting with
        # that prefix.
        root = os.path.abspath(root)
        if not root.endswith(os.path.sep):
            # abspath always removes a trailing slash, except when
            # root is '/'. This is an unusual case, but several projects
            # have independently discovered this technique to disable
            # Tornado's path validation and (hopefully) do their own,
            # so we need to support it.
            root += os.path.sep
        # The trailing slash also needs to be temporarily added back
        # the requested path so a request to root/ will match.
        if not (absolute_path + os.path.sep).startswith(root):
            raise HTTPError(403, ""%s is not in root static directory"", self.path)
        if os.path.isdir(absolute_path) and self.default_filename is not None:
            # need to look at the request.path here for when path is empty
            # but there is some prefix to the path that was already
            # trimmed by the routing
            if not self.request.path.endswith(""/""):
                self.redirect(self.request.path + ""/"", permanent=True)
                return None
            absolute_path = os.path.join(absolute_path, self.default_filename)
        if not os.path.exists(absolute_path):
            raise HTTPError(404)
        if not os.path.isfile(absolute_path):
            raise HTTPError(403, ""%s is not a file"", self.path)
        return absolute_path"
numeric_columns;"def numeric_columns(self, include_bool=True):
        """"""Returns the numeric columns of the Manager.

        Returns:
            List of index names.
        """"""
        columns = []
        for col, dtype in zip(self.columns, self.dtypes):
            if is_numeric_dtype(dtype) and (
                include_bool or (not include_bool and dtype != np.bool_)
            ):
                columns.append(col)
        return columns"
export_coreml;"def export_coreml(self, filename):
        """"""
        Save the model in Core ML format.
        The exported model calculates the distance between a query image and
        each row of the model's stored data. It does not sort and retrieve
        the k nearest neighbors of the query image.

        See Also
        --------
        save

        Examples
        --------
        >>> # Train an image similarity model
        >>> model = turicreate.image_similarity.create(data)
        >>>
        >>> # Query the model for similar images
        >>> similar_images = model.query(data)
        +-------------+-----------------+---------------+------+
        | query_label | reference_label |    distance   | rank |
        +-------------+-----------------+---------------+------+
        |      0      |        0        |      0.0      |  1   |
        |      0      |        2        | 24.9664942809 |  2   |
        |      0      |        1        | 28.4416069428 |  3   |
        |      1      |        1        |      0.0      |  1   |
        |      1      |        2        | 21.8715131191 |  2   |
        |      1      |        0        | 28.4416069428 |  3   |
        |      2      |        2        |      0.0      |  1   |
        |      2      |        1        | 21.8715131191 |  2   |
        |      2      |        0        | 24.9664942809 |  3   |
        +-------------+-----------------+---------------+------+
        [9 rows x 4 columns]
        >>>
        >>> # Export the model to Core ML format
        >>> model.export_coreml('myModel.mlmodel')
        >>>
        >>> # Load the Core ML model
        >>> import coremltools
        >>> ml_model = coremltools.models.MLModel('myModel.mlmodel')
        >>>
        >>> # Prepare the first image of reference data for consumption
        >>> # by the Core ML model
        >>> import PIL
        >>> image = tc.image_analysis.resize(data['image'][0], *reversed(model.input_image_shape))
        >>> image = PIL.Image.fromarray(image.pixel_data)
        >>>
        >>> # Calculate distances using the Core ML model
        >>> ml_model.predict(data={'image': image})
        {'distance': array([ 0., 28.453125, 24.96875 ])}
        """"""
        import numpy as _np
        import coremltools as _cmt
        from coremltools.models import datatypes as _datatypes, neural_network as _neural_network
        from .._mxnet._mxnet_to_coreml import _mxnet_converter
        from turicreate.toolkits import _coreml_utils

        # Get the reference data from the model
        proxy = self.similarity_model.__proxy__
        reference_data = _np.array(_tc.extensions._nearest_neighbors._nn_get_reference_data(proxy))
        num_examples, embedding_size = reference_data.shape

        output_name = 'distance'
        output_features = [(output_name, _datatypes.Array(num_examples))]

        if self.model != 'VisionFeaturePrint_Scene':
            # Convert the MxNet model to Core ML
            ptModel = _pre_trained_models.MODELS[self.model]()
            feature_extractor = _image_feature_extractor.MXFeatureExtractor(ptModel)

            input_name = feature_extractor.data_layer
            input_features = [(input_name, _datatypes.Array(*(self.input_image_shape)))]

            # Create a neural network
            builder = _neural_network.NeuralNetworkBuilder(
                input_features, output_features, mode=None)

            # Convert the feature extraction network
            mx_feature_extractor = feature_extractor._get_mx_module(
                feature_extractor.ptModel.mxmodel,
                feature_extractor.data_layer,
                feature_extractor.feature_layer,
                feature_extractor.context,
                self.input_image_shape
            )
            batch_input_shape = (1, ) + self.input_image_shape
            _mxnet_converter.convert(mx_feature_extractor, mode=None,
                                     input_shape=[(input_name, batch_input_shape)],
                                     builder=builder, verbose=False)
            feature_layer = feature_extractor.feature_layer

        else:     # self.model == VisionFeaturePrint_Scene
            # Create a pipleline that contains a VisionFeaturePrint followed by a
            # neural network.
            BGR_VALUE = _cmt.proto.FeatureTypes_pb2.ImageFeatureType.ColorSpace.Value('BGR')
            DOUBLE_ARRAY_VALUE = _cmt.proto.FeatureTypes_pb2.ArrayFeatureType.ArrayDataType.Value('DOUBLE')
            INPUT_IMAGE_SHAPE = 299

            top_spec = _cmt.proto.Model_pb2.Model()
            top_spec.specificationVersion = 3
            desc = top_spec.description

            input = desc.input.add()
            input.name = self.feature
            input.type.imageType.width = INPUT_IMAGE_SHAPE
            input.type.imageType.height = INPUT_IMAGE_SHAPE
            input.type.imageType.colorSpace = BGR_VALUE

            output = desc.output.add()
            output.name = output_name
            output.type.multiArrayType.shape.append(num_examples)
            output.type.multiArrayType.dataType = DOUBLE_ARRAY_VALUE

            # VisionFeaturePrint extractor
            pipeline = top_spec.pipeline
            scene_print = pipeline.models.add()
            scene_print.specificationVersion = 3
            scene_print.visionFeaturePrint.scene.version = 1

            input = scene_print.description.input.add()
            input.name = self.feature
            input.type.imageType.width = 299
            input.type.imageType.height = 299
            input.type.imageType.colorSpace = BGR_VALUE

            feature_layer = 'VisionFeaturePrint_Scene_output'
            output = scene_print.description.output.add()
            output.name = feature_layer
            output.type.multiArrayType.dataType = DOUBLE_ARRAY_VALUE
            output.type.multiArrayType.shape.append(2048)

            # Neural network builder
            input_features = [(feature_layer, _datatypes.Array(2048))]
            builder = _neural_network.NeuralNetworkBuilder(input_features, output_features)

        # To add the nearest neighbors model we add calculation of the euclidean 
        # distance between the newly extracted query features (denoted by the vector u)
        # and each extracted reference feature (denoted by the rows of matrix V).
        # Calculation of sqrt((v_i-u)^2) = sqrt(v_i^2 - 2v_i*u + u^2) ensues.
        V = reference_data
        v_squared = (V * V).sum(axis=1)
        builder.add_inner_product('v^2-2vu', W=-2 * V, b=v_squared, has_bias=True,
                                  input_channels=embedding_size, output_channels=num_examples,
                                  input_name=feature_layer, output_name='v^2-2vu')

        builder.add_unary('element_wise-u^2', mode='power', alpha=2,
                          input_name=feature_layer, output_name='element_wise-u^2')

        # Produce a vector of length num_examples with all values equal to u^2
        builder.add_inner_product('u^2', W=_np.ones((embedding_size, num_examples)),
                                  b=None, has_bias=False,
                                  input_channels=embedding_size, output_channels=num_examples,
                                  input_name='element_wise-u^2', output_name='u^2')

        builder.add_elementwise('v^2-2vu+u^2', mode='ADD',
                                input_names=['v^2-2vu', 'u^2'],
                                output_name='v^2-2vu+u^2')

        # v^2-2vu+u^2=(v-u)^2 is non-negative but some computations on GPU may result in
        # small negative values. Apply RELU so we don't take the square root of negative values.
        builder.add_activation('relu', non_linearity='RELU',
                               input_name='v^2-2vu+u^2', output_name='relu')
        builder.add_unary('sqrt', mode='sqrt', input_name='relu', output_name=output_name)

        # Finalize model
        if self.model != 'VisionFeaturePrint_Scene':
            _mxnet_converter._set_input_output_layers(builder, [input_name], [output_name])
            builder.set_input([input_name], [self.input_image_shape])
            builder.set_output([output_name], [(num_examples,)])
            _cmt.models.utils.rename_feature(builder.spec, input_name, self.feature)
            builder.set_pre_processing_parameters(image_input_names=self.feature)
            mlmodel = _cmt.models.MLModel(builder.spec)
        else:
            top_spec.pipeline.models.extend([builder.spec])
            mlmodel = _cmt.models.MLModel(top_spec)

        # Add metadata
        model_type = 'image similarity'
        mlmodel.short_description = _coreml_utils._mlmodel_short_description(model_type)
        mlmodel.input_description[self.feature] = u'Input image'
        mlmodel.output_description[output_name] = u'Distances between the input and reference images'

        _coreml_utils._set_model_metadata(mlmodel, self.__class__.__name__, {
            'model': self.model,
            'num_examples': str(self.num_examples)
        }, version=ImageSimilarityModel._PYTHON_IMAGE_SIMILARITY_VERSION)

        mlmodel.save(filename)"
delete_collection_cluster_role_binding;"def delete_collection_cluster_role_binding(self, **kwargs):
        """"""
        delete collection of ClusterRoleBinding
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.delete_collection_cluster_role_binding(async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str pretty: If 'true', then the output is pretty printed.
        :param str _continue: The continue option should be set when retrieving more results from the server. Since this value is server defined, clients may only use the continue value from a previous query result with identical query parameters (except for the value of continue) and the server may reject a continue value it does not recognize. If the specified continue value is no longer valid whether due to expiration (generally five to fifteen minutes) or a configuration change on the server, the server will respond with a 410 ResourceExpired error together with a continue token. If the client needs a consistent list, it must restart their list without the continue field. Otherwise, the client may send another list request with the token received with the 410 error, the server will respond with a list starting from the next key, but from the latest snapshot, which is inconsistent from the previous list results - objects that are created, modified, or deleted after the first list request will be included in the response, as long as their keys are after the \""next key\"".  This field is not supported when watch is true. Clients may start a watch from the last resourceVersion value returned by the server and not miss any modifications.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param int limit: limit is a maximum number of responses to return for a list call. If more items exist, the server will set the `continue` field on the list metadata to a value that can be used with the same initial query to retrieve the next set of results. Setting a limit may return fewer than the requested amount of items (up to zero items) in the event all requested objects are filtered out and clients should only use the presence of the continue field to determine whether more results are available. Servers may choose not to support the limit argument and will return all of the available results. If limit is specified and the continue field is empty, clients may assume that no more results are available. This field is not supported if watch is true.  The server guarantees that the objects returned when using continue will be identical to issuing a single list call without a limit - that is, no objects created, modified, or deleted after the first request is issued will be included in any subsequent continued requests. This is sometimes referred to as a consistent snapshot, and ensures that a client that is using limit to receive smaller chunks of a very large result can ensure they see all possible objects. If objects are updated during a chunked list the version of the object that was present at the time the first list result was calculated is returned.
        :param str resource_version: When specified with a watch call, shows changes that occur after that particular version of a resource. Defaults to changes from the beginning of history. When specified for list: - if unset, then the result is returned from remote storage based on quorum-read flag; - if it's 0, then we simply return what we currently have in cache, no guarantee; - if set to non zero, then the result is at least as fresh as given rv.
        :param int timeout_seconds: Timeout for the list/watch call. This limits the duration of the call, regardless of any activity or inactivity.
        :param bool watch: Watch for changes to the described resources and return them as a stream of add, update, and remove notifications. Specify resourceVersion.
        :return: V1Status
                 If the method is called asynchronously,
                 returns the request thread.
        """"""
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_collection_cluster_role_binding_with_http_info(**kwargs)
        else:
            (data) = self.delete_collection_cluster_role_binding_with_http_info(**kwargs)
            return data"
clear_rtag;"def clear_rtag(opts):
    '''
    Remove the rtag file
    '''
    try:
        os.remove(rtag(opts))
    except OSError as exc:
        if exc.errno != errno.ENOENT:
            # Using __str__() here to get the fully-formatted error message
            # (error number, error message, path)
            log.warning('Encountered error removing rtag: %s', exc.__str__())"
create_swap_disk;"def create_swap_disk(vm_, linode_id, swap_size=None):
    r'''
    Creates the disk for the specified Linode.

    vm\_
        The VM profile to create the swap disk for.

    linode_id
        The ID of the Linode to create the swap disk for.

    swap_size
        The size of the disk, in MB.
    '''
    kwargs = {}

    if not swap_size:
        swap_size = get_swap_size(vm_)

    kwargs.update({'LinodeID': linode_id,
                   'Label': vm_['name'],
                   'Type': 'swap',
                   'Size': swap_size
                  })

    result = _query('linode', 'disk.create', args=kwargs)

    return _clean_data(result)"
get_value;"def get_value(self, index):
        """"""Return current value""""""
        if index.column() == 0:
            return self.keys[ index.row() ]
        elif index.column() == 1:
            return self.types[ index.row() ]
        elif index.column() == 2:
            return self.sizes[ index.row() ]
        else:
            return self._data[ self.keys[index.row()] ]"
begin_transaction;"def begin_transaction(self, project_id, transaction_options=None):
        """"""Perform a ``beginTransaction`` request.

        :type project_id: str
        :param project_id: The project to connect to. This is
                           usually your project name in the cloud console.

        :type transaction_options: ~.datastore_v1.types.TransactionOptions
        :param transaction_options: (Optional) Options for a new transaction.

        :rtype: :class:`.datastore_pb2.BeginTransactionResponse`
        :returns: The returned protobuf response object.
        """"""
        request_pb = _datastore_pb2.BeginTransactionRequest()
        return _rpc(
            self.client._http,
            project_id,
            ""beginTransaction"",
            self.client._base_url,
            request_pb,
            _datastore_pb2.BeginTransactionResponse,
        )"
get_all_params;"def get_all_params(cls):
        """"""
        Compiles and returns all parameters for all :py:class:`Task`.

        :return: a generator of tuples (TODO: we should make this more elegant)
        """"""
        for task_name, task_cls in six.iteritems(cls._get_reg()):
            if task_cls == cls.AMBIGUOUS_CLASS:
                continue
            for param_name, param_obj in task_cls.get_params():
                yield task_name, (not task_cls.use_cmdline_section), param_name, param_obj"
check_input_files_for_numbered_seq;"def check_input_files_for_numbered_seq(sourceDir, suffix, containers):
    """"""Check if files, used as input when pre-processing MPL-containers in their numbered form, need fixing.""""""
    # Check input files for each MPL-container type.
    for container in containers:
        files = glob.glob( os.path.join( sourceDir, container, container + '*' + suffix ) )
        for currentFile in sorted( files ):
            if check_header_comment( currentFile ):
                return True
    return False"
add_tags;"def add_tags(self, *tags):
        """"""
        Add a list of strings to the statement as tags.
        (Overrides the method from StatementMixin)
        """"""
        for _tag in tags:
            self.tags.get_or_create(name=_tag)"
decode_message;"def decode_message(self, message, is_key=False):
        """"""
        Decode a message from kafka that has been encoded for use with
        the schema registry.
        :param str|bytes or None message: message key or value to be decoded
        :returns: Decoded message contents.
        :rtype dict:
        """"""

        if message is None:
            return None

        if len(message) <= 5:
            raise SerializerError(""message is too small to decode"")

        with ContextStringIO(message) as payload:
            magic, schema_id = struct.unpack('>bI', payload.read(5))
            if magic != MAGIC_BYTE:
                raise SerializerError(""message does not start with magic byte"")
            decoder_func = self._get_decoder_func(schema_id, payload, is_key)
            return decoder_func(payload)"
clear_header;"def clear_header(self, name: str) -> None:
        """"""Clears an outgoing header, undoing a previous `set_header` call.

        Note that this method does not apply to multi-valued headers
        set by `add_header`.
        """"""
        if name in self._headers:
            del self._headers[name]"
read_nonblocking;"def read_nonblocking(self, size=1, timeout=None):
        """"""This reads data from the file descriptor.

        This is a simple implementation suitable for a regular file. Subclasses using ptys or pipes should override it.

        The timeout parameter is ignored.
        """"""

        try:
            s = os.read(self.child_fd, size)
        except OSError as err:
            if err.args[0] == errno.EIO:
                # Linux-style EOF
                self.flag_eof = True
                raise EOF('End Of File (EOF). Exception style platform.')
            raise
        if s == b'':
            # BSD-style EOF
            self.flag_eof = True
            raise EOF('End Of File (EOF). Empty string style platform.')

        s = self._decoder.decode(s, final=False)
        self._log(s, 'read')
        return s"
update_model;"def update_model(self, model, fields, retry=DEFAULT_RETRY):
        """"""[Beta] Change some fields of a model.

        Use ``fields`` to specify which fields to update. At least one field
        must be provided. If a field is listed in ``fields`` and is ``None``
        in ``model``, it will be deleted.

        If ``model.etag`` is not ``None``, the update will only succeed if
        the model on the server has the same ETag. Thus reading a model with
        ``get_model``, changing its fields, and then passing it to
        ``update_model`` will ensure that the changes will only be saved if
        no modifications to the model occurred since the read.

        Args:
            model (google.cloud.bigquery.model.Model): The model to update.
            fields (Sequence[str]):
                The fields of ``model`` to change, spelled as the Model
                properties (e.g. ""friendly_name"").
            retry (google.api_core.retry.Retry):
                (Optional) A description of how to retry the API call.

        Returns:
            google.cloud.bigquery.model.Model:
                The model resource returned from the API call.
        """"""
        partial = model._build_resource(fields)
        if model.etag:
            headers = {""If-Match"": model.etag}
        else:
            headers = None
        api_response = self._call_api(
            retry, method=""PATCH"", path=model.path, data=partial, headers=headers
        )
        return Model.from_api_repr(api_response)"
resolve_name;"def resolve_name(name, arch, osarch=None):
    '''
    Resolve the package name and arch into a unique name referred to by salt.
    For example, on a 64-bit OS, a 32-bit package will be pkgname.i386.
    '''
    if osarch is None:
        osarch = get_osarch()

    if not check_32(arch, osarch) and arch not in (ARCHMAP.get(osarch, osarch), 'noarch'):
        name += '.{0}'.format(arch)
    return name"
wrap_inference_results;"def wrap_inference_results(inference_result_proto):
  """"""Returns packaged inference results from the provided proto.

  Args:
    inference_result_proto: The classification or regression response proto.

  Returns:
    An InferenceResult proto with the result from the response.
  """"""
  inference_proto = inference_pb2.InferenceResult()
  if isinstance(inference_result_proto,
                classification_pb2.ClassificationResponse):
    inference_proto.classification_result.CopyFrom(
        inference_result_proto.result)
  elif isinstance(inference_result_proto, regression_pb2.RegressionResponse):
    inference_proto.regression_result.CopyFrom(inference_result_proto.result)
  return inference_proto"
create_network;"def create_network(batch_size, update_freq):
    """"""Create a linear regression network for performing SVRG optimization.
    :return: an instance of mx.io.NDArrayIter
    :return: an instance of mx.mod.svrgmodule for performing SVRG optimization
    """"""
    head = '%(asctime)-15s %(message)s'
    logging.basicConfig(level=logging.INFO, format=head)
    data = np.random.randint(1, 5, [1000, 2])

    #Test_Train data split
    n_train = int(data.shape[0] * 0.8)
    weights = np.array([1.0, 2.0])
    label = data.dot(weights)

    di = mx.io.NDArrayIter(data[:n_train, :], label[:n_train], batch_size=batch_size, shuffle=True, label_name='lin_reg_label')
    val_iter = mx.io.NDArrayIter(data[n_train:, :], label[n_train:], batch_size=batch_size)

    X = mx.sym.Variable('data')
    Y = mx.symbol.Variable('lin_reg_label')
    fully_connected_layer = mx.sym.FullyConnected(data=X, name='fc1', num_hidden=1)
    lro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=""lro"")

    mod = SVRGModule(
        symbol=lro,
        data_names=['data'],
        label_names=['lin_reg_label'], update_freq=update_freq, logger=logging)

    return di, val_iter, mod"
after_loop;"def after_loop(self, coro):
        """"""A function that also acts as a decorator to register a coroutine to be
        called after the loop finished running.

        Parameters
        ------------
        coro: :term:`py:awaitable`
            The coroutine to register after the loop finishes.

        Raises
        -------
        TypeError
            The function was not a coroutine.
        """"""

        if not (inspect.iscoroutinefunction(coro) or inspect.isawaitable(coro)):
            raise TypeError('Expected coroutine or awaitable, received {0.__name__!r}.'.format(type(coro)))

        self._after_loop = coro"
make_madry_ngpu;"def make_madry_ngpu(nb_classes=10, input_shape=(None, 28, 28, 1), **kwargs):
  """"""
  Create a multi-GPU model similar to Madry et al. (arXiv:1706.06083).
  """"""
  layers = [Conv2DnGPU(32, (5, 5), (1, 1), ""SAME""),
            ReLU(),
            MaxPool((2, 2), (2, 2), ""SAME""),
            Conv2DnGPU(64, (5, 5), (1, 1), ""SAME""),
            ReLU(),
            MaxPool((2, 2), (2, 2), ""SAME""),
            Flatten(),
            LinearnGPU(1024),
            ReLU(),
            LinearnGPU(nb_classes),
            Softmax()]

  model = MLPnGPU(nb_classes, layers, input_shape)
  return model"
ext_pillar;"def ext_pillar(minion_id,
               pillar,  # pylint: disable=W0613
               url,
               with_grains=False):
    '''
    Read pillar data from HTTP response.

    :param str url: Url to request.
    :param bool with_grains: Whether to substitute strings in the url with their grain values.

    :return: A dictionary of the pillar data to add.
    :rtype: dict
    '''

    url = url.replace('%s', _quote(minion_id))

    grain_pattern = r'<(?P<grain_name>.*?)>'

    if with_grains:
        # Get the value of the grain and substitute each grain
        # name for the url-encoded version of its grain value.
        for match in re.finditer(grain_pattern, url):
            grain_name = match.group('grain_name')
            grain_value = __salt__['grains.get'](grain_name, None)

            if not grain_value:
                log.error(""Unable to get minion '%s' grain: %s"", minion_id, grain_name)
                return {}

            grain_value = _quote(six.text_type(grain_value))
            url = re.sub('<{0}>'.format(grain_name), grain_value, url)

    log.debug('Getting url: %s', url)
    data = __salt__['http.query'](url=url, decode=True, decode_type='json')

    if 'dict' in data:
        return data['dict']

    log.error(""Error on minion '%s' http query: %s\nMore Info:\n"", minion_id, url)

    for key in data:
        log.error('%s: %s', key, data[key])

    return {}"
avg_pooling;"def avg_pooling(attrs, inputs, proto_obj):
    """""" Average pooling""""""
    new_attrs = translation_utils._fix_attribute_names(attrs,
                                                       {'kernel_shape': 'kernel',
                                                        'strides': 'stride',
                                                        'pads': 'pad',
                                                       })
    new_attrs = translation_utils._add_extra_attributes(new_attrs,
                                                        {'pooling_convention': 'valid'
                                                        })
    new_op = translation_utils._fix_pooling('avg', inputs, new_attrs)

    return new_op, new_attrs, inputs"
get_dtype_counts;"def get_dtype_counts(self):
        """"""Get the counts of dtypes in this object.

        Returns:
            The counts of dtypes in this object.
        """"""
        if hasattr(self, ""dtype""):
            return pandas.Series({str(self.dtype): 1})
        result = self.dtypes.value_counts()
        result.index = result.index.map(lambda x: str(x))
        return result"
remove_column;"def remove_column(self, column_name, inplace=False):
        """"""
        Returns an SFrame with a column removed.

        If inplace == False (default) this operation does not modify the
        current SFrame, returning a new SFrame.

        If inplace == True, this operation modifies the current
        SFrame, returning self.

        Parameters
        ----------
        column_name : string
            The name of the column to remove.

        inplace : bool, optional. Defaults to False.
            Whether the SFrame is modified in place.

        Returns
        -------
        out : SFrame
            The SFrame with given column removed.

        Examples
        --------
        >>> sf = turicreate.SFrame({'id': [1, 2, 3], 'val': ['A', 'B', 'C']})
        >>> # This is equivalent to `del sf['val']`
        >>> res = sf.remove_column('val')
        >>> res
        +----+
        | id |
        +----+
        | 1  |
        | 2  |
        | 3  |
        +----+
        [3 rows x 1 columns]
        """"""
        column_name = str(column_name)
        if column_name not in self.column_names():
            raise KeyError('Cannot find column %s' % column_name)
        colid = self.column_names().index(column_name)

        if inplace:
            ret = self
        else:
            ret = self.copy()

        with cython_context():
            ret.__proxy__.remove_column(colid)

        ret._cache = None
        return ret"
get_authenticated_user;"async def get_authenticated_user(
        self, http_client: httpclient.AsyncHTTPClient = None
    ) -> Dict[str, Any]:
        """"""Gets the OAuth authorized user and access token.

        This method should be called from the handler for your
        OAuth callback URL to complete the registration process. We run the
        callback with the authenticated user dictionary.  This dictionary
        will contain an ``access_key`` which can be used to make authorized
        requests to this service on behalf of the user.  The dictionary will
        also contain other fields such as ``name``, depending on the service
        used.

        .. versionchanged:: 6.0

           The ``callback`` argument was removed. Use the returned
           awaitable object instead.
        """"""
        handler = cast(RequestHandler, self)
        request_key = escape.utf8(handler.get_argument(""oauth_token""))
        oauth_verifier = handler.get_argument(""oauth_verifier"", None)
        request_cookie = handler.get_cookie(""_oauth_request_token"")
        if not request_cookie:
            raise AuthError(""Missing OAuth request token cookie"")
        handler.clear_cookie(""_oauth_request_token"")
        cookie_key, cookie_secret = [
            base64.b64decode(escape.utf8(i)) for i in request_cookie.split(""|"")
        ]
        if cookie_key != request_key:
            raise AuthError(""Request token does not match cookie"")
        token = dict(
            key=cookie_key, secret=cookie_secret
        )  # type: Dict[str, Union[str, bytes]]
        if oauth_verifier:
            token[""verifier""] = oauth_verifier
        if http_client is None:
            http_client = self.get_auth_http_client()
        assert http_client is not None
        response = await http_client.fetch(self._oauth_access_token_url(token))
        access_token = _oauth_parse_response(response.body)
        user = await self._oauth_get_user_future(access_token)
        if not user:
            raise AuthError(""Error getting user"")
        user[""access_token""] = access_token
        return user"
get_logits;"def get_logits(self, x, **kwargs):
    """"""
    :param x: A symbolic representation (Tensor) of the network input
    :return: A symbolic representation (Tensor) of the output logits
    (i.e., the values fed as inputs to the softmax layer).
    """"""
    outputs = self.fprop(x, **kwargs)
    if self.O_LOGITS in outputs:
      return outputs[self.O_LOGITS]
    raise NotImplementedError(str(type(self)) + ""must implement `get_logits`""
                              "" or must define a "" + self.O_LOGITS +
                              "" output in `fprop`"")"
parse_class_names;"def parse_class_names(args):
    """""" parse # classes and class_names if applicable """"""
    num_class = args.num_class
    if len(args.class_names) > 0:
        if os.path.isfile(args.class_names):
            # try to open it to read class names
            with open(args.class_names, 'r') as f:
                class_names = [l.strip() for l in f.readlines()]
        else:
            class_names = [c.strip() for c in args.class_names.split(',')]
        assert len(class_names) == num_class, str(len(class_names))
        for name in class_names:
            assert len(name) > 0
    else:
        class_names = None
    return class_names"
create_table;"def create_table(self, engine):
        """"""
        Override to provide code for creating the target table.

        By default it will be created using types specified in columns.
        If the table exists, then it binds to the existing table.

        If overridden, use the provided connection object for setting up the table in order to
        create the table and insert data using the same transaction.
        :param engine: The sqlalchemy engine instance
        :type engine: object
        """"""
        def construct_sqla_columns(columns):
            retval = [sqlalchemy.Column(*c[0], **c[1]) for c in columns]
            return retval

        needs_setup = (len(self.columns) == 0) or (False in [len(c) == 2 for c in self.columns]) if not self.reflect else False
        if needs_setup:
            # only names of columns specified, no types
            raise NotImplementedError(""create_table() not implemented for %r and columns types not specified"" % self.table)
        else:
            # if columns is specified as (name, type) tuples
            with engine.begin() as con:

                if self.schema:
                    metadata = sqlalchemy.MetaData(schema=self.schema)
                else:
                    metadata = sqlalchemy.MetaData()

                try:
                    if not con.dialect.has_table(con, self.table, self.schema or None):
                        sqla_columns = construct_sqla_columns(self.columns)
                        self.table_bound = sqlalchemy.Table(self.table, metadata, *sqla_columns)
                        metadata.create_all(engine)
                    else:
                        full_table = '.'.join([self.schema, self.table]) if self.schema else self.table
                        metadata.reflect(only=[self.table], bind=engine)
                        self.table_bound = metadata.tables[full_table]
                except Exception as e:
                    self._logger.exception(self.table + str(e))"
make_aliased_type;"def make_aliased_type(cls, other_base):
        """"""
        Factory for making Aliased{Filter,Factor,Classifier}.
        """"""
        docstring = dedent(
            """"""
            A {t} that names another {t}.

            Parameters
            ----------
            term : {t}
            {{name}}
            """"""
        ).format(t=other_base.__name__)

        doc = format_docstring(
            owner_name=other_base.__name__,
            docstring=docstring,
            formatters={'name': PIPELINE_ALIAS_NAME_DOC},
        )

        return type(
            'Aliased' + other_base.__name__,
            (cls, other_base),
            {'__doc__': doc,
             '__module__': other_base.__module__},
        )"
unjoin_domain;"def unjoin_domain(username=None,
                  password=None,
                  domain=None,
                  workgroup='WORKGROUP',
                  disable=False,
                  restart=False):
    # pylint: disable=anomalous-backslash-in-string
    '''
    Unjoin a computer from an Active Directory Domain. Requires a restart.

    Args:

        username (str):
            Username of an account which is authorized to manage computer
            accounts on the domain. Needs to be a fully qualified name like
            ``user@domain.tld`` or ``domain.tld\\user``. If the domain is not
            specified, the passed domain will be used. If the computer account
            doesn't need to be disabled after the computer is unjoined, this can
            be ``None``.

        password (str):
            The password of the specified user

        domain (str):
            The domain from which to unjoin the computer. Can be ``None``

        workgroup (str):
            The workgroup to join the computer to. Default is ``WORKGROUP``

            .. versionadded:: 2015.8.2/2015.5.7

        disable (bool):
            ``True`` to disable the computer account in Active Directory.
            Default is ``False``

        restart (bool):
            ``True`` will restart the computer after successful unjoin. Default
            is ``False``

            .. versionadded:: 2015.8.2/2015.5.7

    Returns:
        dict: Returns a dictionary if successful, otherwise ``False``

    CLI Example:

    .. code-block:: bash

        salt 'minion-id' system.unjoin_domain restart=True

        salt 'minion-id' system.unjoin_domain username='unjoinuser' \\
                         password='unjoinpassword' disable=True \\
                         restart=True
    '''
    # pylint: enable=anomalous-backslash-in-string
    if six.PY2:
        username = _to_unicode(username)
        password = _to_unicode(password)
        domain = _to_unicode(domain)

    status = get_domain_workgroup()
    if 'Workgroup' in status:
        if status['Workgroup'] == workgroup:
            return 'Already joined to {0}'.format(workgroup)

    if username and '\\' not in username and '@' not in username:
        if domain:
            username = '{0}@{1}'.format(username, domain)
        else:
            return 'Must specify domain if not supplied in username'

    if username and password is None:
        return 'Must specify a password if you pass a username'

    NETSETUP_ACCT_DELETE = 0x4  # pylint: disable=invalid-name

    unjoin_options = 0x0
    if disable:
        unjoin_options |= NETSETUP_ACCT_DELETE

    with salt.utils.winapi.Com():
        conn = wmi.WMI()
    comp = conn.Win32_ComputerSystem()[0]
    err = comp.UnjoinDomainOrWorkgroup(Password=password,
                                       UserName=username,
                                       FUnjoinOptions=unjoin_options)

    # you have to do this because UnjoinDomainOrWorkgroup returns a
    # strangely formatted value that looks like (0,)
    if not err[0]:
        err = comp.JoinDomainOrWorkgroup(Name=workgroup)
        if not err[0]:
            ret = {'Workgroup': workgroup,
                   'Restart': False}
            if restart:
                ret['Restart'] = reboot()

            return ret
        else:
            log.error(win32api.FormatMessage(err[0]).rstrip())
            log.error('Failed to join the computer to %s', workgroup)
            return False
    else:
        log.error(win32api.FormatMessage(err[0]).rstrip())
        log.error('Failed to unjoin computer from %s', status['Domain'])
        return False"
client_list_entries;"def client_list_entries(client, to_delete):  # pylint: disable=unused-argument
    """"""List entries via client.""""""

    # [START client_list_entries_default]
    for entry in client.list_entries():  # API call(s)
        do_something_with(entry)
    # [END client_list_entries_default]

    # [START client_list_entries_filter]
    FILTER = ""logName:log_name AND textPayload:simple""
    for entry in client.list_entries(filter_=FILTER):  # API call(s)
        do_something_with(entry)
    # [END client_list_entries_filter]

    # [START client_list_entries_order_by]
    from google.cloud.logging import DESCENDING

    for entry in client.list_entries(order_by=DESCENDING):  # API call(s)
        do_something_with(entry)
    # [END client_list_entries_order_by]

    # [START client_list_entries_paged]
    iterator = client.list_entries()
    pages = iterator.pages

    page1 = next(pages)  # API call
    for entry in page1:
        do_something_with(entry)

    page2 = next(pages)  # API call
    for entry in page2:
        do_something_with(entry)"
import_and_get_task;"def import_and_get_task(task_path):
    """"""
    Given a modular path to a function, import that module
    and return the function.
    """"""
    module, function = task_path.rsplit('.', 1)
    app_module = importlib.import_module(module)
    app_function = getattr(app_module, function)
    return app_function"
find_best_candidate;"def find_best_candidate(
        self,
        package_name,  # type: str
        target_package_version=None,  # type:  Union[str, None]
        allow_prereleases=False,  # type: bool
    ):  # type: (...) -> Union[Package, bool]
        """"""
        Given a package name and optional version,
        returns the latest Package that matches
        """"""
        if target_package_version:
            constraint = parse_constraint(target_package_version)
        else:
            constraint = parse_constraint(""*"")

        candidates = self._pool.find_packages(
            package_name, constraint, allow_prereleases=allow_prereleases
        )

        if not candidates:
            return False

        dependency = Dependency(package_name, constraint)

        # Select highest version if we have many
        package = candidates[0]
        for candidate in candidates:
            if candidate.is_prerelease() and not dependency.allows_prereleases():
                continue

            # Select highest version of the two
            if package.version < candidate.version:
                package = candidate

        return package"
delete_dataset;"def delete_dataset(self, dataset, delete_nonempty=True):
        """"""Deletes a dataset (and optionally any tables in it), if it exists.

           :param dataset:
           :type dataset: BQDataset
           :param delete_nonempty: if true, will delete any tables before deleting the dataset
        """"""

        if not self.dataset_exists(dataset):
            return

        self.client.datasets().delete(projectId=dataset.project_id,
                                      datasetId=dataset.dataset_id,
                                      deleteContents=delete_nonempty).execute()"
current_hold_price;"def current_hold_price(self):
        """"""计算目前持仓的成本  用于模拟盘和实盘查询

        Returns:
            [type] -- [description]
        """"""
        
        def weights(x):
            n=len(x)
            res=1
            while res>0 or res<0:
                res=sum(x[:n]['amount'])
                n=n-1
            
            x=x[n+1:]     
            
            if sum(x['amount']) != 0:
                return np.average(
                    x['price'],
                    weights=x['amount'],
                    returned=True
                )
            else:
                return np.nan
        return self.history_table.set_index(
            'datetime',
            drop=False
        ).sort_index().groupby('code').apply(weights).dropna()"
update_model;"def update_model(client, model_id):
    """"""Sample ID: go/samples-tracker/1533""""""

    # [START bigquery_update_model_description]
    from google.cloud import bigquery

    # TODO(developer): Construct a BigQuery client object.
    # client = bigquery.Client()

    # TODO(developer): Set model_id to the ID of the model to fetch.
    # model_id = 'your-project.your_dataset.your_model'

    model = client.get_model(model_id)
    model.description = ""This model was modified from a Python program.""
    model = client.update_model(model, [""description""])

    full_model_id = ""{}.{}.{}"".format(model.project, model.dataset_id, model.model_id)
    print(
        ""Updated model '{}' with description '{}'."".format(
            full_model_id, model.description
        )
    )"
replace_route_table_association;"def replace_route_table_association(association_id, route_table_id, region=None, key=None, keyid=None, profile=None):
    '''
    Replaces a route table association.

    CLI Example:

    .. code-block:: bash

        salt myminion boto_vpc.replace_route_table_association 'rtbassoc-d8ccddba' 'rtb-1f382e7d'

    '''

    try:
        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
        association_id = conn.replace_route_table_association_with_assoc(association_id, route_table_id)
        log.info('Route table %s was reassociated with association id %s',
                 route_table_id, association_id)
        return {'replaced': True, 'association_id': association_id}
    except BotoServerError as e:
        return {'replaced': False, 'error': __utils__['boto.get_error'](e)}"
database_admin_api;"def database_admin_api(self):
        """"""Helper for session-related API calls.""""""
        if self._database_admin_api is None:
            self._database_admin_api = DatabaseAdminClient(
                credentials=self.credentials, client_info=_CLIENT_INFO
            )
        return self._database_admin_api"
