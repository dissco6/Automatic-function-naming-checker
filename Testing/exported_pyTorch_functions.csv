name;code
_get_package_path;"def _get_package_path(package_name):
    spec = importlib.util.find_spec(package_name)
    if spec:
        # The package might be a namespace package, so get_data may fail
        try:
            loader = spec.loader
            if loader is not None:
                file_path = loader.get_filename()  # type: ignore[attr-defined]
                return os.path.dirname(file_path)
        except AttributeError:
            pass
    return None"
get_submodule_folders;"def get_submodule_folders():
    git_modules_path = os.path.join(cwd, "".gitmodules"")
    default_modules_path = [
        os.path.join(third_party_path, name)
        for name in [
            ""gloo"",
            ""cpuinfo"",
            ""onnx"",
            ""fbgemm"",
            ""cutlass"",
        ]
    ]
    if not os.path.exists(git_modules_path):
        return default_modules_path
    with open(git_modules_path) as f:
        return [
            os.path.join(cwd, line.split(""="", 1)[1].strip())
            for line in f
            if line.strip().startswith(""path"")
        ]"
check_submodules;"def check_submodules():
    def check_for_files(folder, files):
        if not any(os.path.exists(os.path.join(folder, f)) for f in files):
            report(""Could not find any of {} in {}"".format("", "".join(files), folder))
            report(""Did you run 'git submodule update --init --recursive'?"")
            sys.exit(1)

    def not_exists_or_empty(folder):
        return not os.path.exists(folder) or (
            os.path.isdir(folder) and len(os.listdir(folder)) == 0
        )

    if bool(os.getenv(""USE_SYSTEM_LIBS"", False)):
        return
    folders = get_submodule_folders()
    # If none of the submodule folders exists, try to initialize them
    if all(not_exists_or_empty(folder) for folder in folders):
        try:
            report("" --- Trying to initialize submodules"")
            start = time.time()
            subprocess.check_call(
                [""git"", ""submodule"", ""update"", ""--init"", ""--recursive""], cwd=cwd
            )
            end = time.time()
            report(f"" --- Submodule initialization took {end - start:.2f} sec"")
        except Exception:
            report("" --- Submodule initalization failed"")
            report(""Please run:\n\tgit submodule update --init --recursive"")
            sys.exit(1)
    for folder in folders:
        check_for_files(
            folder,
            [
                ""CMakeLists.txt"",
                ""Makefile"",
                ""setup.py"",
                ""LICENSE"",
                ""LICENSE.md"",
                ""LICENSE.txt"",
            ],
        )
    check_for_files(
        os.path.join(third_party_path, ""fbgemm"", ""third_party"", ""asmjit""),
        [""CMakeLists.txt""],
    )"
mirror_files_into_torchgen;"def mirror_files_into_torchgen():
    # (new_path, orig_path)
    # Directories are OK and are recursively mirrored.
    paths = [
        (
            ""torchgen/packaged/ATen/native/native_functions.yaml"",
            ""aten/src/ATen/native/native_functions.yaml"",
        ),
        (""torchgen/packaged/ATen/native/tags.yaml"", ""aten/src/ATen/native/tags.yaml""),
        (""torchgen/packaged/ATen/templates"", ""aten/src/ATen/templates""),
        (""torchgen/packaged/autograd"", ""tools/autograd""),
        (""torchgen/packaged/autograd/templates"", ""tools/autograd/templates""),
    ]
    for new_path, orig_path in paths:
        # Create the dirs involved in new_path if they don't exist
        if not os.path.exists(new_path):
            os.makedirs(os.path.dirname(new_path), exist_ok=True)

        # Copy the files from the orig location to the new location
        if os.path.isfile(orig_path):
            shutil.copyfile(orig_path, new_path)
            continue
        if os.path.isdir(orig_path):
            if os.path.exists(new_path):
                # copytree fails if the tree exists already, so remove it.
                shutil.rmtree(new_path)
            shutil.copytree(orig_path, new_path)
            continue
        raise RuntimeError(""Check the file paths in `mirror_files_into_torchgen()`"")"
build_deps;"def build_deps():
    report(""-- Building version "" + version)
    check_submodules()
    check_pydep(""yaml"", ""pyyaml"")
    build_python = not BUILD_LIBTORCH_WHL
    build_pytorch(
        version=version,
        cmake_python_library=cmake_python_library,
        build_python=build_python,
        rerun_cmake=RERUN_CMAKE,
        cmake_only=CMAKE_ONLY,
        cmake=cmake,
    )

    if CMAKE_ONLY:
        report(
            'Finished running cmake. Run ""ccmake build"" or '
            '""cmake-gui build"" to adjust build options and '
            '""python setup.py install"" to build.'
        )
        sys.exit()

    # Use copies instead of symbolic files.
    # Windows has very poor support for them.
    sym_files = [
        ""tools/shared/_utils_internal.py"",
        ""torch/utils/benchmark/utils/valgrind_wrapper/callgrind.h"",
        ""torch/utils/benchmark/utils/valgrind_wrapper/valgrind.h"",
    ]
    orig_files = [
        ""torch/_utils_internal.py"",
        ""third_party/valgrind-headers/callgrind.h"",
        ""third_party/valgrind-headers/valgrind.h"",
    ]
    for sym_file, orig_file in zip(sym_files, orig_files):
        same = False
        if os.path.exists(sym_file):
            if filecmp.cmp(sym_file, orig_file):
                same = True
            else:
                os.remove(sym_file)
        if not same:
            shutil.copyfile(orig_file, sym_file)"
check_pydep;"def check_pydep(importname, module):
    try:
        importlib.import_module(importname)
    except ImportError as e:
        raise RuntimeError(
            missing_pydep.format(importname=importname, module=module)
        ) from e"
get_cmake_cache_vars;"def get_cmake_cache_vars():
    try:
        return defaultdict(lambda: False, cmake.get_cmake_cache_variables())
    except FileNotFoundError:
        # CMakeCache.txt does not exist. Probably running ""python setup.py clean"" over a clean directory.
        return defaultdict(lambda: False)"
configure_extension_build;"def configure_extension_build():
    r""""""Configures extension build options according to system environment and user's choice.

    Returns:
      The input to parameters ext_modules, cmdclass, packages, and entry_points as required in setuptools.setup.
    """"""

    cmake_cache_vars = get_cmake_cache_vars()

    ################################################################################
    # Configure compile flags
    ################################################################################

    library_dirs = []
    extra_install_requires = []

    if IS_WINDOWS:
        # /NODEFAULTLIB makes sure we only link to DLL runtime
        # and matches the flags set for protobuf and ONNX
        extra_link_args = [""/NODEFAULTLIB:LIBCMT.LIB""]
        # /MD links against DLL runtime
        # and matches the flags set for protobuf and ONNX
        # /EHsc is about standard C++ exception handling
        extra_compile_args = [""/MD"", ""/FS"", ""/EHsc""]
    else:
        extra_link_args = []
        extra_compile_args = [
            ""-Wall"",
            ""-Wextra"",
            ""-Wno-strict-overflow"",
            ""-Wno-unused-parameter"",
            ""-Wno-missing-field-initializers"",
            ""-Wno-unknown-pragmas"",
            # Python 2.6 requires -fno-strict-aliasing, see
            # http://legacy.python.org/dev/peps/pep-3123/
            # We also depend on it in our code (even Python 3).
            ""-fno-strict-aliasing"",
        ]

    library_dirs.append(lib_path)

    main_compile_args = []
    main_libraries = [""torch_python""]

    main_link_args = []
    main_sources = [""torch/csrc/stub.c""]

    if BUILD_LIBTORCH_WHL:
        main_libraries = [""torch""]
        main_sources = []

    if build_type.is_debug():
        if IS_WINDOWS:
            extra_compile_args.append(""/Z7"")
            extra_link_args.append(""/DEBUG:FULL"")
        else:
            extra_compile_args += [""-O0"", ""-g""]
            extra_link_args += [""-O0"", ""-g""]

    if build_type.is_rel_with_deb_info():
        if IS_WINDOWS:
            extra_compile_args.append(""/Z7"")
            extra_link_args.append(""/DEBUG:FULL"")
        else:
            extra_compile_args += [""-g""]
            extra_link_args += [""-g""]

    # pypi cuda package that requires installation of cuda runtime, cudnn and cublas
    # should be included in all wheels uploaded to pypi
    pytorch_extra_install_requirements = os.getenv(
        ""PYTORCH_EXTRA_INSTALL_REQUIREMENTS"", """"
    )
    if pytorch_extra_install_requirements:
        report(
            f""pytorch_extra_install_requirements: {pytorch_extra_install_requirements}""
        )
        extra_install_requires += pytorch_extra_install_requirements.split(""|"")

    # Cross-compile for M1
    if IS_DARWIN:
        macos_target_arch = os.getenv(""CMAKE_OSX_ARCHITECTURES"", """")
        if macos_target_arch in [""arm64"", ""x86_64""]:
            macos_sysroot_path = os.getenv(""CMAKE_OSX_SYSROOT"")
            if macos_sysroot_path is None:
                macos_sysroot_path = (
                    subprocess.check_output(
                        [""xcrun"", ""--show-sdk-path"", ""--sdk"", ""macosx""]
                    )
                    .decode(""utf-8"")
                    .strip()
                )
            extra_compile_args += [
                ""-arch"",
                macos_target_arch,
                ""-isysroot"",
                macos_sysroot_path,
            ]
            extra_link_args += [""-arch"", macos_target_arch]

    def make_relative_rpath_args(path):
        if IS_DARWIN:
            return [""-Wl,-rpath,@loader_path/"" + path]
        elif IS_WINDOWS:
            return []
        else:
            return [""-Wl,-rpath,$ORIGIN/"" + path]

    ################################################################################
    # Declare extensions and package
    ################################################################################

    extensions = []
    excludes = [""tools"", ""tools.*"", ""caffe2"", ""caffe2.*""]
    if not cmake_cache_vars[""BUILD_FUNCTORCH""]:
        excludes.extend([""functorch"", ""functorch.*""])
    packages = find_packages(exclude=excludes)
    C = Extension(
        ""torch._C"",
        libraries=main_libraries,
        sources=main_sources,
        language=""c"",
        extra_compile_args=main_compile_args + extra_compile_args,
        include_dirs=[],
        library_dirs=library_dirs,
        extra_link_args=extra_link_args
        + main_link_args
        + make_relative_rpath_args(""lib""),
    )
    extensions.append(C)

    # These extensions are built by cmake and copied manually in build_extensions()
    # inside the build_ext implementation
    if cmake_cache_vars[""BUILD_FUNCTORCH""]:
        extensions.append(
            Extension(name=""functorch._C"", sources=[]),
        )

    cmdclass = {
        ""bdist_wheel"": wheel_concatenate,
        ""build_ext"": build_ext,
        ""clean"": clean,
        ""install"": install,
        ""sdist"": sdist,
    }

    entry_points = {
        ""console_scripts"": [
            ""torchrun = torch.distributed.run:main"",
        ],
        ""torchrun.logs_specs"": [
            ""default = torch.distributed.elastic.multiprocessing:DefaultLogsSpecs"",
        ],
    }

    if cmake_cache_vars[""USE_DISTRIBUTED""]:
        # Only enable fr_trace command if distributed is enabled
        entry_points[""console_scripts""].append(
            ""torchfrtrace = tools.flight_recorder.fr_trace:main"",
        )
    return extensions, cmdclass, packages, entry_points, extra_install_requires"
check_for_files;"def check_for_files(folder, files):
        if not any(os.path.exists(os.path.join(folder, f)) for f in files):
            report(""Could not find any of {} in {}"".format("", "".join(files), folder))
            report(""Did you run 'git submodule update --init --recursive'?"")
            sys.exit(1)"
not_exists_or_empty;"def not_exists_or_empty(folder):
        return not os.path.exists(folder) or (
            os.path.isdir(folder) and len(os.listdir(folder)) == 0
        )"
_embed_libomp;"def _embed_libomp(self):
        # Copy libiomp5.dylib/libomp.dylib inside the wheel package on MacOS
        lib_dir = os.path.join(self.build_lib, ""torch"", ""lib"")
        libtorch_cpu_path = os.path.join(lib_dir, ""libtorch_cpu.dylib"")
        if not os.path.exists(libtorch_cpu_path):
            return
        # Parse libtorch_cpu load commands
        otool_cmds = (
            subprocess.check_output([""otool"", ""-l"", libtorch_cpu_path])
            .decode(""utf-8"")
            .split(""\n"")
        )
        rpaths, libs = [], []
        for idx, line in enumerate(otool_cmds):
            if line.strip() == ""cmd LC_LOAD_DYLIB"":
                lib_name = otool_cmds[idx + 2].strip()
                assert lib_name.startswith(""name "")
                libs.append(lib_name.split("" "", 1)[1].rsplit(""("", 1)[0][:-1])

            if line.strip() == ""cmd LC_RPATH"":
                rpath = otool_cmds[idx + 2].strip()
                assert rpath.startswith(""path "")
                rpaths.append(rpath.split("" "", 1)[1].rsplit(""("", 1)[0][:-1])

        omplib_path = get_cmake_cache_vars()[""OpenMP_libomp_LIBRARY""]
        omplib_name = get_cmake_cache_vars()[""OpenMP_C_LIB_NAMES""] + "".dylib""
        omplib_rpath_path = os.path.join(""@rpath"", omplib_name)

        # This logic is fragile and checks only two cases:
        # - libtorch_cpu depends on `@rpath/libomp.dylib`e (happens when built inside miniconda environment)
        # - libtorch_cpu depends on `/abs/path/to/libomp.dylib` (happens when built with libomp from homebrew)
        if not any(c in libs for c in [omplib_path, omplib_rpath_path]):
            return

        # Copy libomp/libiomp5 from rpath locations
        target_lib = os.path.join(self.build_lib, ""torch"", ""lib"", omplib_name)
        libomp_relocated = False
        for rpath in rpaths:
            source_lib = os.path.join(rpath, omplib_name)
            if not os.path.exists(source_lib):
                continue
            self.copy_file(source_lib, target_lib)
            # Delete old rpath and add @loader_lib to the rpath
            # This should prevent delocate from attempting to package another instance
            # of OpenMP library in torch wheel as well as loading two libomp.dylib into
            # the address space, as libraries are cached by their unresolved names
            install_name_tool_args = [
                ""-rpath"",
                rpath,
                ""@loader_path"",
            ]
            libomp_relocated = True
            break
        if not libomp_relocated and os.path.exists(omplib_path):
            self.copy_file(omplib_path, target_lib)
            install_name_tool_args = [
                ""-change"",
                omplib_path,
                omplib_rpath_path,
            ]
            if ""@loader_path"" not in rpaths:
                install_name_tool_args += [
                    ""-add_rpath"",
                    ""@loader_path"",
                ]
            libomp_relocated = True
        if libomp_relocated:
            install_name_tool_args.insert(0, ""install_name_tool"")
            install_name_tool_args.append(libtorch_cpu_path)
            subprocess.check_call(install_name_tool_args)
        # Copy omp.h from OpenMP_C_FLAGS and copy it into include folder
        omp_cflags = get_cmake_cache_vars()[""OpenMP_C_FLAGS""]
        if not omp_cflags:
            return
        for include_dir in [f[2:] for f in omp_cflags.split("" "") if f.startswith(""-I"")]:
            omp_h = os.path.join(include_dir, ""omp.h"")
            if not os.path.exists(omp_h):
                continue
            target_omp_h = os.path.join(self.build_lib, ""torch"", ""include"", ""omp.h"")
            self.copy_file(omp_h, target_omp_h)
            break"
run;"def run(self):
        # Report build options. This is run after the build completes so # `CMakeCache.txt` exists and we can get an
        # accurate report on what is used and what is not.
        cmake_cache_vars = defaultdict(lambda: False, cmake.get_cmake_cache_variables())
        if cmake_cache_vars[""USE_NUMPY""]:
            report(""-- Building with NumPy bindings"")
        else:
            report(""-- NumPy not found"")
        if cmake_cache_vars[""USE_CUDNN""]:
            report(
                ""-- Detected cuDNN at ""
                + cmake_cache_vars[""CUDNN_LIBRARY""]
                + "", ""
                + cmake_cache_vars[""CUDNN_INCLUDE_DIR""]
            )
        else:
            report(""-- Not using cuDNN"")
        if cmake_cache_vars[""USE_CUDA""]:
            report(""-- Detected CUDA at "" + cmake_cache_vars[""CUDA_TOOLKIT_ROOT_DIR""])
        else:
            report(""-- Not using CUDA"")
        if cmake_cache_vars[""USE_XPU""]:
            report(""-- Detected XPU runtime at "" + cmake_cache_vars[""SYCL_LIBRARY_DIR""])
        else:
            report(""-- Not using XPU"")
        if cmake_cache_vars[""USE_MKLDNN""]:
            report(""-- Using MKLDNN"")
            if cmake_cache_vars[""USE_MKLDNN_ACL""]:
                report(""-- Using Compute Library for the Arm architecture with MKLDNN"")
            else:
                report(
                    ""-- Not using Compute Library for the Arm architecture with MKLDNN""
                )
            if cmake_cache_vars[""USE_MKLDNN_CBLAS""]:
                report(""-- Using CBLAS in MKLDNN"")
            else:
                report(""-- Not using CBLAS in MKLDNN"")
        else:
            report(""-- Not using MKLDNN"")
        if cmake_cache_vars[""USE_NCCL""] and cmake_cache_vars[""USE_SYSTEM_NCCL""]:
            report(
                ""-- Using system provided NCCL library at {}, {}"".format(
                    cmake_cache_vars[""NCCL_LIBRARIES""],
                    cmake_cache_vars[""NCCL_INCLUDE_DIRS""],
                )
            )
        elif cmake_cache_vars[""USE_NCCL""]:
            report(""-- Building NCCL library"")
        else:
            report(""-- Not using NCCL"")
        if cmake_cache_vars[""USE_DISTRIBUTED""]:
            if IS_WINDOWS:
                report(""-- Building without distributed package"")
            else:
                report(""-- Building with distributed package: "")
                report(
                    ""  -- USE_TENSORPIPE={}"".format(cmake_cache_vars[""USE_TENSORPIPE""])
                )
                report(""  -- USE_GLOO={}"".format(cmake_cache_vars[""USE_GLOO""]))
                report(""  -- USE_MPI={}"".format(cmake_cache_vars[""USE_OPENMPI""]))
        else:
            report(""-- Building without distributed package"")
        if cmake_cache_vars[""STATIC_DISPATCH_BACKEND""]:
            report(
                ""-- Using static dispatch with backend {}"".format(
                    cmake_cache_vars[""STATIC_DISPATCH_BACKEND""]
                )
            )
        if cmake_cache_vars[""USE_LIGHTWEIGHT_DISPATCH""]:
            report(""-- Using lightweight dispatch"")
        if cmake_cache_vars[""BUILD_EXECUTORCH""]:
            report(""-- Building Executorch"")

        if cmake_cache_vars[""USE_ITT""]:
            report(""-- Using ITT"")
        else:
            report(""-- Not using ITT"")

        # Do not use clang to compile extensions if `-fstack-clash-protection` is defined
        # in system CFLAGS
        c_flags = str(os.getenv(""CFLAGS"", """"))
        if (
            IS_LINUX
            and ""-fstack-clash-protection"" in c_flags
            and ""clang"" in os.environ.get(""CC"", """")
        ):
            os.environ[""CC""] = str(os.environ[""CC""])

        # It's an old-style class in Python 2.7...
        setuptools.command.build_ext.build_ext.run(self)

        if IS_DARWIN:
            self._embed_libomp()

        # Copy the essential export library to compile C++ extensions.
        if IS_WINDOWS:
            build_temp = self.build_temp

            ext_filename = self.get_ext_filename(""_C"")
            lib_filename = ""."".join(ext_filename.split(""."")[:-1]) + "".lib""

            export_lib = os.path.join(
                build_temp, ""torch"", ""csrc"", lib_filename
            ).replace(""\\"", ""/"")

            build_lib = self.build_lib

            target_lib = os.path.join(build_lib, ""torch"", ""lib"", ""_C.lib"").replace(
                ""\\"", ""/""
            )

            # Create ""torch/lib"" directory if not exists.
            # (It is not created yet in ""develop"" mode.)
            target_dir = os.path.dirname(target_lib)
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)

            self.copy_file(export_lib, target_lib)"
build_extensions;"def build_extensions(self):
        self.create_compile_commands()

        # Copy functorch extension
        for i, ext in enumerate(self.extensions):
            if ext.name != ""functorch._C"":
                continue
            fullname = self.get_ext_fullname(ext.name)
            filename = self.get_ext_filename(fullname)
            fileext = os.path.splitext(filename)[1]
            src = os.path.join(os.path.dirname(filename), ""functorch"" + fileext)
            dst = os.path.join(os.path.realpath(self.build_lib), filename)
            if os.path.exists(src):
                report(f""Copying {ext.name} from {src} to {dst}"")
                dst_dir = os.path.dirname(dst)
                if not os.path.exists(dst_dir):
                    os.makedirs(dst_dir)
                self.copy_file(src, dst)

        setuptools.command.build_ext.build_ext.build_extensions(self)"
get_outputs;"def get_outputs(self):
        outputs = setuptools.command.build_ext.build_ext.get_outputs(self)
        outputs.append(os.path.join(self.build_lib, ""caffe2""))
        report(f""setup.py::get_outputs returning {outputs}"")
        return outputs"
create_compile_commands;"def create_compile_commands(self):
        def load(filename):
            with open(filename) as f:
                return json.load(f)

        ninja_files = glob.glob(""build/*compile_commands.json"")
        cmake_files = glob.glob(""torch/lib/build/*/compile_commands.json"")
        all_commands = [entry for f in ninja_files + cmake_files for entry in load(f)]

        # cquery does not like c++ compiles that start with gcc.
        # It forgets to include the c++ header directories.
        # We can work around this by replacing the gcc calls that python
        # setup.py generates with g++ calls instead
        for command in all_commands:
            if command[""command""].startswith(""gcc ""):
                command[""command""] = ""g++ "" + command[""command""][4:]

        new_contents = json.dumps(all_commands, indent=2)
        contents = """"
        if os.path.exists(""compile_commands.json""):
            with open(""compile_commands.json"") as f:
                contents = f.read()
        if contents != new_contents:
            with open(""compile_commands.json"", ""w"") as f:
                f.write(new_contents)"
initialize_options;"def initialize_options(self):
        pass"
finalize_options;"def finalize_options(self):
        pass"
make_relative_rpath_args;"def make_relative_rpath_args(path):
        if IS_DARWIN:
            return [""-Wl,-rpath,@loader_path/"" + path]
        elif IS_WINDOWS:
            return []
        else:
            return [""-Wl,-rpath,$ORIGIN/"" + path]"
load;"def load(filename):
            with open(filename) as f:
                return json.load(f)"
write_wheelfile;"def write_wheelfile(self, *args, **kwargs):
            super().write_wheelfile(*args, **kwargs)

            if BUILD_LIBTORCH_WHL:
                # Remove extraneneous files in the libtorch wheel
                for root, dirs, files in os.walk(self.bdist_dir):
                    for file in files:
                        if file.endswith(("".a"", "".so"")) and os.path.isfile(
                            os.path.join(self.bdist_dir, file)
                        ):
                            os.remove(os.path.join(root, file))
                        elif file.endswith("".py""):
                            os.remove(os.path.join(root, file))
                # need an __init__.py file otherwise we wouldn't have a package
                open(os.path.join(self.bdist_dir, ""torch"", ""__init__.py""), ""w"").close()"
list_dir;"def list_dir(path: str) -> list[str]:
    """"""'
    Helper for getting paths for Python
    """"""
    return check_output([""ls"", ""-1"", path]).decode().split(""\n"")"
build_ArmComputeLibrary;"def build_ArmComputeLibrary() -> None:
    """"""
    Using ArmComputeLibrary for aarch64 PyTorch
    """"""
    print(""Building Arm Compute Library"")
    acl_build_flags = [
        ""debug=0"",
        ""neon=1"",
        ""opencl=0"",
        ""os=linux"",
        ""openmp=1"",
        ""cppthreads=0"",
        ""arch=armv8a"",
        ""multi_isa=1"",
        ""fixed_format_kernels=1"",
        ""build=native"",
    ]
    acl_install_dir = ""/acl""
    acl_checkout_dir = ""ComputeLibrary""
    os.makedirs(acl_install_dir)
    check_call(
        [
            ""git"",
            ""clone"",
            ""https://github.com/ARM-software/ComputeLibrary.git"",
            ""-b"",
            ""v25.02"",
            ""--depth"",
            ""1"",
            ""--shallow-submodules"",
        ]
    )

    check_call(
        [""scons"", ""Werror=1"", ""-j8"", f""build_dir=/{acl_install_dir}/build""]
        + acl_build_flags,
        cwd=acl_checkout_dir,
    )
    for d in [""arm_compute"", ""include"", ""utils"", ""support"", ""src""]:
        shutil.copytree(f""{acl_checkout_dir}/{d}"", f""{acl_install_dir}/{d}"")"
update_wheel;"def update_wheel(wheel_path, desired_cuda) -> None:
    """"""
    Update the cuda wheel libraries
    """"""
    folder = os.path.dirname(wheel_path)
    wheelname = os.path.basename(wheel_path)
    os.mkdir(f""{folder}/tmp"")
    os.system(f""unzip {wheel_path} -d {folder}/tmp"")
    libs_to_copy = [
        ""/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.12"",
        ""/usr/local/cuda/lib64/libcudnn.so.9"",
        ""/usr/local/cuda/lib64/libcublas.so.12"",
        ""/usr/local/cuda/lib64/libcublasLt.so.12"",
        ""/usr/local/cuda/lib64/libcudart.so.12"",
        ""/usr/local/cuda/lib64/libcufft.so.11"",
        ""/usr/local/cuda/lib64/libcusparse.so.12"",
        ""/usr/local/cuda/lib64/libcusparseLt.so.0"",
        ""/usr/local/cuda/lib64/libcusolver.so.11"",
        ""/usr/local/cuda/lib64/libcurand.so.10"",
        ""/usr/local/cuda/lib64/libnvToolsExt.so.1"",
        ""/usr/local/cuda/lib64/libnvJitLink.so.12"",
        ""/usr/local/cuda/lib64/libnvrtc.so.12"",
        ""/usr/local/cuda/lib64/libcudnn_adv.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_cnn.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_graph.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_ops.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_engines_runtime_compiled.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_engines_precompiled.so.9"",
        ""/usr/local/cuda/lib64/libcudnn_heuristic.so.9"",
        ""/lib64/libgomp.so.1"",
        ""/usr/lib64/libgfortran.so.5"",
        ""/acl/build/libarm_compute.so"",
        ""/acl/build/libarm_compute_graph.so"",
    ]
    if enable_cuda:
        libs_to_copy += [
            ""/usr/local/lib/libnvpl_lapack_lp64_gomp.so.0"",
            ""/usr/local/lib/libnvpl_blas_lp64_gomp.so.0"",
            ""/usr/local/lib/libnvpl_lapack_core.so.0"",
            ""/usr/local/lib/libnvpl_blas_core.so.0"",
        ]
        if ""126"" in desired_cuda:
            libs_to_copy += [
                ""/usr/local/cuda/lib64/libnvrtc-builtins.so.12.6"",
                ""/usr/local/cuda/lib64/libcufile.so.0"",
                ""/usr/local/cuda/lib64/libcufile_rdma.so.1"",
            ]
        elif ""128"" in desired_cuda:
            libs_to_copy += [
                ""/usr/local/cuda/lib64/libnvrtc-builtins.so.12.8"",
                ""/usr/local/cuda/lib64/libcufile.so.0"",
                ""/usr/local/cuda/lib64/libcufile_rdma.so.1"",
            ]
    else:
        libs_to_copy += [
            ""/opt/OpenBLAS/lib/libopenblas.so.0"",
        ]
    # Copy libraries to unzipped_folder/a/lib
    for lib_path in libs_to_copy:
        lib_name = os.path.basename(lib_path)
        shutil.copy2(lib_path, f""{folder}/tmp/torch/lib/{lib_name}"")
        os.system(
            f""cd {folder}/tmp/torch/lib/; ""
            f""patchelf --set-rpath '$ORIGIN' --force-rpath {folder}/tmp/torch/lib/{lib_name}""
        )
    os.mkdir(f""{folder}/cuda_wheel"")
    os.system(f""cd {folder}/tmp/; zip -r {folder}/cuda_wheel/{wheelname} *"")
    shutil.move(
        f""{folder}/cuda_wheel/{wheelname}"",
        f""{folder}/{wheelname}"",
        copy_function=shutil.copy2,
    )
    os.system(f""rm -rf {folder}/tmp/ {folder}/cuda_wheel/"")"
complete_wheel;"def complete_wheel(folder: str) -> str:
    """"""
    Complete wheel build and put in artifact location
    """"""
    wheel_name = list_dir(f""/{folder}/dist"")[0]

    # Please note for cuda we don't run auditwheel since we use custom script to package
    # the cuda dependencies to the wheel file using update_wheel() method.
    # However we need to make sure filename reflects the correct Manylinux platform.
    if ""pytorch"" in folder and not enable_cuda:
        print(""Repairing Wheel with AuditWheel"")
        check_call([""auditwheel"", ""repair"", f""dist/{wheel_name}""], cwd=folder)
        repaired_wheel_name = list_dir(f""/{folder}/wheelhouse"")[0]

        print(f""Moving {repaired_wheel_name} wheel to /{folder}/dist"")
        os.rename(
            f""/{folder}/wheelhouse/{repaired_wheel_name}"",
            f""/{folder}/dist/{repaired_wheel_name}"",
        )
    else:
        repaired_wheel_name = wheel_name.replace(
            ""linux_aarch64"", ""manylinux_2_28_aarch64""
        )
        print(f""Renaming {wheel_name} wheel to {repaired_wheel_name}"")
        os.rename(
            f""/{folder}/dist/{wheel_name}"",
            f""/{folder}/dist/{repaired_wheel_name}"",
        )

    print(f""Copying {repaired_wheel_name} to artifacts"")
    shutil.copy2(
        f""/{folder}/dist/{repaired_wheel_name}"", f""/artifacts/{repaired_wheel_name}""
    )

    return repaired_wheel_name"
parse_arguments;"def parse_arguments():
    """"""
    Parse inline arguments
    """"""
    from argparse import ArgumentParser

    parser = ArgumentParser(""AARCH64 wheels python CD"")
    parser.add_argument(""--debug"", action=""store_true"")
    parser.add_argument(""--build-only"", action=""store_true"")
    parser.add_argument(""--test-only"", type=str)
    parser.add_argument(""--enable-mkldnn"", action=""store_true"")
    parser.add_argument(""--enable-cuda"", action=""store_true"")
    return parser.parse_args()"
compute_keyfile_path;"def compute_keyfile_path(key_name: Optional[str] = None) -> tuple[str, str]:
    if key_name is None:
        key_name = os.getenv(""AWS_KEY_NAME"")
        if key_name is None:
            return os.getenv(""SSH_KEY_PATH"", """"), """"

    homedir_path = os.path.expanduser(""~"")
    default_path = os.path.join(homedir_path, "".ssh"", f""{key_name}.pem"")
    return os.getenv(""SSH_KEY_PATH"", default_path), key_name"
ec2_get_instances;"def ec2_get_instances(filter_name, filter_value):
    return ec2.instances.filter(
        Filters=[{""Name"": filter_name, ""Values"": [filter_value]}]
    )"
ec2_instances_of_type;"def ec2_instances_of_type(instance_type=""t4g.2xlarge""):
    return ec2_get_instances(""instance-type"", instance_type)"
ec2_instances_by_id;"def ec2_instances_by_id(instance_id):
    rc = list(ec2_get_instances(""instance-id"", instance_id))
    return rc[0] if len(rc) > 0 else None"
start_instance;"def start_instance(
    key_name, ami=ubuntu20_04_ami, instance_type=""t4g.2xlarge"", ebs_size: int = 50
):
    inst = ec2.create_instances(
        ImageId=ami,
        InstanceType=instance_type,
        SecurityGroups=[""ssh-allworld""],
        KeyName=key_name,
        MinCount=1,
        MaxCount=1,
        BlockDeviceMappings=[
            {
                ""DeviceName"": ""/dev/sda1"",
                ""Ebs"": {
                    ""DeleteOnTermination"": True,
                    ""VolumeSize"": ebs_size,
                    ""VolumeType"": ""standard"",
                },
            }
        ],
    )[0]
    print(f""Create instance {inst.id}"")
    inst.wait_until_running()
    running_inst = ec2_instances_by_id(inst.id)
    print(f""Instance started at {running_inst.public_dns_name}"")
    return running_inst"
wait_for_connection;"def wait_for_connection(addr, port, timeout=15, attempt_cnt=5):
    import socket

    for i in range(attempt_cnt):
        try:
            with socket.create_connection((addr, port), timeout=timeout):
                return
        except (ConnectionRefusedError, socket.timeout):  # noqa: PERF203
            if i == attempt_cnt - 1:
                raise
            time.sleep(timeout)"
update_apt_repo;"def update_apt_repo(host: RemoteHost) -> None:
    time.sleep(5)
    host.run_cmd(""sudo systemctl stop apt-daily.service || true"")
    host.run_cmd(""sudo systemctl stop unattended-upgrades.service || true"")
    host.run_cmd(
        ""while systemctl is-active --quiet apt-daily.service; do sleep 1; done""
    )
    host.run_cmd(
        ""while systemctl is-active --quiet unattended-upgrades.service; do sleep 1; done""
    )
    host.run_cmd(""sudo apt-get update"")
    time.sleep(3)
    host.run_cmd(""sudo apt-get update"")"
install_condaforge;"def install_condaforge(
    host: RemoteHost, suffix: str = ""latest/download/Miniforge3-Linux-aarch64.sh""
) -> None:
    print(""Install conda-forge"")
    host.run_cmd(f""curl -OL https://github.com/conda-forge/miniforge/releases/{suffix}"")
    host.run_cmd(f""sh -f {os.path.basename(suffix)} -b"")
    host.run_cmd(f""rm -f {os.path.basename(suffix)}"")
    if host.using_docker():
        host.run_cmd(""echo 'PATH=$HOME/miniforge3/bin:$PATH'>>.bashrc"")
    else:
        host.run_cmd(
            [
                ""sed"",
                ""-i"",
                ""'/^# If not running interactively.*/i PATH=$HOME/miniforge3/bin:$PATH'"",
                "".bashrc"",
            ]
        )"
install_condaforge_python;"def install_condaforge_python(host: RemoteHost, python_version=""3.8"") -> None:
    if python_version == ""3.6"":
        # Python-3.6 EOLed and not compatible with conda-4.11
        install_condaforge(
            host, suffix=""download/4.10.3-10/Miniforge3-4.10.3-10-Linux-aarch64.sh""
        )
        host.run_cmd(f""conda install -y python={python_version} numpy pyyaml"")
    else:
        install_condaforge(
            host, suffix=""download/4.11.0-4/Miniforge3-4.11.0-4-Linux-aarch64.sh""
        )
        # Pytorch-1.10 or older are not compatible with setuptools=59.6 or newer
        host.run_cmd(
            f""conda install -y python={python_version} numpy pyyaml setuptools>=59.5.0""
        )"
build_OpenBLAS;"def build_OpenBLAS(host: RemoteHost, git_clone_flags: str = """") -> None:
    print(""Building OpenBLAS"")
    host.run_cmd(
        f""git clone https://github.com/xianyi/OpenBLAS -b v0.3.28 {git_clone_flags}""
    )
    make_flags = ""NUM_THREADS=64 USE_OPENMP=1 NO_SHARED=1 DYNAMIC_ARCH=1 TARGET=ARMV8""
    host.run_cmd(
        f""pushd OpenBLAS && make {make_flags} -j8 && sudo make {make_flags} install && popd && rm -rf OpenBLAS""
    )"
build_ArmComputeLibrary;"def build_ArmComputeLibrary(host: RemoteHost, git_clone_flags: str = """") -> None:
    print(""Building Arm Compute Library"")
    acl_build_flags = "" "".join(
        [
            ""debug=0"",
            ""neon=1"",
            ""opencl=0"",
            ""os=linux"",
            ""openmp=1"",
            ""cppthreads=0"",
            ""arch=armv8a"",
            ""multi_isa=1"",
            ""fixed_format_kernels=1"",
            ""build=native"",
        ]
    )
    host.run_cmd(
        f""git clone https://github.com/ARM-software/ComputeLibrary.git -b v25.02 {git_clone_flags}""
    )

    host.run_cmd(f""cd ComputeLibrary && scons Werror=1 -j8 {acl_build_flags}"")"
embed_libgomp;"def embed_libgomp(host: RemoteHost, use_conda, wheel_name) -> None:
    host.run_cmd(""pip3 install auditwheel"")
    host.run_cmd(
        ""conda install -y patchelf"" if use_conda else ""sudo apt-get install -y patchelf""
    )
    from tempfile import NamedTemporaryFile

    with NamedTemporaryFile() as tmp:
        tmp.write(embed_library_script.encode(""utf-8""))
        tmp.flush()
        host.upload_file(tmp.name, ""embed_library.py"")

    print(""Embedding libgomp into wheel"")
    if host.using_docker():
        host.run_cmd(f""python3 embed_library.py {wheel_name} --update-tag"")
    else:
        host.run_cmd(f""python3 embed_library.py {wheel_name}"")"
checkout_repo;"def checkout_repo(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    url: str,
    git_clone_flags: str,
    mapping: dict[str, tuple[str, str]],
) -> Optional[str]:
    for prefix in mapping:
        if not branch.startswith(prefix):
            continue
        tag = f""v{mapping[prefix][0]}-{mapping[prefix][1]}""
        host.run_cmd(f""git clone {url} -b {tag} {git_clone_flags}"")
        return mapping[prefix][0]

    host.run_cmd(f""git clone {url} -b {branch} {git_clone_flags}"")
    return None"
build_torchvision;"def build_torchvision(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    use_conda: bool = True,
    git_clone_flags: str,
    run_smoke_tests: bool = True,
) -> str:
    print(""Checking out TorchVision repo"")
    build_version = checkout_repo(
        host,
        branch=branch,
        url=""https://github.com/pytorch/vision"",
        git_clone_flags=git_clone_flags,
        mapping={
            ""v1.7.1"": (""0.8.2"", ""rc2""),
            ""v1.8.0"": (""0.9.0"", ""rc3""),
            ""v1.8.1"": (""0.9.1"", ""rc1""),
            ""v1.9.0"": (""0.10.0"", ""rc1""),
            ""v1.10.0"": (""0.11.1"", ""rc1""),
            ""v1.10.1"": (""0.11.2"", ""rc1""),
            ""v1.10.2"": (""0.11.3"", ""rc1""),
            ""v1.11.0"": (""0.12.0"", ""rc1""),
            ""v1.12.0"": (""0.13.0"", ""rc4""),
            ""v1.12.1"": (""0.13.1"", ""rc6""),
            ""v1.13.0"": (""0.14.0"", ""rc4""),
            ""v1.13.1"": (""0.14.1"", ""rc2""),
            ""v2.0.0"": (""0.15.1"", ""rc2""),
            ""v2.0.1"": (""0.15.2"", ""rc2""),
        },
    )
    print(""Building TorchVision wheel"")

    # Please note libnpg and jpeg are required to build image.so extension
    if use_conda:
        host.run_cmd(""conda install -y libpng jpeg"")
        # Remove .so files to force static linking
        host.run_cmd(
            ""rm miniforge3/lib/libpng.so miniforge3/lib/libpng16.so miniforge3/lib/libjpeg.so""
        )
        # And patch setup.py to include libz dependency for libpng
        host.run_cmd(
            [
                'sed -i -e \'s/image_link_flags\\.append(""png"")/image_link_flags += [""png"", ""z""]/\' vision/setup.py'
            ]
        )

    build_vars = """"
    if branch == ""nightly"":
        version = host.check_output(
            [""if [ -f vision/version.txt ]; then cat vision/version.txt; fi""]
        ).strip()
        if len(version) == 0:
            # In older revisions, version was embedded in setup.py
            version = (
                host.check_output([""grep"", '""version = \'""', ""vision/setup.py""])
                .strip()
                .split(""'"")[1][:-2]
            )
        build_date = (
            host.check_output(""cd vision && git log --pretty=format:%s -1"")
            .strip()
            .split()[0]
            .replace(""-"", """")
        )
        build_vars += f""BUILD_VERSION={version}.dev{build_date}""
    elif build_version is not None:
        build_vars += (
            f""BUILD_VERSION={build_version} PYTORCH_VERSION={branch[1:].split('-')[0]}""
        )
    if host.using_docker():
        build_vars += "" CMAKE_SHARED_LINKER_FLAGS=-Wl,-z,max-page-size=0x10000""

    host.run_cmd(f""cd vision && {build_vars} python3 setup.py bdist_wheel"")
    vision_wheel_name = host.list_dir(""vision/dist"")[0]
    embed_libgomp(host, use_conda, os.path.join(""vision"", ""dist"", vision_wheel_name))

    print(""Copying TorchVision wheel"")
    host.download_wheel(os.path.join(""vision"", ""dist"", vision_wheel_name))
    if run_smoke_tests:
        host.run_cmd(
            f""pip3 install {os.path.join('vision', 'dist', vision_wheel_name)}""
        )
        host.run_cmd(""python3 vision/test/smoke_test.py"")
    print(""Delete vision checkout"")
    host.run_cmd(""rm -rf vision"")

    return vision_wheel_name"
build_torchdata;"def build_torchdata(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    use_conda: bool = True,
    git_clone_flags: str = """",
) -> str:
    print(""Checking out TorchData repo"")
    git_clone_flags += "" --recurse-submodules""
    build_version = checkout_repo(
        host,
        branch=branch,
        url=""https://github.com/pytorch/data"",
        git_clone_flags=git_clone_flags,
        mapping={
            ""v1.13.1"": (""0.5.1"", """"),
            ""v2.0.0"": (""0.6.0"", ""rc5""),
            ""v2.0.1"": (""0.6.1"", ""rc1""),
        },
    )
    print(""Building TorchData wheel"")
    build_vars = """"
    if branch == ""nightly"":
        version = host.check_output(
            [""if [ -f data/version.txt ]; then cat data/version.txt; fi""]
        ).strip()
        build_date = (
            host.check_output(""cd data && git log --pretty=format:%s -1"")
            .strip()
            .split()[0]
            .replace(""-"", """")
        )
        build_vars += f""BUILD_VERSION={version}.dev{build_date}""
    elif build_version is not None:
        build_vars += (
            f""BUILD_VERSION={build_version} PYTORCH_VERSION={branch[1:].split('-')[0]}""
        )
    if host.using_docker():
        build_vars += "" CMAKE_SHARED_LINKER_FLAGS=-Wl,-z,max-page-size=0x10000""

    host.run_cmd(f""cd data && {build_vars} python3 setup.py bdist_wheel"")
    wheel_name = host.list_dir(""data/dist"")[0]
    embed_libgomp(host, use_conda, os.path.join(""data"", ""dist"", wheel_name))

    print(""Copying TorchData wheel"")
    host.download_wheel(os.path.join(""data"", ""dist"", wheel_name))

    return wheel_name"
build_torchtext;"def build_torchtext(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    use_conda: bool = True,
    git_clone_flags: str = """",
) -> str:
    print(""Checking out TorchText repo"")
    git_clone_flags += "" --recurse-submodules""
    build_version = checkout_repo(
        host,
        branch=branch,
        url=""https://github.com/pytorch/text"",
        git_clone_flags=git_clone_flags,
        mapping={
            ""v1.9.0"": (""0.10.0"", ""rc1""),
            ""v1.10.0"": (""0.11.0"", ""rc2""),
            ""v1.10.1"": (""0.11.1"", ""rc1""),
            ""v1.10.2"": (""0.11.2"", ""rc1""),
            ""v1.11.0"": (""0.12.0"", ""rc1""),
            ""v1.12.0"": (""0.13.0"", ""rc2""),
            ""v1.12.1"": (""0.13.1"", ""rc5""),
            ""v1.13.0"": (""0.14.0"", ""rc3""),
            ""v1.13.1"": (""0.14.1"", ""rc1""),
            ""v2.0.0"": (""0.15.1"", ""rc2""),
            ""v2.0.1"": (""0.15.2"", ""rc2""),
        },
    )
    print(""Building TorchText wheel"")
    build_vars = """"
    if branch == ""nightly"":
        version = host.check_output(
            [""if [ -f text/version.txt ]; then cat text/version.txt; fi""]
        ).strip()
        build_date = (
            host.check_output(""cd text && git log --pretty=format:%s -1"")
            .strip()
            .split()[0]
            .replace(""-"", """")
        )
        build_vars += f""BUILD_VERSION={version}.dev{build_date}""
    elif build_version is not None:
        build_vars += (
            f""BUILD_VERSION={build_version} PYTORCH_VERSION={branch[1:].split('-')[0]}""
        )
    if host.using_docker():
        build_vars += "" CMAKE_SHARED_LINKER_FLAGS=-Wl,-z,max-page-size=0x10000""

    host.run_cmd(f""cd text && {build_vars} python3 setup.py bdist_wheel"")
    wheel_name = host.list_dir(""text/dist"")[0]
    embed_libgomp(host, use_conda, os.path.join(""text"", ""dist"", wheel_name))

    print(""Copying TorchText wheel"")
    host.download_wheel(os.path.join(""text"", ""dist"", wheel_name))

    return wheel_name"
build_torchaudio;"def build_torchaudio(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    use_conda: bool = True,
    git_clone_flags: str = """",
) -> str:
    print(""Checking out TorchAudio repo"")
    git_clone_flags += "" --recurse-submodules""
    build_version = checkout_repo(
        host,
        branch=branch,
        url=""https://github.com/pytorch/audio"",
        git_clone_flags=git_clone_flags,
        mapping={
            ""v1.9.0"": (""0.9.0"", ""rc2""),
            ""v1.10.0"": (""0.10.0"", ""rc5""),
            ""v1.10.1"": (""0.10.1"", ""rc1""),
            ""v1.10.2"": (""0.10.2"", ""rc1""),
            ""v1.11.0"": (""0.11.0"", ""rc1""),
            ""v1.12.0"": (""0.12.0"", ""rc3""),
            ""v1.12.1"": (""0.12.1"", ""rc5""),
            ""v1.13.0"": (""0.13.0"", ""rc4""),
            ""v1.13.1"": (""0.13.1"", ""rc2""),
            ""v2.0.0"": (""2.0.1"", ""rc3""),
            ""v2.0.1"": (""2.0.2"", ""rc2""),
        },
    )
    print(""Building TorchAudio wheel"")
    build_vars = """"
    if branch == ""nightly"":
        version = (
            host.check_output([""grep"", '""version = \'""', ""audio/setup.py""])
            .strip()
            .split(""'"")[1][:-2]
        )
        build_date = (
            host.check_output(""cd audio && git log --pretty=format:%s -1"")
            .strip()
            .split()[0]
            .replace(""-"", """")
        )
        build_vars += f""BUILD_VERSION={version}.dev{build_date}""
    elif build_version is not None:
        build_vars += (
            f""BUILD_VERSION={build_version} PYTORCH_VERSION={branch[1:].split('-')[0]}""
        )
    if host.using_docker():
        build_vars += "" CMAKE_SHARED_LINKER_FLAGS=-Wl,-z,max-page-size=0x10000""

    host.run_cmd(
        f""cd audio && export FFMPEG_ROOT=$(pwd)/third_party/ffmpeg && export USE_FFMPEG=1 \
        && ./packaging/ffmpeg/build.sh \
        && {build_vars} python3 setup.py bdist_wheel""
    )

    wheel_name = host.list_dir(""audio/dist"")[0]
    embed_libgomp(host, use_conda, os.path.join(""audio"", ""dist"", wheel_name))

    print(""Copying TorchAudio wheel"")
    host.download_wheel(os.path.join(""audio"", ""dist"", wheel_name))

    return wheel_name"
configure_system;"def configure_system(
    host: RemoteHost,
    *,
    compiler: str = ""gcc-8"",
    use_conda: bool = True,
    python_version: str = ""3.8"",
) -> None:
    if use_conda:
        install_condaforge_python(host, python_version)

    print(""Configuring the system"")
    if not host.using_docker():
        update_apt_repo(host)
        host.run_cmd(""sudo apt-get install -y ninja-build g++ git cmake gfortran unzip"")
    else:
        host.run_cmd(""yum install -y sudo"")
        host.run_cmd(""conda install -y ninja scons"")

    if not use_conda:
        host.run_cmd(
            ""sudo apt-get install -y python3-dev python3-yaml python3-setuptools python3-wheel python3-pip""
        )
    host.run_cmd(""pip3 install dataclasses typing-extensions"")
    if not use_conda:
        print(""Installing Cython + numpy from PyPy"")
        host.run_cmd(""sudo pip3 install Cython"")
        host.run_cmd(""sudo pip3 install numpy"")"
build_domains;"def build_domains(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    use_conda: bool = True,
    git_clone_flags: str = """",
) -> tuple[str, str, str, str]:
    vision_wheel_name = build_torchvision(
        host, branch=branch, use_conda=use_conda, git_clone_flags=git_clone_flags
    )
    audio_wheel_name = build_torchaudio(
        host, branch=branch, use_conda=use_conda, git_clone_flags=git_clone_flags
    )
    data_wheel_name = build_torchdata(
        host, branch=branch, use_conda=use_conda, git_clone_flags=git_clone_flags
    )
    text_wheel_name = build_torchtext(
        host, branch=branch, use_conda=use_conda, git_clone_flags=git_clone_flags
    )
    return (vision_wheel_name, audio_wheel_name, data_wheel_name, text_wheel_name)"
start_build;"def start_build(
    host: RemoteHost,
    *,
    branch: str = ""main"",
    compiler: str = ""gcc-8"",
    use_conda: bool = True,
    python_version: str = ""3.8"",
    pytorch_only: bool = False,
    pytorch_build_number: Optional[str] = None,
    shallow_clone: bool = True,
    enable_mkldnn: bool = False,
) -> tuple[str, str, str, str, str]:
    git_clone_flags = "" --depth 1 --shallow-submodules"" if shallow_clone else """"
    if host.using_docker() and not use_conda:
        print(""Auto-selecting conda option for docker images"")
        use_conda = True
    if not host.using_docker():
        print(""Disable mkldnn for host builds"")
        enable_mkldnn = False

    configure_system(
        host, compiler=compiler, use_conda=use_conda, python_version=python_version
    )
    build_OpenBLAS(host, git_clone_flags)

    if host.using_docker():
        print(""Move libgfortant.a into a standard location"")
        # HACK: pypa gforntran.a is compiled without PIC, which leads to the following error
        # libgfortran.a(error.o)(.text._gfortrani_st_printf+0x34): unresolvable R_AARCH64_ADR_PREL_PG_HI21 relocation against symbol `__stack_chk_guard@@GLIBC_2.17'  # noqa: E501, B950
        # Workaround by copying gfortran library from the host
        host.run_ssh_cmd(""sudo apt-get install -y gfortran-8"")
        host.run_cmd(""mkdir -p /usr/lib/gcc/aarch64-linux-gnu/8"")
        host.run_ssh_cmd(
            [
                ""docker"",
                ""cp"",
                ""/usr/lib/gcc/aarch64-linux-gnu/8/libgfortran.a"",
                f""{host.container_id}:/opt/rh/devtoolset-10/root/usr/lib/gcc/aarch64-redhat-linux/10/"",
            ]
        )

    print(""Checking out PyTorch repo"")
    host.run_cmd(
        f""git clone --recurse-submodules -b {branch} https://github.com/pytorch/pytorch {git_clone_flags}""
    )

    print(""Building PyTorch wheel"")
    build_opts = """"
    if pytorch_build_number is not None:
        build_opts += f"" --build-number {pytorch_build_number}""
    # Breakpad build fails on aarch64
    build_vars = ""USE_BREAKPAD=0 ""
    if branch == ""nightly"":
        build_date = (
            host.check_output(""cd pytorch && git log --pretty=format:%s -1"")
            .strip()
            .split()[0]
            .replace(""-"", """")
        )
        version = host.check_output(""cat pytorch/version.txt"").strip()[:-2]
        build_vars += f""BUILD_TEST=0 PYTORCH_BUILD_VERSION={version}.dev{build_date} PYTORCH_BUILD_NUMBER=1""
    if branch.startswith((""v1."", ""v2."")):
        build_vars += f""BUILD_TEST=0 PYTORCH_BUILD_VERSION={branch[1 : branch.find('-')]} PYTORCH_BUILD_NUMBER=1""
    if host.using_docker():
        build_vars += "" CMAKE_SHARED_LINKER_FLAGS=-Wl,-z,max-page-size=0x10000""
    if enable_mkldnn:
        build_ArmComputeLibrary(host, git_clone_flags)
        print(""build pytorch with mkldnn+acl backend"")
        build_vars += "" USE_MKLDNN=ON USE_MKLDNN_ACL=ON""
        host.run_cmd(
            f""cd $HOME/pytorch && export ACL_ROOT_DIR=$HOME/ComputeLibrary && {build_vars} python3 setup.py bdist_wheel{build_opts}""
        )
        print(""Repair the wheel"")
        pytorch_wheel_name = host.list_dir(""pytorch/dist"")[0]
        ld_library_path = ""$HOME/acl/build:$HOME/pytorch/build/lib""
        host.run_cmd(
            f""export LD_LIBRARY_PATH={ld_library_path} && auditwheel repair $HOME/pytorch/dist/{pytorch_wheel_name}""
        )
        print(""replace the original wheel with the repaired one"")
        pytorch_repaired_wheel_name = host.list_dir(""wheelhouse"")[0]
        host.run_cmd(
            f""cp $HOME/wheelhouse/{pytorch_repaired_wheel_name} $HOME/pytorch/dist/{pytorch_wheel_name}""
        )
    else:
        print(""build pytorch without mkldnn backend"")
        host.run_cmd(
            f""cd pytorch && {build_vars} python3 setup.py bdist_wheel{build_opts}""
        )

    print(""Deleting build folder"")
    host.run_cmd(""cd pytorch && rm -rf build"")
    pytorch_wheel_name = host.list_dir(""pytorch/dist"")[0]
    embed_libgomp(host, use_conda, os.path.join(""pytorch"", ""dist"", pytorch_wheel_name))
    print(""Copying the wheel"")
    host.download_wheel(os.path.join(""pytorch"", ""dist"", pytorch_wheel_name))

    print(""Installing PyTorch wheel"")
    host.run_cmd(f""pip3 install pytorch/dist/{pytorch_wheel_name}"")

    if pytorch_only:
        return (pytorch_wheel_name, None, None, None, None)
    domain_wheels = build_domains(
        host, branch=branch, use_conda=use_conda, git_clone_flags=git_clone_flags
    )

    return (pytorch_wheel_name, *domain_wheels)"
get_instance_name;"def get_instance_name(instance) -> Optional[str]:
    if instance.tags is None:
        return None
    for tag in instance.tags:
        if tag[""Key""] == ""Name"":
            return tag[""Value""]
    return None"
list_instances;"def list_instances(instance_type: str) -> None:
    print(f""All instances of type {instance_type}"")
    for instance in ec2_instances_of_type(instance_type):
        ifaces = instance.network_interfaces
        az = ifaces[0].subnet.availability_zone if len(ifaces) > 0 else None
        print(
            f""{instance.id} {get_instance_name(instance)} {instance.public_dns_name} {instance.state['Name']} {az}""
        )"
terminate_instances;"def terminate_instances(instance_type: str) -> None:
    print(f""Terminating all instances of type {instance_type}"")
    instances = list(ec2_instances_of_type(instance_type))
    for instance in instances:
        print(f""Terminating {instance.id}"")
        instance.terminate()
    print(""Waiting for termination to complete"")
    for instance in instances:
        instance.wait_until_terminated()"
parse_arguments;"def parse_arguments():
    from argparse import ArgumentParser

    parser = ArgumentParser(""Builid and test AARCH64 wheels using EC2"")
    parser.add_argument(""--key-name"", type=str)
    parser.add_argument(""--debug"", action=""store_true"")
    parser.add_argument(""--build-only"", action=""store_true"")
    parser.add_argument(""--test-only"", type=str)
    group = parser.add_mutually_exclusive_group()
    group.add_argument(""--os"", type=str, choices=list(os_amis.keys()))
    group.add_argument(""--ami"", type=str)
    parser.add_argument(
        ""--python-version"",
        type=str,
        choices=[f""3.{d}"" for d in range(6, 12)],
        default=None,
    )
    parser.add_argument(""--alloc-instance"", action=""store_true"")
    parser.add_argument(""--list-instances"", action=""store_true"")
    parser.add_argument(""--pytorch-only"", action=""store_true"")
    parser.add_argument(""--keep-running"", action=""store_true"")
    parser.add_argument(""--terminate-instances"", action=""store_true"")
    parser.add_argument(""--instance-type"", type=str, default=""t4g.2xlarge"")
    parser.add_argument(""--ebs-size"", type=int, default=50)
    parser.add_argument(""--branch"", type=str, default=""main"")
    parser.add_argument(""--use-docker"", action=""store_true"")
    parser.add_argument(
        ""--compiler"",
        type=str,
        choices=[""gcc-7"", ""gcc-8"", ""gcc-9"", ""clang""],
        default=""gcc-8"",
    )
    parser.add_argument(""--use-torch-from-pypi"", action=""store_true"")
    parser.add_argument(""--pytorch-build-number"", type=str, default=None)
    parser.add_argument(""--disable-mkldnn"", action=""store_true"")
    return parser.parse_args()"
start_docker;"def start_docker(self, image=""quay.io/pypa/manylinux2014_aarch64:latest"") -> None:
        self.run_ssh_cmd(""sudo apt-get install -y docker.io"")
        self.run_ssh_cmd(f""sudo usermod -a -G docker {self.login_name}"")
        self.run_ssh_cmd(""sudo service docker start"")
        self.run_ssh_cmd(f""docker pull {image}"")
        self.container_id = self.check_ssh_output(
            f""docker run -t -d -w /root {image}""
        ).strip()"
using_docker;"def using_docker(self) -> bool:
        return self.container_id is not None"
run_cmd;"def run_cmd(self, args: Union[str, list[str]]) -> None:
        if not self.using_docker():
            return self.run_ssh_cmd(args)
        assert self.container_id is not None
        docker_cmd = self._gen_ssh_prefix() + [
            ""docker"",
            ""exec"",
            ""-i"",
            self.container_id,
            ""bash"",
        ]
        p = subprocess.Popen(docker_cmd, stdin=subprocess.PIPE)
        p.communicate(
            input="" "".join([""source .bashrc && ""] + self._split_cmd(args)).encode(
                ""utf-8""
            )
        )
        rc = p.wait()
        if rc != 0:
            raise subprocess.CalledProcessError(rc, docker_cmd)"
check_output;"def check_output(self, args: Union[str, list[str]]) -> str:
        if not self.using_docker():
            return self.check_ssh_output(args)
        assert self.container_id is not None
        docker_cmd = self._gen_ssh_prefix() + [
            ""docker"",
            ""exec"",
            ""-i"",
            self.container_id,
            ""bash"",
        ]
        p = subprocess.Popen(docker_cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
        (out, err) = p.communicate(
            input="" "".join([""source .bashrc && ""] + self._split_cmd(args)).encode(
                ""utf-8""
            )
        )
        rc = p.wait()
        if rc != 0:
            raise subprocess.CalledProcessError(rc, docker_cmd, output=out, stderr=err)
        return out.decode(""utf-8"")"
upload_file;"def upload_file(self, local_file: str, remote_file: str) -> None:
        if not self.using_docker():
            return self.scp_upload_file(local_file, remote_file)
        tmp_file = os.path.join(""/tmp"", os.path.basename(local_file))
        self.scp_upload_file(local_file, tmp_file)
        self.run_ssh_cmd(
            [""docker"", ""cp"", tmp_file, f""{self.container_id}:/root/{remote_file}""]
        )
        self.run_ssh_cmd([""rm"", tmp_file])"
download_file;"def download_file(self, remote_file: str, local_file: Optional[str] = None) -> None:
        if not self.using_docker():
            return self.scp_download_file(remote_file, local_file)
        tmp_file = os.path.join(""/tmp"", os.path.basename(remote_file))
        self.run_ssh_cmd(
            [""docker"", ""cp"", f""{self.container_id}:/root/{remote_file}"", tmp_file]
        )
        self.scp_download_file(tmp_file, local_file)
        self.run_ssh_cmd([""rm"", tmp_file])"
download_wheel;"def download_wheel(
        self, remote_file: str, local_file: Optional[str] = None
    ) -> None:
        if self.using_docker() and local_file is None:
            basename = os.path.basename(remote_file)
            local_file = basename.replace(
                ""-linux_aarch64.whl"", ""-manylinux2014_aarch64.whl""
            )
        self.download_file(remote_file, local_file)"
list_dir;"def list_dir(self, path: str) -> list[str]:
        return self.check_output([""ls"", ""-1"", path]).split(""\n"")"
replace_tag;"def replace_tag(filename):
    with open(filename) as f:
        lines = f.read().split(""\\n"")
    for i, line in enumerate(lines):
        if not line.startswith(""Tag: ""):
            continue
        lines[i] = line.replace(""-linux_"", ""-manylinux2014_"")
        print(f""Updated tag from {line} to {lines[i]}"")

    with open(filename, ""w"") as f:
        f.write(""\\n"".join(lines))"
embed_library;"def embed_library(whl_path, lib_soname, update_tag=False):
    patcher = AlignedPatchelf()
    out_dir = TemporaryDirectory()
    whl_name = os.path.basename(whl_path)
    tmp_whl_name = os.path.join(out_dir.name, whl_name)
    with InWheelCtx(whl_path) as ctx:
        torchlib_path = os.path.join(ctx._tmpdir.name, ""torch"", ""lib"")
        ctx.out_wheel = tmp_whl_name
        new_lib_path, new_lib_soname = None, None
        for filename, _ in elf_file_filter(ctx.iter_files()):
            if not filename.startswith(""torch/lib""):
                continue
            libtree = lddtree(filename)
            if lib_soname not in libtree[""needed""]:
                continue
            lib_path = libtree[""libs""][lib_soname][""path""]
            if lib_path is None:
                print(f""Can't embed {lib_soname} as it could not be found"")
                break
            if lib_path.startswith(torchlib_path):
                continue

            if new_lib_path is None:
                new_lib_soname, new_lib_path = copylib(lib_path, torchlib_path, patcher)
            patcher.replace_needed(filename, lib_soname, new_lib_soname)
            print(f""Replacing {lib_soname} with {new_lib_soname} for {filename}"")
        if update_tag:
            # Add manylinux2014 tag
            for filename in ctx.iter_files():
                if os.path.basename(filename) != ""WHEEL"":
                    continue
                replace_tag(filename)
    shutil.move(tmp_whl_name, whl_path)"
set_soname;"def set_soname(self, file_name: str, new_soname: str) -> None:
        check_call(
            [""patchelf"", ""--page-size"", ""65536"", ""--set-soname"", new_soname, file_name]
        )"
replace_needed;"def replace_needed(self, file_name: str, soname: str, new_soname: str) -> None:
        check_call(
            [
                ""patchelf"",
                ""--page-size"",
                ""65536"",
                ""--replace-needed"",
                soname,
                new_soname,
                file_name,
            ]
        )"
is_manylinux1_compatible;"def is_manylinux1_compatible():
    # Only Linux, and only x86-64 / i686
    from distutils.util import get_platform

    if get_platform() not in [""linux-x86_64"", ""linux-i686"", ""linux-s390x""]:
        return False

    # Check for presence of _manylinux module
    try:
        import _manylinux

        return bool(_manylinux.manylinux1_compatible)
    except (ImportError, AttributeError):
        # Fall through to heuristic check below
        pass

    # Check glibc version. CentOS 5 uses glibc 2.5.
    return have_compatible_glibc(2, 5)"
have_compatible_glibc;"def have_compatible_glibc(major, minimum_minor):
    import ctypes

    process_namespace = ctypes.CDLL(None)
    try:
        gnu_get_libc_version = process_namespace.gnu_get_libc_version
    except AttributeError:
        # Symbol doesn't exist -> therefore, we are not linked to
        # glibc.
        return False

    # Call gnu_get_libc_version, which returns a string like ""2.5"".
    gnu_get_libc_version.restype = ctypes.c_char_p
    version_str = gnu_get_libc_version()
    # py2 / py3 compatibility:
    if not isinstance(version_str, str):
        version_str = version_str.decode(""ascii"")

    # Parse string and check against requested version.
    version = [int(piece) for piece in version_str.split(""."")]
    assert len(version) == 2
    if major != version[0]:
        return False
    if minimum_minor > version[1]:
        return False
    return True"
sign_certificate_request;"def sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):
    cert = (
        x509.CertificateBuilder()
        .subject_name(csr_cert.subject)
        .issuer_name(ca_cert.subject)
        .public_key(csr_cert.public_key())
        .serial_number(x509.random_serial_number())
        .not_valid_before(datetime.now(timezone.utc))
        .not_valid_after(
            # Our certificate will be valid for 10 days
            datetime.now(timezone.utc) + timedelta(days=10)
            # Sign our certificate with our private key
        )
        .sign(private_ca_key, hashes.SHA256())
    )
    with open(path, ""wb"") as f:
        f.write(cert.public_bytes(serialization.Encoding.PEM))
    return cert"
_apply_libtorch_symbols;"def _apply_libtorch_symbols(symbols):
    return [
        re.compile(f""{x}.*{y}"")
        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)
    ]"
get_symbols;"def get_symbols(lib: str) -> list[tuple[str, str, str]]:
    from subprocess import check_output

    lines = check_output(f'nm ""{lib}""|c++filt', shell=True)
    return [x.split("" "", 2) for x in lines.decode(""latin1"").split(""\n"")[:-1]]"
grep_symbols;"def grep_symbols(lib: str, patterns: list[Any]) -> list[str]:
    def _grep_symbols(
        symbols: list[tuple[str, str, str]], patterns: list[Any]
    ) -> list[str]:
        rc = []
        for _s_addr, _s_type, s_name in symbols:
            for pattern in patterns:
                if pattern.match(s_name):
                    rc.append(s_name)
                    continue
        return rc

    all_symbols = get_symbols(lib)
    num_workers = 32
    chunk_size = (len(all_symbols) + num_workers - 1) // num_workers

    def _get_symbols_chunk(i):
        return all_symbols[i * chunk_size : (i + 1) * chunk_size]

    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:
        tasks = [
            executor.submit(_grep_symbols, _get_symbols_chunk(i), patterns)
            for i in range(num_workers)
        ]
        return functools.reduce(list.__add__, (x.result() for x in tasks), [])"
check_lib_symbols_for_abi_correctness;"def check_lib_symbols_for_abi_correctness(lib: str) -> None:
    print(f""lib: {lib}"")
    cxx11_symbols = grep_symbols(lib, LIBTORCH_CXX11_PATTERNS)
    pre_cxx11_symbols = grep_symbols(lib, LIBTORCH_PRE_CXX11_PATTERNS)
    num_cxx11_symbols = len(cxx11_symbols)
    num_pre_cxx11_symbols = len(pre_cxx11_symbols)
    print(f""num_cxx11_symbols: {num_cxx11_symbols}"")
    print(f""num_pre_cxx11_symbols: {num_pre_cxx11_symbols}"")
    if num_pre_cxx11_symbols > 0:
        raise RuntimeError(
            f""Found pre-cxx11 symbols, but there shouldn't be any, see: {pre_cxx11_symbols[:100]}""
        )
    if num_cxx11_symbols < 100:
        raise RuntimeError(""Didn't find enought cxx11 symbols"")"
load_json_from_basedir;"def load_json_from_basedir(filename: str):
    try:
        with open(BASE_DIR / filename) as fptr:
            return json.load(fptr)
    except FileNotFoundError as exc:
        raise ImportError(f""File {filename} not found error: {exc.strerror}"") from exc
    except json.JSONDecodeError as exc:
        raise ImportError(f""Invalid JSON {filename}"") from exc"
read_release_matrix;"def read_release_matrix():
    return load_json_from_basedir(""release_matrix.json"")"
check_version;"def check_version(package: str) -> None:
    release_version = os.getenv(""RELEASE_VERSION"")
    # if release_version is specified, use it to validate the packages
    if release_version:
        release_matrix = read_release_matrix()
        stable_version = release_matrix[""torch""]
    else:
        stable_version = os.getenv(""MATRIX_STABLE_VERSION"")

    # only makes sense to check nightly package where dates are known
    if channel == ""nightly"":
        check_nightly_binaries_date(package)
    elif stable_version is not None:
        if not torch.__version__.startswith(stable_version):
            raise RuntimeError(
                f""Torch version mismatch, expected {stable_version} for channel {channel}. But its {torch.__version__}""
            )

        if release_version and package == ""all"":
            for module in MODULES:
                imported_module = importlib.import_module(module[""name""])
                module_version = imported_module.__version__
                if not module_version.startswith(release_matrix[module[""name""]]):
                    raise RuntimeError(
                        f""{module['name']} version mismatch, expected: \
                            {release_matrix[module['name']]} for channel {channel}. But its {module_version}""
                    )
                else:
                    print(
                        f""{module['name']} version actual: {module_version} expected: \
                        {release_matrix[module['name']]} for channel {channel}.""
                    )

    else:
        print(f""Skip version check for channel {channel} as stable version is None"")"
check_nightly_binaries_date;"def check_nightly_binaries_date(package: str) -> None:
    from datetime import datetime

    format_dt = ""%Y%m%d""

    date_t_str = re.findall(""dev\\d+"", torch.__version__)
    date_t_delta = datetime.now() - datetime.strptime(date_t_str[0][3:], format_dt)
    if date_t_delta.days >= NIGHTLY_ALLOWED_DELTA:
        raise RuntimeError(
            f""the binaries are from {date_t_str} and are more than {NIGHTLY_ALLOWED_DELTA} days old!""
        )

    if package == ""all"":
        for module in MODULES:
            imported_module = importlib.import_module(module[""name""])
            module_version = imported_module.__version__
            date_m_str = re.findall(""dev\\d+"", module_version)
            date_m_delta = datetime.now() - datetime.strptime(
                date_m_str[0][3:], format_dt
            )
            print(f""Nightly date check for {module['name']} version {module_version}"")
            if date_m_delta.days > NIGHTLY_ALLOWED_DELTA:
                raise RuntimeError(
                    f""Expected {module['name']} to be less then {NIGHTLY_ALLOWED_DELTA} days. But its {date_m_delta}""
                )"
find_pypi_package_version;"def find_pypi_package_version(package: str) -> Optional[str]:
    from importlib import metadata

    dists = metadata.distributions()
    for dist in dists:
        if dist.metadata[""Name""].startswith(package):
            return dist.version
    return None"
cudnn_to_version_str;"def cudnn_to_version_str(cudnn_version: int) -> str:
    patch = int(cudnn_version % 10)
    minor = int((cudnn_version / 100) % 100)
    major = int((cudnn_version / 10000) % 10000)
    return f""{major}.{minor}.{patch}"""
compare_pypi_to_torch_versions;"def compare_pypi_to_torch_versions(
    package: str, pypi_version: str, torch_version: str
) -> None:
    if pypi_version is None:
        raise RuntimeError(f""Can't find {package} in PyPI for Torch: {torch_version}"")
    if pypi_version.startswith(torch_version):
        print(f""Found matching {package}. Torch: {torch_version} PyPI {pypi_version}"")
    else:
        raise RuntimeError(
            f""Wrong {package} version. Torch: {torch_version} PyPI: {pypi_version}""
        )"
smoke_test_cuda;"def smoke_test_cuda(
    package: str,
    runtime_error_check: str,
    torch_compile_check: str,
    pypi_pkg_check: str,
) -> None:
    if not torch.cuda.is_available() and is_cuda_system:
        raise RuntimeError(f""Expected CUDA {gpu_arch_ver}. However CUDA is not loaded."")

    if package == ""all"" and is_cuda_system:
        for module in MODULES:
            imported_module = importlib.import_module(module[""name""])
            # TBD for vision move extension module to private so it will
            # be _extention.
            version = ""N/A""
            if module[""extension""] == ""extension"":
                version = imported_module.extension._check_cuda_version()
            else:
                version = imported_module._extension._check_cuda_version()
            print(f""{module['name']} CUDA: {version}"")

    # torch.compile is available on macos-arm64 and Linux for python 3.8-3.13
    if (
        torch_compile_check == ""enabled""
        and sys.version_info < (3, 14, 0)
        and target_os in [""linux"", ""linux-aarch64"", ""macos-arm64"", ""darwin""]
    ):
        smoke_test_compile(""cuda"" if torch.cuda.is_available() else ""cpu"")

    if torch.cuda.is_available():
        if torch.version.cuda != gpu_arch_ver:
            raise RuntimeError(
                f""Wrong CUDA version. Loaded: {torch.version.cuda} Expected: {gpu_arch_ver}""
            )

        print(f""torch cuda: {torch.version.cuda}"")
        torch.cuda.init()
        print(""CUDA initialized successfully"")
        print(f""Number of CUDA devices: {torch.cuda.device_count()}"")
        for i in range(torch.cuda.device_count()):
            print(f""Device {i}: {torch.cuda.get_device_name(i)}"")

        print(f""cuDNN enabled? {torch.backends.cudnn.enabled}"")
        torch_cudnn_version = cudnn_to_version_str(torch.backends.cudnn.version())
        print(f""Torch cuDNN version: {torch_cudnn_version}"")

        if sys.platform in [""linux"", ""linux2""]:
            torch_nccl_version = ""."".join(str(v) for v in torch.cuda.nccl.version())
            print(f""Torch nccl; version: {torch_nccl_version}"")

        # Pypi dependencies are installed on linux ony and nccl is availbale only on Linux.
        if pypi_pkg_check == ""enabled"" and sys.platform in [""linux"", ""linux2""]:
            compare_pypi_to_torch_versions(
                ""cudnn"", find_pypi_package_version(""nvidia-cudnn""), torch_cudnn_version
            )
            compare_pypi_to_torch_versions(
                ""nccl"", find_pypi_package_version(""nvidia-nccl""), torch_nccl_version
            )

        if runtime_error_check == ""enabled"":
            test_cuda_runtime_errors_captured()"
smoke_test_conv2d;"def smoke_test_conv2d() -> None:
    import torch.nn as nn

    print(""Testing smoke_test_conv2d"")
    # With square kernels and equal stride
    m = nn.Conv2d(16, 33, 3, stride=2)
    # non-square kernels and unequal stride and with padding
    m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
    assert m is not None
    # non-square kernels and unequal stride and with padding and dilation
    basic_conv = nn.Conv2d(
        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)
    )
    input = torch.randn(20, 16, 50, 100)
    output = basic_conv(input)

    if is_cuda_system:
        print(""Testing smoke_test_conv2d with cuda"")
        conv = nn.Conv2d(3, 3, 3).cuda()
        x = torch.randn(1, 3, 24, 24, device=""cuda"")
        with torch.cuda.amp.autocast():
            out = conv(x)
        assert out is not None

        supported_dtypes = [torch.float16, torch.float32, torch.float64]
        for dtype in supported_dtypes:
            print(f""Testing smoke_test_conv2d with cuda for {dtype}"")
            conv = basic_conv.to(dtype).cuda()
            input = torch.randn(20, 16, 50, 100, device=""cuda"").type(dtype)
            output = conv(input)
            assert output is not None"
smoke_test_compile;"def smoke_test_compile(device: str = ""cpu"") -> None:
    supported_dtypes = [torch.float16, torch.float32, torch.float64]

    def foo(x: torch.Tensor) -> torch.Tensor:
        return torch.sin(x) + torch.cos(x)

    for dtype in supported_dtypes:
        print(f""Testing smoke_test_compile for {device} and {dtype}"")
        x = torch.rand(3, 3, device=device).type(dtype)
        x_eager = foo(x)
        x_pt2 = torch.compile(foo)(x)
        torch.testing.assert_close(x_eager, x_pt2)

    # Check that SIMD were detected for the architecture
    if device == ""cpu"":
        from torch._inductor.codecache import pick_vec_isa

        isa = pick_vec_isa()
        if not isa:
            raise RuntimeError(""Can't detect vectorized ISA for CPU"")
        print(f""Picked CPU ISA {type(isa).__name__} bit width {isa.bit_width()}"")

    # Reset torch dynamo since we are changing mode
    torch._dynamo.reset()
    dtype = torch.float32
    torch.set_float32_matmul_precision(""high"")
    print(f""Testing smoke_test_compile with mode 'max-autotune' for {dtype}"")
    x = torch.rand(64, 1, 28, 28, device=device).type(torch.float32)
    model = Net().to(device=device)
    x_pt2 = torch.compile(model, mode=""max-autotune"")(x)"
smoke_test_modules;"def smoke_test_modules():
    cwd = os.getcwd()
    for module in MODULES:
        if module[""repo""]:
            if not os.path.exists(f""{cwd}/{module['repo_name']}""):
                print(f""Path does not exist: {cwd}/{module['repo_name']}"")
                try:
                    subprocess.check_output(
                        f""git clone --depth 1 {module['repo']}"",
                        stderr=subprocess.STDOUT,
                        shell=True,
                    )
                except subprocess.CalledProcessError as exc:
                    raise RuntimeError(
                        f""Cloning {module['repo']} FAIL: {exc.returncode} Output: {exc.output}""
                    ) from exc
            try:
                smoke_test_command = f""python3 {module['smoke_test']}""
                if target_os == ""windows"":
                    smoke_test_command = f""python {module['smoke_test']}""
                output = subprocess.check_output(
                    smoke_test_command,
                    stderr=subprocess.STDOUT,
                    shell=True,
                    universal_newlines=True,
                )
            except subprocess.CalledProcessError as exc:
                raise RuntimeError(
                    f""Module {module['name']} FAIL: {exc.returncode} Output: {exc.output}""
                ) from exc
            else:
                print(f""Output: \n{output}\n"")"
regurgitate;"def regurgitate(depth, use_pyyaml_formatter=False):
    data = yaml.safe_load(sys.stdin)

    if use_pyyaml_formatter:
        output = yaml.dump(data, sort_keys=True)
        sys.stdout.write(output)
    else:
        miniyaml.render(sys.stdout, data, depth)"
submit_build;"def submit_build(pipeline_id, project_id, source_branch, source_version):
    print(""Submitting build for branch: "" + source_branch)
    print(""Commit SHA1: "", source_version)

    run_build_raw = s.post(
        build_base_url,
        json={
            ""definition"": {""id"": pipeline_id},
            ""project"": {""id"": project_id},
            ""sourceBranch"": source_branch,
            ""sourceVersion"": source_version,
        },
    )

    try:
        run_build_json = run_build_raw.json()
    except json.decoder.JSONDecodeError as e:
        print(e)
        print(
            ""Failed to parse the response. Check if the Azure DevOps PAT is incorrect or expired.""
        )
        sys.exit(-1)

    build_id = run_build_json[""id""]

    print(""Submitted bulid: "" + str(build_id))
    print(""Bulid URL: "" + run_build_json[""url""])
    return build_id"
get_build;"def get_build(_id):
    get_build_url = (
        AZURE_PIPELINE_BASE_URL + f""/_apis/build/builds/{_id}?api-version=6.0""
    )
    get_build_raw = s.get(get_build_url)
    return get_build_raw.json()"
get_build_logs;"def get_build_logs(_id):
    get_build_logs_url = (
        AZURE_PIPELINE_BASE_URL + f""/_apis/build/builds/{_id}/logs?api-version=6.0""
    )
    get_build_logs_raw = s.get(get_build_logs_url)
    return get_build_logs_raw.json()"
get_log_content;"def get_log_content(url):
    resp = s.get(url)
    return resp.text"
wait_for_build;"def wait_for_build(_id):
    build_detail = get_build(_id)
    build_status = build_detail[""status""]

    while build_status == ""notStarted"":
        print(""Waiting for run to start: "" + str(_id))
        sys.stdout.flush()
        try:
            build_detail = get_build(_id)
            build_status = build_detail[""status""]
        except Exception as e:
            print(""Error getting build"")
            print(e)

        time.sleep(30)

    print(""Bulid started: "", str(_id))

    handled_logs = set()
    while build_status == ""inProgress"":
        try:
            print(""Waiting for log: "" + str(_id))
            logs = get_build_logs(_id)
        except Exception as e:
            print(""Error fetching logs"")
            print(e)
            time.sleep(30)
            continue

        for log in logs[""value""]:
            log_id = log[""id""]
            if log_id in handled_logs:
                continue
            handled_logs.add(log_id)
            print(""Fetching log: \n"" + log[""url""])
            try:
                log_content = get_log_content(log[""url""])
                print(log_content)
            except Exception as e:
                print(""Error getting log content"")
                print(e)
            sys.stdout.flush()
        build_detail = get_build(_id)
        build_status = build_detail[""status""]
        time.sleep(30)

    build_result = build_detail[""result""]

    print(""Bulid status: "" + build_status)
    print(""Bulid result: "" + build_result)

    return build_status, build_result"
get_merge_base;"def get_merge_base() -> str:
    merge_base = subprocess.check_output(
        [""git"", ""merge-base"", ""HEAD"", ""origin/main""],
        text=True,
        stderr=subprocess.DEVNULL,
    ).strip()
    return merge_base"
get_head_sha;"def get_head_sha() -> str:
    sha = subprocess.check_output(
        [""git"", ""rev-parse"", ""HEAD""],
        text=True,
        stderr=subprocess.DEVNULL,
    ).strip()
    return sha"
is_main_branch;"def is_main_branch() -> bool:
    return False"
query_github_api;"def query_github_api(url: str) -> Any:
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""Bearer {os.environ['GITHUB_TOKEN']}"",
    }
    response = requests.get(url, headers=headers)
    return response.json()"
check_labels_for_pr;"def check_labels_for_pr() -> bool:
    # Check if the current commit is part of a PR and if it has the
    # FORCE_REBUILD_LABEL
    head_sha = get_head_sha()
    url = f""https://api.github.com/repos/pytorch/pytorch/commits/{head_sha}/pulls""
    response = query_github_api(url)

    print(
        f""Found {len(response)} PRs for commit {head_sha}: {[pr['number'] for pr in response]}""
    )
    for pr in response:
        labels = pr.get(""labels"", [])
        for label in labels:
            if label[""name""] == FORCE_REBUILD_LABEL:
                print(f""Found label {FORCE_REBUILD_LABEL} in PR {pr['number']}."")
                return True
    return False"
check_issue_open;"def check_issue_open() -> bool:
    # Check if issue #153759 is open.  This is the config issue for quickly
    # forcing everyone to build
    url = ""https://api.github.com/repos/pytorch/pytorch/issues/153759""
    response = query_github_api(url)
    if response.get(""state"") == ""open"":
        print(""Issue #153759 is open."")
        return True
    else:
        print(""Issue #153759 is not open."")
        return False"
get_workflow_id;"def get_workflow_id(run_id: str) -> Optional[str]:
    # Get the workflow ID that corresponds to the file for the run ID
    url = f""https://api.github.com/repos/pytorch/pytorch/actions/runs/{run_id}""
    response = query_github_api(url)
    if ""workflow_id"" in response:
        print(f""Found workflow ID for run ID {run_id}: {response['workflow_id']}"")
        return cast(str, response[""workflow_id""])
    else:
        print(""No workflow ID found."")
        return None"
ok_changed_file;"def ok_changed_file(file: str) -> bool:
    # Return true if the file is in the list of allowed files to be changed to
    # reuse the old whl
    if (
        file.startswith(""torch/"")
        and file.endswith("".py"")
        and not file.startswith(""torch/csrc/"")
    ):
        return True
    if file.startswith(""test/"") and file.endswith("".py""):
        return True
    return False"
check_changed_files;"def check_changed_files(sha: str) -> bool:
    # Return true if all the changed files are in the list of allowed files to
    # be changed to reuse the old whl
    changed_files = (
        subprocess.check_output(
            [""git"", ""diff"", ""--name-only"", sha, ""HEAD""],
            text=True,
            stderr=subprocess.DEVNULL,
        )
        .strip()
        .split()
    )
    print(f""Checking changed files between {sha} and HEAD:"")
    for file in changed_files:
        if not ok_changed_file(file):
            print(f""  File {file} is not allowed to be changed."")
            return False
        else:
            print(f""  File {file} is allowed to be changed."")
    return True"
find_old_whl;"def find_old_whl(workflow_id: str, build_environment: str, sha: str) -> bool:
    # Find the old whl on s3 and download it to artifacts.zip
    if build_environment is None:
        print(""BUILD_ENVIRONMENT is not set."")
        return False
    print(f""SHA: {sha}, workflow_id: {workflow_id}"")

    workflow_runs = query_github_api(
        f""https://api.github.com/repos/pytorch/pytorch/actions/workflows/{workflow_id}/runs?head_sha={sha}&branch=main&per_page=100""
    )
    if workflow_runs.get(""total_count"", 0) == 0:
        print(""No workflow runs found."")
        return False
    for run in workflow_runs.get(""workflow_runs"", []):
        # Look in s3 for the old whl
        run_id = run[""id""]
        try:
            url = f""https://gha-artifacts.s3.amazonaws.com/pytorch/pytorch/{run_id}/{build_environment}/artifacts.zip""
            print(f""Checking for old whl at {url}"")
            response = requests.get(
                url,
            )
            if response.status_code == 200:
                with open(""artifacts.zip"", ""wb"") as f:
                    f.write(response.content)
                    print(f""Found old whl file from s3: {url}"")
                    return True
        except requests.RequestException as e:
            print(f""Error checking for old whl: {e}"")
            continue
    return False"
unzip_artifact_and_replace_files;"def unzip_artifact_and_replace_files() -> None:
    # Unzip the artifact and replace files
    subprocess.check_output(
        [""unzip"", ""-o"", ""artifacts.zip"", ""-d"", ""artifacts""],
    )
    os.remove(""artifacts.zip"")

    # Rename wheel into zip
    wheel_path = Path(""artifacts/dist"").glob(""*.whl"")
    for path in wheel_path:
        new_path = path.with_suffix("".zip"")
        os.rename(path, new_path)
        print(f""Renamed {path} to {new_path}"")
        print(new_path.stem)
        # Unzip the wheel
        subprocess.check_output(
            [""unzip"", ""-o"", new_path, ""-d"", f""artifacts/dist/{new_path.stem}""],
        )
        # Copy python files into the artifact
        subprocess.check_output(
            [""rsync"", ""-avz"", ""torch"", f""artifacts/dist/{new_path.stem}""],
        )

        # Zip the wheel back
        subprocess.check_output(
            [""zip"", ""-r"", f""{new_path.stem}.zip"", "".""],
            cwd=f""artifacts/dist/{new_path.stem}"",
        )
        subprocess.check_output(
            [
                ""mv"",
                f""artifacts/dist/{new_path.stem}/{new_path.stem}.zip"",
                f""artifacts/dist/{new_path.stem}.whl"",
            ],
        )

        # Remove the extracted folder
        subprocess.check_output(
            [""rm"", ""-rf"", f""artifacts/dist/{new_path.stem}""],
        )

    # Rezip the artifact
    subprocess.check_output([""zip"", ""-r"", ""artifacts.zip"", "".""], cwd=""artifacts"")
    subprocess.check_output(
        [""mv"", ""artifacts/artifacts.zip"", "".""],
    )
    return None"
set_output;"def set_output() -> None:
    # Disable for now so we can monitor first
    # pass
    if os.getenv(""GITHUB_OUTPUT""):
        with open(str(os.getenv(""GITHUB_OUTPUT"")), ""a"") as env:
            print(""reuse=true"", file=env)
    else:
        print(""::set-output name=reuse::true"")"
parse_args;"def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Check for old whl files."")
    parser.add_argument(""--run-id"", type=str, required=True, help=""Workflow ID"")
    parser.add_argument(
        ""--build-environment"", type=str, required=True, help=""Build environment""
    )
    parser.add_argument(
        ""--github-ref"",
        type=str,
    )
    return parser.parse_args()"
can_reuse_whl;"def can_reuse_whl(args: argparse.Namespace) -> bool:
    if is_main_branch() or (
        args.github_ref
        and any(
            args.github_ref.startswith(x)
            for x in [""refs/heads/release"", ""refs/tags/v"", ""refs/heads/main""]
        )
    ):
        print(""On main branch or release branch, rebuild whl"")
        return False

    if check_labels_for_pr():
        print(f""Found {FORCE_REBUILD_LABEL} label on PR, rebuild whl"")
        return False

    if check_issue_open():
        print(""Issue #153759 is open, rebuild whl"")
        return False

    if not check_changed_files(get_merge_base()):
        print(""Cannot use old whl due to the changed files, rebuild whl"")
        return False

    workflow_id = get_workflow_id(args.run_id)
    if workflow_id is None:
        print(""No workflow ID found, rebuild whl"")
        return False

    if not find_old_whl(workflow_id, args.build_environment, get_merge_base()):
        print(""No old whl found, rebuild whl"")
        # TODO: go backwards from merge base to find more runs
        return False

    return True"
read_triton_pin;"def read_triton_pin(device: str = ""cuda"") -> str:
    triton_file = ""triton.txt""
    if device == ""xpu"":
        triton_file = ""triton-xpu.txt""
    with open(REPO_DIR / "".ci"" / ""docker"" / ""ci_commit_pins"" / triton_file) as f:
        return f.read().strip()"
read_triton_version;"def read_triton_version() -> str:
    with open(REPO_DIR / "".ci"" / ""docker"" / ""triton_version.txt"") as f:
        return f.read().strip()"
check_and_replace;"def check_and_replace(inp: str, src: str, dst: str) -> str:
    """"""Checks that `src` can be found in `input` and replaces it with `dst`""""""
    if src not in inp:
        raise RuntimeError(f""Can't find ${src} in the input"")
    return inp.replace(src, dst)"
patch_init_py;"def patch_init_py(
    path: Path, *, version: str, expected_version: Optional[str] = None
) -> None:
    if not expected_version:
        expected_version = read_triton_version()
    with open(path) as f:
        orig = f.read()
    # Replace version
    orig = check_and_replace(
        orig, f""__version__ = '{expected_version}'"", f'__version__ = ""{version}""'
    )
    with open(path, ""w"") as f:
        f.write(orig)"
build_triton;"def build_triton(
    *,
    version: str,
    commit_hash: str,
    device: str = ""cuda"",
    py_version: Optional[str] = None,
    release: bool = False,
    with_clang_ldd: bool = False,
) -> Path:
    env = os.environ.copy()
    if ""MAX_JOBS"" not in env:
        max_jobs = os.cpu_count() or 1
        env[""MAX_JOBS""] = str(max_jobs)

    with TemporaryDirectory() as tmpdir:
        triton_basedir = Path(tmpdir) / ""triton""
        triton_pythondir = triton_basedir / ""python""
        triton_repo = ""https://github.com/openai/triton""
        if device == ""rocm"":
            triton_pkg_name = ""pytorch-triton-rocm""
        elif device == ""xpu"":
            triton_pkg_name = ""pytorch-triton-xpu""
            triton_repo = ""https://github.com/intel/intel-xpu-backend-for-triton""
        else:
            triton_pkg_name = ""pytorch-triton""
        check_call([""git"", ""clone"", triton_repo, ""triton""], cwd=tmpdir)
        if release:
            ver, rev, patch = version.split(""."")
            check_call(
                [""git"", ""checkout"", f""release/{ver}.{rev}.x""], cwd=triton_basedir
            )
        else:
            check_call([""git"", ""checkout"", commit_hash], cwd=triton_basedir)

        # change built wheel name and version
        env[""TRITON_WHEEL_NAME""] = triton_pkg_name
        if with_clang_ldd:
            env[""TRITON_BUILD_WITH_CLANG_LLD""] = ""1""

        patch_init_py(
            triton_pythondir / ""triton"" / ""__init__.py"",
            version=f""{version}"",
            expected_version=None,
        )

        if device == ""rocm"":
            check_call(
                [f""{SCRIPT_DIR}/amd/package_triton_wheel.sh""],
                cwd=triton_basedir,
                shell=True,
            )
            print(""ROCm libraries setup for triton installation..."")

        check_call(
            [sys.executable, ""setup.py"", ""bdist_wheel""], cwd=triton_pythondir, env=env
        )

        whl_path = next(iter((triton_pythondir / ""dist"").glob(""*.whl"")))
        shutil.copy(whl_path, Path.cwd())

        if device == ""rocm"":
            check_call(
                [f""{SCRIPT_DIR}/amd/patch_triton_wheel.sh"", Path.cwd()],
                cwd=triton_basedir,
            )

        return Path.cwd() / whl_path.name"
delete_all_label_err_comments;"def delete_all_label_err_comments(pr: ""GitHubPR"") -> None:
    for comment in pr.get_comments():
        if is_label_err_comment(comment):
            gh_delete_comment(pr.org, pr.project, comment.database_id)"
add_label_err_comment;"def add_label_err_comment(pr: ""GitHubPR"") -> None:
    # Only make a comment if one doesn't exist already
    if not any(is_label_err_comment(comment) for comment in pr.get_comments()):
        gh_post_pr_comment(pr.org, pr.project, pr.pr_num, LABEL_ERR_MSG)"
get_merge_commit_sha;"def get_merge_commit_sha(repo: GitRepo, pr: GitHubPR) -> Optional[str]:
    """"""
    Return the merge commit SHA iff the PR has been merged. For simplicity, we
    will only cherry pick PRs that have been merged into main
    """"""
    commit_sha = get_pr_commit_sha(repo, pr)
    return commit_sha if pr.is_closed() else None"
get_release_version;"def get_release_version(onto_branch: str) -> Optional[str]:
    """"""
    Return the release version if the target branch is a release branch
    """"""
    m = re.match(RELEASE_BRANCH_REGEX, onto_branch)
    return m.group(""version"") if m else """""
get_tracker_issues;"def get_tracker_issues(
    org: str, project: str, onto_branch: str
) -> list[dict[str, Any]]:
    """"""
    Find the tracker issue from the repo. The tracker issue needs to have the title
    like [VERSION] Release Tracker following the convention on PyTorch
    """"""
    version = get_release_version(onto_branch)
    if not version:
        return []

    tracker_issues = gh_query_issues_by_labels(org, project, labels=[""release tracker""])
    if not tracker_issues:
        return []

    # Figure out the tracker issue from the list by looking at the title
    return [issue for issue in tracker_issues if version in issue.get(""title"", """")]"
cherry_pick;"def cherry_pick(
    github_actor: str,
    repo: GitRepo,
    pr: GitHubPR,
    commit_sha: str,
    onto_branch: str,
    classification: str,
    fixes: str,
    dry_run: bool = False,
) -> None:
    """"""
    Create a local branch to cherry pick the commit and submit it as a pull request
    """"""
    current_branch = repo.current_branch()
    cherry_pick_branch = create_cherry_pick_branch(
        github_actor, repo, pr, commit_sha, onto_branch
    )

    try:
        org, project = repo.gh_owner_and_name()

        cherry_pick_pr = """"
        if not dry_run:
            cherry_pick_pr = submit_pr(repo, pr, cherry_pick_branch, onto_branch)

        tracker_issues_comments = []
        tracker_issues = get_tracker_issues(org, project, onto_branch)
        for issue in tracker_issues:
            issue_number = int(str(issue.get(""number"", ""0"")))
            if not issue_number:
                continue

            res = cast(
                dict[str, Any],
                post_tracker_issue_comment(
                    org,
                    project,
                    issue_number,
                    pr.pr_num,
                    cherry_pick_pr,
                    classification,
                    fixes,
                    dry_run,
                ),
            )

            comment_url = res.get(""html_url"", """")
            if comment_url:
                tracker_issues_comments.append(comment_url)

        msg = f""The cherry pick PR is at {cherry_pick_pr}""
        if fixes:
            msg += f"" and it is linked with issue {fixes}.""
        elif classification in REQUIRES_ISSUE:
            msg += f"" and it is recommended to link a {classification} cherry pick PR with an issue.""

        if tracker_issues_comments:
            msg += "" The following tracker issues are updated:\n""
            for tracker_issues_comment in tracker_issues_comments:
                msg += f""* {tracker_issues_comment}\n""

        post_pr_comment(org, project, pr.pr_num, msg, dry_run)

    finally:
        if current_branch:
            repo.checkout(branch=current_branch)"
create_cherry_pick_branch;"def create_cherry_pick_branch(
    github_actor: str, repo: GitRepo, pr: GitHubPR, commit_sha: str, onto_branch: str
) -> str:
    """"""
    Create a local branch and cherry pick the commit. Return the name of the local
    cherry picking branch.
    """"""
    repo.checkout(branch=onto_branch)
    repo._run_git(""submodule"", ""update"", ""--init"", ""--recursive"")

    # Remove all special characters if we want to include the actor in the branch name
    github_actor = re.sub(""[^0-9a-zA-Z]+"", ""_"", github_actor)

    cherry_pick_branch = f""cherry-pick-{pr.pr_num}-by-{github_actor}""
    repo.create_branch_and_checkout(branch=cherry_pick_branch)

    # We might want to support ghstack later
    # We don't want to resolve conflicts here.
    repo._run_git(""cherry-pick"", ""-x"", commit_sha)
    repo.push(branch=cherry_pick_branch, dry_run=False)

    return cherry_pick_branch"
post_pr_comment;"def post_pr_comment(
    org: str, project: str, pr_num: int, msg: str, dry_run: bool = False
) -> list[dict[str, Any]]:
    """"""
    Post a comment on the PR itself to point to the cherry picking PR when success
    or print the error when failure
    """"""
    internal_debugging = """"

    run_url = os.getenv(""GH_RUN_URL"")
    # Post a comment to tell folks that the PR is being cherry picked
    if run_url is not None:
        internal_debugging = ""\n"".join(
            line
            for line in (
                ""<details><summary>Details for Dev Infra team</summary>"",
                f'Raised by <a href=""{run_url}"">workflow job</a>\n',
                ""</details>"",
            )
            if line
        )

    comment = ""\n"".join(
        (f""### Cherry picking #{pr_num}"", f""{msg}"", """", f""{internal_debugging}"")
    )
    return gh_post_pr_comment(org, project, pr_num, comment, dry_run)"
post_tracker_issue_comment;"def post_tracker_issue_comment(
    org: str,
    project: str,
    issue_num: int,
    pr_num: int,
    cherry_pick_pr: str,
    classification: str,
    fixes: str,
    dry_run: bool = False,
) -> list[dict[str, Any]]:
    """"""
    Post a comment on the tracker issue (if any) to record the cherry pick
    """"""
    comment = ""\n"".join(
        (
            ""Link to landed trunk PR (if applicable):"",
            f""* https://github.com/{org}/{project}/pull/{pr_num}"",
            """",
            ""Link to release branch PR:"",
            f""* {cherry_pick_pr}"",
            """",
            ""Criteria Category:"",
            "" - "".join((classification.capitalize(), fixes.capitalize())),
        )
    )
    return gh_post_pr_comment(org, project, issue_num, comment, dry_run)"
download_log_worker;"def download_log_worker(temp_dir: str, id: int, name: str) -> None:
    url = f""https://ossci-raw-job-status.s3.amazonaws.com/log/{id}""
    data = requests.get(url).text
    with open(f""{temp_dir}/{name.replace('/', '_')} {id}.txt"", ""x"") as f:
        f.write(data)"
close_issue;"def close_issue(num: int) -> None:
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {os.environ['GITHUB_TOKEN']}"",
    }
    response = requests.post(
        f""https://api.github.com/repos/pytorch/pytorch/issues/{num}/comments"",
        data=json.dumps({""body"": CLOSING_COMMENT}),
        headers=headers,
    )
    if response.status_code != 201:
        raise RuntimeError(f""Failed to comment on issue {num}: {response.text}"")
    response = requests.patch(
        f""https://api.github.com/repos/pytorch/pytorch/issues/{num}"",
        data=json.dumps({""state"": ""closed""}),
        headers=headers,
    )
    if response.status_code != 200:
        raise RuntimeError(f""Failed to close issue {num}: {response.text}"")"
check_if_exists;"def check_if_exists(
    item: tuple[str, tuple[int, str, list[str]]], all_logs: list[str]
) -> tuple[bool, str]:
    test, (_, link, _) = item
    # Test names should look like `test_a (module.path.classname)`
    reg = re.match(r""(\S+) \((\S*)\)"", test)
    if reg is None:
        return False, ""poorly formed""

    name = reg[1]
    classname = reg[2].split(""."")[-1]

    # Check if there is any mention of the link or the test name in the logs.
    # The link usually shows up in the skip reason.
    present = False
    for log in all_logs:
        if link in log:
            present = True
            break
        if f""{classname}::{name}"" in log:
            present = True
            break
    if present:
        return True, ""found in logs""

    # Query DB to see if the test is there
    count = query_db(
        TEST_EXISTS_QUERY, {""name"": f""{name}%"", ""classname"": f""{classname}%""}
    )
    if len(count) == 0:
        return False, ""not found""
    return True, ""found in DB"""
get_workflows_push_tags;"def get_workflows_push_tags() -> set[str]:
    ""Extract all known push tags from workflows""
    rc: set[str] = set()
    for fname in (GITHUB_DIR / ""workflows"").glob(""*.yml""):
        with fname.open(""r"") as f:
            wf_yml = yaml.safe_load(f)
        # ""on"" is alias to True in yaml
        on_tag = wf_yml.get(True, None)
        push_tag = on_tag.get(""push"", None) if isinstance(on_tag, dict) else None
        tags_tag = push_tag.get(""tags"", None) if isinstance(push_tag, dict) else None
        if isinstance(tags_tag, list):
            rc.update(tags_tag)
    return rc"
filter_ciflow_tags;"def filter_ciflow_tags(tags: set[str]) -> list[str]:
    ""Return sorted list of ciflow tags""
    return sorted(
        tag[:-2] for tag in tags if tag.startswith(""ciflow/"") and tag.endswith(""/*"")
    )"
read_probot_config;"def read_probot_config() -> dict[str, Any]:
    with (GITHUB_DIR / ""pytorch-probot.yml"").open(""r"") as f:
        return cast(dict[str, Any], yaml.safe_load(f))"
update_probot_config;"def update_probot_config(labels: set[str]) -> None:
    orig = read_probot_config()
    orig[""ciflow_push_tags""] = filter_ciflow_tags(labels)
    with (GITHUB_DIR / ""pytorch-probot.yml"").open(""w"") as f:
        yaml.dump(orig, f, indent=4, sort_keys=False)"
is_protected;"def is_protected(branch: str) -> bool:
    try:
        ESTIMATED_TOKENS[0] += 1
        res = gh_fetch_json_dict(
            f""https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/branches/{branch}""
        )
        return bool(res[""protected""])
    except Exception as e:
        print(f""[{branch}] Failed to fetch branch protections: {e}"")
        return True"
convert_gh_timestamp;"def convert_gh_timestamp(date: str) -> float:
    return datetime.strptime(date, ""%Y-%m-%dT%H:%M:%SZ"").timestamp()"
get_branches;"def get_branches(repo: GitRepo) -> dict[str, Any]:
    # Query locally for branches, group by branch base name (e.g. gh/blah/base -> gh/blah), and get the most recent branch
    git_response = repo._run_git(
        ""for-each-ref"",
        ""--sort=creatordate"",
        ""--format=%(refname) %(committerdate:iso-strict)"",
        ""refs/remotes/origin"",
    )
    branches_by_base_name: dict[str, Any] = {}
    for line in git_response.splitlines():
        branch, date = line.split("" "")
        re_branch = re.match(r""refs/remotes/origin/(.*)"", branch)
        assert re_branch
        branch = branch_base_name = re_branch.group(1)
        if x := re.match(r""(gh\/.+)\/(head|base|orig)"", branch):
            branch_base_name = x.group(1)
        date = datetime.fromisoformat(date).timestamp()
        if branch_base_name not in branches_by_base_name:
            branches_by_base_name[branch_base_name] = [date, [branch]]
        else:
            branches_by_base_name[branch_base_name][1].append(branch)
            if date > branches_by_base_name[branch_base_name][0]:
                branches_by_base_name[branch_base_name][0] = date
    return branches_by_base_name"
paginate_graphql;"def paginate_graphql(
    query: str,
    kwargs: dict[str, Any],
    termination_func: Callable[[list[dict[str, Any]]], bool],
    get_data: Callable[[dict[str, Any]], list[dict[str, Any]]],
    get_page_info: Callable[[dict[str, Any]], dict[str, Any]],
) -> list[Any]:
    hasNextPage = True
    endCursor = None
    data: list[dict[str, Any]] = []
    while hasNextPage:
        ESTIMATED_TOKENS[0] += 1
        res = gh_graphql(query, cursor=endCursor, **kwargs)
        data.extend(get_data(res))
        hasNextPage = get_page_info(res)[""hasNextPage""]
        endCursor = get_page_info(res)[""endCursor""]
        if termination_func(data):
            break
    return data"
get_recent_prs;"def get_recent_prs() -> dict[str, Any]:
    now = datetime.now().timestamp()

    # Grab all PRs updated in last CLOSED_PR_RETENTION days
    pr_infos: list[dict[str, Any]] = paginate_graphql(
        GRAPHQL_ALL_PRS_BY_UPDATED_AT,
        {""owner"": ""pytorch"", ""repo"": ""pytorch""},
        lambda data: (
            PR_WINDOW is not None
            and (now - convert_gh_timestamp(data[-1][""updatedAt""]) > PR_WINDOW)
        ),
        lambda res: res[""data""][""repository""][""pullRequests""][""nodes""],
        lambda res: res[""data""][""repository""][""pullRequests""][""pageInfo""],
    )

    # Get the most recent PR for each branch base (group gh together)
    prs_by_branch_base = {}
    for pr in pr_infos:
        pr[""updatedAt""] = convert_gh_timestamp(pr[""updatedAt""])
        branch_base_name = pr[""headRefName""]
        if x := re.match(r""(gh\/.+)\/(head|base|orig)"", branch_base_name):
            branch_base_name = x.group(1)
        if branch_base_name not in prs_by_branch_base:
            prs_by_branch_base[branch_base_name] = pr
        else:
            if pr[""updatedAt""] > prs_by_branch_base[branch_base_name][""updatedAt""]:
                prs_by_branch_base[branch_base_name] = pr
    return prs_by_branch_base"
get_open_prs;"def get_open_prs() -> list[dict[str, Any]]:
    return paginate_graphql(
        GRAPHQL_OPEN_PRS,
        {""owner"": ""pytorch"", ""repo"": ""pytorch""},
        lambda data: False,
        lambda res: res[""data""][""repository""][""pullRequests""][""nodes""],
        lambda res: res[""data""][""repository""][""pullRequests""][""pageInfo""],
    )"
get_branches_with_magic_label_or_open_pr;"def get_branches_with_magic_label_or_open_pr() -> set[str]:
    pr_infos: list[dict[str, Any]] = paginate_graphql(
        GRAPHQL_NO_DELETE_BRANCH_LABEL,
        {""owner"": ""pytorch"", ""repo"": ""pytorch""},
        lambda data: False,
        lambda res: res[""data""][""repository""][""label""][""pullRequests""][""nodes""],
        lambda res: res[""data""][""repository""][""label""][""pullRequests""][""pageInfo""],
    )

    pr_infos.extend(get_open_prs())

    # Get the most recent PR for each branch base (group gh together)
    branch_bases = set()
    for pr in pr_infos:
        branch_base_name = pr[""headRefName""]
        if x := re.match(r""(gh\/.+)\/(head|base|orig)"", branch_base_name):
            branch_base_name = x.group(1)
        branch_bases.add(branch_base_name)
    return branch_bases"
delete_branch;"def delete_branch(repo: GitRepo, branch: str) -> None:
    repo._run_git(""push"", ""origin"", ""-d"", branch)"
delete_branches;"def delete_branches() -> None:
    now = datetime.now().timestamp()
    git_repo = GitRepo(str(REPO_ROOT), ""origin"", debug=True)
    branches = get_branches(git_repo)
    prs_by_branch = get_recent_prs()
    keep_branches = get_branches_with_magic_label_or_open_pr()

    delete = []
    # Do not delete if:
    # * associated PR is open, closed but updated recently, or contains the magic string
    # * no associated PR and branch was updated in last 1.5 years
    # * is protected
    # Setting different values of PR_WINDOW will change how branches with closed
    # PRs are treated depending on how old the branch is.  The default value of
    # 90 will allow branches with closed PRs to be deleted if the PR hasn't been
    # updated in 90 days and the branch hasn't been updated in 1.5 years
    for base_branch, (date, sub_branches) in branches.items():
        print(f""[{base_branch}] Updated {(now - date) / SEC_IN_DAY} days ago"")
        if base_branch in keep_branches:
            print(f""[{base_branch}] Has magic label or open PR, skipping"")
            continue
        pr = prs_by_branch.get(base_branch)
        if pr:
            print(
                f""[{base_branch}] Has PR {pr['number']}: {pr['state']}, updated {(now - pr['updatedAt']) / SEC_IN_DAY} days ago""
            )
            if (
                now - pr[""updatedAt""] < CLOSED_PR_RETENTION
                or (now - date) < CLOSED_PR_RETENTION
            ):
                continue
        elif now - date < NO_PR_RETENTION:
            continue
        print(f""[{base_branch}] Checking for branch protections"")
        if any(is_protected(sub_branch) for sub_branch in sub_branches):
            print(f""[{base_branch}] Is protected"")
            continue
        for sub_branch in sub_branches:
            print(f""[{base_branch}] Deleting {sub_branch}"")
            delete.append(sub_branch)
        if ESTIMATED_TOKENS[0] > 400:
            print(""Estimated tokens exceeded, exiting"")
            break

    print(f""To delete ({len(delete)}):"")
    for branch in delete:
        print(f""About to delete branch {branch}"")
        delete_branch(git_repo, branch)"
delete_old_ciflow_tags;"def delete_old_ciflow_tags() -> None:
    # Deletes ciflow tags if they are associated with a closed PR or a specific
    # commit.  Lightweight tags don't have information about the date they were
    # created, so we can't check how old they are.  The script just assumes that
    # ciflow tags should be deleted regardless of creation date.
    git_repo = GitRepo(str(REPO_ROOT), ""origin"", debug=True)

    def delete_tag(tag: str) -> None:
        print(f""Deleting tag {tag}"")
        ESTIMATED_TOKENS[0] += 1
        delete_branch(git_repo, f""refs/tags/{tag}"")

    tags = git_repo._run_git(""tag"").splitlines()
    open_pr_numbers = [x[""number""] for x in get_open_prs()]

    for tag in tags:
        try:
            if ESTIMATED_TOKENS[0] > 400:
                print(""Estimated tokens exceeded, exiting"")
                break
            if not tag.startswith(""ciflow/""):
                continue
            re_match_pr = re.match(r""^ciflow\/.*\/(\d{5,6})$"", tag)
            re_match_sha = re.match(r""^ciflow\/.*\/([0-9a-f]{40})$"", tag)
            if re_match_pr:
                pr_number = int(re_match_pr.group(1))
                if pr_number in open_pr_numbers:
                    continue
                delete_tag(tag)
            elif re_match_sha:
                delete_tag(tag)
        except Exception as e:
            print(f""Failed to check tag {tag}: {e}"")"
delete_tag;"def delete_tag(tag: str) -> None:
        print(f""Deleting tag {tag}"")
        ESTIMATED_TOKENS[0] += 1
        delete_branch(git_repo, f""refs/tags/{tag}"")"
load_json_file;"def load_json_file(file_path: Path) -> Any:
    """"""
    Returns the deserialized json object
    """"""
    with open(file_path) as f:
        return json.load(f)"
write_json_file;"def write_json_file(file_path: Path, content: Any) -> None:
    dir = file_path.parent
    ensure_dir_exists(dir)

    with open(file_path, ""w"") as f:
        json.dump(content, f, indent=2)"
sanitize_for_s3;"def sanitize_for_s3(text: str) -> str:
    """"""
    S3 keys can only contain alphanumeric characters, underscores, and dashes.
    This function replaces all other characters with underscores.
    """"""
    return re.sub(r""[^a-zA-Z0-9_-]"", ""_"", text)"
upload_file_to_s3;"def upload_file_to_s3(file_name: Path, bucket: str, key: str) -> None:
    print(f""Uploading {file_name}"")
    print(f""       to s3://{bucket}/{key}"")

    boto3.client(""s3"").upload_file(
        str(file_name),
        bucket,
        key,
    )"
download_s3_objects_with_prefix;"def download_s3_objects_with_prefix(
    bucket_name: str, prefix: str, download_folder: Path
) -> list[Path]:
    s3 = boto3.resource(""s3"")
    bucket = s3.Bucket(bucket_name)

    downloads = []

    for obj in bucket.objects.filter(Prefix=prefix):
        download_path = download_folder / obj.key

        ensure_dir_exists(download_path.parent)
        print(f""Downloading s3://{bucket.name}/{obj.key}"")
        print(f""         to {download_path}"")

        s3.Object(bucket.name, obj.key).download_file(str(download_path))
        downloads.append(download_path)

    if len(downloads) == 0:
        print(
            f""There were no files matching the prefix `{prefix}` in bucket `{bucket.name}`""
        )

    return downloads"
is_cuda_or_rocm_job;"def is_cuda_or_rocm_job(job_name: Optional[str]) -> bool:
    if not job_name:
        return False

    return ""cuda"" in job_name or ""rocm"" in job_name"
parse_args;"def parse_args() -> Any:
    from argparse import ArgumentParser

    parser = ArgumentParser(
        ""Filter all test configurations and keep only requested ones""
    )
    parser.add_argument(
        ""--test-matrix"", type=str, required=True, help=""the original test matrix""
    )
    parser.add_argument(
        ""--selected-test-configs"",
        type=str,
        default="""",
        help=""a comma-separated list of test configurations from the test matrix to keep"",
    )
    parser.add_argument(
        ""--workflow"", type=str, help=""the name of the current workflow, i.e. pull""
    )
    parser.add_argument(
        ""--job-name"",
        type=str,
        help=""the name of the current job, i.e. linux-focal-py3.8-gcc7 / build"",
    )
    parser.add_argument(""--pr-number"", type=str, help=""the pull request number"")
    parser.add_argument(""--tag"", type=str, help=""the associated tag if it exists"")
    parser.add_argument(
        ""--event-name"",
        type=str,
        help=""name of the event that triggered the job (pull, schedule, etc)"",
    )
    parser.add_argument(
        ""--schedule"",
        type=str,
        help=""cron schedule that triggered the job"",
    )
    parser.add_argument(
        ""--branch"",
        type=str,
        default=""main"",
        help=""the branch name"",
    )
    return parser.parse_args()"
filter_labels;"def filter_labels(labels: set[str], label_regex: Any) -> set[str]:
    """"""
    Return the list of matching labels
    """"""
    return {l for l in labels if re.match(label_regex, l)}"
filter_selected_test_configs;"def filter_selected_test_configs(
    test_matrix: dict[str, list[Any]], selected_test_configs: set[str]
) -> dict[str, list[Any]]:
    """"""
    Keep only the selected configs if the list if not empty. Otherwise, keep all test configs.
    This filter is used when the workflow is dispatched manually.
    """"""
    if not selected_test_configs:
        return test_matrix

    filtered_test_matrix: dict[str, list[Any]] = {""include"": []}
    for entry in test_matrix.get(""include"", []):
        config_name = entry.get(""config"", """")
        if not config_name:
            continue

        if config_name in selected_test_configs:
            filtered_test_matrix[""include""].append(entry)

    return filtered_test_matrix"
set_periodic_modes;"def set_periodic_modes(
    test_matrix: dict[str, list[Any]], job_name: Optional[str]
) -> dict[str, list[Any]]:
    """"""
    Apply all periodic modes when running under a schedule
    """"""
    scheduled_test_matrix: dict[str, list[Any]] = {
        ""include"": [],
    }

    for config in test_matrix.get(""include"", []):
        for mode, cond in SUPPORTED_PERIODICAL_MODES.items():
            if not cond(job_name):
                continue

            cfg = config.copy()
            cfg[mode] = mode
            scheduled_test_matrix[""include""].append(cfg)

    return scheduled_test_matrix"
mark_unstable_jobs;"def mark_unstable_jobs(
    workflow: str, job_name: str, test_matrix: dict[str, list[Any]]
) -> dict[str, list[Any]]:
    """"""
    Check the list of unstable jobs and mark them accordingly. Note that if a job
    is unstable, all its dependents will also be marked accordingly
    """"""
    return process_jobs(
        workflow=workflow,
        job_name=job_name,
        test_matrix=test_matrix,
        issue_type=IssueType.UNSTABLE,
        url=UNSTABLE_JOBS_URL,
    )"
remove_disabled_jobs;"def remove_disabled_jobs(
    workflow: str, job_name: str, test_matrix: dict[str, list[Any]]
) -> dict[str, list[Any]]:
    """"""
    Check the list of disabled jobs, remove the current job and all its dependents
    if it exists in the list
    """"""
    return process_jobs(
        workflow=workflow,
        job_name=job_name,
        test_matrix=test_matrix,
        issue_type=IssueType.DISABLED,
        url=DISABLED_JOBS_URL,
    )"
process_jobs;"def process_jobs(
    workflow: str,
    job_name: str,
    test_matrix: dict[str, list[Any]],
    issue_type: IssueType,
    url: str,
) -> dict[str, list[Any]]:
    """"""
    Both disabled and unstable jobs are in the following format:

    {
        ""WORKFLOW / PLATFORM / JOB (CONFIG)"": [
            AUTHOR,
            ISSUE_NUMBER,
            ISSUE_URL,
            WORKFLOW,
            PLATFORM,
            JOB (CONFIG),
        ],
        ""pull / linux-bionic-py3.8-clang9 / test (dynamo)"": [
            ""pytorchbot"",
            ""94861"",
            ""https://github.com/pytorch/pytorch/issues/94861"",
            ""pull"",
            ""linux-bionic-py3.8-clang9"",
            ""test (dynamo)"",
        ],
    }
    """"""
    try:
        # The job name from github is in the PLATFORM / JOB (CONFIG) format, so breaking
        # it into its two components first
        current_platform, _ = (n.strip() for n in job_name.split(JOB_NAME_SEP, 1) if n)
    except ValueError:
        warnings.warn(f""Invalid job name {job_name}, returning"")
        return test_matrix

    for record in download_json(url=url, headers={}).values():
        (
            author,
            _,
            target_url,
            target_workflow,
            target_platform,
            target_job_cfg,
        ) = record

        if target_workflow != workflow:
            # The current workflow doesn't match this record
            continue

        cleanup_regex = rf""(-{BUILD_JOB_NAME}|-{TEST_JOB_NAME})$""
        # There is an exception here for binary build workflows in which the platform
        # names have the build and test suffix. For example, we have a build job called
        # manywheel-py3-cuda11_8-build / build and its subsequent test job called
        # manywheel-py3-cuda11_8-test / test. So they are linked, but their suffixes
        # are different
        target_platform_no_suffix = re.sub(cleanup_regex, """", target_platform)
        current_platform_no_suffix = re.sub(cleanup_regex, """", current_platform)

        if (
            target_platform != current_platform
            and target_platform_no_suffix != current_platform_no_suffix
        ):
            # The current platform doesn't match this record
            continue

        # The logic after this is fairly complicated:
        #
        # - If the target record doesn't have the optional job (config) name,
        #   i.e. pull / linux-bionic-py3.8-clang9, all build and test jobs will
        #   be skipped if it's a disabled record or marked as unstable if it's
        #   an unstable record
        #
        # - If the target record has the job name and it's a build job, i.e.
        #   pull / linux-bionic-py3.8-clang9 / build, all build and test jobs
        #   will be skipped if it's a disabled record or marked as unstable if
        #   it's an unstable record, because the latter requires the former
        #
        # - If the target record has the job name and it's a test job without
        #   the config part, i.e. pull / linux-bionic-py3.8-clang9 / test, all
        #   test jobs will be skipped if it's a disabled record or marked as
        #   unstable if it's an unstable record
        #
        # - If the target record has the job (config) name, only that test config
        #   will be skipped or marked as unstable
        if not target_job_cfg:
            msg = (
                f""Issue {target_url} created by {author} has {issue_type.value} ""
                + f""all CI jobs for {workflow} / {job_name}""
            )
            info(msg)
            return _filter_jobs(
                test_matrix=test_matrix,
                issue_type=issue_type,
            )

        if target_job_cfg == BUILD_JOB_NAME:
            msg = (
                f""Issue {target_url} created by {author} has {issue_type.value} ""
                + f""the build job for {workflow} / {job_name}""
            )
            info(msg)
            return _filter_jobs(
                test_matrix=test_matrix,
                issue_type=issue_type,
            )

        if target_job_cfg in (TEST_JOB_NAME, BUILD_AND_TEST_JOB_NAME):
            msg = (
                f""Issue {target_url} created by {author} has {issue_type.value} ""
                + f""all the test jobs for {workflow} / {job_name}""
            )
            info(msg)
            return _filter_jobs(
                test_matrix=test_matrix,
                issue_type=issue_type,
            )

        m = JOB_NAME_CFG_REGEX.match(target_job_cfg)
        if m:
            target_job = m.group(""job"")
            # Make sure that the job name is a valid test job name first before checking the config
            if target_job in (TEST_JOB_NAME, BUILD_AND_TEST_JOB_NAME):
                target_cfg = m.group(""cfg"")

                # NB: There can be multiple unstable configurations, i.e. inductor, inductor_huggingface
                test_matrix = _filter_jobs(
                    test_matrix=test_matrix,
                    issue_type=issue_type,
                    target_cfg=target_cfg,
                )
        else:
            warnings.warn(
                f""Found a matching {issue_type.value} issue {target_url} for {workflow} / {job_name}, ""
                + f""but the name {target_job_cfg} is invalid""
            )

    # Found no matching target, return the same input test matrix
    return test_matrix"
download_json;"def download_json(url: str, headers: dict[str, str], num_retries: int = 3) -> Any:
    for _ in range(num_retries):
        try:
            req = Request(url=url, headers=headers)
            content = urlopen(req, timeout=5).read().decode(""utf-8"")
            return json.loads(content)
        except Exception as e:
            warnings.warn(f""Could not download {url}: {e}"")

    warnings.warn(f""All {num_retries} retries exhausted, downloading {url} failed"")
    return {}"
set_output;"def set_output(name: str, val: Any) -> None:
    if os.getenv(""GITHUB_OUTPUT""):
        with open(str(os.getenv(""GITHUB_OUTPUT"")), ""a"") as env:
            print(f""{name}={val}"", file=env)
    else:
        print(f""::set-output name={name}::{val}"")"
parse_reenabled_issues;"def parse_reenabled_issues(s: Optional[str]) -> list[str]:
    # NB: When the PR body is empty, GitHub API returns a None value, which is
    # passed into this function
    if not s:
        return []

    # The regex is meant to match all *case-insensitive* keywords that
    # GitHub has delineated would link PRs to issues, more details here:
    # https://docs.github.com/en/issues/tracking-your-work-with-issues/linking-a-pull-request-to-an-issue.
    # E.g., ""Close #62851"", ""fixES #62851"" and ""RESOLVED #62851"" would all match, but not
    # ""closes  #62851"" --> extra space, ""fixing #62851"" --> not a keyword, nor ""fix 62851"" --> no #
    issue_numbers = [x[5] for x in re.findall(REENABLE_TEST_REGEX, s)]
    return issue_numbers"
get_reenabled_issues;"def get_reenabled_issues(pr_body: str = """") -> list[str]:
    default_branch = f""origin/{os.environ.get('GIT_DEFAULT_BRANCH', 'main')}""
    try:
        commit_messages = subprocess.check_output(
            f""git cherry -v {default_branch}"".split("" "")
        ).decode(""utf-8"")
    except Exception as e:
        warnings.warn(f""failed to get commit messages: {e}"")
        commit_messages = """"
    return parse_reenabled_issues(pr_body) + parse_reenabled_issues(commit_messages)"
check_for_setting;"def check_for_setting(labels: set[str], body: str, setting: str) -> bool:
    return setting in labels or f""[{setting}]"" in body"
perform_misc_tasks;"def perform_misc_tasks(
    labels: set[str], test_matrix: dict[str, list[Any]], job_name: str, pr_body: str
) -> None:
    """"""
    In addition to apply the filter logic, the script also does the following
    misc tasks to set keep-going and is-unstable variables
    """"""
    set_output(""keep-going"", check_for_setting(labels, pr_body, ""keep-going""))
    set_output(
        ""ci-verbose-test-logs"",
        check_for_setting(labels, pr_body, ""ci-verbose-test-logs""),
    )
    set_output(
        ""ci-test-showlocals"", check_for_setting(labels, pr_body, ""ci-test-showlocals"")
    )
    set_output(
        ""ci-no-test-timeout"", check_for_setting(labels, pr_body, ""ci-no-test-timeout"")
    )
    set_output(""ci-no-td"", check_for_setting(labels, pr_body, ""ci-no-td""))
    # Only relevant for the one linux distributed cuda job, delete this when TD
    # is rolled out completely
    set_output(
        ""ci-td-distributed"", check_for_setting(labels, pr_body, ""ci-td-distributed"")
    )

    # Obviously, if the job name includes unstable, then this is an unstable job
    is_unstable = job_name and IssueType.UNSTABLE.value in job_name
    if not is_unstable and test_matrix and test_matrix.get(""include""):
        # Even when the job name doesn't mention unstable, we will also mark it as
        # unstable when the test matrix only includes unstable jobs. Basically, this
        # logic allows build or build-and-test jobs to be marked as unstable too.
        #
        # Basically, when a build job is unstable, all the subsequent test jobs are
        # also unstable. And when all test jobs are unstable, we will also treat the
        # build job as unstable. It's simpler this way
        is_unstable = all(IssueType.UNSTABLE.value in r for r in test_matrix[""include""])

    set_output(
        ""is-unstable"",
        is_unstable,
    )

    set_output(""reenabled-issues"", "","".join(get_reenabled_issues(pr_body=pr_body)))

    if MEM_LEAK_LABEL in labels:
        # Enable mem leak check if label is added
        for config in test_matrix.get(""include"", []):
            if is_cuda_or_rocm_job(job_name):
                config[""mem_leak_check""] = ""mem_leak_check"""
get_nccl_wheel_version;"def get_nccl_wheel_version(arch_version: str) -> str:
    import re

    requirements = map(
        str.strip, re.split(""[;|]"", PYTORCH_EXTRA_INSTALL_REQUIREMENTS[arch_version])
    )
    return next(x for x in requirements if x.startswith(""nvidia-nccl-cu"")).split(""=="")[
        1
    ]"
read_nccl_pin;"def read_nccl_pin(arch_version: str) -> str:
    from pathlib import Path

    nccl_pin_path = os.path.join(
        Path(__file__).absolute().parents[2],
        "".ci"",
        ""docker"",
        ""ci_commit_pins"",
        f""nccl-cu{arch_version[:2]}.txt"",
    )
    with open(nccl_pin_path) as f:
        return f.read().strip()"
validate_nccl_dep_consistency;"def validate_nccl_dep_consistency(arch_version: str) -> None:
    nccl_release_tag = read_nccl_pin(arch_version)
    wheel_ver = get_nccl_wheel_version(arch_version)
    if not nccl_release_tag.startswith(f""v{wheel_ver}""):
        raise RuntimeError(
            f""{arch_version} NCCL release tag version {nccl_release_tag} does not correspond to wheel version {wheel_ver}""
        )"
arch_type;"def arch_type(arch_version: str) -> str:
    if arch_version in CUDA_ARCHES:
        return ""cuda""
    elif arch_version in ROCM_ARCHES:
        return ""rocm""
    elif arch_version in XPU_ARCHES:
        return ""xpu""
    elif arch_version in CPU_AARCH64_ARCH:
        return ""cpu-aarch64""
    elif arch_version in CPU_S390X_ARCH:
        return ""cpu-s390x""
    elif arch_version in CUDA_AARCH64_ARCHES:
        return ""cuda-aarch64""
    else:  # arch_version should always be ""cpu"" in this case
        return ""cpu"""
translate_desired_cuda;"def translate_desired_cuda(gpu_arch_type: str, gpu_arch_version: str) -> str:
    return {
        ""cpu"": ""cpu"",
        ""cpu-aarch64"": ""cpu"",
        ""cpu-s390x"": ""cpu"",
        ""cuda"": f""cu{gpu_arch_version.replace('.', '')}"",
        ""cuda-aarch64"": f""cu{gpu_arch_version.replace('-aarch64', '').replace('.', '')}"",
        ""rocm"": f""rocm{gpu_arch_version}"",
        ""xpu"": ""xpu"",
    }.get(gpu_arch_type, gpu_arch_version)"
list_without;"def list_without(in_list: list[str], without: list[str]) -> list[str]:
    return [item for item in in_list if item not in without]"
generate_libtorch_matrix;"def generate_libtorch_matrix(
    os: str,
    release_type: str,
    arches: Optional[list[str]] = None,
    libtorch_variants: Optional[list[str]] = None,
) -> list[dict[str, str]]:
    if arches is None:
        arches = [""cpu""]
        if os == ""linux"":
            arches += CUDA_ARCHES
            arches += ROCM_ARCHES
        elif os == ""windows"":
            arches += CUDA_ARCHES
    if libtorch_variants is None:
        libtorch_variants = [
            ""shared-with-deps"",
            ""shared-without-deps"",
            ""static-with-deps"",
            ""static-without-deps"",
        ]

    ret: list[dict[str, str]] = []
    for arch_version in arches:
        for libtorch_variant in libtorch_variants:
            gpu_arch_type = arch_type(arch_version)
            gpu_arch_version = """" if arch_version == ""cpu"" else arch_version
            # ROCm builds without-deps failed even in ROCm runners; skip for now
            if gpu_arch_type == ""rocm"" and (""without-deps"" in libtorch_variant):
                continue
            ret.append(
                {
                    ""gpu_arch_type"": gpu_arch_type,
                    ""gpu_arch_version"": gpu_arch_version,
                    ""desired_cuda"": translate_desired_cuda(
                        gpu_arch_type, gpu_arch_version
                    ),
                    ""libtorch_config"": release_type,
                    ""libtorch_variant"": libtorch_variant,
                    ""container_image"": (
                        LIBTORCH_CONTAINER_IMAGES[arch_version].split("":"")[0]
                        if os not in (""windows"", ""windows-arm64"")
                        else """"
                    ),
                    ""container_image_tag_prefix"": (
                        LIBTORCH_CONTAINER_IMAGES[arch_version].split("":"")[1]
                        if os not in (""windows"", ""windows-arm64"")
                        else """"
                    ),
                    ""package_type"": ""libtorch"",
                    ""build_name"": f""libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{release_type}"".replace(
                        ""."", ""_""
                    ),
                }
            )
    return ret"
generate_wheels_matrix;"def generate_wheels_matrix(
    os: str,
    arches: Optional[list[str]] = None,
    python_versions: Optional[list[str]] = None,
    use_split_build: bool = False,
) -> list[dict[str, str]]:
    package_type = ""wheel""
    if os == ""linux"" or os == ""linux-aarch64"" or os == ""linux-s390x"":
        # NOTE: We only build manywheel packages for x86_64 and aarch64 and s390x linux
        package_type = ""manywheel""

    if python_versions is None:
        python_versions = FULL_PYTHON_VERSIONS

    if arches is None:
        # Define default compute archivectures
        arches = [""cpu""]
        if os == ""linux"":
            arches += CUDA_ARCHES + ROCM_ARCHES + XPU_ARCHES
        elif os == ""windows"":
            arches += CUDA_ARCHES + XPU_ARCHES
        elif os == ""linux-aarch64"":
            # Separate new if as the CPU type is different and
            # uses different build/test scripts
            arches = CPU_AARCH64_ARCH + CUDA_AARCH64_ARCHES
        elif os == ""linux-s390x"":
            # Only want the one arch as the CPU type is different and
            # uses different build/test scripts
            arches = [""cpu-s390x""]

    ret: list[dict[str, str]] = []
    for python_version in python_versions:
        for arch_version in arches:
            gpu_arch_type = arch_type(arch_version)
            gpu_arch_version = (
                """"
                if arch_version == ""cpu""
                or arch_version == ""cpu-aarch64""
                or arch_version == ""cpu-s390x""
                or arch_version == ""xpu""
                else arch_version
            )

            # TODO: Enable python 3.13t on cpu-s390x
            if gpu_arch_type == ""cpu-s390x"" and python_version == ""3.13t"":
                continue

            if use_split_build and (
                arch_version not in [""12.6"", ""12.8"", ""11.8"", ""cpu""] or os != ""linux""
            ):
                raise RuntimeError(
                    ""Split build is only supported on linux with cuda 12*, 11.8, and cpu.\n""
                    f""Currently attempting to build on arch version {arch_version} and os {os}.\n""
                    ""Please modify the matrix generation to exclude this combination.""
                )

            # cuda linux wheels require PYTORCH_EXTRA_INSTALL_REQUIREMENTS to install

            if (
                arch_version in [""12.8"", ""12.6"", ""11.8""]
                and os == ""linux""
                or arch_version in CUDA_AARCH64_ARCHES
            ):
                desired_cuda = translate_desired_cuda(gpu_arch_type, gpu_arch_version)
                ret.append(
                    {
                        ""python_version"": python_version,
                        ""gpu_arch_type"": gpu_arch_type,
                        ""gpu_arch_version"": gpu_arch_version,
                        ""desired_cuda"": desired_cuda,
                        ""use_split_build"": ""True"" if use_split_build else ""False"",
                        ""container_image"": WHEEL_CONTAINER_IMAGES[arch_version].split(
                            "":""
                        )[0],
                        ""container_image_tag_prefix"": WHEEL_CONTAINER_IMAGES[
                            arch_version
                        ].split("":"")[1],
                        ""package_type"": package_type,
                        ""pytorch_extra_install_requirements"": (
                            PYTORCH_EXTRA_INSTALL_REQUIREMENTS[
                                f""{desired_cuda[2:4]}.{desired_cuda[4:]}""  # for cuda-aarch64: cu126 -> 12.6
                            ]
                            if os == ""linux-aarch64""
                            else PYTORCH_EXTRA_INSTALL_REQUIREMENTS[arch_version]
                        ),
                        ""build_name"": (
                            f""{package_type}-py{python_version}-{gpu_arch_type}""
                            f""{'-' if 'aarch64' in gpu_arch_type else ''}{gpu_arch_version.replace('-aarch64', '')}"".replace(
                                ""."", ""_""
                            )
                        ),  # include special case for aarch64 build, remove the -aarch64 postfix
                    }
                )
                # Special build building to use on Colab. Python 3.11 for 12.6 CUDA
                if python_version == ""3.11"" and arch_version == CUDA_STABLE:
                    ret.append(
                        {
                            ""python_version"": python_version,
                            ""gpu_arch_type"": gpu_arch_type,
                            ""gpu_arch_version"": gpu_arch_version,
                            ""desired_cuda"": translate_desired_cuda(
                                gpu_arch_type, gpu_arch_version
                            ),
                            ""use_split_build"": ""True"" if use_split_build else ""False"",
                            ""container_image"": WHEEL_CONTAINER_IMAGES[
                                arch_version
                            ].split("":"")[0],
                            ""container_image_tag_prefix"": WHEEL_CONTAINER_IMAGES[
                                arch_version
                            ].split("":"")[1],
                            ""package_type"": package_type,
                            ""pytorch_extra_install_requirements"": """",
                            ""build_name"": f""{package_type}-py{python_version}-{gpu_arch_type}{gpu_arch_version}-full"".replace(  # noqa: B950
                                ""."", ""_""
                            ),
                        }
                    )
            else:
                ret.append(
                    {
                        ""python_version"": python_version,
                        ""gpu_arch_type"": gpu_arch_type,
                        ""gpu_arch_version"": gpu_arch_version,
                        ""desired_cuda"": translate_desired_cuda(
                            gpu_arch_type, gpu_arch_version
                        ),
                        ""use_split_build"": ""True"" if use_split_build else ""False"",
                        ""container_image"": WHEEL_CONTAINER_IMAGES[arch_version].split(
                            "":""
                        )[0],
                        ""container_image_tag_prefix"": WHEEL_CONTAINER_IMAGES[
                            arch_version
                        ].split("":"")[1],
                        ""package_type"": package_type,
                        ""build_name"": f""{package_type}-py{python_version}-{gpu_arch_type}{gpu_arch_version}"".replace(
                            ""."", ""_""
                        ),
                        ""pytorch_extra_install_requirements"": (
                            PYTORCH_EXTRA_INSTALL_REQUIREMENTS[""xpu""]
                            if gpu_arch_type == ""xpu""
                            else """"
                        ),
                    }
                )

    return ret"
generate_workflow_file;"def generate_workflow_file(self, workflow_template: jinja2.Template) -> None:
        output_file_path = (
            GITHUB_DIR
            / f""workflows/generated-{self.build_environment}-{self.branches}.yml""
        )
        with open(output_file_path, ""w"") as output_file:
            GENERATED = ""generated""  # Note that please keep the variable GENERATED otherwise phabricator will hide the whole file
            output_file.writelines([f""# @{GENERATED} DO NOT EDIT MANUALLY\n""])
            try:
                content = workflow_template.render(asdict(self))
            except Exception as e:
                print(f""Failed on template: {workflow_template}"", file=sys.stderr)
                raise e
            output_file.write(content)
            if content[-1] != ""\n"":
                output_file.write(""\n"")
        print(output_file_path)"
generate_docker_matrix;"def generate_docker_matrix() -> dict[str, list[dict[str, str]]]:
    ret: list[dict[str, str]] = []
    # CUDA amd64 Docker images are available as both runtime and devel while
    # CPU arm64 image is only available as runtime.
    for cuda, version in generate_binary_build_matrix.CUDA_ARCHES_FULL_VERSION.items():
        for image in DOCKER_IMAGE_TYPES:
            ret.append(
                {
                    ""cuda"": cuda,
                    ""cuda_full_version"": version,
                    ""cudnn_version"": generate_binary_build_matrix.CUDA_ARCHES_CUDNN_VERSION[
                        cuda
                    ],
                    ""image_type"": image,
                    ""platform"": ""linux/amd64"",
                }
            )
    ret.append(
        {
            ""cuda"": ""cpu"",
            ""cuda_full_version"": """",
            ""cudnn_version"": """",
            ""image_type"": ""runtime"",
            ""platform"": ""linux/arm64"",
        }
    )

    return {""include"": ret}"
get_pytorch_root;"def get_pytorch_root() -> Path:
    return Path(
        subprocess.check_output([""git"", ""rev-parse"", ""--show-toplevel""])
        .decode(""ascii"")
        .strip()
    )"
get_tag;"def get_tag() -> str:
    root = get_pytorch_root()
    try:
        dirty_tag = (
            subprocess.check_output([""git"", ""describe"", ""--tags"", ""--exact""], cwd=root)
            .decode(""ascii"")
            .strip()
        )
    except subprocess.CalledProcessError:
        return """"
    # Strip leading v that we typically do when we tag branches
    # ie: v1.7.1 -> 1.7.1
    tag = re.sub(LEADING_V_PATTERN, """", dirty_tag)
    # Strip trailing rc pattern
    # ie: 1.7.1-rc1 -> 1.7.1
    tag = re.sub(TRAILING_RC_PATTERN, """", tag)
    # Ignore ciflow tags
    if tag.startswith(""ciflow/""):
        return """"
    return tag"
get_base_version;"def get_base_version() -> str:
    root = get_pytorch_root()
    dirty_version = open(root / ""version.txt"").read().strip()
    # Strips trailing a0 from version.txt, not too sure why it's there in the
    # first place
    return re.sub(LEGACY_BASE_VERSION_SUFFIX_PATTERN, """", dirty_version)"
get_post_build_suffix;"def get_post_build_suffix(self) -> str:
        if self.no_build_suffix:
            return """"
        if self.gpu_arch_type == ""cuda"":
            return f""+cu{self.gpu_arch_version.replace('.', '')}""
        return f""+{self.gpu_arch_type}{self.gpu_arch_version}"""
get_release_version;"def get_release_version(self) -> str:
        if not get_tag():
            raise NoGitTagException(
                ""Not on a git tag, are you sure you want a release version?""
            )
        return f""{get_tag()}{self.get_post_build_suffix()}"""
get_nightly_version;"def get_nightly_version(self) -> str:
        date_str = datetime.today().strftime(""%Y%m%d"")
        build_suffix = self.get_post_build_suffix()
        return f""{get_base_version()}.dev{date_str}{build_suffix}"""
parse_json_and_links;"def parse_json_and_links(conn: Any) -> tuple[Any, dict[str, dict[str, str]]]:
    links = {}
    # Extract links which GH uses for pagination
    # see https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Link
    if ""Link"" in conn.headers:
        for elem in re.split("", *<"", conn.headers[""Link""]):
            try:
                url, params_ = elem.split("";"", 1)
            except ValueError:
                continue
            url = urllib.parse.unquote(url.strip(""<> ""))
            qparams = urllib.parse.parse_qs(params_.strip(), separator="";"")
            params = {
                k: v[0].strip('""')
                for k, v in qparams.items()
                if type(v) is list and len(v) > 0
            }
            params[""url""] = url
            if ""rel"" in params:
                links[params[""rel""]] = params

    return json.load(conn), links"
find_job_id_name;"def find_job_id_name(args: Any) -> tuple[str, str]:
    # From https://docs.github.com/en/actions/learn-github-actions/environment-variables
    PYTORCH_REPO = os.environ.get(""GITHUB_REPOSITORY"", ""pytorch/pytorch"")
    PYTORCH_GITHUB_API = f""https://api.github.com/repos/{PYTORCH_REPO}""
    GITHUB_TOKEN = os.environ[""GITHUB_TOKEN""]
    REQUEST_HEADERS = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": ""token "" + GITHUB_TOKEN,
    }

    url = f""{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100""
    jobs = fetch_jobs(url, REQUEST_HEADERS)

    # Sort the jobs list by start time, in descending order. We want to get the most
    # recently scheduled job on the runner.
    jobs.sort(key=operator.itemgetter(""started_at""), reverse=True)

    for job in jobs:
        if job[""runner_name""] == args.runner_name:
            return (job[""id""], job[""name""])

    raise RuntimeError(f""Can't find job id for runner {args.runner_name}"")"
set_output;"def set_output(name: str, val: Any) -> None:
    if os.getenv(""GITHUB_OUTPUT""):
        with open(str(os.getenv(""GITHUB_OUTPUT"")), ""a"") as env:
            print(f""{name}={val}"", file=env)
        print(f""setting {name}={val}"")
    else:
        print(f""::set-output name={name}::{val}"")"
gh_fetch_url_and_headers;"def gh_fetch_url_and_headers(
    url: str,
    *,
    headers: Optional[dict[str, str]] = None,
    data: Union[Optional[dict[str, Any]], str] = None,
    method: Optional[str] = None,
    reader: Callable[[Any], Any] = lambda x: x.read(),
) -> tuple[Any, Any]:
    if headers is None:
        headers = {}
    token = os.environ.get(""GITHUB_TOKEN"")
    if token is not None and url.startswith(f""{GITHUB_API_URL}/""):
        headers[""Authorization""] = f""token {token}""

    data_ = None
    if data is not None:
        data_ = data.encode() if isinstance(data, str) else json.dumps(data).encode()

    try:
        with urlopen(Request(url, headers=headers, data=data_, method=method)) as conn:
            return conn.headers, reader(conn)
    except HTTPError as err:
        if (
            err.code == 403
            and all(
                key in err.headers
                for key in [""X-RateLimit-Limit"", ""X-RateLimit-Remaining""]
            )
            and int(err.headers[""X-RateLimit-Remaining""]) == 0
        ):
            print(
                f""""""{url}
                Rate limit exceeded:
                Used: {err.headers[""X-RateLimit-Used""]}
                Limit: {err.headers[""X-RateLimit-Limit""]}
                Remaining: {err.headers[""X-RateLimit-Remaining""]}
                Resets at: {err.headers[""x-RateLimit-Reset""]}""""""
            )
        else:
            print(f""Error fetching {url} {err}"")
        raise"
gh_fetch_url;"def gh_fetch_url(
    url: str,
    *,
    headers: Optional[dict[str, str]] = None,
    data: Union[Optional[dict[str, Any]], str] = None,
    method: Optional[str] = None,
    reader: Callable[[Any], Any] = json.load,
) -> Any:
    return gh_fetch_url_and_headers(
        url, headers=headers, data=data, reader=reader, method=method
    )[1]"
gh_fetch_json;"def gh_fetch_json(
    url: str,
    params: Optional[dict[str, Any]] = None,
    data: Optional[dict[str, Any]] = None,
    method: Optional[str] = None,
) -> list[dict[str, Any]]:
    headers = {""Accept"": ""application/vnd.github.v3+json""}
    if params is not None and len(params) > 0:
        url += ""?"" + ""&"".join(
            f""{name}={quote(str(val))}"" for name, val in params.items()
        )
    return cast(
        list[dict[str, Any]],
        gh_fetch_url(url, headers=headers, data=data, reader=json.load, method=method),
    )"
_gh_fetch_json_any;"def _gh_fetch_json_any(
    url: str,
    params: Optional[dict[str, Any]] = None,
    data: Optional[dict[str, Any]] = None,
) -> Any:
    headers = {""Accept"": ""application/vnd.github.v3+json""}
    if params is not None and len(params) > 0:
        url += ""?"" + ""&"".join(
            f""{name}={quote(str(val))}"" for name, val in params.items()
        )
    return gh_fetch_url(url, headers=headers, data=data, reader=json.load)"
gh_fetch_json_list;"def gh_fetch_json_list(
    url: str,
    params: Optional[dict[str, Any]] = None,
    data: Optional[dict[str, Any]] = None,
) -> list[dict[str, Any]]:
    return cast(list[dict[str, Any]], _gh_fetch_json_any(url, params, data))"
gh_fetch_json_dict;"def gh_fetch_json_dict(
    url: str,
    params: Optional[dict[str, Any]] = None,
    data: Optional[dict[str, Any]] = None,
) -> dict[str, Any]:
    return cast(dict[str, Any], _gh_fetch_json_any(url, params, data))"
branches_containing_ref;"def branches_containing_ref(
        self, ref: str, *, include_remote: bool = True
    ) -> list[str]:
        rc = (
            self._run_git(""branch"", ""--remote"", ""--contains"", ref)
            if include_remote
            else self._run_git(""branch"", ""--contains"", ref)
        )
        return [x.strip() for x in rc.split(""\n"") if x.strip()] if len(rc) > 0 else []"
current_branch;"def current_branch(self) -> Optional[str]:
        try:
            return self._run_git(""symbolic-ref"", ""--short"", ""HEAD"").strip()
        except RuntimeError:
            # we are in detached HEAD state
            return None"
create_branch_and_checkout;"def create_branch_and_checkout(self, branch: str) -> None:
        self._run_git(""checkout"", ""-b"", branch)"
rev_parse;"def rev_parse(self, name: str) -> str:
        return self._run_git(""rev-parse"", ""--verify"", name).strip()"
get_merge_base;"def get_merge_base(self, from_ref: str, to_ref: str) -> str:
        return self._run_git(""merge-base"", from_ref, to_ref).strip()"
commits_resolving_gh_pr;"def commits_resolving_gh_pr(self, pr_num: int) -> list[str]:
        owner, name = self.gh_owner_and_name()
        msg = f""Pull Request resolved: https://github.com/{owner}/{name}/pull/{pr_num}""
        rc = self._run_git(""log"", ""--format=%H"", ""--grep"", msg).strip()
        return rc.split(""\n"") if len(rc) > 0 else []"
get_commit;"def get_commit(self, ref: str) -> GitCommit:
        return parse_fuller_format(
            self._run_git(""show"", ""--format=fuller"", ""--date=unix"", ""--shortstat"", ref)
        )"
cherry_pick;"def cherry_pick(self, ref: str) -> None:
        self._run_git(""cherry-pick"", ""-x"", ref)"
compute_branch_diffs;"def compute_branch_diffs(
        self, from_branch: str, to_branch: str
    ) -> tuple[list[str], list[str]]:
        """"""
        Returns list of commmits that are missing in each other branch since their merge base
        Might be slow if merge base is between two branches is pretty far off
        """"""
        from_ref = self.rev_parse(from_branch)
        to_ref = self.rev_parse(to_branch)
        merge_base = self.get_merge_base(from_ref, to_ref)
        from_commits = self.revlist(f""{merge_base}..{from_ref}"")
        to_commits = self.revlist(f""{merge_base}..{to_ref}"")
        from_ids = fuzzy_list_to_dict(self.patch_id(from_commits))
        to_ids = fuzzy_list_to_dict(self.patch_id(to_commits))
        for patch_id in set(from_ids).intersection(set(to_ids)):
            from_values = from_ids[patch_id]
            to_values = to_ids[patch_id]
            if len(from_values) != len(to_values):
                # Eliminate duplicate commits+reverts from the list
                while len(from_values) > 0 and len(to_values) > 0:
                    frc = self.get_commit(from_values.pop())
                    toc = self.get_commit(to_values.pop())
                    # FRC branch might have PR number added to the title
                    if frc.title != toc.title or frc.author_date != toc.author_date:
                        # HACK: Same commit were merged, reverted and landed again
                        # which creates a tracking problem
                        if (
                            ""pytorch/pytorch"" not in self.remote_url()
                            or frc.commit_hash
                            not in {
                                ""0a6a1b27a464ba5be5f587cce2ee12ab8c504dbf"",
                                ""6d0f4a1d545a8f161df459e8d4ccafd4b9017dbe"",
                                ""edf909e58f06150f7be41da2f98a3b9de3167bca"",
                                ""a58c6aea5a0c9f8759a4154e46f544c8b03b8db1"",
                                ""7106d216c29ca16a3504aa2bedad948ebcf4abc2"",
                            }
                        ):
                            raise RuntimeError(
                                f""Unexpected differences between {frc} and {toc}""
                            )
                    from_commits.remove(frc.commit_hash)
                    to_commits.remove(toc.commit_hash)
                continue
            for commit in from_values:
                from_commits.remove(commit)
            for commit in to_values:
                to_commits.remove(commit)
        # Another HACK: Patch-id is not stable for commits with binary files or for big changes across commits
        # I.e. cherry-picking those from one branch into another will change patchid
        if ""pytorch/pytorch"" in self.remote_url():
            for excluded_commit in {
                ""8e09e20c1dafcdbdb45c2d1574da68a32e54a3a5"",
                ""5f37e5c2a39c3acb776756a17730b865f0953432"",
                ""b5222584e6d6990c6585981a936defd1af14c0ba"",
                ""84d9a2e42d5ed30ec3b8b4140c38dd83abbce88d"",
                ""f211ec90a6cdc8a2a5795478b5b5c8d7d7896f7e"",
            }:
                if excluded_commit in from_commits:
                    from_commits.remove(excluded_commit)

        return (from_commits, to_commits)"
cherry_pick_commits;"def cherry_pick_commits(self, from_branch: str, to_branch: str) -> None:
        orig_branch = self.current_branch()
        assert orig_branch is not None, ""Must be on a branch""
        self.checkout(to_branch)
        from_commits, to_commits = self.compute_branch_diffs(from_branch, to_branch)
        if len(from_commits) == 0:
            print(""Nothing to do"")
            self.checkout(orig_branch)
            return
        for commit in reversed(from_commits):
            print(f""Cherry picking commit {commit}"")
            self.cherry_pick(commit)
        self.checkout(orig_branch)"
remote_url;"def remote_url(self) -> str:
        return self._run_git(""remote"", ""get-url"", self.remote)"
gh_owner_and_name;"def gh_owner_and_name(self) -> tuple[str, str]:
        url = os.getenv(""GIT_REMOTE_URL"", None)
        if url is None:
            url = self.remote_url()
        rc = RE_GITHUB_URL_MATCH.match(url)
        if rc is None:
            raise RuntimeError(f""Unexpected url format {url}"")
        return cast(tuple[str, str], rc.groups())"
commit_message;"def commit_message(self, ref: str) -> str:
        return self._run_git(""log"", ""-1"", ""--format=%B"", ref)"
amend_commit_message;"def amend_commit_message(self, msg: str) -> None:
        self._run_git(""commit"", ""--amend"", ""-m"", msg)"
request_for_labels;"def request_for_labels(url: str) -> tuple[Any, Any]:
    headers = {""Accept"": ""application/vnd.github.v3+json""}
    return gh_fetch_url_and_headers(
        url, headers=headers, reader=lambda x: x.read().decode(""utf-8"")
    )"
update_labels;"def update_labels(labels: list[str], info: str) -> None:
    labels_json = json.loads(info)
    labels.extend([x[""name""] for x in labels_json])"
get_last_page_num_from_header;"def get_last_page_num_from_header(header: Any) -> int:
    # Link info looks like: <https://api.github.com/repositories/65600975/labels?per_page=100&page=2>;
    # rel=""next"", <https://api.github.com/repositories/65600975/labels?per_page=100&page=3>; rel=""last""
    link_info = header[""link""]
    # Docs does not specify that it should be present for projects with just few labels
    # And https://github.com/malfet/deleteme/actions/runs/7334565243/job/19971396887 it's not the case  # @lint-ignore
    if link_info is None:
        return 1
    prefix = ""&page=""
    suffix = "">;""
    return int(
        link_info[link_info.rindex(prefix) + len(prefix) : link_info.rindex(suffix)]
    )"
gh_get_labels;"def gh_get_labels(org: str, repo: str) -> list[str]:
    prefix = f""https://api.github.com/repos/{org}/{repo}/labels?per_page=100""
    header, info = request_for_labels(prefix + ""&page=1"")
    labels: list[str] = []
    update_labels(labels, info)

    last_page = get_last_page_num_from_header(header)
    assert last_page > 0, (
        ""Error reading header info to determine total number of pages of labels""
    )
    for page_number in range(2, last_page + 1):  # skip page 1
        _, info = request_for_labels(prefix + f""&page={page_number}"")
        update_labels(labels, info)

    return labels"
gh_add_labels;"def gh_add_labels(
    org: str, repo: str, pr_num: int, labels: Union[str, list[str]], dry_run: bool
) -> None:
    if dry_run:
        print(f""Dryrun: Adding labels {labels} to PR {pr_num}"")
        return
    gh_fetch_url_and_headers(
        url=f""https://api.github.com/repos/{org}/{repo}/issues/{pr_num}/labels"",
        data={""labels"": labels},
    )"
gh_remove_label;"def gh_remove_label(
    org: str, repo: str, pr_num: int, label: str, dry_run: bool
) -> None:
    if dry_run:
        print(f""Dryrun: Removing {label} from PR {pr_num}"")
        return
    gh_fetch_url_and_headers(
        url=f""https://api.github.com/repos/{org}/{repo}/issues/{pr_num}/labels/{label}"",
        method=""DELETE"",
    )"
get_release_notes_labels;"def get_release_notes_labels(org: str, repo: str) -> list[str]:
    return [
        label
        for label in gh_get_labels(org, repo)
        if label.lstrip().startswith(""release notes:"")
    ]"
has_required_labels;"def has_required_labels(pr: ""GitHubPR"") -> bool:
    pr_labels = pr.get_labels()
    # Check if PR is not user facing
    is_not_user_facing_pr = any(
        label.strip() == ""topic: not user facing"" for label in pr_labels
    )
    return is_not_user_facing_pr or any(
        label.strip() in get_release_notes_labels(pr.org, pr.project)
        for label in pr_labels
    )"
is_label_err_comment;"def is_label_err_comment(comment: GitHubComment) -> bool:
    # comment.body_text returns text without markdown
    no_format_title = LABEL_ERR_MSG_TITLE.replace(""`"", """")
    return (
        comment.body_text.lstrip("" #"").startswith(no_format_title)
        and comment.author_login in BOT_AUTHORS
    )"
set_output;"def set_output(name: str, val: str) -> None:
    if os.getenv(""GITHUB_OUTPUT""):
        with open(str(os.getenv(""GITHUB_OUTPUT"")), ""a"") as env:
            print(f""{name}={val}"", file=env)
    else:
        print(f""::set-output name={name}::{val}"")"
main;"def main() -> None:
    args = parse_args()

    pr_identifier = PRIdentifier(args.pr_identifier)
    print(f""PR identifier for `{args.pr_identifier}` is `{pr_identifier}`"")

    repo = GithubRepo.from_string(args.repo)
    cache_dir = Path(args.cache_dir)
    if args.temp_dir:
        temp_dir = Path(args.temp_dir)
    else:
        temp_dir = Path(TEMP_DIR)

    if args.upload:
        print(f""Uploading cache with args {args}"")

        # verify the cache dir exists
        if not cache_dir.exists():
            print(f""The pytest cache dir `{cache_dir}` does not exist. Skipping upload"")
            return

        upload_pytest_cache(
            pr_identifier=pr_identifier,
            repo=repo,
            job_identifier=args.job_identifier,
            sha=args.sha,
            test_config=args.test_config,
            shard=args.shard,
            cache_dir=cache_dir,
            bucket=args.bucket,
            temp_dir=temp_dir,
        )

    if args.download:
        print(f""Downloading cache with args {args}"")
        download_pytest_cache(
            pr_identifier=pr_identifier,
            repo=repo,
            job_identifier=args.job_identifier,
            dest_cache_dir=cache_dir,
            bucket=args.bucket,
            temp_dir=temp_dir,
        )"
upload_pytest_cache;"def upload_pytest_cache(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    sha: str,
    test_config: str,
    shard: str,
    cache_dir: Path,
    temp_dir: Path,
    bucket: str = BUCKET,
) -> None:
    """"""
    Uploads the pytest cache to S3, merging it with any previous caches from previous runs of the same job.
    In particular, this keeps all the failed tests across all runs of this job in the cache, so that
    future jobs that download this cache will prioritize running tests that have failed in the past.

    Args:
        pr_identifier: A unique, human readable identifier for the PR
        job: The name of the job that is uploading the cache
    """"""

    if not isinstance(pr_identifier, PRIdentifier):
        raise ValueError(
            f""pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}""
        )

    if not bucket:
        bucket = BUCKET

    # Upload the cache
    obj_key_prefix = _get_s3_key_prefix(
        pr_identifier, repo, job_identifier, sha, test_config, shard
    )
    zip_file_path = zip_folder(cache_dir, temp_dir / ZIP_UPLOAD / obj_key_prefix)
    obj_key = f""{obj_key_prefix}{os.path.splitext(zip_file_path)[1]}""  # Keep the new file extension
    upload_file_to_s3(zip_file_path, bucket, obj_key)"
download_pytest_cache;"def download_pytest_cache(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    dest_cache_dir: Path,
    temp_dir: Path,
    bucket: str = BUCKET,
) -> None:
    """"""
    Downloads the pytest cache from S3. The goal is to detect any tests that have failed in the past
    and run them first, so that the dev can get faster feedback on them.

    We merge the cache from all shards since tests can get shuffled around from one shard to another
    (based on when we last updated our stats on how long each test takes to run). This ensures that
    even if a test moves to a different shard, that shard will know to run it first if had failed previously.
    """"""
    if not bucket:
        bucket = BUCKET

    if not isinstance(pr_identifier, PRIdentifier):
        raise ValueError(
            f""pr_identifier must be of type PRIdentifier, not {type(pr_identifier)}""
        )

    obj_key_prefix = _get_s3_key_prefix(pr_identifier, repo, job_identifier)

    zip_download_dir = temp_dir / CACHE_ZIP_DOWNLOADS / obj_key_prefix

    # downloads the cache zips for all shards
    downloads = download_s3_objects_with_prefix(
        bucket, obj_key_prefix, zip_download_dir
    )

    for downloaded_zip in downloads:
        # Unzip into random folder, then merge with the current cache
        cache_dir_for_shard = (
            temp_dir / UNZIPPED_CACHES / os.urandom(16).hex() / PYTEST_CACHE_DIR_NAME
        )

        unzip_folder(downloaded_zip, cache_dir_for_shard)
        print(f""Merging cache from {downloaded_zip}"")
        _merge_pytest_caches(cache_dir_for_shard, dest_cache_dir)"
_get_s3_key_prefix;"def _get_s3_key_prefix(
    pr_identifier: PRIdentifier,
    repo: GithubRepo,
    job_identifier: str,
    sha: str = """",
    test_config: str = """",
    shard: str = """",
) -> str:
    """"""
    The prefix to any S3 object key for a pytest cache. It's only a prefix though, not a full path to an object.
    For example, it won't include the file extension.
    """"""
    prefix = f""{PYTEST_CACHE_KEY_PREFIX}/{repo.owner}/{repo.name}/{pr_identifier}/{sanitize_for_s3(job_identifier)}""

    if sha:
        prefix += f""/{sha}""
    if test_config:
        prefix += f""/{sanitize_for_s3(test_config)}""
    if shard:
        prefix += f""/{shard}""

    return prefix"
_merge_pytest_caches;"def _merge_pytest_caches(
    pytest_cache_dir_to_merge_from: Path, pytest_cache_dir_to_merge_into: Path
) -> None:
    # LASTFAILED_FILE_PATH is the only file we actually care about in the cache
    # since it contains all the tests that failed.
    #
    # The remaining files are static supporting files that don't really matter. They
    # make the cache folder play nice with other tools devs tend to use (e.g. git).
    # But since pytest doesn't recreate these files if the .pytest_cache folder already exists,
    # we'll copy them over as a way to protect against future bugs where a certain tool
    # may need those files to exist to work properly (their combined file size is negligible)
    static_files_to_copy = [
        "".gitignore"",
        ""CACHEDIR.TAG"",
        ""README.md"",
    ]

    # Copy over the static files. These files never change, so only copy them
    # if they don't already exist in the new cache
    for static_file in static_files_to_copy:
        source_file = pytest_cache_dir_to_merge_from / static_file
        if not source_file.is_file():
            continue

        dest_file = pytest_cache_dir_to_merge_into / static_file
        if not dest_file.exists():
            copy_file(source_file, dest_file)

    # Handle the v/cache/lastfailed file
    _merge_lastfailed_files(
        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into
    )

    _merge_additional_failures_files(
        pytest_cache_dir_to_merge_from, pytest_cache_dir_to_merge_into
    )"
_merge_lastfailed_files;"def _merge_lastfailed_files(source_pytest_cache: Path, dest_pytest_cache: Path) -> None:
    # Simple cases where one of the files doesn't exist
    source_lastfailed_file = source_pytest_cache / LASTFAILED_FILE_PATH
    dest_lastfailed_file = dest_pytest_cache / LASTFAILED_FILE_PATH

    if not source_lastfailed_file.exists():
        return
    if not dest_lastfailed_file.exists():
        copy_file(source_lastfailed_file, dest_lastfailed_file)
        return

    # Both files exist, so we need to merge them
    from_lastfailed = load_json_file(source_lastfailed_file)
    to_lastfailed = load_json_file(dest_lastfailed_file)
    merged_content = _merged_lastfailed_content(from_lastfailed, to_lastfailed)

    # Save the results
    write_json_file(dest_lastfailed_file, merged_content)"
_merged_lastfailed_content;"def _merged_lastfailed_content(
    from_lastfailed: dict[str, bool], to_lastfailed: dict[str, bool]
) -> dict[str, bool]:
    """"""
    The lastfailed files are dictionaries where the key is the test identifier.
    Each entry's value appears to always be `true`, but let's not count on that.
    An empty dictionary is represented with a single value with an empty string as the key.
    """"""

    # If an entry in from_lastfailed doesn't exist in to_lastfailed, add it and it's value
    for key in from_lastfailed:
        if key not in to_lastfailed:
            to_lastfailed[key] = from_lastfailed[key]

    if len(to_lastfailed) > 1:
        # Remove the empty entry if it exists since we have actual entries now
        if """" in to_lastfailed:
            del to_lastfailed[""""]

    return to_lastfailed"
_merge_additional_failures_files;"def _merge_additional_failures_files(
    source_pytest_cache: Path, dest_pytest_cache: Path
) -> None:
    # Simple cases where one of the files doesn't exist
    source_lastfailed_file = (
        source_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL
    )
    dest_lastfailed_file = dest_pytest_cache / TD_HEURISTIC_PREVIOUSLY_FAILED_ADDITIONAL

    if not source_lastfailed_file.exists():
        return
    if not dest_lastfailed_file.exists():
        copy_file(source_lastfailed_file, dest_lastfailed_file)
        return

    # Both files exist, so we need to merge them
    from_lastfailed = load_json_file(source_lastfailed_file)
    to_lastfailed = load_json_file(dest_lastfailed_file)
    merged_content = list(set(from_lastfailed + to_lastfailed))

    # Save the results
    write_json_file(dest_lastfailed_file, merged_content)"
from_string;"def from_string(cls, repo_string: str) -> ""GithubRepo"":
        if ""/"" not in repo_string:
            raise ValueError(
                f""repo_string must be of the form 'owner/repo', not {repo_string}""
            )

        owner, name = repo_string.split(""/"")
        return cls(owner, name)"
set_github_output;"def set_github_output(key: str, value: str) -> None:
    """"""
    Defines outputs of the github action that invokes this script
    """"""
    if not GITHUB_OUTPUT:
        # See https://github.blog/changelog/2022-10-11-github-actions-deprecating-save-state-and-set-output-commands/ for deprecation notice
        log.warning(
            ""No env var found for GITHUB_OUTPUT, you must be running this code locally. Falling back to the deprecated print method.""
        )
        print(f""::set-output name={key}::{value}"")
        return

    with open(GITHUB_OUTPUT, ""a"") as f:
        log.info(f""Setting output: {key}='{value}'"")
        f.write(f""{key}={value}\n"")"
_str_comma_separated_to_set;"def _str_comma_separated_to_set(value: str) -> frozenset[str]:
    return frozenset(
        filter(lambda itm: itm != """", map(str.strip, value.strip("" \n\t"").split("","")))
    )"
parse_args;"def parse_args() -> Any:
    parser = ArgumentParser(""Get dynamic rollout settings"")
    parser.add_argument(""--github-token"", type=str, required=True, help=""GitHub token"")
    parser.add_argument(
        ""--github-issue-repo"",
        type=str,
        required=False,
        default=""pytorch/test-infra"",
        help=""GitHub repo to get the issue"",
    )
    parser.add_argument(
        ""--github-repo"",
        type=str,
        required=True,
        help=""GitHub repo where CI is running"",
    )
    parser.add_argument(
        ""--github-issue"", type=int, required=True, help=""GitHub issue number""
    )
    parser.add_argument(
        ""--github-actor"", type=str, required=True, help=""GitHub triggering_actor""
    )
    parser.add_argument(
        ""--github-issue-owner"", type=str, required=True, help=""GitHub issue owner""
    )
    parser.add_argument(
        ""--github-branch"", type=str, required=True, help=""Current GitHub branch or tag""
    )
    parser.add_argument(
        ""--github-ref-type"",
        type=str,
        required=True,
        help=""Current GitHub ref type, branch or tag"",
    )
    parser.add_argument(
        ""--eligible-experiments"",
        type=_str_comma_separated_to_set,
        required=False,
        default="""",
        help=""comma separated list of experiments to check, if omitted all experiments marked with default=True are checked"",
    )
    parser.add_argument(
        ""--opt-out-experiments"",
        type=_str_comma_separated_to_set,
        required=False,
        default="""",
        help=(
            ""comma separated list of experiments to opt-out of. If unset, no opt-outs will occur. ""
            ""If the same experiment is listed both here and in '--eligible-experiments' opt-out will take priority.""
        ),
    )
    parser.add_argument(
        ""--pr-number"",
        type=str,
        required=False,
        default="""",
        help=""the optional PR number where this is run"",
    )

    return parser.parse_args()"
get_gh_client;"def get_gh_client(github_token: str) -> Github:  # type: ignore[no-any-unimported]
    auth = Auth.Token(github_token)
    return Github(auth=auth)"
get_issue;"def get_issue(gh: Github, repo: str, issue_num: int) -> Issue:  # type: ignore[no-any-unimported]
    repo = gh.get_repo(repo)
    return repo.get_issue(number=issue_num)"
get_potential_pr_author;"def get_potential_pr_author(
    github_token: str, repo: str, username: str, ref_type: str, ref_name: str
) -> str:
    # If the trigger was a new tag added by a bot, this is a ciflow case
    # Fetch the actual username from the original PR. The PR number is
    # embedded in the tag name: ciflow/<name>/<pr-number>

    gh = get_gh_client(github_token)

    if username == ""pytorch-bot[bot]"" and ref_type == ""tag"":
        split_tag = ref_name.split(""/"")
        if (
            len(split_tag) == 3
            and split_tag[0] == ""ciflow""
            and split_tag[2].isnumeric()
        ):
            pr_number = split_tag[2]
            try:
                repository = gh.get_repo(repo)
                pull = repository.get_pull(number=int(pr_number))
            except Exception as e:
                raise Exception(  # noqa: TRY002
                    f""issue with pull request {pr_number} from repo {repository}""
                ) from e
            return pull.user.login  # type: ignore[no-any-return]
    # In all other cases, return the original input username
    return username"
is_exception_branch;"def is_exception_branch(branch: str) -> bool:
    """"""
    Branches that get opted out of experiments by default, until they're explicitly enabled.
    """"""
    return branch.split(""/"")[0] in {""main"", ""nightly"", ""release"", ""landchecks""}"
load_yaml;"def load_yaml(yaml_text: str) -> Any:
    try:
        data = yaml.safe_load(yaml_text)
        return data
    except yaml.YAMLError:
        log.exception(""Error loading YAML"")
        raise"
extract_settings_user_opt_in_from_text;"def extract_settings_user_opt_in_from_text(rollout_state: str) -> tuple[str, str]:
    """"""
    Extracts the text with settings, if any, and the opted in users from the rollout state.

    If the issue body contains ""---"" then the text above that is the settings
    and the text below is the list of opted in users.

    If it doesn't contain ""---"" then the settings are empty and the rest is the users.
    """"""
    rollout_state_parts = rollout_state.split(""---"")
    if len(rollout_state_parts) >= 2:
        return rollout_state_parts[0], rollout_state_parts[1]
    else:
        return """", rollout_state"
parse_user_opt_in_from_text;"def parse_user_opt_in_from_text(user_optin_text: str) -> UserOptins:
    """"""
    Parse the user opt-in text into a key value pair of username and the list of features they have opted into

    Users are GitHub usernames with the @ prefix. Each user is also a comma-separated list of features/experiments to enable.
        - Example line: ""@User1,lf,split_build""
        - A ""#"" prefix indicates the user is opted out of all experiments


    """"""
    optins = UserOptins()
    for user in user_optin_text.split(""\n""):
        user = user.strip(""\r\n\t -"")
        if not user or not user.startswith(""@""):
            # Not a valid user. Skip
            continue

        if user:
            usr_name = user.split("","")[0].strip(""@"")
            optins[usr_name] = [exp.strip("" "") for exp in user.split("","")[1:]]

    return optins"
is_valid_experiment_name;"def is_valid_experiment_name(experiment_name: str) -> bool:
    """"""
    Check if the experiment name is valid.
    A valid name:
        - Contains only alphanumeric characters and the special characters ""_"" & ""-""
        - The special characters ""_"" & ""-"" shouldn't be the first or last characters
        - Cannot contain spaces
    """"""

    valid_char_regex = r""^[a-zA-Z0-9]([\w-]*[a-zA-Z0-9])?$""
    valid = bool(re.match(valid_char_regex, experiment_name))

    if valid:
        return True

    log.error(
        f""Invalid experiment name: {experiment_name}. Experiment names should only contain alphanumeric characters, '_', and '-'. They cannot contain spaces, and the special characters '_' and '-' cannot be the first or last characters.""
    )
    return False"
parse_settings_from_text;"def parse_settings_from_text(settings_text: str) -> Settings:
    """"""
    Parse the experiments from the issue body into a list of ExperimentSettings
    """"""
    try:
        if settings_text:
            # Escape the backtick as well so that we can have the settings in a code block on the GH issue
            # for easy reading
            # Note: Using ascii for the backtick so that the cat step in _runner-determinator.yml doesn't choke on
            #       the backtick character in shell commands.
            backtick = chr(96)  # backtick character
            settings_text = settings_text.strip(f""\r\n\t{backtick} "")
            settings = load_yaml(settings_text)

            # For now we just load experiments. We can expand this if/when we add more settings
            experiments = {}

            for exp_name, exp_settings in settings.get(SETTING_EXPERIMENTS).items():
                if not is_valid_experiment_name(exp_name):
                    # Exclude invalid experiments from the list. We log an error, but don't raise an exception so that other experiments can still be processed.
                    continue

                valid_settings = {}
                for setting in exp_settings:
                    if setting not in Experiment._fields:
                        log.warning(
                            f""Unexpected setting in experiment: {setting} = {exp_settings[setting]}""
                        )
                    else:
                        valid_settings[setting] = exp_settings[setting]

                experiments[exp_name] = Experiment(**valid_settings)
            return Settings(experiments)

    except Exception:
        log.exception(""Failed to parse settings"")

    return Settings()"
parse_settings;"def parse_settings(rollout_state: str) -> Settings:
    """"""
    Parse settings, if any, from the rollout state.

    If the issue body contains ""---"" then the text above that is the settings
    and the text below is the list of opted in users.

    If it doesn't contain ""---"" then the settings are empty and the default values are used.
    """"""
    settings_text, _ = extract_settings_user_opt_in_from_text(rollout_state)
    return parse_settings_from_text(settings_text)"
parse_users;"def parse_users(rollout_state: str) -> UserOptins:
    """"""
    Parse users from the rollout state.

    """"""
    _, users_text = extract_settings_user_opt_in_from_text(rollout_state)
    return parse_user_opt_in_from_text(users_text)"
is_user_opted_in;"def is_user_opted_in(user: str, user_optins: UserOptins, experiment_name: str) -> bool:
    """"""
    Check if a user is opted into an experiment
    """"""
    return experiment_name in user_optins.get(user, [])"
is_user_opted_out;"def is_user_opted_out(user: str, user_optins: UserOptins, experiment_name: str) -> bool:
    """"""
    Check if a user explicitly opted out of an experiment
    """"""
    # if the experiment is prefixed with a ""-"", then it's an opt-out
    experiment_optout = ""-"" + experiment_name
    if experiment_optout not in user_optins.get(user, []):
        return False

    if is_user_opted_in(user, user_optins, experiment_name):
        log.warning(
            f""User {user} is opted into experiment {experiment_name}, but also opted out of it. Defaulting to opting out""
        )

    return True"
get_runner_prefix;"def get_runner_prefix(
    rollout_state: str,
    workflow_requestors: Iterable[str],
    branch: str,
    eligible_experiments: frozenset[str] = frozenset(),
    opt_out_experiments: frozenset[str] = frozenset(),
    is_canary: bool = False,
) -> str:
    settings = parse_settings(rollout_state)
    user_optins = parse_users(rollout_state)

    fleet_prefix = """"
    prefixes = []
    for experiment_name, experiment_settings in settings.experiments.items():
        if not experiment_settings.all_branches and is_exception_branch(branch):
            log.info(
                f""Branch {branch} is an exception branch. Not enabling experiment {experiment_name}.""
            )
            continue

        if opt_out_experiments:
            if experiment_name in opt_out_experiments:
                opt_out_exp_list = "", "".join(opt_out_experiments)
                log.info(
                    f""Skipping experiment '{experiment_name}', as this workflow has opted-out (opted out experiments are: {opt_out_exp_list})""
                )
                continue

        if eligible_experiments:
            if experiment_name not in eligible_experiments:
                exp_list = "", "".join(eligible_experiments)
                log.info(
                    f""Skipping experiment '{experiment_name}', as it is not in the eligible_experiments list: {exp_list}""
                )
                continue
        elif not experiment_settings.default:
            log.info(
                f""Skipping experiment '{experiment_name}', as it is not a default experiment""
            )
            continue

        # Is any workflow_requestor opted out to this experiment?
        opted_out_users = [
            requestor
            for requestor in workflow_requestors
            if is_user_opted_out(requestor, user_optins, experiment_name)
        ]

        if opted_out_users:
            log.info(
                f""{', '.join(opted_out_users)} have opted out of experiment {experiment_name}.""
            )
            continue

        # Is any workflow_requestor opted in to this experiment?
        opted_in_users = [
            requestor
            for requestor in workflow_requestors
            if is_user_opted_in(requestor, user_optins, experiment_name)
        ]

        enabled = False
        if opted_in_users:
            log.info(
                f""{', '.join(opted_in_users)} have opted into experiment {experiment_name}.""
            )
            enabled = True

        elif experiment_settings.rollout_perc:
            # If no user is opted in, then we randomly enable the experiment based on the rollout percentage
            if random.uniform(0, 100) <= experiment_settings.rollout_perc:
                log.info(
                    f""Based on rollout percentage of {experiment_settings.rollout_perc}%, enabling experiment {experiment_name}.""
                )
                enabled = True

        if enabled:
            label = experiment_name
            if experiment_name == LF_FLEET_EXPERIMENT:
                # We give some special treatment to the ""lf"" experiment since determines the fleet we use
                #  - If it's enabled, then we always list it's prefix first
                #  - If we're in the canary branch, then we append "".c"" to the lf prefix
                if is_canary:
                    label += CANARY_FLEET_SUFFIX
                fleet_prefix = label
            else:
                prefixes.append(label)

    if len(prefixes) > 1:
        log.error(
            f""Only a fleet and one other experiment can be enabled for a job at any time. Enabling {prefixes[0]} and ignoring the rest, which are {', '.join(prefixes[1:])}""
        )
        prefixes = prefixes[:1]

    # Fleet always comes first
    if fleet_prefix:
        prefixes.insert(0, fleet_prefix)

    return ""."".join(prefixes) + ""."" if prefixes else """""
get_rollout_state_from_issue;"def get_rollout_state_from_issue(github_token: str, repo: str, issue_num: int) -> str:
    """"""
    Gets the first comment of the issue, which contains the desired rollout state.

    The default issue we use - https://github.com/pytorch/test-infra/issues/5132
    """"""
    gh = get_gh_client(github_token)
    issue = get_issue(gh, repo, issue_num)
    return str(issue.get_comments()[0].body.strip(""\n\t ""))"
download_json;"def download_json(url: str, headers: dict[str, str], num_retries: int = 3) -> Any:
    for _ in range(num_retries):
        try:
            req = Request(url=url, headers=headers)
            content = urlopen(req, timeout=5).read().decode(""utf-8"")
            return json.loads(content)
        except Exception as e:
            log.warning(f""Could not download {url}: {e}"")

    log.warning(f""All {num_retries} retries exhausted, downloading {url} failed"")
    return {}"
get_pr_info;"def get_pr_info(github_repo: str, github_token: str, pr_number: int) -> dict[str, Any]:
    """"""
    Dynamically get PR information
    """"""
    github_api = f""https://api.github.com/repos/{github_repo}""
    headers = {
        ""Accept"": ""application/vnd.github.v3+json"",
        ""Authorization"": f""token {github_token}"",
    }
    json_response: dict[str, Any] = download_json(
        url=f""{github_api}/issues/{pr_number}"",
        headers=headers,
    )

    if not json_response:
        log.warning(f""Failed to get the labels for #{pr_number}"")
        return {}

    return json_response"
get_labels;"def get_labels(github_repo: str, github_token: str, pr_number: int) -> set[str]:
    """"""
    Dynamically get the latest list of labels from the pull request
    """"""
    pr_info = get_pr_info(github_repo, github_token, pr_number)
    return {
        label.get(""name"") for label in pr_info.get(""labels"", []) if label.get(""name"")
    }"
tag_image;"def tag_image(
    image: str,
    default_tag: str,
    release_version: str,
    dry_run: str,
    tagged_images: dict[str, bool],
) -> None:
    if image in tagged_images:
        return
    release_image = image.replace(f""-{default_tag}"", f""-{release_version}"")
    print(f""Tagging {image} to {release_image} , dry_run: {dry_run}"")

    if dry_run == ""disabled"":
        subprocess.check_call([""docker"", ""pull"", image])
        subprocess.check_call([""docker"", ""tag"", image, release_image])
        subprocess.check_call([""docker"", ""push"", release_image])
    tagged_images[image] = True"
mock_parse_args;"def mock_parse_args() -> object:
    class Object:
        def __init__(self) -> None:
            self.pr_num = 76123
            self.exit_non_zero = False

    return Object()"
mock_add_label_err_comment;"def mock_add_label_err_comment(pr: ""GitHubPR"") -> None:
    pass"
mock_delete_all_label_err_comments;"def mock_delete_all_label_err_comments(pr: ""GitHubPR"") -> None:
    pass"
mock_get_comments;"def mock_get_comments() -> list[GitHubComment]:
    return [
        # Case 1 - a non label err comment
        GitHubComment(
            body_text=""mock_body_text"",
            created_at="""",
            author_login="""",
            author_association="""",
            editor_login=None,
            database_id=1,
            url="""",
        ),
        # Case 2 - a label err comment
        GitHubComment(
            body_text="" #"" + LABEL_ERR_MSG_TITLE.replace(""`"", """"),
            created_at="""",
            author_login=BOT_AUTHORS[1],
            author_association="""",
            editor_login=None,
            database_id=2,
            url="""",
        ),
    ]"
_gen_expected_string;"def _gen_expected_string(
            keep_going: bool = False,
            ci_verbose_test_logs: bool = False,
            ci_test_showlocals: bool = False,
            ci_no_test_timeout: bool = False,
            ci_no_td: bool = False,
            ci_td_distributed: bool = False,
            is_unstable: bool = False,
            reenabled_issues: str = """",
        ) -> str:
            return (
                f""keep-going={keep_going}\n""
                f""ci-verbose-test-logs={ci_verbose_test_logs}\n""
                f""ci-test-showlocals={ci_test_showlocals}\n""
                f""ci-no-test-timeout={ci_no_test_timeout}\n""
                f""ci-no-td={ci_no_td}\n""
                f""ci-td-distributed={ci_td_distributed}\n""
                f""is-unstable={is_unstable}\n""
                f""reenabled-issues={reenabled_issues}\n""
            )"
_skip_if_ref_does_not_exist;"def _skip_if_ref_does_not_exist(self, ref: str) -> None:
        """"""Skip test if ref is missing as stale branches are deleted with time""""""
        try:
            self.repo.show_ref(ref)
        except RuntimeError as e:
            raise SkipTest(f""Can't find head ref {ref} due to {str(e)}"") from e"
foo;"def foo(x: int, y: int) -> int:
            return x + y"
mock_query;"def mock_query(
    fallback_function: Any,
    file_name: str,
    key_function: Any,
    *args: Any,
) -> Any:
    gql_db_fname = os.path.join(os.path.dirname(__file__), file_name)

    def get_mocked_queries() -> Any:
        if not os.path.exists(gql_db_fname):
            return {}
        with gzip.open(gql_db_fname, encoding=""utf-8"", mode=""rt"") as f:
            return json.load(f)

    def save_mocked_queries(obj: Any) -> None:
        with gzip.open(gql_db_fname, encoding=""utf-8"", mode=""wt"") as f:
            json.dump(obj, f, indent=2)
            f.write(""\n"")

    key = key_function(*args)
    mocked_queries = get_mocked_queries()

    if key in mocked_queries:
        return mocked_queries[key]

    try:
        rc = fallback_function(*args)
    except HTTPError as err:
        if err.code == 401 or err.code == 403:
            err_msg = f""If you are seeing this message during workflow run, please make sure to update {file_name}""
            err_msg += f"" locally, by deleting it and running {os.path.basename(__file__)} with""
            err_msg += "" GitHub Personal Access Token passed via GITHUB_TOKEN""
            err_msg += "" and drci api key passed via DRCI_BOT_KEY environment variables""
            if os.getenv(""GITHUB_TOKEN"") is None or os.getenv(""DRCI_BOT_KEY"") is None:
                err_msg = (
                    ""Failed to update cached queries as GITHUB_TOKEN or DRCI_BOT_KEY ""
                    + ""is not defined. ""
                    + err_msg
                )
            raise RuntimeError(err_msg) from err
    mocked_queries[key] = rc

    save_mocked_queries(mocked_queries)

    return rc"
mocked_gh_graphql;"def mocked_gh_graphql(query: str, **kwargs: Any) -> Any:
    def key_function(query: str, kwargs: Any) -> str:
        return f""query_sha={sha256(query.encode('utf-8')).hexdigest()} "" + "" "".join(
            [f""{k}={kwargs[k]}"" for k in sorted(kwargs.keys())]
        )

    def gh_graphql_wrapper(query: str, kwargs: Any) -> Any:
        return gh_graphql(query, **kwargs)

    return mock_query(gh_graphql_wrapper, GQL_MOCKS, key_function, query, kwargs)"
mocked_drci_classifications;"def mocked_drci_classifications(pr_num: int, project: str, num_retries: int = 3) -> Any:
    return mock_query(
        get_drci_classifications,
        DRCI_MOCKS,
        lambda x, y: f""{x} {y}"",
        pr_num,
        project,
    )"
mock_parse_args;"def mock_parse_args(revert: bool = False, force: bool = False) -> Any:
    class Object:
        def __init__(self) -> None:
            self.revert = revert
            self.force = force
            self.pr_num = 76123
            self.dry_run = True
            self.comment_id = 0
            self.reason = ""this is for testing""
            self.ignore_current = False
            self.check_mergeability = False

    return Object()"
mock_remove_label;"def mock_remove_label(
    org: str, repo: str, pr_num: str, label: str, dry_run: bool
) -> None:
    pass"
mock_revert;"def mock_revert(
    repo: GitRepo,
    pr: GitHubPR,
    *,
    dry_run: bool = False,
    comment_id: Optional[int] = None,
    reason: Optional[str] = None,
) -> None:
    pass"
mock_merge;"def mock_merge(
    pr: GitHubPR,
    repo: GitRepo,
    dry_run: bool = False,
    skip_mandatory_checks: bool = False,
    comment_id: Optional[int] = None,
    timeout_minutes: int = 400,
    stale_pr_days: int = 3,
    ignore_current: bool = False,
) -> None:
    pass"
mock_gh_get_info;"def mock_gh_get_info() -> Any:
    return {
        ""closed"": False,
        ""isCrossRepository"": False,
        ""headRefName"": ""foo"",
        ""baseRefName"": ""bar"",
        ""baseRepository"": {""defaultBranchRef"": {""name"": ""bar""}},
        ""files"": {""nodes"": [], ""pageInfo"": {""hasNextPage"": False}},
        ""changedFiles"": 0,
    }"
mocked_read_merge_rules_NE;"def mocked_read_merge_rules_NE(repo: Any, org: str, project: str) -> list[MergeRule]:
    return [
        MergeRule(
            name=""mock with nonexistent check"",
            patterns=[""*""],
            approved_by=[],
            mandatory_checks_name=[""Lint"", ""Facebook CLA Check"", ""nonexistent""],
            ignore_flaky_failures=True,
        ),
    ]"
mocked_read_merge_rules;"def mocked_read_merge_rules(repo: Any, org: str, project: str) -> list[MergeRule]:
    return [
        MergeRule(
            name=""super"",
            patterns=[""*""],
            approved_by=[""pytorch/metamates"", ""ngimel""],
            mandatory_checks_name=[
                ""Lint"",
                ""pull / linux-xenial-cuda11.3-py3.7-gcc7 / build"",
            ],
            ignore_flaky_failures=True,
        ),
        MergeRule(
            name=""xla"",
            patterns=["".github/ci_commit_pins/xla.txt""],
            approved_by=[""pytorchbot""],
            mandatory_checks_name=[
                ""Lint"",
                ""EasyCLA"",
                ""pull / linux-focal-py3_8-clang9-xla / build"",
                ""pull / linux-focal-py3_8-clang9-xla / test (xla, 1, 1, linux.12xlarge)"",
            ],
            ignore_flaky_failures=True,
        ),
    ]"
mocked_read_merge_rules_approvers;"def mocked_read_merge_rules_approvers(
    repo: Any, org: str, project: str
) -> list[MergeRule]:
    return [
        MergeRule(
            name=""Core Reviewers"",
            patterns=[""*""],
            approved_by=[""1"", ""2"", ""3"", ""4"", ""5"", ""6""],
            mandatory_checks_name=[
                ""Lint"",
                ""pull"",
            ],
        ),
        MergeRule(
            name=""Core Maintainers"",
            patterns=[""*""],
            approved_by=[""1"", ""2"", ""malfet""],
            mandatory_checks_name=[
                ""Lint"",
                ""pull"",
            ],
        ),
    ]"
mocked_read_merge_rules_raise;"def mocked_read_merge_rules_raise(repo: Any, org: str, project: str) -> list[MergeRule]:
    raise RuntimeError(""testing"")"
xla_merge_rules;"def xla_merge_rules(repo: Any, org: str, project: str) -> list[MergeRule]:
    return [
        MergeRule(
            name="" OSS CI / pytorchbot / XLA"",
            patterns=["".github/ci_commit_pins/xla.txt""],
            approved_by=[""pytorchbot""],
            mandatory_checks_name=[
                ""Lint"",
                ""EasyCLA"",
                ""pull / linux-bionic-py3_8-clang8-xla / build"",
                ""pull / linux-bionic-py3_8-clang8-xla / test (xla, 1, 1, linux.4xlarge)"",
                ""inductor / cuda11.8-py3.10-gcc7-sm86 / test (inductor_torchbench_dynamic, 1, 1, linux.g5.4xlarge.nvidia.gpu)"",
            ],
            ignore_flaky_failures=False,
        ),
    ]"
get_mocked_queries;"def get_mocked_queries() -> Any:
        if not os.path.exists(gql_db_fname):
            return {}
        with gzip.open(gql_db_fname, encoding=""utf-8"", mode=""rt"") as f:
            return json.load(f)"
save_mocked_queries;"def save_mocked_queries(obj: Any) -> None:
        with gzip.open(gql_db_fname, encoding=""utf-8"", mode=""wt"") as f:
            json.dump(obj, f, indent=2)
            f.write(""\n"")"
gh_graphql_wrapper;"def gh_graphql_wrapper(query: str, kwargs: Any) -> Any:
        return gh_graphql(query, **kwargs)"
commits_resolving_gh_pr;"def commits_resolving_gh_pr(self, pr_num: int) -> list[str]:
        return [""FakeCommitSha""]"
commit_message;"def commit_message(self, ref: str) -> str:
        return ""super awsome commit message"""
get_co_authors;"def get_co_authors(self, *args: Any) -> None:
        """"""Tests that co-authors are recognized""""""
        pr = GitHubPR(""pytorch"", ""pytorch"", 118347)
        authors = pr.get_authors()
        self.assertIn(""kit1980"", authors)
        self.assertIn(""Co-authored-by:"", pr.gen_commit_message())"
commit_message;"def commit_message(self, ref: str) -> str:
                return pr.get_body()"
mocked_rev_parse;"def mocked_rev_parse(branch: str) -> str:
    return branch"
gh_get_pr_info;"def gh_get_pr_info(org: str, proj: str, pr_no: int) -> Any:
    rc = gh_graphql(GH_GET_PR_INFO_QUERY, name=proj, owner=org, number=pr_no)
    return rc[""data""][""repository""][""pullRequest""]"
gh_get_team_members;"def gh_get_team_members(org: str, name: str) -> list[str]:
    rc: list[str] = []
    team_members: dict[str, Any] = {
        ""pageInfo"": {""hasNextPage"": ""true"", ""endCursor"": None}
    }
    while bool(team_members[""pageInfo""][""hasNextPage""]):
        query = gh_graphql(
            GH_GET_TEAM_MEMBERS_QUERY,
            org=org,
            name=name,
            cursor=team_members[""pageInfo""][""endCursor""],
        )
        team = query[""data""][""organization""][""team""]
        if team is None:
            warn(f""Requested non-existing team {org}/{name}"")
            return []
        team_members = team[""members""]
        rc += [member[""login""] for member in team_members[""nodes""]]
    return rc"
get_check_run_name_prefix;"def get_check_run_name_prefix(workflow_run: Any) -> str:
    if workflow_run is None:
        return """"
    else:
        return f""{workflow_run['workflow']['name']} / """
is_passing_status;"def is_passing_status(status: Optional[str]) -> bool:
    return status is not None and status.upper() in [""SUCCESS"", ""SKIPPED"", ""NEUTRAL""]"
add_workflow_conclusions;"def add_workflow_conclusions(
    checksuites: Any,
    get_next_checkruns_page: Callable[[list[dict[str, dict[str, Any]]], int, Any], Any],
    get_next_checksuites: Callable[[Any], Any],
) -> JobNameToStateDict:
    # graphql seems to favor the most recent workflow run, so in theory we
    # shouldn't need to account for reruns, but do it just in case

    # workflow -> job -> job info
    workflows: dict[str, WorkflowCheckState] = {}

    # for the jobs that don't have a workflow
    no_workflow_obj: WorkflowCheckState = WorkflowCheckState("""", """", 0, None)

    def add_conclusions(edges: Any) -> None:
        for edge_idx, edge in enumerate(edges):
            node = edge[""node""]
            workflow_run = node[""workflowRun""]
            checkruns = node[""checkRuns""]

            workflow_obj: WorkflowCheckState = no_workflow_obj

            if workflow_run is not None:
                # This is the usual workflow run ID we see on GitHub
                workflow_run_id = workflow_run[""databaseId""]
                # While this is the metadata name and ID of the workflow itself
                workflow_name = workflow_run[""workflow""][""name""]
                workflow_id = workflow_run[""workflow""][""databaseId""]

                workflow_conclusion = node[""conclusion""]
                # Do not override existing status with cancelled
                if workflow_conclusion == ""CANCELLED"" and workflow_name in workflows:
                    continue

                # Only keep the latest workflow run for each workflow, heuristically,
                # it's the run with largest run ID
                if (
                    workflow_id not in workflows
                    or workflows[workflow_id].run_id < workflow_run_id
                ):
                    workflows[workflow_id] = WorkflowCheckState(
                        name=workflow_name,
                        status=workflow_conclusion,
                        url=workflow_run[""url""],
                        run_id=workflow_run_id,
                    )
                workflow_obj = workflows[workflow_id]

            while checkruns is not None:
                for checkrun_node in checkruns[""nodes""]:
                    if not isinstance(checkrun_node, dict):
                        warn(f""Expected dictionary, but got {type(checkrun_node)}"")
                        continue
                    checkrun_name = f""{get_check_run_name_prefix(workflow_run)}{checkrun_node['name']}""
                    existing_checkrun = workflow_obj.jobs.get(checkrun_name)
                    if existing_checkrun is None or not is_passing_status(
                        existing_checkrun.status
                    ):
                        workflow_obj.jobs[checkrun_name] = JobCheckState(
                            checkrun_name,
                            checkrun_node[""detailsUrl""],
                            checkrun_node[""conclusion""],
                            classification=None,
                            job_id=checkrun_node[""databaseId""],
                            title=checkrun_node[""title""],
                            summary=checkrun_node[""summary""],
                        )

                if bool(checkruns[""pageInfo""][""hasNextPage""]):
                    checkruns = get_next_checkruns_page(edges, edge_idx, checkruns)
                else:
                    checkruns = None

    all_edges = checksuites[""edges""].copy()
    while bool(checksuites[""pageInfo""][""hasNextPage""]):
        checksuites = get_next_checksuites(checksuites)
        all_edges.extend(checksuites[""edges""])

    add_conclusions(all_edges)

    # Flatten the dictionaries.  If there exists jobs in the workflow run, put
    # the jobs in but don't put the workflow in.  We care more about the jobs in
    # the workflow that ran than the container workflow.
    res: JobNameToStateDict = {}
    for workflow in workflows.values():
        if len(workflow.jobs) > 0:
            for job_name, job in workflow.jobs.items():
                res[job_name] = job
        else:
            res[workflow.name] = JobCheckState(
                workflow.name,
                workflow.url,
                workflow.status,
                classification=None,
                job_id=None,
                title=None,
                summary=None,
            )
    for job_name, job in no_workflow_obj.jobs.items():
        res[job_name] = job
    return res"
parse_args;"def parse_args() -> Any:
    from argparse import ArgumentParser

    parser = ArgumentParser(""Merge PR into default branch"")
    parser.add_argument(""--dry-run"", action=""store_true"")
    parser.add_argument(""--revert"", action=""store_true"")
    parser.add_argument(""--force"", action=""store_true"")
    parser.add_argument(""--ignore-current"", action=""store_true"")
    parser.add_argument(""--check-mergeability"", action=""store_true"")
    parser.add_argument(""--comment-id"", type=int)
    parser.add_argument(""--reason"", type=str)
    parser.add_argument(""pr_num"", type=int)
    return parser.parse_args()"
can_skip_internal_checks;"def can_skip_internal_checks(pr: ""GitHubPR"", comment_id: Optional[int] = None) -> bool:
    if comment_id is None:
        return False
    comment = pr.get_comment_by_id(comment_id)
    if comment.editor_login is not None:
        return False
    return comment.author_login == ""facebook-github-bot"""
_revlist_to_prs;"def _revlist_to_prs(
    repo: GitRepo,
    pr: ""GitHubPR"",
    rev_list: Iterable[str],
    should_skip: Optional[Callable[[int, ""GitHubPR""], bool]] = None,
) -> list[tuple[""GitHubPR"", str]]:
    rc: list[tuple[GitHubPR, str]] = []
    for idx, rev in enumerate(rev_list):
        msg = repo.commit_message(rev)
        m = RE_PULL_REQUEST_RESOLVED.search(msg)
        if m is None:
            raise RuntimeError(
                f""Could not find PR-resolved string in {msg} of ghstacked PR {pr.pr_num}""
            )
        if pr.org != m.group(""owner"") or pr.project != m.group(""repo""):
            raise RuntimeError(
                f""PR {m.group('number')} resolved to wrong owner/repo pair""
            )
        pr_num = int(m.group(""number""))
        candidate = GitHubPR(pr.org, pr.project, pr_num) if pr_num != pr.pr_num else pr
        if should_skip is not None and should_skip(idx, candidate):
            continue
        rc.append((candidate, rev))
    return rc"
get_ghstack_prs;"def get_ghstack_prs(
    repo: GitRepo, pr: ""GitHubPR"", open_only: bool = True
) -> list[tuple[""GitHubPR"", str]]:
    """"""
    Get the PRs in the stack that are below this PR (inclusive).  Throws error if any of the open PRs are out of sync.
    @:param open_only: Only return open PRs
    """"""
    # For ghstack, cherry-pick commits based from origin
    orig_ref = f""{repo.remote}/{pr.get_ghstack_orig_ref()}""
    rev_list = repo.revlist(f""{pr.default_branch()}..{orig_ref}"")

    def skip_func(idx: int, candidate: ""GitHubPR"") -> bool:
        if not open_only or not candidate.is_closed():
            return False
        print(
            f""Skipping {idx + 1} of {len(rev_list)} PR (#{candidate.pr_num}) as its already been merged""
        )
        return True

    assert pr.is_ghstack_pr()
    entire_stack = _revlist_to_prs(repo, pr, reversed(rev_list), skip_func)

    for stacked_pr, rev in entire_stack:
        if stacked_pr.is_closed():
            continue
        base_ref = stacked_pr.base_ref()
        if base_ref == pr.default_branch():
            base_ref = repo.get_merge_base(
                f""{repo.remote}/{base_ref}"", f""{repo.remote}/{stacked_pr.head_ref()}""
            )
        if not are_ghstack_branches_in_sync(repo, stacked_pr.head_ref(), base_ref):
            raise RuntimeError(
                f""PR {stacked_pr.pr_num} is out of sync with the corresponding revision {rev} on ""
                + f""branch {stacked_pr.get_ghstack_orig_ref()} that would be merged into {stacked_pr.default_branch()}.  ""
                + ""This usually happens because there is a non ghstack change in the PR.  ""
                + f""Please sync them and try again (ex. make the changes on {orig_ref} and run ghstack).""
            )
    return entire_stack"
gen_new_issue_link;"def gen_new_issue_link(
    org: str, project: str, labels: list[str], template: str = ""bug-report.yml""
) -> str:
    labels_str = "","".join(labels)
    return (
        f""https://github.com/{org}/{project}/issues/new?""
        f""labels={urllib.parse.quote(labels_str)}&""
        f""template={urllib.parse.quote(template)}""
    )"
read_merge_rules;"def read_merge_rules(
    repo: Optional[GitRepo], org: str, project: str
) -> list[MergeRule]:
    """"""Returns the list of all merge rules for the repo or project.

    NB: this function is used in Meta-internal workflows, see the comment
    at the top of this file for details.
    """"""
    repo_relative_rules_path = MERGE_RULE_PATH
    if repo is None:
        json_data = gh_fetch_url(
            f""https://api.github.com/repos/{org}/{project}/contents/{repo_relative_rules_path}"",
            headers={""Accept"": ""application/vnd.github.v3+json""},
            reader=json.load,
        )
        content = base64.b64decode(json_data[""content""])
        return [MergeRule(**x) for x in yaml.safe_load(content)]
    else:
        rules_path = Path(repo.repo_dir) / repo_relative_rules_path
        if not rules_path.exists():
            print(f""{rules_path} does not exist, returning empty rules"")
            return []
        with open(rules_path) as fp:
            rc = yaml.safe_load(fp)
        return [MergeRule(**x) for x in rc]"
find_matching_merge_rule;"def find_matching_merge_rule(
    pr: GitHubPR,
    repo: Optional[GitRepo] = None,
    skip_mandatory_checks: bool = False,
    skip_internal_checks: bool = False,
    ignore_current_checks: Optional[list[str]] = None,
) -> tuple[
    MergeRule,
    list[tuple[str, Optional[str], Optional[int]]],
    list[tuple[str, Optional[str], Optional[int]]],
    dict[str, list[Any]],
]:
    """"""
    Returns merge rule matching to this pr together with the list of associated pending
    and failing jobs OR raises an exception.

    NB: this function is used in Meta-internal workflows, see the comment at the top of
    this file for details.
    """"""
    changed_files = pr.get_changed_files()
    approved_by = set(pr.get_approved_by())

    issue_link = gen_new_issue_link(
        org=pr.org,
        project=pr.project,
        labels=[""module: ci""],
    )
    reject_reason = f""No rule found to match PR. Please [report]{issue_link} this issue to DevX team.""

    rules = read_merge_rules(repo, pr.org, pr.project)
    if not rules:
        reject_reason = f""Rejecting the merge as no rules are defined for the repository in {MERGE_RULE_PATH}""
        raise RuntimeError(reject_reason)

    checks = pr.get_checkrun_conclusions()
    checks = get_classifications(
        pr.pr_num,
        pr.project,
        checks,
        ignore_current_checks=ignore_current_checks,
    )

    # This keeps the list of all approvers that could stamp the change
    all_rule_approvers = {}

    # PRs can fail multiple merge rules, but it only needs to pass one rule to be approved.
    # If it fails all rules, we need to find the rule that it came closest to passing and report
    # that to the dev.
    #
    # reject_reason_score ranks rules by relevancy. The higher the score, the more relevant the
    # rule & rejection reason, and we only care about the most relevant rule/reason
    #
    # reject_reason_score intrepretation:
    # Score 0 to 10K - how many files rule matched
    # Score 10K - matched all files, but no overlapping approvers
    # Score 20K - matched all files and approvers, but mandatory checks are pending
    # Score 30k - Matched all files and approvers, but mandatory checks failed
    reject_reason_score = 0
    for rule in rules:
        rule_name = rule.name
        patterns_re = patterns_to_regex(rule.patterns)
        non_matching_files = []

        # Does this rule apply to all the files?
        for fname in changed_files:
            if not patterns_re.match(fname):
                non_matching_files.append(fname)
        if len(non_matching_files) > 0:
            num_matching_files = len(changed_files) - len(non_matching_files)
            if num_matching_files > reject_reason_score:
                reject_reason_score = num_matching_files
                reject_reason = ""\n"".join(
                    (
                        f""Not all files match rule `{rule_name}`."",
                        f""{num_matching_files} files matched, but there are still non-matching files:"",
                        f""{','.join(non_matching_files[:5])}{', ...' if len(non_matching_files) > 5 else ''}"",
                    )
                )
            continue

        # If rule needs approvers but PR has not been reviewed, skip it
        if len(rule.approved_by) > 0 and len(approved_by) == 0:
            if reject_reason_score < 10000:
                reject_reason_score = 10000
                reject_reason = f""PR #{pr.pr_num} has not been reviewed yet""
            continue

        # Does the PR have the required approvals for this rule?
        rule_approvers = set()
        for approver in rule.approved_by:
            if ""/"" in approver:
                org, name = approver.split(""/"")
                rule_approvers.update(gh_get_team_members(org, name))
            else:
                rule_approvers.add(approver)
        approvers_intersection = approved_by.intersection(rule_approvers)
        # If rule requires approvers but they aren't the ones that reviewed PR
        if len(approvers_intersection) == 0 and len(rule_approvers) > 0:
            # Less than or equal is intentionally used here to gather all potential
            # approvers
            if reject_reason_score <= 10000:
                reject_reason_score = 10000

                all_rule_approvers[rule.name] = rule.approved_by
                # Prepare the reject reason
                all_rule_approvers_msg = [
                    f""- {name} ({', '.join(approved_by[:5])}{', ...' if len(approved_by) > 5 else ''})""
                    for name, approved_by in all_rule_approvers.items()
                ]

                reject_reason = ""Approvers from one of the following sets are needed:\n""
                reject_reason += ""\n"".join(all_rule_approvers_msg)

            continue

        # Does the PR pass the checks required by this rule?
        mandatory_checks = (
            rule.mandatory_checks_name if rule.mandatory_checks_name is not None else []
        )
        required_checks = list(
            filter(
                lambda x: (""EasyCLA"" in x)
                or (""Facebook CLA Check"" in x)
                or not skip_mandatory_checks,
                mandatory_checks,
            )
        )
        pending_checks, failed_checks, _ = categorize_checks(
            checks,
            required_checks,
            ok_failed_checks_threshold=IGNORABLE_FAILED_CHECKS_THESHOLD
            if rule.ignore_flaky_failures
            else 0,
        )

        # categorize_checks assumes all tests are required if required_checks is empty.
        # this is a workaround as we want to keep that behavior for categorize_checks
        # generally.
        if not required_checks:
            pending_checks = []
            failed_checks = []

        hud_link = f""https://hud.pytorch.org/{pr.org}/{pr.project}/commit/{pr.last_commit()['oid']}""
        if len(failed_checks) > 0:
            if reject_reason_score < 30000:
                reject_reason_score = 30000
                reject_reason = ""\n"".join(
                    (
                        f""{len(failed_checks)} mandatory check(s) failed.  The first few are:"",
                        *checks_to_markdown_bullets(failed_checks),
                        """",
                        f""Dig deeper by [viewing the failures on hud]({hud_link})"",
                    )
                )
            continue
        elif len(pending_checks) > 0:
            if reject_reason_score < 20000:
                reject_reason_score = 20000
                reject_reason = ""\n"".join(
                    (
                        f""{len(pending_checks)} mandatory check(s) are pending/not yet run.  The first few are:"",
                        *checks_to_markdown_bullets(pending_checks),
                        """",
                        f""Dig deeper by [viewing the pending checks on hud]({hud_link})"",
                    )
                )
            continue

        if not skip_internal_checks and pr.has_internal_changes():
            raise RuntimeError(
                ""This PR has internal changes and must be landed via Phabricator! Please try reimporting/rexporting the PR!""
            )

        # Categorize all checks when skip_mandatory_checks (force merge) is set. Do it here
        # where the list of checks is readily available. These records will be saved into
        # s3 merge records
        (
            pending_mandatory_checks,
            failed_mandatory_checks,
            ignorable_checks,
        ) = categorize_checks(
            checks,
            [],
            ok_failed_checks_threshold=IGNORABLE_FAILED_CHECKS_THESHOLD,
        )
        return (
            rule,
            pending_mandatory_checks,
            failed_mandatory_checks,
            ignorable_checks,
        )

    if reject_reason_score == 20000:
        raise MandatoryChecksMissingError(reject_reason, rule)
    raise MergeRuleFailedError(reject_reason, rule)"
checks_to_str;"def checks_to_str(checks: list[tuple[str, Optional[str]]]) -> str:
    return "", "".join(f""[{c[0]}]({c[1]})"" if c[1] is not None else c[0] for c in checks)"
checks_to_markdown_bullets;"def checks_to_markdown_bullets(
    checks: list[tuple[str, Optional[str], Optional[int]]],
) -> list[str]:
    return [
        f""- [{c[0]}]({c[1]})"" if c[1] is not None else f""- {c[0]}"" for c in checks[:5]
    ]"
post_starting_merge_comment;"def post_starting_merge_comment(
    repo: GitRepo,
    pr: GitHubPR,
    explainer: TryMergeExplainer,
    dry_run: bool,
    ignore_current_checks_info: Optional[
        list[tuple[str, Optional[str], Optional[int]]]
    ] = None,
) -> None:
    """"""Post the initial merge starting message on the PR. Also post a short
    message on all PRs in the stack.""""""
    gh_post_pr_comment(
        pr.org,
        pr.project,
        pr.pr_num,
        explainer.get_merge_message(ignore_current_checks_info),
        dry_run=dry_run,
    )
    if pr.is_ghstack_pr():
        for additional_prs, _ in get_ghstack_prs(repo, pr):
            if additional_prs.pr_num != pr.pr_num:
                gh_post_pr_comment(
                    additional_prs.org,
                    additional_prs.project,
                    additional_prs.pr_num,
                    f""Starting merge as part of PR stack under #{pr.pr_num}"",
                    dry_run=dry_run,
                )"
manually_close_merged_pr;"def manually_close_merged_pr(
    pr: GitHubPR,
    additional_merged_prs: list[GitHubPR],
    merge_commit_sha: str,
    dry_run: bool,
) -> None:
    def _comment_and_close(pr: GitHubPR, comment: str) -> None:
        pr = GitHubPR(pr.org, pr.project, pr.pr_num)  # Refresh the PR
        if not pr.is_closed():
            gh_post_pr_comment(pr.org, pr.project, pr.pr_num, comment, dry_run)
            gh_close_pr(pr.org, pr.project, pr.pr_num, dry_run)

    message = (
        f""This PR (#{pr.pr_num}) was merged in {merge_commit_sha} but it is still open, likely due to a Github bug, ""
        ""so mergebot is closing it manually.  If you think this is a mistake, please feel free to reopen and contact Dev Infra.""
    )
    _comment_and_close(pr, message)
    for additional_pr in additional_merged_prs:
        message = (
            f""This PR (#{additional_pr.pr_num}) was merged as part of PR #{pr.pr_num} in the stack under {merge_commit_sha} ""
            ""but it is still open, likely due to a Github bug, so mergebot is closing it manually. ""
            ""If you think this is a mistake, please feel free to reopen and contact Dev Infra.""
        )
        _comment_and_close(additional_pr, message)

    print(f""PR {pr.pr_num} and all additional PRs in the stack have been closed."")"
save_merge_record;"def save_merge_record(
    comment_id: int,
    pr_num: int,
    owner: str,
    project: str,
    author: str,
    pending_checks: list[tuple[str, Optional[str], Optional[int]]],
    failed_checks: list[tuple[str, Optional[str], Optional[int]]],
    ignore_current_checks: list[tuple[str, Optional[str], Optional[int]]],
    broken_trunk_checks: list[tuple[str, Optional[str], Optional[int]]],
    flaky_checks: list[tuple[str, Optional[str], Optional[int]]],
    unstable_checks: list[tuple[str, Optional[str], Optional[int]]],
    last_commit_sha: str,
    merge_base_sha: str,
    merge_commit_sha: str = """",
    is_failed: bool = False,
    skip_mandatory_checks: bool = False,
    ignore_current: bool = False,
    error: str = """",
) -> None:
    """"""
    This saves the merge records as a json, which can later be uploaded to s3
    """"""

    # Prepare the record to be written into s3
    data = [
        {
            ""comment_id"": comment_id,
            ""pr_num"": pr_num,
            ""owner"": owner,
            ""project"": project,
            ""author"": author,
            ""pending_checks"": pending_checks,
            ""failed_checks"": failed_checks,
            ""ignore_current_checks"": ignore_current_checks,
            ""broken_trunk_checks"": broken_trunk_checks,
            ""flaky_checks"": flaky_checks,
            ""unstable_checks"": unstable_checks,
            ""last_commit_sha"": last_commit_sha,
            ""merge_base_sha"": merge_base_sha,
            ""merge_commit_sha"": merge_commit_sha,
            ""is_failed"": is_failed,
            ""skip_mandatory_checks"": skip_mandatory_checks,
            ""ignore_current"": ignore_current,
            ""error"": error,
            # This is a unique identifier for the record for deduping purposes
            # in Rockset.  Any unique string would work.  This will not be used
            # after we migrate off Rockset
            ""_id"": f""{project}-{pr_num}-{comment_id}-{os.environ.get('GITHUB_RUN_ID')}"",
        }
    ]
    repo_root = Path(__file__).resolve().parent.parent.parent

    with open(repo_root / ""merge_record.json"", ""w"") as f:
        json.dump(data, f)"
get_drci_classifications;"def get_drci_classifications(pr_num: int, project: str = ""pytorch"") -> Any:
    """"""
    Query HUD API to find similar failures to decide if they are flaky
    """"""
    # NB: This doesn't work internally atm because this requires making an
    # external API call to HUD
    failures = gh_fetch_url(
        f""https://hud.pytorch.org/api/drci/drci?prNumber={pr_num}"",
        data=f""repo={project}"",
        headers={
            ""Authorization"": os.getenv(""DRCI_BOT_KEY"", """"),
            ""Accept"": ""application/vnd.github.v3+json"",
        },
        method=""POST"",
        reader=json.load,
    )

    return failures.get(str(pr_num), {}) if failures else {}"
remove_job_name_suffix;"def remove_job_name_suffix(name: str, replacement: str = "")"") -> str:
    return re.sub(REMOVE_JOB_NAME_SUFFIX_REGEX, replacement, name)"
is_broken_trunk;"def is_broken_trunk(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # Consult the list of broken trunk failures from Dr.CI
    return any(
        (name == broken_trunk[""name""]) or (job_id and job_id == broken_trunk[""id""])
        for broken_trunk in drci_classifications.get(""BROKEN_TRUNK"", [])
    )"
is_unstable;"def is_unstable(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # The job name has the unstable keyword. This is the original way to mark a job
    # as unstable on HUD, Dr.CI, and trymerge
    if ""unstable"" in name:
        return True

    # Consult the list of unstable failures from Dr.CI
    return any(
        (name == unstable[""name""] or (job_id and job_id == unstable[""id""]))
        for unstable in drci_classifications.get(""UNSTABLE"", [])
    )"
is_flaky;"def is_flaky(
    check: JobCheckState,
    drci_classifications: Any,
) -> bool:
    if not check or not drci_classifications:
        return False

    name = check.name
    job_id = check.job_id

    # Consult the list of flaky failures from Dr.CI
    return any(
        (name == flaky[""name""] or (job_id and job_id == flaky[""id""]))
        for flaky in drci_classifications.get(""FLAKY"", [])
    )"
is_invalid_cancel;"def is_invalid_cancel(
    name: str,
    conclusion: Optional[str],
    drci_classifications: Any,
) -> bool:
    """"""
    After https://github.com/pytorch/test-infra/pull/4579, invalid cancelled
    signals have been removed from HUD and Dr.CI. The same needs to be done
    here for consistency
    """"""
    if (
        not name
        or not drci_classifications
        or not conclusion
        or conclusion.upper() != ""CANCELLED""
    ):
        return False

    # If a job is cancelled and not listed as a failure by Dr.CI, it's an
    # invalid signal and can be ignored
    return all(
        name != failure[""name""] for failure in drci_classifications.get(""FAILED"", [])
    )"
get_classifications;"def get_classifications(
    pr_num: int,
    project: str,
    checks: dict[str, JobCheckState],
    ignore_current_checks: Optional[list[str]],
) -> dict[str, JobCheckState]:
    # Get the failure classification from Dr.CI, which is the source of truth
    # going forward. It's preferable to try calling Dr.CI API directly first
    # to get the latest results as well as update Dr.CI PR comment
    drci_classifications = get_drci_classifications(pr_num=pr_num, project=project)

    def get_readable_drci_results(drci_classifications: Any) -> str:
        try:
            s = f""From Dr.CI API ({pr_num}):\n""
            for classification, jobs in drci_classifications.items():
                s += f""  {classification}: \n""
                for job in jobs:
                    s += f""    {job['id']} {job['name']}\n""
            return s
        except Exception:
            return f""From Dr.CI API: {json.dumps(drci_classifications)}""

    print(get_readable_drci_results(drci_classifications))

    # NB: if the latest results from Dr.CI is not available, i.e. when calling from
    # SandCastle, we fallback to any results we can find on Dr.CI check run summary
    if (
        not drci_classifications
        and DRCI_CHECKRUN_NAME in checks
        and checks[DRCI_CHECKRUN_NAME]
        and checks[DRCI_CHECKRUN_NAME].summary
    ):
        drci_summary = checks[DRCI_CHECKRUN_NAME].summary
        try:
            print(f""From Dr.CI checkrun summary: {drci_summary}"")
            drci_classifications = json.loads(str(drci_summary))
        except json.JSONDecodeError:
            warn(""Invalid Dr.CI checkrun summary"")
            drci_classifications = {}

    checks_with_classifications = checks.copy()
    for name, check in checks.items():
        if check.status == ""SUCCESS"" or check.status == ""NEUTRAL"":
            continue

        if is_unstable(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                ""UNSTABLE"",
                check.job_id,
                check.title,
                check.summary,
            )
            continue

        # NB: It's important to note that when it comes to ghstack and broken trunk classification,
        # Dr.CI uses the base of the whole stack
        if is_broken_trunk(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                ""BROKEN_TRUNK"",
                check.job_id,
                check.title,
                check.summary,
            )
            continue

        elif is_flaky(check, drci_classifications):
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                ""FLAKY"",
                check.job_id,
                check.title,
                check.summary,
            )
            continue

        elif is_invalid_cancel(name, check.status, drci_classifications):
            # NB: Create a new category here for invalid cancelled signals because
            # there are usually many of them when they happen. So, they shouldn't
            # be counted toward ignorable failures threshold
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                ""INVALID_CANCEL"",
                check.job_id,
                check.title,
                check.summary,
            )
            continue

        if ignore_current_checks is not None and name in ignore_current_checks:
            checks_with_classifications[name] = JobCheckState(
                check.name,
                check.url,
                check.status,
                ""IGNORE_CURRENT_CHECK"",
                check.job_id,
                check.title,
                check.summary,
            )

    return checks_with_classifications"
filter_checks_with_lambda;"def filter_checks_with_lambda(
    checks: JobNameToStateDict, status_filter: Callable[[Optional[str]], bool]
) -> list[JobCheckState]:
    return [check for check in checks.values() if status_filter(check.status)]"
get_pr_commit_sha;"def get_pr_commit_sha(repo: GitRepo, pr: GitHubPR) -> str:
    commit_sha = pr.get_merge_commit()
    if commit_sha is not None:
        return commit_sha
    commits = repo.commits_resolving_gh_pr(pr.pr_num)
    if len(commits) == 0:
        raise PostCommentError(""Can't find any commits resolving PR"")
    return commits[0]"
validate_revert;"def validate_revert(
    repo: GitRepo, pr: GitHubPR, *, comment_id: Optional[int] = None
) -> tuple[str, str]:
    comment = (
        pr.get_last_comment()
        if comment_id is None
        else pr.get_comment_by_id(comment_id)
    )
    if comment.editor_login is not None:
        raise PostCommentError(""Don't want to revert based on edited command"")
    author_association = comment.author_association
    author_login = comment.author_login
    allowed_reverters = [""COLLABORATOR"", ""MEMBER"", ""OWNER""]
    # For some reason, one can not be a member of private repo, only CONTRIBUTOR
    if pr.is_base_repo_private():
        allowed_reverters.append(""CONTRIBUTOR"")
    if author_association not in allowed_reverters:
        raise PostCommentError(
            f""Will not revert as @{author_login} is not one of ""
            f""[{', '.join(allowed_reverters)}], but instead is {author_association}.""
        )

    # Raises exception if matching rule is not found, but ignores all status checks
    find_matching_merge_rule(
        pr, repo, skip_mandatory_checks=True, skip_internal_checks=True
    )
    commit_sha = get_pr_commit_sha(repo, pr)
    return (author_login, commit_sha)"
get_ghstack_dependent_prs;"def get_ghstack_dependent_prs(
    repo: GitRepo, pr: GitHubPR, only_closed: bool = True
) -> list[tuple[str, GitHubPR]]:
    """"""
    Get the PRs in the stack that are above this PR (inclusive).
    Throws error if stack have branched or original branches are gone
    """"""
    assert pr.is_ghstack_pr()
    orig_ref = f""{repo.remote}/{pr.get_ghstack_orig_ref()}""
    rev_list = repo.revlist(f""{pr.default_branch()}..{orig_ref}"")
    if len(rev_list) == 0:
        raise RuntimeError(
            f""PR {pr.pr_num} does not have any revisions associated with it""
        )
    skip_len = len(rev_list) - 1
    for branch in repo.branches_containing_ref(orig_ref):
        candidate = repo.revlist(f""{pr.default_branch()}..{branch}"")
        # Pick longest candidate
        if len(candidate) > len(rev_list):
            candidate, rev_list = rev_list, candidate
        # Validate that candidate always ends rev-list
        if rev_list[-len(candidate) :] != candidate:
            raise RuntimeError(
                f""Branch {branch} revlist {', '.join(candidate)} is not a subset of {', '.join(rev_list)}""
            )
    # Remove commits original PR depends on
    if skip_len > 0:
        rev_list = rev_list[:-skip_len]
    rc: list[tuple[str, GitHubPR]] = []
    for pr_, sha in _revlist_to_prs(repo, pr, rev_list):
        if not pr_.is_closed():
            if not only_closed:
                rc.append(("""", pr_))
            continue
        commit_sha = get_pr_commit_sha(repo, pr_)
        rc.append((commit_sha, pr_))
    return rc"
do_revert_prs;"def do_revert_prs(
    repo: GitRepo,
    original_pr: GitHubPR,
    shas_and_prs: list[tuple[str, GitHubPR]],
    *,
    author_login: str,
    extra_msg: str = """",
    skip_internal_checks: bool = False,
    dry_run: bool = False,
) -> None:
    # Prepare and push revert commits
    for commit_sha, pr in shas_and_prs:
        revert_msg = f""\nReverted {pr.get_pr_url()} on behalf of {prefix_with_github_url(author_login)}""
        revert_msg += extra_msg
        repo.checkout(pr.default_branch())
        repo.revert(commit_sha)
        msg = repo.commit_message(""HEAD"")
        msg = re.sub(RE_PULL_REQUEST_RESOLVED, """", msg)
        msg += revert_msg
        repo.amend_commit_message(msg)
    repo.push(shas_and_prs[0][1].default_branch(), dry_run)

    # Comment/reopen PRs
    for commit_sha, pr in shas_and_prs:
        revert_message = """"
        if pr.pr_num == original_pr.pr_num:
            revert_message += (
                f""@{pr.get_pr_creator_login()} your PR has been successfully reverted.""
            )
        else:
            revert_message += (
                f""@{pr.get_pr_creator_login()} your PR has been reverted as part of the stack under ""
                f""#{original_pr.pr_num}.\n""
            )
        if (
            pr.has_internal_changes()
            and not pr.has_no_connected_diff()
            and not skip_internal_checks
        ):
            revert_message += ""\n:warning: This PR might contain internal changes""
            revert_message += ""\ncc: @pytorch/pytorch-dev-infra""
        gh_post_pr_comment(
            pr.org, pr.project, pr.pr_num, revert_message, dry_run=dry_run
        )

        pr.add_numbered_label(""reverted"", dry_run)
        pr.add_label(""ci-no-td"", dry_run)
        if not dry_run:
            gh_post_commit_comment(pr.org, pr.project, commit_sha, revert_msg)
            gh_update_pr_state(pr.org, pr.project, pr.pr_num)"
try_revert;"def try_revert(
    repo: GitRepo,
    pr: GitHubPR,
    *,
    dry_run: bool = False,
    comment_id: Optional[int] = None,
    reason: Optional[str] = None,
) -> None:
    try:
        author_login, commit_sha = validate_revert(repo, pr, comment_id=comment_id)
    except PostCommentError as e:
        gh_post_pr_comment(pr.org, pr.project, pr.pr_num, str(e), dry_run=dry_run)
        return

    extra_msg = f"" due to {reason}"" if reason is not None else """"
    extra_msg += (
        f"" ([comment]({pr.get_comment_by_id(comment_id).url}))\n""
        if comment_id is not None
        else ""\n""
    )
    shas_and_prs = [(commit_sha, pr)]
    if pr.is_ghstack_pr():
        try:
            shas_and_prs = get_ghstack_dependent_prs(repo, pr)
            prs_to_revert = "" "".join([t[1].get_pr_url() for t in shas_and_prs])
            print(f""About to stack of PRs: {prs_to_revert}"")
        except Exception as e:
            print(
                f""Failed to fetch dependent PRs: {str(e)}, fall over to single revert""
            )

    do_revert_prs(
        repo,
        pr,
        shas_and_prs,
        author_login=author_login,
        extra_msg=extra_msg,
        dry_run=dry_run,
        skip_internal_checks=can_skip_internal_checks(pr, comment_id),
    )"
prefix_with_github_url;"def prefix_with_github_url(suffix_str: str) -> str:
    return f""https://github.com/{suffix_str}"""
check_for_sev;"def check_for_sev(org: str, project: str, skip_mandatory_checks: bool) -> None:
    if skip_mandatory_checks:
        return
    response = cast(
        dict[str, Any],
        gh_fetch_json_list(
            ""https://api.github.com/search/issues"",  # @lint-ignore
            # Having two label: queries is an AND operation
            params={
                ""q"": f'repo:{org}/{project} is:open is:issue label:""ci: sev"" label:""merge blocking""'
            },
        ),
    )
    if response[""total_count""] != 0:
        raise RuntimeError(
            ""Not merging any PRs at the moment because there is a ""
            + ""merge blocking https://github.com/pytorch/pytorch/labels/ci:%20sev issue open at: \n""
            + f""{response['items'][0]['html_url']}""
        )
    return"
has_label;"def has_label(labels: list[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:
    return len(list(filter(pattern.match, labels))) > 0"
categorize_checks;"def categorize_checks(
    check_runs: JobNameToStateDict,
    required_checks: list[str],
    ok_failed_checks_threshold: Optional[int] = None,
) -> tuple[
    list[tuple[str, Optional[str], Optional[int]]],
    list[tuple[str, Optional[str], Optional[int]]],
    dict[str, list[Any]],
]:
    """"""
    Categories all jobs into the list of pending and failing jobs. All known flaky
    failures and broken trunk are ignored by defaults when ok_failed_checks_threshold
    is not set (unlimited)
    """"""
    pending_checks: list[tuple[str, Optional[str], Optional[int]]] = []
    failed_checks: list[tuple[str, Optional[str], Optional[int]]] = []

    # failed_checks_categorization is used to keep track of all ignorable failures when saving the merge record on s3
    failed_checks_categorization: dict[str, list[Any]] = defaultdict(list)

    # If required_checks is not set or empty, consider all names are relevant
    relevant_checknames = [
        name
        for name in check_runs.keys()
        if not required_checks or any(x in name for x in required_checks)
    ]

    for checkname in required_checks:
        if all(checkname not in x for x in check_runs.keys()):
            pending_checks.append((checkname, None, None))

    for checkname in relevant_checknames:
        status = check_runs[checkname].status
        url = check_runs[checkname].url
        classification = check_runs[checkname].classification
        job_id = check_runs[checkname].job_id

        if status is None and classification != ""UNSTABLE"":
            # NB: No need to wait if the job classification is unstable as it would be
            # ignored anyway. This is useful to not need to wait for scarce resources
            # like ROCm, which is also frequently in unstable mode
            pending_checks.append((checkname, url, job_id))
        elif classification == ""INVALID_CANCEL"":
            continue
        elif not is_passing_status(check_runs[checkname].status):
            target = (
                failed_checks_categorization[classification]
                if classification
                in (""IGNORE_CURRENT_CHECK"", ""BROKEN_TRUNK"", ""FLAKY"", ""UNSTABLE"")
                else failed_checks
            )
            target.append((checkname, url, job_id))

    flaky_or_broken_trunk = (
        failed_checks_categorization[""BROKEN_TRUNK""]
        + failed_checks_categorization[""FLAKY""]
    )

    if flaky_or_broken_trunk:
        warn(
            f""The following {len(flaky_or_broken_trunk)} checks failed but were likely due flakiness or broken trunk: ""
            + "", "".join([x[0] for x in flaky_or_broken_trunk])
            + (
                f"" but this is greater than the threshold of {ok_failed_checks_threshold} so merge will fail""
                if ok_failed_checks_threshold is not None
                and len(flaky_or_broken_trunk) > ok_failed_checks_threshold
                else """"
            )
        )

    if (
        ok_failed_checks_threshold is not None
        and len(flaky_or_broken_trunk) > ok_failed_checks_threshold
    ):
        failed_checks = failed_checks + flaky_or_broken_trunk

    # The list of failed_checks_categorization is returned so that it can be saved into the s3 merge record
    return (pending_checks, failed_checks, failed_checks_categorization)"
add_conclusions;"def add_conclusions(edges: Any) -> None:
        for edge_idx, edge in enumerate(edges):
            node = edge[""node""]
            workflow_run = node[""workflowRun""]
            checkruns = node[""checkRuns""]

            workflow_obj: WorkflowCheckState = no_workflow_obj

            if workflow_run is not None:
                # This is the usual workflow run ID we see on GitHub
                workflow_run_id = workflow_run[""databaseId""]
                # While this is the metadata name and ID of the workflow itself
                workflow_name = workflow_run[""workflow""][""name""]
                workflow_id = workflow_run[""workflow""][""databaseId""]

                workflow_conclusion = node[""conclusion""]
                # Do not override existing status with cancelled
                if workflow_conclusion == ""CANCELLED"" and workflow_name in workflows:
                    continue

                # Only keep the latest workflow run for each workflow, heuristically,
                # it's the run with largest run ID
                if (
                    workflow_id not in workflows
                    or workflows[workflow_id].run_id < workflow_run_id
                ):
                    workflows[workflow_id] = WorkflowCheckState(
                        name=workflow_name,
                        status=workflow_conclusion,
                        url=workflow_run[""url""],
                        run_id=workflow_run_id,
                    )
                workflow_obj = workflows[workflow_id]

            while checkruns is not None:
                for checkrun_node in checkruns[""nodes""]:
                    if not isinstance(checkrun_node, dict):
                        warn(f""Expected dictionary, but got {type(checkrun_node)}"")
                        continue
                    checkrun_name = f""{get_check_run_name_prefix(workflow_run)}{checkrun_node['name']}""
                    existing_checkrun = workflow_obj.jobs.get(checkrun_name)
                    if existing_checkrun is None or not is_passing_status(
                        existing_checkrun.status
                    ):
                        workflow_obj.jobs[checkrun_name] = JobCheckState(
                            checkrun_name,
                            checkrun_node[""detailsUrl""],
                            checkrun_node[""conclusion""],
                            classification=None,
                            job_id=checkrun_node[""databaseId""],
                            title=checkrun_node[""title""],
                            summary=checkrun_node[""summary""],
                        )

                if bool(checkruns[""pageInfo""][""hasNextPage""]):
                    checkruns = get_next_checkruns_page(edges, edge_idx, checkruns)
                else:
                    checkruns = None"
is_closed;"def is_closed(self) -> bool:
        return bool(self.info[""closed""])"
is_cross_repo;"def is_cross_repo(self) -> bool:
        return bool(self.info[""isCrossRepository""])"
base_ref;"def base_ref(self) -> str:
        return cast(str, self.info[""baseRefName""])"
default_branch;"def default_branch(self) -> str:
        return cast(str, self.info[""baseRepository""][""defaultBranchRef""][""name""])"
head_ref;"def head_ref(self) -> str:
        return cast(str, self.info[""headRefName""])"
is_ghstack_pr;"def is_ghstack_pr(self) -> bool:
        return RE_GHSTACK_HEAD_REF.match(self.head_ref()) is not None"
get_ghstack_orig_ref;"def get_ghstack_orig_ref(self) -> str:
        assert self.is_ghstack_pr()
        return re.sub(r""/head$"", ""/orig"", self.head_ref())"
is_base_repo_private;"def is_base_repo_private(self) -> bool:
        return bool(self.info[""baseRepository""][""isPrivate""])"
get_changed_files_count;"def get_changed_files_count(self) -> int:
        return int(self.info[""changedFiles""])"
last_commit;"def last_commit(self) -> Any:
        return self.info[""commits""][""nodes""][-1][""commit""]"
get_merge_base;"def get_merge_base(self) -> str:
        if self.merge_base:
            return self.merge_base

        last_commit_oid = self.last_commit()[""oid""]
        # NB: We could use self.base_ref() here for regular PR, however, that doesn't
        # work for ghstack where the base is the custom branch, i.e. gh/USER/ID/base,
        # so let's just use main instead
        self.merge_base = gh_fetch_merge_base(
            self.org, self.project, last_commit_oid, self.default_branch()
        )

        # Fallback to baseRefOid if the API call fails, i.e. rate limit. Note that baseRefOid
        # points to the base ref associated with the PR or, in other words, the head of main
        # when the PR is created or rebased. This is not necessarily the merge base commit,
        # but it could serve as a fallback in most cases and it's readily available as part
        # of the PR info
        if not self.merge_base:
            self.merge_base = cast(str, self.info[""baseRefOid""])

        return self.merge_base"
get_changed_files;"def get_changed_files(self) -> list[str]:
        if self.changed_files is None:
            info = self.info
            unique_changed_files = set()
            # Do not try to fetch more than 10K files
            for _ in range(100):
                unique_changed_files.update([x[""path""] for x in info[""files""][""nodes""]])
                if not info[""files""][""pageInfo""][""hasNextPage""]:
                    break
                rc = gh_graphql(
                    GH_GET_PR_NEXT_FILES_QUERY,
                    name=self.project,
                    owner=self.org,
                    number=self.pr_num,
                    cursor=info[""files""][""pageInfo""][""endCursor""],
                )
                info = rc[""data""][""repository""][""pullRequest""]
            self.changed_files = list(unique_changed_files)

        if len(self.changed_files) != self.get_changed_files_count():
            raise RuntimeError(""Changed file count mismatch"")
        return self.changed_files"
get_submodules;"def get_submodules(self) -> list[str]:
        if self.submodules is None:
            rc = gh_graphql(GH_GET_REPO_SUBMODULES, name=self.project, owner=self.org)
            info = rc[""data""][""repository""][""submodules""]
            self.submodules = [s[""path""] for s in info[""nodes""]]
        return self.submodules"
get_changed_submodules;"def get_changed_submodules(self) -> list[str]:
        submodules = self.get_submodules()
        return [f for f in self.get_changed_files() if f in submodules]"
has_invalid_submodule_updates;"def has_invalid_submodule_updates(self) -> bool:
        """"""Submodule updates in PR are invalid if submodule keyword
        is not mentioned in neither the title nor body/description
        nor in any of the labels.
        """"""
        return (
            len(self.get_changed_submodules()) > 0
            and ""submodule"" not in self.get_title().lower()
            and ""submodule"" not in self.get_body().lower()
            and all(""submodule"" not in label for label in self.get_labels())
        )"
_get_reviews;"def _get_reviews(self) -> list[tuple[str, str]]:
        if self._reviews is None:
            self._reviews = []
            info = self.info
            for _ in range(100):
                nodes = info[""reviews""][""nodes""]
                self._reviews = [
                    (node[""author""][""login""], node[""state""]) for node in nodes
                ] + self._reviews
                if not info[""reviews""][""pageInfo""][""hasPreviousPage""]:
                    break
                rc = gh_graphql(
                    GH_GET_PR_PREV_REVIEWS_QUERY,
                    name=self.project,
                    owner=self.org,
                    number=self.pr_num,
                    cursor=info[""reviews""][""pageInfo""][""startCursor""],
                )
                info = rc[""data""][""repository""][""pullRequest""]
        reviews = {
            author: state for author, state in self._reviews if state != ""COMMENTED""
        }
        return list(reviews.items())"
get_approved_by;"def get_approved_by(self) -> list[str]:
        return [login for (login, state) in self._get_reviews() if state == ""APPROVED""]"
get_commit_count;"def get_commit_count(self) -> int:
        return int(self.info[""commits_with_authors""][""totalCount""])"
get_pr_creator_login;"def get_pr_creator_login(self) -> str:
        return cast(str, self.info[""author""][""login""])"
_fetch_authors;"def _fetch_authors(self) -> list[tuple[str, str]]:
        if self._authors is not None:
            return self._authors
        authors: list[tuple[str, str]] = []

        def add_authors(info: dict[str, Any]) -> None:
            for node in info[""commits_with_authors""][""nodes""]:
                for author_node in node[""commit""][""authors""][""nodes""]:
                    user_node = author_node[""user""]
                    author = f""{author_node['name']} <{author_node['email']}>""
                    if user_node is None:
                        # If author is not github user, user node will be null
                        authors.append(("""", author))
                    else:
                        authors.append((cast(str, user_node[""login""]), author))

        info = self.info
        for _ in range(100):
            add_authors(info)
            if not info[""commits_with_authors""][""pageInfo""][""hasNextPage""]:
                break
            rc = gh_graphql(
                GH_GET_PR_NEXT_AUTHORS_QUERY,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=info[""commits_with_authors""][""pageInfo""][""endCursor""],
            )
            info = rc[""data""][""repository""][""pullRequest""]
        self._authors = authors
        return authors"
get_committer_login;"def get_committer_login(self, num: int = 0) -> str:
        return self._fetch_authors()[num][0]"
get_committer_author;"def get_committer_author(self, num: int = 0) -> str:
        return self._fetch_authors()[num][1]"
get_labels;"def get_labels(self) -> list[str]:
        if self.labels is not None:
            return self.labels
        labels = (
            [node[""node""][""name""] for node in self.info[""labels""][""edges""]]
            if ""labels"" in self.info
            else []
        )
        self.labels = labels
        return self.labels"
get_checkrun_conclusions;"def get_checkrun_conclusions(self) -> JobNameToStateDict:
        """"""Returns dict of checkrun -> [conclusion, url]""""""
        if self.conclusions is not None:
            return self.conclusions
        orig_last_commit = self.last_commit()

        def get_pr_next_check_runs(
            edges: list[dict[str, dict[str, Any]]], edge_idx: int, checkruns: Any
        ) -> Any:
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECK_RUNS,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cs_cursor=edges[edge_idx - 1][""cursor""] if edge_idx > 0 else None,
                cr_cursor=checkruns[""pageInfo""][""endCursor""],
            )
            last_commit = rc[""data""][""repository""][""pullRequest""][""commits""][""nodes""][
                -1
            ][""commit""]
            checkruns = last_commit[""checkSuites""][""nodes""][-1][""checkRuns""]
            return checkruns

        def get_pr_next_checksuites(checksuites: Any) -> Any:
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECKSUITES,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=checksuites[""edges""][-1][""cursor""],
            )
            info = rc[""data""][""repository""][""pullRequest""]
            last_commit = info[""commits""][""nodes""][-1][""commit""]
            if last_commit[""oid""] != orig_last_commit[""oid""]:
                raise RuntimeError(""Last commit changed on PR"")
            return last_commit[""checkSuites""]

        checksuites = orig_last_commit[""checkSuites""]

        self.conclusions = add_workflow_conclusions(
            checksuites, get_pr_next_check_runs, get_pr_next_checksuites
        )

        # Append old style statuses(like ones populated by CircleCI or EasyCLA) to conclusions
        if orig_last_commit[""status""] and orig_last_commit[""status""][""contexts""]:
            for status in orig_last_commit[""status""][""contexts""]:
                name = status[""context""]
                self.conclusions[name] = JobCheckState(
                    name,
                    status[""targetUrl""],
                    status[""state""],
                    classification=None,
                    job_id=None,
                    title=None,
                    summary=None,
                )

        return self.conclusions"
get_authors;"def get_authors(self) -> dict[str, str]:
        rc = {}
        for idx in range(len(self._fetch_authors())):
            rc[self.get_committer_login(idx)] = self.get_committer_author(idx)

        return rc"
get_author;"def get_author(self) -> str:
        authors = self.get_authors()
        if len(authors) == 1:
            return next(iter(authors.values()))
        creator = self.get_pr_creator_login()
        # If PR creator is not among authors
        # Assume it was authored by first commit author
        if creator not in authors:
            return self.get_committer_author(0)
        return authors[creator]"
get_title;"def get_title(self) -> str:
        return cast(str, self.info[""title""])"
get_body;"def get_body(self) -> str:
        return cast(str, self.info[""body""])"
get_merge_commit;"def get_merge_commit(self) -> Optional[str]:
        mc = self.info[""mergeCommit""]
        return mc[""oid""] if mc is not None else None"
get_pr_url;"def get_pr_url(self) -> str:
        return f""https://github.com/{self.org}/{self.project}/pull/{self.pr_num}"""
_comment_from_node;"def _comment_from_node(node: Any) -> GitHubComment:
        editor = node[""editor""]
        return GitHubComment(
            body_text=node[""bodyText""],
            created_at=node[""createdAt""] if ""createdAt"" in node else """",
            author_login=node[""author""][""login""],
            author_association=node[""authorAssociation""],
            editor_login=editor[""login""] if editor else None,
            database_id=node[""databaseId""],
            url=node[""url""],
        )"
get_comments;"def get_comments(self) -> list[GitHubComment]:
        if self.comments is not None:
            return self.comments
        self.comments = []
        info = self.info[""comments""]
        # Do not try to fetch more than 10K comments
        for _ in range(100):
            self.comments = [
                self._comment_from_node(node) for node in info[""nodes""]
            ] + self.comments
            if not info[""pageInfo""][""hasPreviousPage""]:
                break
            rc = gh_graphql(
                GH_GET_PR_PREV_COMMENTS,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=info[""pageInfo""][""startCursor""],
            )
            info = rc[""data""][""repository""][""pullRequest""][""comments""]
        return self.comments"
get_last_comment;"def get_last_comment(self) -> GitHubComment:
        return self._comment_from_node(self.info[""comments""][""nodes""][-1])"
get_comment_by_id;"def get_comment_by_id(self, database_id: int) -> GitHubComment:
        if self.comments is None:
            # Fastpath - try searching in partial prefetched comments
            for node in self.info[""comments""][""nodes""]:
                comment = self._comment_from_node(node)
                if comment.database_id == database_id:
                    return comment

        for comment in self.get_comments():
            if comment.database_id == database_id:
                return comment

        # The comment could have actually been a review left on the PR (the message written alongside the review).
        # (This is generally done to trigger the merge right when a comment is left)
        # Check those review comments to see if one of those was the comment in question.
        for node in self.info[""reviews""][""nodes""]:
            # These review comments contain all the fields regular comments need
            comment = self._comment_from_node(node)
            if comment.database_id == database_id:
                return comment

        raise RuntimeError(f""Comment with id {database_id} not found"")"
get_diff_revision;"def get_diff_revision(self) -> Optional[str]:
        rc = RE_DIFF_REV.search(self.get_body())
        return rc.group(1) if rc is not None else None"
has_internal_changes;"def has_internal_changes(self) -> bool:
        checkrun_name = INTERNAL_CHANGES_CHECKRUN_NAME
        if self.get_diff_revision() is None:
            return False
        checks = self.get_checkrun_conclusions()
        if checks is None or checkrun_name not in checks:
            return False
        return checks[checkrun_name].status != ""SUCCESS"""
has_no_connected_diff;"def has_no_connected_diff(self) -> bool:
        checkrun_name = INTERNAL_CHANGES_CHECKRUN_NAME
        checks = self.get_checkrun_conclusions()
        if checks is None or checkrun_name not in checks:
            return False
        return checks[checkrun_name].title == HAS_NO_CONNECTED_DIFF_TITLE"
merge_ghstack_into;"def merge_ghstack_into(
        self,
        repo: GitRepo,
        skip_mandatory_checks: bool,
        comment_id: Optional[int] = None,
        skip_all_rule_checks: bool = False,
    ) -> list[""GitHubPR""]:
        assert self.is_ghstack_pr()
        ghstack_prs = get_ghstack_prs(
            repo, self, open_only=False
        )  # raises error if out of sync
        pr_dependencies = []
        for pr, rev in ghstack_prs:
            if pr.is_closed():
                pr_dependencies.append(pr)
                continue

            commit_msg = pr.gen_commit_message(
                filter_ghstack=True, ghstack_deps=pr_dependencies
            )
            if pr.pr_num != self.pr_num and not skip_all_rule_checks:
                # Raises exception if matching rule is not found
                find_matching_merge_rule(
                    pr,
                    repo,
                    skip_mandatory_checks=skip_mandatory_checks,
                    skip_internal_checks=can_skip_internal_checks(self, comment_id),
                )
            repo.cherry_pick(rev)
            repo.amend_commit_message(commit_msg)
            pr_dependencies.append(pr)
        return [x for x, _ in ghstack_prs if not x.is_closed()]"
gen_commit_message;"def gen_commit_message(
        self,
        filter_ghstack: bool = False,
        ghstack_deps: Optional[list[""GitHubPR""]] = None,
    ) -> str:
        """"""Fetches title and body from PR description
        adds reviewed by, pull request resolved and optionally
        filters out ghstack info""""""
        # Adding the url here makes it clickable within the Github UI
        approved_by_urls = "", "".join(
            prefix_with_github_url(login) for login in self.get_approved_by()
        )
        # Remove ""cc: "" line from the message body
        msg_body = re.sub(RE_PR_CC_LINE, """", self.get_body())
        if filter_ghstack:
            msg_body = re.sub(RE_GHSTACK_DESC, """", msg_body)
        msg = self.get_title() + f"" (#{self.pr_num})\n\n""
        msg += msg_body

        msg += f""\nPull Request resolved: {self.get_pr_url()}\n""
        msg += f""Approved by: {approved_by_urls}\n""
        if ghstack_deps:
            msg += f""ghstack dependencies: {', '.join([f'#{pr.pr_num}' for pr in ghstack_deps])}\n""

        # Mention PR co-authors, which should be at the end of the message
        # And separated from the body by two newlines
        first_coauthor = True
        for author_login, author_name in self.get_authors().items():
            if author_login != self.get_pr_creator_login():
                if first_coauthor:
                    msg, first_coauthor = (msg + ""\n"", False)
                msg += f""\nCo-authored-by: {author_name}""

        return msg"
add_numbered_label;"def add_numbered_label(self, label_base: str, dry_run: bool) -> None:
        labels = self.get_labels() if self.labels is not None else []
        full_label = label_base
        count = 0
        for label in labels:
            if label_base in label:
                count += 1
                full_label = f""{label_base}X{count}""
        self.add_label(full_label, dry_run)"
add_label;"def add_label(self, label: str, dry_run: bool) -> None:
        gh_add_labels(self.org, self.project, self.pr_num, [label], dry_run)"
merge_into;"def merge_into(
        self,
        repo: GitRepo,
        *,
        skip_mandatory_checks: bool = False,
        dry_run: bool = False,
        comment_id: Optional[int] = None,
        ignore_current_checks: Optional[list[str]] = None,
    ) -> None:
        # Raises exception if matching rule is not found
        (
            merge_rule,
            pending_checks,
            failed_checks,
            ignorable_checks,
        ) = find_matching_merge_rule(
            self,
            repo,
            skip_mandatory_checks=skip_mandatory_checks,
            skip_internal_checks=can_skip_internal_checks(self, comment_id),
            ignore_current_checks=ignore_current_checks,
        )
        additional_merged_prs = self.merge_changes(
            repo, skip_mandatory_checks, comment_id
        )

        repo.push(self.default_branch(), dry_run)
        if not dry_run:
            self.add_numbered_label(MERGE_COMPLETE_LABEL, dry_run)
            for pr in additional_merged_prs:
                pr.add_numbered_label(MERGE_COMPLETE_LABEL, dry_run)

        # When the merge process reaches this part, we can assume that the commit
        # has been successfully pushed to trunk
        merge_commit_sha = repo.rev_parse(name=self.default_branch())

        if comment_id and self.pr_num:
            # Finally, upload the record to s3. The list of pending and failed
            # checks are at the time of the merge
            save_merge_record(
                comment_id=comment_id,
                pr_num=self.pr_num,
                owner=self.org,
                project=self.project,
                author=self.get_author(),
                pending_checks=pending_checks,
                failed_checks=failed_checks,
                ignore_current_checks=ignorable_checks.get(""IGNORE_CURRENT_CHECK"", []),
                broken_trunk_checks=ignorable_checks.get(""BROKEN_TRUNK"", []),
                flaky_checks=ignorable_checks.get(""FLAKY"", []),
                unstable_checks=ignorable_checks.get(""UNSTABLE"", []),
                last_commit_sha=self.last_commit().get(""oid"", """"),
                merge_base_sha=self.get_merge_base(),
                merge_commit_sha=merge_commit_sha,
                is_failed=False,
                skip_mandatory_checks=skip_mandatory_checks,
                ignore_current=bool(ignore_current_checks),
            )
        else:
            print(""Missing comment ID or PR number, couldn't upload to s3"")

        # Usually Github will see that the commit has ""resolves <pr_num>"" in the
        # commit message and close the PR, but sometimes it doesn't, leading to
        # confusion.  When it doesn't, we close it manually.
        time.sleep(60)  # Give Github some time to close the PR
        manually_close_merged_pr(
            pr=self,
            additional_merged_prs=additional_merged_prs,
            merge_commit_sha=merge_commit_sha,
            dry_run=dry_run,
        )"
merge_changes;"def merge_changes(
        self,
        repo: GitRepo,
        skip_mandatory_checks: bool = False,
        comment_id: Optional[int] = None,
        branch: Optional[str] = None,
        skip_all_rule_checks: bool = False,
    ) -> list[""GitHubPR""]:
        """"""
        :param skip_all_rule_checks: If true, skips all rule checks, useful for dry-running merge locally
        """"""
        branch_to_merge_into = self.default_branch() if branch is None else branch
        if repo.current_branch() != branch_to_merge_into:
            repo.checkout(branch_to_merge_into)
        if not self.is_ghstack_pr():
            msg = self.gen_commit_message()
            pr_branch_name = f""__pull-request-{self.pr_num}__init__""
            repo.fetch(self.last_commit()[""oid""], pr_branch_name)
            repo._run_git(""merge"", ""--squash"", pr_branch_name)
            repo._run_git(""commit"", f'--author=""{self.get_author()}""', ""-m"", msg)

            # Did the PR change since we started the merge?
            pulled_sha = repo.show_ref(pr_branch_name)
            latest_pr_status = GitHubPR(self.org, self.project, self.pr_num)
            if pulled_sha != latest_pr_status.last_commit()[""oid""]:
                raise RuntimeError(
                    ""PR has been updated since CI checks last passed. Please rerun the merge command.""
                )
            return []
        else:
            return self.merge_ghstack_into(
                repo,
                skip_mandatory_checks,
                comment_id=comment_id,
                skip_all_rule_checks=skip_all_rule_checks,
            )"
__init__;"def __init__(self, message: str, rule: Optional[""MergeRule""] = None) -> None:
        super().__init__(message)
        self.rule = rule"
_comment_and_close;"def _comment_and_close(pr: GitHubPR, comment: str) -> None:
        pr = GitHubPR(pr.org, pr.project, pr.pr_num)  # Refresh the PR
        if not pr.is_closed():
            gh_post_pr_comment(pr.org, pr.project, pr.pr_num, comment, dry_run)
            gh_close_pr(pr.org, pr.project, pr.pr_num, dry_run)"
get_readable_drci_results;"def get_readable_drci_results(drci_classifications: Any) -> str:
        try:
            s = f""From Dr.CI API ({pr_num}):\n""
            for classification, jobs in drci_classifications.items():
                s += f""  {classification}: \n""
                for job in jobs:
                    s += f""    {job['id']} {job['name']}\n""
            return s
        except Exception:
            return f""From Dr.CI API: {json.dumps(drci_classifications)}"""
handle_exception;"def handle_exception(e: Exception, title: str = ""Merge failed"") -> None:
        exception = f""**Reason**: {e}""

        failing_rule = None
        if isinstance(e, MergeRuleFailedError):
            failing_rule = e.rule.name if e.rule else None

        internal_debugging = """"
        run_url = os.getenv(""GH_RUN_URL"")
        if run_url is not None:
            # Hide this behind a collapsed bullet since it's not helpful to most devs
            internal_debugging = ""\n"".join(
                line
                for line in (
                    ""<details><summary>Details for Dev Infra team</summary>"",
                    f'Raised by <a href=""{run_url}"">workflow job</a>\n',
                    f""Failing merge rule: {failing_rule}"" if failing_rule else """",
                    ""</details>"",
                )
                if line
            )  # ignore empty lines during the join

        msg = ""\n"".join((f""## {title}"", f""{exception}"", """", f""{internal_debugging}""))

        gh_post_pr_comment(org, project, args.pr_num, msg, dry_run=args.dry_run)
        import traceback

        traceback.print_exc()"
add_authors;"def add_authors(info: dict[str, Any]) -> None:
            for node in info[""commits_with_authors""][""nodes""]:
                for author_node in node[""commit""][""authors""][""nodes""]:
                    user_node = author_node[""user""]
                    author = f""{author_node['name']} <{author_node['email']}>""
                    if user_node is None:
                        # If author is not github user, user node will be null
                        authors.append(("""", author))
                    else:
                        authors.append((cast(str, user_node[""login""]), author))"
get_pr_next_check_runs;"def get_pr_next_check_runs(
            edges: list[dict[str, dict[str, Any]]], edge_idx: int, checkruns: Any
        ) -> Any:
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECK_RUNS,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cs_cursor=edges[edge_idx - 1][""cursor""] if edge_idx > 0 else None,
                cr_cursor=checkruns[""pageInfo""][""endCursor""],
            )
            last_commit = rc[""data""][""repository""][""pullRequest""][""commits""][""nodes""][
                -1
            ][""commit""]
            checkruns = last_commit[""checkSuites""][""nodes""][-1][""checkRuns""]
            return checkruns"
get_pr_next_checksuites;"def get_pr_next_checksuites(checksuites: Any) -> Any:
            rc = gh_graphql(
                GH_GET_PR_NEXT_CHECKSUITES,
                name=self.project,
                owner=self.org,
                number=self.pr_num,
                cursor=checksuites[""edges""][-1][""cursor""],
            )
            info = rc[""data""][""repository""][""pullRequest""]
            last_commit = info[""commits""][""nodes""][-1][""commit""]
            if last_commit[""oid""] != orig_last_commit[""oid""]:
                raise RuntimeError(""Last commit changed on PR"")
            return last_commit[""checkSuites""]"
has_label;"def has_label(labels: list[str], pattern: Pattern[str] = CIFLOW_LABEL) -> bool:
    return len(list(filter(pattern.match, labels))) > 0"
get_revert_message;"def get_revert_message(org: str, project: str, pr_num: int) -> str:
    msg = (
        ""@pytorchbot successfully started a revert job.""
        + f"" Check the current status [here]({os.getenv('GH_RUN_URL')}).\n""
    )
    msg += CONTACT_US
    return msg"
__init__;"def __init__(
        self,
        force: bool,
        labels: list[str],
        pr_num: int,
        org: str,
        project: str,
        ignore_current: bool,
    ):
        self.force = force
        self.labels = labels
        self.pr_num = pr_num
        self.org = org
        self.project = project
        self.ignore_current = ignore_current"
_get_flag_msg;"def _get_flag_msg(
        self,
        ignore_current_checks: Optional[
            list[tuple[str, Optional[str], Optional[int]]]
        ] = None,
    ) -> str:
        if self.force:
            return (
                ""Your change will be merged immediately since you used the force (-f) flag, ""
                + ""**bypassing any CI checks** (ETA: 1-5 minutes).  ""
                + ""Please use `-f` as last resort and instead consider `-i/--ignore-current` ""
                + ""to continue the merge ignoring current failures.  This will allow ""
                + ""currently pending tests to finish and report signal before the merge.""
            )
        elif self.ignore_current and ignore_current_checks is not None:
            msg = f""Your change will be merged while ignoring the following {len(ignore_current_checks)} checks: ""
            msg += "", "".join(f""[{x[0]}]({x[1]})"" for x in ignore_current_checks)
            return msg
        else:
            return ""Your change will be merged once all checks pass (ETA 0-4 Hours)."""
get_merge_message;"def get_merge_message(
        self,
        ignore_current_checks: Optional[
            list[tuple[str, Optional[str], Optional[int]]]
        ] = None,
    ) -> str:
        title = ""### Merge started""
        main_message = self._get_flag_msg(ignore_current_checks)

        advanced_debugging = ""\n"".join(
            (
                ""<details><summary>Advanced Debugging</summary>"",
                ""Check the merge workflow status "",
                f'<a href=""{os.getenv(""GH_RUN_URL"")}"">here</a>',
                ""</details>"",
            )
        )

        msg = title + ""\n""
        msg += main_message + ""\n\n""
        msg += ALTERNATIVES + ""\n\n""
        msg += CONTACT_US
        msg += advanced_debugging
        return msg"
parse_args;"def parse_args() -> Any:
    from argparse import ArgumentParser

    parser = ArgumentParser(""Rebase PR into branch"")
    parser.add_argument(""--dry-run"", action=""store_true"")
    parser.add_argument(""--branch"", type=str)
    parser.add_argument(""pr_num"", type=int)
    return parser.parse_args()"
post_already_uptodate;"def post_already_uptodate(
    pr: GitHubPR, repo: GitRepo, onto_branch: str, dry_run: bool
) -> None:
    msg = f""Tried to rebase and push PR #{pr.pr_num}, but it was already up to date.""
    def_branch = pr.default_branch()
    def_branch_fcn = f""refs/remotes/{repo.remote}/{def_branch}""
    if onto_branch != def_branch_fcn and repo.rev_parse(
        def_branch_fcn
    ) != repo.rev_parse(onto_branch):
        def_branch_url = f""https://github.com/{pr.org}/{pr.project}/tree/{def_branch}""
        msg += f"" Try rebasing against [{def_branch}]({def_branch_url}) by issuing:""
        msg += f""\n`@pytorchbot rebase -b {def_branch}`""

    gh_post_comment(
        pr.org,
        pr.project,
        pr.pr_num,
        msg,
        dry_run=dry_run,
    )"
rebase_onto;"def rebase_onto(
    pr: GitHubPR, repo: GitRepo, onto_branch: str, dry_run: bool = False
) -> bool:
    branch = f""pull/{pr.pr_num}/head""
    remote_url = f""https://github.com/{pr.info['headRepository']['nameWithOwner']}.git""
    refspec = f""{branch}:{pr.head_ref()}""

    repo.fetch(branch, branch)
    repo._run_git(""rebase"", onto_branch, branch)

    if repo.rev_parse(branch) == repo.rev_parse(onto_branch):
        raise Exception(SAME_SHA_ERROR)  # noqa: TRY002

    if dry_run:
        push_result = repo._run_git(""push"", ""--dry-run"", ""-f"", remote_url, refspec)
    else:
        push_result = repo._run_git(""push"", ""-f"", remote_url, refspec)
    if ""Everything up-to-date"" in push_result:
        post_already_uptodate(pr, repo, onto_branch, dry_run)
        return False
    else:
        gh_post_comment(
            pr.org,
            pr.project,
            pr.pr_num,
            f""Successfully rebased `{pr.head_ref()}` onto `{onto_branch}`, please pull locally ""
            + f""before adding more changes (for example, via `git checkout {pr.head_ref()} && ""
            + ""git pull --rebase`)"",
            dry_run=dry_run,
        )
        return True"
rebase_ghstack_onto;"def rebase_ghstack_onto(
    pr: GitHubPR, repo: GitRepo, onto_branch: str, dry_run: bool = False
) -> bool:
    if (
        subprocess.run(
            [sys.executable, ""-m"", ""ghstack"", ""--help""],
            capture_output=True,
            check=False,
        ).returncode
        != 0
    ):
        subprocess.run([sys.executable, ""-m"", ""pip"", ""install"", ""ghstack""], check=True)
    orig_ref = f""{re.sub(r'/head$', '/orig', pr.head_ref())}""

    repo.fetch(orig_ref, orig_ref)
    repo._run_git(""rebase"", onto_branch, orig_ref)

    if repo.rev_parse(orig_ref) == repo.rev_parse(onto_branch):
        raise Exception(SAME_SHA_ERROR)  # noqa: TRY002

    # steal the identity of the committer of the commit on the orig branch
    email = repo._run_git(""log"", orig_ref, ""--pretty=format:%ae"", ""-1"")
    name = repo._run_git(""log"", orig_ref, ""--pretty=format:%an"", ""-1"")
    repo._run_git(""config"", ""--global"", ""user.email"", email)
    repo._run_git(""config"", ""--global"", ""user.name"", name)

    os.environ[""OAUTH_TOKEN""] = os.environ[""GITHUB_TOKEN""]
    with open("".ghstackrc"", ""w+"") as f:
        f.write(
            ""[ghstack]\n""
            + ""github_url=github.com\n""
            + ""github_username=pytorchmergebot\n""
            + ""remote_name=origin""
        )

    if dry_run:
        print(""Don't know how to dry-run ghstack"")
        return False
    else:
        ghstack_result = subprocess.run([""ghstack""], capture_output=True, check=True)
        push_result = ghstack_result.stdout.decode(""utf-8"")
        print(push_result)
        if ghstack_result.returncode != 0:
            print(ghstack_result.stderr.decode(""utf-8""))
            raise Exception(f""\n```{push_result}```"")  # noqa: TRY002
        # The contents of a successful push result should look like:
        # Summary of changes (ghstack 0.6.0)

        #  - Updated https://github.com/clee2000/random-testing-public/pull/2
        #  - Updated https://github.com/clee2000/random-testing-public/pull/1

        # Facebook employees can import your changes by running
        # (on a Facebook machine):

        #     ghimport -s https://github.com/clee2000/random-testing-public/pull/2

        # If you want to work on this diff stack on another machine:

        #     ghstack checkout https://github.com/clee2000/random-testing-public/pull/2
        org, project = repo.gh_owner_and_name()
        for line in push_result.splitlines():
            if ""Updated"" in line:
                pr_num = int(line.split(""/"")[-1])
                if pr_num != pr.pr_num:
                    gh_post_comment(
                        pr.org,
                        pr.project,
                        pr_num,
                        f""Rebased `{orig_ref}` onto `{onto_branch}` because #{pr.pr_num} was rebased, ""
                        ""please pull locally before adding more changes (for example, via `ghstack ""
                        + f""checkout https://github.com/{org}/{project}/pull/{pr_num}`)"",
                        dry_run=dry_run,
                    )
                else:
                    gh_post_comment(
                        pr.org,
                        pr.project,
                        pr_num,
                        f""Successfully rebased `{orig_ref}` onto `{onto_branch}`, please pull locally ""
                        + ""before adding more changes (for example, via `ghstack ""
                        + f""checkout https://github.com/{org}/{project}/pull/{pr.pr_num}`)"",
                        dry_run=dry_run,
                    )

        if (
            f""Skipped https://github.com/{org}/{project}/pull/{pr.pr_num}""
            in push_result
        ):
            post_already_uptodate(pr, repo, onto_branch, dry_run)
            return False
        return True"
additional_rebase_failure_info;"def additional_rebase_failure_info(e: Exception) -> str:
    if re.search(
        r""remote: Permission to .* denied to .*\.\nfatal: unable to access"", str(e)
    ):
        return (
            ""\nThis is likely because the author did not allow edits from maintainers on the PR or because the ""
            ""repo has additional permissions settings that mergebot does not qualify.""
        )
    return """""
git_config_guard;"def git_config_guard(repo: GitRepo) -> Generator[None, None, None]:
    """"""Restores user.name and user.email global properties after context is finished""""""
    user_email = repo._run_git(""config"", ""user.email"")
    user_name = repo._run_git(""config"", ""user.name"")
    try:
        yield
    finally:
        if user_email:
            repo._run_git(""config"", ""--global"", ""user.email"", user_email)
        if user_name:
            repo._run_git(""config"", ""--global"", ""user.name"", user_name)"
eqDictStrKeyIntValue;"def eqDictStrKeyIntValue(self, input: dict[str, int]) -> dict[str, int]:
        return input"
eqDictIntKeyIntValue;"def eqDictIntKeyIntValue(self, input: dict[int, int]) -> dict[int, int]:
        return input"
eqDictFloatKeyIntValue;"def eqDictFloatKeyIntValue(self, input: dict[float, int]) -> dict[float, int]:
        return input"
listIntSumReturnTuple;"def listIntSumReturnTuple(self, input: list[int]) -> tuple[list[int], int]:
        sum = 0
        for x in input:
            sum += x
        return (input, sum)"
listBoolConjunction;"def listBoolConjunction(self, input: list[bool]) -> bool:
        res = True
        for x in input:
            res = res and x
        return res"
listBoolDisjunction;"def listBoolDisjunction(self, input: list[bool]) -> bool:
        res = False
        for x in input:
            res = res or x
        return res"
tupleIntSumReturnTuple;"def tupleIntSumReturnTuple(
        self, input: tuple[int, int, int]
    ) -> tuple[tuple[int, int, int], int]:
        sum = 0
        for x in input:
            sum += x
        return (input, sum)"
optionalIntIsNone;"def optionalIntIsNone(self, input: Optional[int]) -> bool:
        return input is None"
intEq0None;"def intEq0None(self, input: int) -> Optional[int]:
        if input == 0:
            return None
        return input"
str3Concat;"def str3Concat(self, input: str) -> str:
        return input + input + input"
newEmptyShapeWithItem;"def newEmptyShapeWithItem(self, input):
        return torch.tensor([int(input.item())])[0]"
contiguous;"def contiguous(self, x: Tensor) -> Tensor:
        return x.contiguous()"
contiguousChannelsLast;"def contiguousChannelsLast(self, x: Tensor) -> Tensor:
        return x.contiguous(memory_format=torch.channels_last)"
contiguousChannelsLast3d;"def contiguousChannelsLast3d(self, x: Tensor) -> Tensor:
        return x.contiguous(memory_format=torch.channels_last_3d)"
write_decl_impl;"def write_decl_impl(
    kernels: list[T],
    family_name: str,
    impl_file: str,
    autogen_dir: Path,
    disable_def: Optional[str] = None,
) -> None:
    cpp_file_header = """"""/*
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 * All rights reserved.
 *
 * This source code is licensed under the BSD-style license found in the
 * LICENSE file in the root directory of this source tree.
 */
// This file is auto-generated. See ""generate_kernels.py""
""""""

    kernels.sort()

    implfile_to_kernels: dict[str, list[T]] = collections.defaultdict(list)
    cat_to_kernels: dict[tuple[str, int, int], list[T]] = collections.defaultdict(list)

    dispatch_all = """"
    declarations = cpp_file_header + ""#pragma once\n""
    # declarations += f""#ifndef {disable_def}\n""
    declarations += f""""""#include {impl_file}\n""""""
    declarations += """"""using namespace PyTorchMemEffAttention;\n""""""

    # Declaration of kernel functions
    for k in kernels:
        implfile_to_kernels[k.impl_group].append(k)
        cat_to_kernels[(k.dtype, k.sm_range[0], k.sm_range[1])].append(k)

    for (cat_dt, cat_sm, cat_sm_max), kernels in cat_to_kernels.items():
        declarations += f""// ======== {cat_dt} / sm{cat_sm} ========\n""
        declarations += ""\n"".join(
            k.cpp_impl.split(""{"")[0].rstrip() + "";"" for k in kernels
        )
        dispatch_category_fn = f""dispatch_{family_name}_{cat_dt}_sm{cat_sm}""
        declarations += (
            f""\n\ntemplate <typename T> void {dispatch_category_fn}(T cb, int cc) {{\n""
        )
        for k in kernels:
            _call = f""cb({k.cpp_class}(), {k.name});\n""
            if k.dispatch_cond is not None:
                _call = f""if ({k.dispatch_cond}) {_call}""
            declarations += f""    {_call}""
        declarations += ""}\n\n""
        dispatch_all += f""""""
    if (std::is_same_v<DT, {DTYPES[cat_dt]}> && {cat_sm} <= cc && cc < {cat_sm_max}) {{
        {dispatch_category_fn}(cb, cc);
    }}""""""

    declarations += f""""""
template <typename DT, typename T>
void dispatch_{family_name}(T cb, int cc = 0) {{
{dispatch_all}
}}
""""""
    # declarations += f""#endif // {disable_def}\n""

    # Write declarations to family header
    (autogen_dir / f""{family_name}.h"").write_text(declarations)

    for f, f_kernels in implfile_to_kernels.items():
        impl_cu = cpp_file_header
        # impl_cu += f""#ifndef {disable_def}\n""
        impl_cu += f""""""#include {impl_file}\n""""""
        impl_cu += """"""using namespace PyTorchMemEffAttention;\n""""""
        for k in f_kernels:
            impl_cu += k.cpp_impl
        # impl_cu += f""#endif // {disable_def}\n""
        (autogen_dir / f""{family_name}_{f}.cu"").write_text(impl_cu)"
cpp_class;"def cpp_class(self) -> str:
        template_args = "", "".join(
            [
                DTYPES[self.dtype],
                f""cutlass::arch::Sm{self.sm_range[0]}"",
                ""true"" if self.aligned else ""false"",
                str(self.q),
                str(self.k),
                str(self.max_k),
                ""true"" if self.supports_dropout else ""false"",
                ""true"" if self.supports_bias else ""false"",
            ]
        )
        return f""AttentionKernel<{template_args}>"""
impl_group;"def impl_group(self) -> str:
        # Maps to file which will contain the implementation
        return f""{self.dtype}_{self._aligned_suffix}"""
cpp_impl;"def cpp_impl(self) -> str:
        return KERNEL_IMPL_TEMPLATE.format(
            CPP_CLASS=self.cpp_class,
            NAME=self.name,
            SM=self.sm_range[0],
            SM_MAX=self.sm_range[1],
        )"
get_all;"def get_all(cls) -> list[""FwdKernel""]:
        kernels: list[FwdKernel] = []
        for aligned, dtype, (sm, sm_max) in itertools.product(
            [True, False], DTYPES.keys(), zip(SM, SM[1:])
        ):
            # Remove some kernels we don't use
            if dtype == ""bf16"" and sm < 80:
                continue
            if not aligned and sm >= 80:
                continue
            for q, k, max_k in [
                (64, 64, 64),
                # We get better perf with 64x128 on A100
                (64 if sm > 75 else 32, 128, 128),
                (32, 128, 2**16),
            ]:
                kernels.append(
                    cls(
                        aligned=aligned,
                        dtype=dtype,
                        sm_range=(sm, sm_max),
                        q=q,
                        k=k,
                        max_k=max_k,
                    )
                )
        return kernels"
__post_init__;"def __post_init__(self) -> None:
        # Set kernel selection priority
        # The lowest value that matches inputs
        # will be selected
        self.sort_index = (
            # First select aligned kernel
            0 if self.aligned else 1,
            # Take a kernel without dropout if possible
            1 if self.apply_dropout else 0,
            # Then take the smallest maxK
            self.max_k,
            # .. and the highest block_i
            -self.block_i,
            # and finally avoid bounds-checks if possible
            0 if self.keys_queries_aligned_to_blocksizes else 1,
        )"
_aligned_suffix;"def _aligned_suffix(self) -> str:
        return ""aligned"" if self.aligned else ""notaligned"""
cpp_class;"def cpp_class(self) -> str:
        template_args = "", "".join(
            [
                f""cutlass::arch::Sm{self.sm_range[0]}"",
                DTYPES[self.dtype],
                ""true"" if self.aligned else ""false"",
                ""true"" if self.apply_dropout else ""false"",
                ""true"" if self.preload_mmas else ""false"",
                str(self.block_i),
                str(self.block_j),
                str(self.max_k),
            ]
        )
        if self.keys_queries_aligned_to_blocksizes:
            template_args += "", true""
        return f""AttentionBackwardKernel<{template_args}>"""
impl_group;"def impl_group(self) -> str:
        # Maps to file which will contain the implementation
        dropout_suffix = ""_dropout"" if self.apply_dropout else """"
        return f""{self.dtype}_{self._aligned_suffix}_k{self.max_k}{dropout_suffix}"""
cpp_impl;"def cpp_impl(self) -> str:
        return KERNEL_IMPL_TEMPLATE.format(
            CPP_CLASS=self.cpp_class,
            NAME=self.name,
            SM=self.sm_range[0],
            SM_MAX=self.sm_range[1],
        )"
get_all;"def get_all(cls) -> list[""BwdKernel""]:
        kernels: list[BwdKernel] = []
        for aligned, dtype, (sm, sm_max), apply_dropout, max_k in itertools.product(
            [True, False],
            DTYPES.keys(),
            zip(SM, SM[1:]),
            [True, False],
            [32, 64, 128, 2**16],
        ):
            if dtype == ""bf16"" and sm < 80:
                continue
            if not aligned and sm >= 80:
                continue
            is_half = dtype in [""bf16"", ""f16""]

            bi_values = [64]
            # Some architectures have more shmem and can use 128
            # We still need fallback to 64 for GPUs with less shmem
            # (Sm75, Sm86 ...)
            if sm >= 80 or (sm >= 70 and is_half):
                if max_k > 64:
                    bi_values.append(128)
            for bi in bi_values:
                output_in_rf = is_half and max_k <= bi
                preload_mmas = is_half and sm >= 80 and output_in_rf
                bj = 128 if (preload_mmas and max_k > 64) else 64
                kernels.append(
                    cls(
                        aligned=aligned,
                        dtype=dtype,
                        sm_range=(sm, sm_max),
                        apply_dropout=apply_dropout,
                        preload_mmas=preload_mmas,
                        block_i=bi,
                        block_j=bj,
                        max_k=max_k,
                    )
                )
                # A few specialized kernels that are faster
                if apply_dropout or max_k > 128 or not is_half or not aligned:
                    continue
                if sm not in [70, 80]:
                    continue
                kernels.append(
                    cls(
                        aligned=aligned,
                        dtype=dtype,
                        sm_range=(sm, sm_max),
                        apply_dropout=apply_dropout,
                        preload_mmas=preload_mmas,
                        block_i=bi,
                        block_j=bj,
                        max_k=max_k,
                        keys_queries_aligned_to_blocksizes=True,
                    )
                )
        # Add some specialized kernels for stable diffusion BW (K=80)
        # This is the only kernel that can keep the outputs on RF on
        # Sm86/Sm89, so it's much faster than the 64x64 one
        for dtype in [""f16"", ""bf16""]:
            kernels.append(
                cls(
                    aligned=True,
                    dtype=dtype,
                    sm_range=(80, SM[SM.index(80) + 1]),
                    apply_dropout=False,
                    preload_mmas=True,
                    block_i=128,
                    block_j=64,
                    max_k=96,
                    # Sm80 has a faster kernel for this case
                    dispatch_cond=""cc == 86 || cc == 89"",
                )
            )
        return kernels"
construct_name;"def construct_name(fwd_bwd, test_name):
    bwd = ""backward"" in fwd_bwd
    suite_name = fwd_bwd.replace(""-backward"", """")
    return f""{suite_name}[{test_name}]:{'bwd' if bwd else 'fwd'}"""
get_times;"def get_times(json_data):
    r = {}
    for fwd_bwd in json_data:
        for test_name in json_data[fwd_bwd]:
            name = construct_name(fwd_bwd, test_name)
            r[name] = json_data[fwd_bwd][test_name]
    return r"
format_message;"def format_message(self, field_dict):
        assert ""time"" in field_dict, ""Missing required Scribe field 'time'""
        message = defaultdict(dict)
        for field, value in field_dict.items():
            if field in self.schema[""normal""]:
                message[""normal""][field] = str(value)
            elif field in self.schema[""int""]:
                message[""int""][field] = int(value)
            elif field in self.schema[""float""]:
                message[""float""][field] = float(value)
            else:
                raise ValueError(
                    f""Field {field} is not currently used, be intentional about adding new fields""
                )
        return message"
_upload_intern;"def _upload_intern(self, messages):
        for m in messages:
            json_str = json.dumps(m)
            cmd = [""scribe_cat"", self.category, json_str]
            subprocess.run(cmd)"
post_pytest_benchmarks;"def post_pytest_benchmarks(self, pytest_json):
        machine_info = pytest_json[""machine_info""]
        commit_info = pytest_json[""commit_info""]
        upload_time = int(time.time())
        messages = []
        for b in pytest_json[""benchmarks""]:
            test = b[""name""].split(""["")[0]
            net_name = b[""params""][""net_name""]
            benchmark_name = f""{test}[{net_name}]""
            executor = b[""params""][""executor""]
            fuser = b[""params""][""fuser""]
            m = self.format_message(
                {
                    ""time"": upload_time,
                    ""benchmark_group"": b[""group""],
                    ""benchmark_name"": benchmark_name,
                    ""benchmark_executor"": executor,
                    ""benchmark_fuser"": fuser,
                    ""benchmark_class"": b[""fullname""],
                    ""benchmark_time"": pytest_json[""datetime""],
                    ""pytorch_commit_id"": commit_info[""id""],
                    ""pytorch_branch"": commit_info[""branch""],
                    ""pytorch_commit_time"": commit_info[""time""],
                    ""pytorch_version"": None,
                    ""pytorch_git_dirty"": commit_info[""dirty""],
                    ""machine_kernel"": machine_info[""release""],
                    ""machine_processor"": machine_info[""processor""],
                    ""machine_hostname"": machine_info[""node""],
                    ""circle_build_num"": os.environ.get(""CIRCLE_BUILD_NUM""),
                    ""circle_project_reponame"": os.environ.get(
                        ""CIRCLE_PROJECT_REPONAME""
                    ),
                    ""stddev"": b[""stats""][""stddev""],
                    ""rounds"": b[""stats""][""rounds""],
                    ""min"": b[""stats""][""min""],
                    ""median"": b[""stats""][""median""],
                    ""max"": b[""stats""][""max""],
                    ""mean"": b[""stats""][""mean""],
                }
            )
            messages.append(m)
        self.upload(messages)"
allgather_object;"def allgather_object(obj):
    out = [None for _ in range(dist.get_world_size())]
    dist.all_gather_object(out, obj)
    return out"
allgather_run;"def allgather_run(cmd):
    proc = subprocess.run(shlex.split(cmd), capture_output=True)
    assert proc.returncode == 0
    return allgather_object(proc.stdout.decode(""utf-8""))"
allequal;"def allequal(iterator):
    iterator = iter(iterator)
    try:
        first = next(iterator)
    except StopIteration:
        return True
    return all(first == rest for rest in iterator)"
benchmark_process_group;"def benchmark_process_group(pg, benchmark, use_ddp_for_single_rank=True):
    torch.manual_seed(pg.rank())
    torch.cuda.manual_seed(pg.rank())

    model = benchmark.create_model()
    data = [(benchmark.generate_inputs(), benchmark.generate_target())]
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), 0.001, momentum=0.9, weight_decay=1e-4)
    if use_ddp_for_single_rank or pg.size() > 1:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[torch.cuda.current_device()],
            broadcast_buffers=False,
            process_group=pg,
            bucket_cap_mb=benchmark.bucket_size,
        )

    measurements = []
    warmup_iterations = 5
    measured_iterations = 10
    for inputs, target in data * (warmup_iterations + measured_iterations):
        start = time.time()
        output = model(*inputs)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        torch.cuda.synchronize()
        measurements.append(time.time() - start)

    # Throw away measurements for warmup iterations
    return measurements[warmup_iterations:]"
run_benchmark;"def run_benchmark(benchmark, ranks, opts):
    group = dist.new_group(ranks=ranks, backend=benchmark.distributed_backend)
    measurements = []
    if dist.get_rank() in set(ranks):
        if not opts:
            opts = {}
        measurements = benchmark_process_group(group, benchmark, **opts)
    dist.destroy_process_group(group)
    dist.barrier()

    # Aggregate measurements for better estimation of percentiles
    return list(itertools.chain(*allgather_object(measurements)))"
append_benchmark;"def append_benchmark(prefix, ranks, opts=None):
        prefix = f""{len(ranks):4} GPUs -- {prefix}""
        benchmarks.append((prefix, ranks, opts))"
local_print;"def local_print(msg):
        if dist.get_rank() == 0:
            print(msg, end="""", flush=True)"
print_header;"def print_header():
        local_print(""\n"")
        local_print("" "" * 22)
        for _ in [50, 75, 90, 95]:
            local_print(f""{'sec/iter':14s}{'ex/sec':10s}"")
        local_print(""\n"")"
print_measurements;"def print_measurements(prefix, nelem, measurements):
        measurements = sorted(measurements)
        local_print(f""{prefix:8s}:"")
        for p in [50, 75, 90, 95]:
            v = np.percentile(measurements, p)
            local_print(f""  p{p:02d}:  {v:1.3f}s  {nelem / v:6d}/s"")
        local_print(""\n"")"
create_model;"def create_model(self):
        raise NotImplementedError"
generate_inputs;"def generate_inputs(self):
        raise NotImplementedError"
generate_target;"def generate_target(self):
        raise NotImplementedError"
create_model;"def create_model(self):
        return torchvision.models.__dict__[self.model]().to(self.device)"
generate_inputs;"def generate_inputs(self):
        return [torch.rand([self.batch_size, 3, 224, 224], device=self.device)]"
generate_target;"def generate_target(self):
        return torch.tensor([1] * self.batch_size, dtype=torch.long, device=self.device)"
model_names;"def model_names(filename: str) -> set[str]:
    names = set()
    with open(filename) as fh:
        lines = fh.readlines()
        lines = [line.rstrip() for line in lines]
        for line in lines:
            line_parts = line.split("" "")
            if len(line_parts) == 1:
                line_parts = line.split("","")
            model_name = line_parts[0]
            names.add(model_name)
    return names"
parse_args;"def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--only"",
        help=""""""Run just one model from whichever model suite it belongs to. Or
        specify the path and class name of the model in format like:
        --only=path:<MODEL_FILE_PATH>,class:<CLASS_NAME>

        Due to the fact that dynamo changes current working directory,
        the path should be an absolute path.

        The class should have a method get_example_inputs to return the inputs
        for the model. An example looks like
        ```
        class LinearModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 10)

            def forward(self, x):
                return self.linear(x)

            def get_example_inputs(self):
                return (torch.randn(2, 10),)
        ```
    """""",
    )
    return parser.parse_known_args(args)"
get_compile_time;"def get_compile_time(file: tempfile._TemporaryFileWrapper) -> float:
    lines = file.readlines()
    # Decode from byte string, remove new lines, parse csv
    lines = [line.decode(""utf-8"").strip().split("","") for line in lines]
    compilation_time_idx = lines[0].index(""compilation_latency"")
    compilation_time = lines[1][compilation_time_idx]
    return float(compilation_time)"
_run_torchbench_from_args;"def _run_torchbench_from_args(
    cmd_args: argparse.Namespace,
    model: str,
    args: list[str],
) -> tuple[list[float], list[float]]:
    cold_compile_time: list[float] = []
    warm_compile_time: list[float] = []

    for _ in range(cmd_args.repeat):
        with fresh_inductor_cache():
            env = os.environ.copy()
            with tempfile.NamedTemporaryFile(suffix="".csv"") as file:
                args.append(""--output="" + file.name)
                logger.info(f""Performing cold-start run for {model}"")  # noqa: G004
                subprocess.check_call(args, timeout=TIMEOUT, env=env)
                cold_compile_time.append(get_compile_time(file))

            args.pop()
            with tempfile.NamedTemporaryFile(suffix="".csv"") as file:
                args.append(""--output="" + file.name)
                logger.info(f""Performing warm-start run for {model}"")  # noqa: G004
                subprocess.check_call(args, timeout=TIMEOUT, env=env)
                warm_compile_time.append(get_compile_time(file))

    return cold_compile_time, warm_compile_time"
_run_torchbench_model;"def _run_torchbench_model(
    cmd_args: argparse.Namespace,
    results: list[RunResult],
    model: str,
) -> None:
    cur_file = os.path.abspath(__file__)
    torchbench_file = os.path.join(
        os.path.dirname(cur_file), BENCHMARK_FILE[cmd_args.benchmark]
    )
    assert os.path.exists(torchbench_file), (
        f""Torchbench does not exist at {torchbench_file}""
    )

    dynamic = cmd_args.dynamic
    dynamic_args = [""--dynamic-shapes"", ""--dynamic-batch-only""] if dynamic else []

    args = (
        [
            sys.executable,
            torchbench_file,
            f""--only={model}"",
            ""--repeat=1"",
            ""--performance"",
            ""--backend=inductor"",
            f""--device={cmd_args.device}"",
        ]
        + MODE_ARGS_DICT[cmd_args.mode]
        + dynamic_args
    )

    logger.info(f""Command: {args}"")  # noqa: G004
    try:
        cold_compile_t, warm_compile_t = _run_torchbench_from_args(
            cmd_args, model, args
        )
        speedup_pct = (1 - (sum(warm_compile_t) / sum(cold_compile_t))) * 100
        results.append(
            RunResult(
                model=model,
                mode=cmd_args.mode,
                benchmark=cmd_args.benchmark,
                dynamic=dynamic,
                device=cmd_args.device,
                cold_compile_s=cold_compile_t,
                warm_compile_s=warm_compile_t,
                speedup_pct=speedup_pct,
            )
        )
    except Exception:
        logger.info(""fail"", exc_info=True)
        return None"
_write_results_to_json;"def _write_results_to_json(
    cmd_args: argparse.Namespace,
    results: list[RunResult],
) -> None:
    if len(results) == 0:
        # do not write empty results
        return

    records = []
    for result in results:
        for metric_name, value in [
            (""Cold compile time (s)"", result.cold_compile_s),
            (""Warm compile time (s)"", result.warm_compile_s),
            (""Speedup (%)"", [result.speedup_pct]),
        ]:
            records.append(
                {
                    ""benchmark"": {
                        ""name"": ""TorchCache Benchmark"",
                        ""mode"": result.mode,
                        ""extra_info"": {
                            ""is_dynamic"": result.dynamic,
                            ""device"": result.device,
                        },
                    },
                    ""model"": {
                        ""name"": result.model,
                        ""backend"": ""inductor"",
                        ""origins"": [result.benchmark],
                    },
                    ""metric"": {
                        ""name"": metric_name,
                        ""type"": ""OSS model"",
                        ""benchmark_values"": value,
                    },
                }
            )
    with open(cmd_args.output, ""w"") as f:
        json.dump(records, f)"
parse_cmd_args;"def parse_cmd_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description=""Run a TorchCache benchmark."")
    parser.add_argument(
        ""-m"",
        ""--model"",
        help=""Name of the model to run"",
    )
    parser.add_argument(
        ""--dynamic"",
        action=""store_true"",
        help=""Whether to run with dynamic enabled"",
    )
    parser.add_argument(
        ""--benchmark"",
        choices=(""torchbench"", ""huggingface""),
        required=True,
        help=""Name of benchmark suite to run"",
    )
    parser.add_argument(
        ""--mode"",
        choices=(""inference"", ""training""),
        default=""training"",
    )
    parser.add_argument(
        ""--device"",
        default=""cuda"",
        choices=(""cuda"", ""cpu""),
    )
    parser.add_argument(
        ""--output"",
        required=True,
        help=""The output filename (json)"",
    )
    parser.add_argument(
        ""--repeat"",
        type=int,
        default=1,
        choices=range(1, 10),
        help=""Number of times to repeat the compilation (reduce noise)"",
    )
    args, _ = parser.parse_known_args()
    return args"
check_accuracy;"def check_accuracy(actual_csv, expected_csv, expected_filename):
    failed = []
    improved = []

    if ""rocm"" in expected_filename:
        flaky_models.update(
            {
                ""alexnet"",
                ""cait_m36_384"",
                ""demucs"",
                ""densenet121"",
                ""detectron2_fcos_r_50_fpn"",
                ""doctr_det_predictor"",
                ""doctr_reco_predictor"",
                ""hf_BigBird"",
                ""hf_Longformer"",
                ""hf_Reformer"",
                ""hf_Roberta_base"",
                ""hf_T5"",
                ""hf_T5_base"",
                ""levit_128"",
                ""llava"",
                ""microbench_unbacked_tolist_sum"",
                ""mnasnet1_0"",
                ""mobilenet_v2"",
                ""pytorch_CycleGAN_and_pix2pix"",
                ""pytorch_stargan"",
                ""resnet152"",
                ""resnet18"",
                ""resnet50"",
                ""resnext50_32x4d"",
                ""sam"",
                ""sam_fast"",
                ""shufflenet_v2_x1_0"",
                ""squeezenet1_1"",
                ""stable_diffusion_text_encoder"",
                ""stable_diffusion_unet"",
                ""timm_efficientdet"",
                ""timm_efficientnet"",
                ""timm_nfnet"",
                ""timm_regnet"",
                ""timm_resnest"",
                ""timm_vovnet"",
                ""torchrec_dlrm"",
                ""vgg16"",
            }
        )

    for model in actual_csv[""name""]:
        accuracy = get_field(actual_csv, model, ""accuracy"")
        expected_accuracy = get_field(expected_csv, model, ""accuracy"")

        if accuracy == expected_accuracy:
            status = ""PASS"" if expected_accuracy == ""pass"" else ""XFAIL""
            print(f""{model:34}  {status}"")
            continue
        elif model in flaky_models:
            if accuracy == ""pass"":
                # model passed but marked xfailed
                status = ""PASS_BUT_FLAKY:""
            else:
                # model failed but marked passe
                status = ""FAIL_BUT_FLAKY:""
        elif accuracy != ""pass"":
            status = ""FAIL:""
            failed.append(model)
        else:
            status = ""IMPROVED:""
            improved.append(model)
        print(
            f""{model:34}  {status:9} accuracy={accuracy}, expected={expected_accuracy}""
        )

    msg = """"
    if failed or improved:
        if failed:
            msg += textwrap.dedent(
                f""""""
            Error: {len(failed)} models have accuracy status regressed:
                {"" "".join(failed)}

            """"""
            )
        if improved:
            msg += textwrap.dedent(
                f""""""
            Improvement: {len(improved)} models have accuracy status improved:
                {"" "".join(improved)}

            """"""
            )
        sha = os.getenv(""SHA1"", ""{your CI commit sha}"")
        msg += textwrap.dedent(
            f""""""
        If this change is expected, you can update `{expected_filename}` to reflect the new baseline.
        from pytorch/pytorch root, run
        `python benchmarks/dynamo/ci_expected_accuracy/update_expected.py {sha}`
        and then `git add` the resulting local changes to expected CSVs to your commit.
        """"""
        )
    return failed or improved, msg"
check_csv;"def check_csv(filename):
    """"""
    Basic accuracy checking.
    """"""

    df = pd.read_csv(filename)

    failed = []
    for _, row in df.iterrows():
        model_name = row[""name""]
        status = row[""accuracy""]
        if ""pass"" not in status:
            failed.append(model_name)

        print(f""{model_name:34} {status}"")

    if failed:
        print(
            textwrap.dedent(
                f""""""
                Error {len(failed)} models failed
                    {"" "".join(failed)}
                """"""
            )
        )
        sys.exit(1)"
check_graph_breaks;"def check_graph_breaks(actual_csv, expected_csv, expected_filename):
    failed = []
    improved = []

    if ""rocm"" in expected_filename:
        flaky_models.update(
            {
                ""alexnet"",
                ""cait_m36_384"",
                ""demucs"",
                ""densenet121"",
                ""detectron2_fcos_r_50_fpn"",
                ""doctr_det_predictor"",
                ""doctr_reco_predictor"",
                ""hf_BigBird"",
                ""hf_Longformer"",
                ""hf_Reformer"",
                ""hf_Roberta_base"",
                ""hf_T5"",
                ""hf_T5_base"",
                ""levit_128"",
                ""llava"",
                ""microbench_unbacked_tolist_sum"",
                ""sam"",
                ""sam_fast"",
                ""stable_diffusion_text_encoder"",
                ""stable_diffusion_unet"",
                ""timm_efficientdet"",
                ""timm_nfnet"",
                ""torchrec_dlrm"",
                ""vgg16"",
            }
        )

    for model in actual_csv[""name""]:
        graph_breaks = get_field(actual_csv, model, ""graph_breaks"")
        expected_graph_breaks = get_field(expected_csv, model, ""graph_breaks"")
        flaky = model in flaky_models

        if expected_graph_breaks is None:
            status = ""MISSING:""
            improved.append(model)
        elif graph_breaks == expected_graph_breaks:
            status = ""PASS_BUT_FLAKY"" if flaky else ""PASS""
            print(f""{model:34}  {status}"")
            continue
        elif graph_breaks > expected_graph_breaks:
            if flaky:
                status = ""FAIL_BUT_FLAKY:""
            else:
                status = ""FAIL:""
                failed.append(model)
        elif graph_breaks < expected_graph_breaks:
            if flaky:
                status = ""IMPROVED_BUT_FLAKY:""
            else:
                status = ""IMPROVED:""
                improved.append(model)
        print(
            f""{model:34}  {status:19} graph_breaks={graph_breaks}, expected={expected_graph_breaks}""
        )

    msg = """"
    if failed or improved:
        if failed:
            msg += textwrap.dedent(
                f""""""
            Error: {len(failed)} models have new dynamo graph breaks:
                {"" "".join(failed)}

            """"""
            )
        if improved:
            msg += textwrap.dedent(
                f""""""
            Improvement: {len(improved)} models have fixed dynamo graph breaks:
                {"" "".join(improved)}

            """"""
            )
        sha = os.getenv(""SHA1"", ""{your CI commit sha}"")
        msg += textwrap.dedent(
            f""""""
        If this change is expected, you can update `{expected_filename}` to reflect the new baseline.
        from pytorch/pytorch root, run
        `python benchmarks/dynamo/ci_expected_accuracy/update_expected.py {sha}`
        and then `git add` the resulting local changes to expected CSVs to your commit.
        """"""
        )
    return failed or improved, msg"
check_perf_csv;"def check_perf_csv(filename, threshold, threshold_scale):
    """"""
    Basic performance checking.
    """"""

    df = pd.read_csv(filename)

    failed = []
    for _, row in df.iterrows():
        model_name = row[""name""]
        speedup = row[""speedup""]
        if speedup < threshold * threshold_scale:
            failed.append(model_name)

        print(f""{model_name:34} {speedup}"")

    if failed:
        print(
            textwrap.dedent(
                f""""""
                Error {len(failed)} models performance regressed
                    {"" "".join(failed)}
                """"""
            )
        )
        sys.exit(1)"
load_yaml_file;"def load_yaml_file(filename):
    filepath = os.path.join(os.path.dirname(__file__), filename)

    with open(filepath) as f:
        data = yaml.safe_load(f)

    internal_file_path = os.path.join(os.path.dirname(__file__), ""fb"", filename)
    if os.path.exists(internal_file_path):
        with open(internal_file_path) as f:
            internal_data = yaml.safe_load(f)
            data.update(internal_data)

    def flatten(lst):
        for item in lst:
            if isinstance(item, list):
                yield from flatten(item)
            else:
                yield item

    def maybe_list_to_set(obj):
        if isinstance(obj, dict):
            return {k: maybe_list_to_set(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return set(flatten(obj))
        return obj

    return maybe_list_to_set(data)"
model_specified_by_path;"def model_specified_by_path(path_and_class_str):
    return "":"" in path_and_class_str"
load_model_from_path;"def load_model_from_path(path_and_class_str):
    configs = {}
    for kvstr in path_and_class_str.split("",""):
        k, v = kvstr.split("":"")
        configs[k] = v

    for name in [""path"", ""class""]:
        if name not in configs:
            raise RuntimeError(
                ""Invalid --only arguments. Check help message for the correct format""
            )

    path = configs[""path""]
    class_name = configs[""class""]

    if path[:1] != ""/"":
        raise RuntimeError(
            ""Use absolute path since dynamo may change the current working directory which makes using relative path tricky""
        )

    spec = importlib.util.spec_from_file_location(""module_name"", path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    model_class = getattr(module, class_name)
    assert issubclass(model_class, torch.nn.Module)
    model = model_class()
    assert hasattr(model, ""get_example_inputs"")
    inputs = model.get_example_inputs()
    return model, inputs"
write_outputs;"def write_outputs(filename, headers, row):
    """"""
    Write both CSV and JSON outputs using the original CSV output interface
    """"""
    global disable_output
    if disable_output:
        return

    output_csv(filename, headers, row)
    output_json(filename, headers, row)"
output_csv;"def output_csv(filename, headers, row):
    if os.path.exists(filename):
        with open(filename) as fd:
            lines = list(csv.reader(fd)) or [[]]
            if headers and len(headers) > len(lines[0]):
                # if prior results failed the header might not be filled in yet
                lines[0] = headers
            else:
                headers = lines[0]
    else:
        lines = [headers]
    lines.append([(f""{x:.6f}"" if isinstance(x, float) else x) for x in row])
    with open(filename, ""w"") as fd:
        writer = csv.writer(fd, lineterminator=""\n"")
        for line in lines:
            writer.writerow(list(line) + [""0""] * (len(headers) - len(line)))"
output_json;"def output_json(filename, headers, row):
    """"""
    Write the result into JSON format, so that it can be uploaded to the benchmark database
    to be displayed on OSS dashboard. The JSON format is defined at
    https://github.com/pytorch/pytorch/wiki/How-to-integrate-with-PyTorch-OSS-benchmark-database
    """"""
    origin = """"
    if ""torchbench"" in filename:
        origin = ""torchbench""
    elif ""huggingface"" in filename:
        origin = ""huggingface""
    elif ""timm_models"" in filename:
        origin = ""timm_models""

    extra_info = {
        ""device"": current_device,
        ""quantization"": current_quantization,
        ""batch_size"": current_batch_size,
    }
    if current_settings:
        extra_info.update(current_settings)

    mapping_headers = {headers[i]: v for i, v in enumerate(row)}
    with open(f""{os.path.splitext(filename)[0]}.json"", ""a"") as f:
        for header, value in mapping_headers.items():
            # These headers are not metric names
            if header in (""dev"", ""name"", ""batch_size""):
                continue

            # Make sure that the record is valid
            if not current_name:
                continue

            record = {
                ""benchmark"": {
                    ""name"": ""TorchInductor"",
                    ""mode"": current_mode,
                    ""dtype"": current_dtype,
                    ""extra_info"": extra_info,
                },
                ""model"": {
                    ""name"": current_name,
                    ""type"": ""OSS model"",
                    ""backend"": current_backend,
                    ""origins"": [origin],
                },
            }

            # NB: When the metric is accuracy, its value is actually a string, i.e. pass, and
            # not a number. ClickHouse doesn't support mix types atm. It has a Variant type
            # https://clickhouse.com/docs/en/sql-reference/data-types/variant, but this isn't
            # recommended by CH team themselves. The workaround here is to store that value
            # in the extra_info field instead.
            if isinstance(value, str):
                record[""metric""] = {
                    ""name"": header,
                    ""extra_info"": {""benchmark_values"": [value]},
                }
            else:
                record[""metric""] = {
                    ""name"": header,
                    ""benchmark_values"": [value],
                }

            print(json.dumps(record), file=f)"
get_suite_from_model_iter_fn;"def get_suite_from_model_iter_fn(model_iter_fn):
    # TODO: This is a bit of a hack
    suite = None
    if (runner := getattr(model_iter_fn, ""__self__"", None)) and hasattr(
        runner, ""suite_name""
    ):
        suite = runner.suite_name
    return suite"
output_signpost;"def output_signpost(data, args, suite, error=None):
    from torch.utils._stats import simple_call_counter

    data = data.copy()

    if ""name"" not in data:
        data[""name""] = current_name

    if ""dev"" not in data:
        data[""dev""] = current_device

    filtered_args = vars(args).copy()
    # I generated this list by reading through all the configs and dropping
    # ones that looked irrelevant or redundant
    for k in [
        ""filter"",
        ""exclude"",
        ""exclude_exact"",
        ""dump_raw_metrics"",
        ""log_operator_inputs"",
        ""distributed_master_port"",
        ""skip_accuracy_check"",
        ""generate_aot_autograd_stats"",
        ""output"",
        ""output_directory"",
        ""disable_output"",
        ""export_profiler_trace"",
        ""profiler_trace_name"",
        ""explain"",
        ""stats"",
        ""print_memory"",
        ""print_compilation_time"",
        ""print_dataframe_summary"",
        ""print_graph_breaks"",
        ""log_graph_breaks"",
        ""timing"",
        ""progress"",
        ""timeout"",
        ""per_process_memory_fraction"",
        ""minify"",
        ""verbose"",
        ""quiet"",
        ""print_fx"",
        ""print_aten_ops"",
        ""log_conv_args"",
        ""recompile_profiler"",
        ""find_batch_sizes"",
        # Redundant
        ""batch_size"",
        ""batch_size_file"",
        ""only"",
        ""diff_branch"",
        ""tag"",
        ""coverage"",
        ""overhead"",
        ""speedup_dynamo_ts"",
        ""speedup_fx2trt"",
        ""speedup_fx2trt_fp16"",
        ""accuracy"",
        ""performance"",
        ""tolerance"",
    ]:
        del filtered_args[k]

    event_name = ""unknown""
    if args.accuracy:
        event_name = ""accuracy""
    elif args.quantization:
        event_name = ""quantization""
    elif args.performance:
        event_name = ""performance""

    from torch._dynamo.utils import calculate_time_spent, compilation_time_metrics

    wall_time_by_phase = calculate_time_spent()

    open_source_signpost(
        subsystem=""dynamo_benchmark"",
        name=event_name,
        parameters=json.dumps(
            {
                **data,
                # TODO: Arguably the rest of these should be in the CSV too
                ""suite"": suite,
                # Better than using compile_times utils directly
                # NB: Externally, compilation_metrics colloquially refers to
                # the coarse-grained phase timings, even though internally
                # they are called something else
                ""compilation_metrics"": wall_time_by_phase,
                ""agg_compilation_metrics"": {
                    k: sum(v) for k, v in compilation_time_metrics.items()
                },
                ""detailed_compilation_metrics"": compilation_time_metrics,
                ""simple_call_counter"": simple_call_counter,
                # NB: args has training vs inference
                ""args"": filtered_args,
                ""error"": error,
            }
        ),
    )

    return wall_time_by_phase[""total_wall_time""]"
nothing;"def nothing(f):
    return f"
patch_torch_manual_seed;"def patch_torch_manual_seed():
    """"""Make torch manual seed deterministic. Helps with accuracy testing.""""""

    def deterministic_torch_manual_seed(*args, **kwargs):
        from torch._C import default_generator

        seed = 1337
        if HAS_CUDA:
            import torch.cuda

            if not torch.cuda._is_in_bad_fork():
                torch.cuda.manual_seed_all(seed)
        if HAS_XPU:
            import torch.xpu

            if not torch.xpu._is_in_bad_fork():
                torch.xpu.manual_seed_all(seed)
        return default_generator.manual_seed(seed)

    torch.manual_seed = deterministic_torch_manual_seed"
empty_gpu_cache;"def empty_gpu_cache(device):
    """"""
    Explicitly empty gpu cache to avoid OOM in subsequent run.
    """"""

    if device not in [""cuda"", ""xpu"", ""mps""]:
        log.warning(
            ""Trying to call the empty_gpu_cache for device: %s, which is not in list [cuda, xpu]"",
            device,
        )
        return

    getattr(torch, device).empty_cache()"
synchronize;"def synchronize():
    pass"
summarize_graph_break;"def summarize_graph_break(filename):
    """"""
    Sorts and de-dupes the graphs breaks on the reason string. Note that this
    function is just a best effort to reduce the logging information. We could
    miss some graph breaks because of de-duping. We can further refine this
    function as need arises.
    """"""
    log_file = f""{filename.rstrip('.csv')}_graph_breaks.csv""
    if os.path.exists(log_file):
        df = pd.read_csv(log_file)
        df = df.sort_values(""reason"").drop_duplicates(subset=""reason"")

        # Specialize for multi tensor sgd as reason is not identical
        multi_tensor_sgd_row = df.loc[df[""reason""].str.contains(""_multi_tensor_sgd"")]
        if len(multi_tensor_sgd_row):
            df = df[
                ~df[""reason""].str.contains(""_multi_tensor_sgd"")
            ]  # Drop all sgd rows
            df = pd.concat(
                [df, pd.DataFrame([multi_tensor_sgd_row.iloc[0]])], axis=0
            )  # Add back a single row
        df.to_csv(f""{log_file.rstrip('.csv')}_deduped.csv"", index=False)"
print_summary;"def print_summary(filename, print_dataframe=False):
    if not (filename and os.path.exists(filename)):
        return
    data = pd.read_csv(filename)
    if ""tag"" in data.columns:
        for tag in data.tag.unique():
            if tag == ""0.0000"":
                continue  # This happens for failed runs
            print(f""\nSummary for tag={tag}:"")
            print_summary_table(data[data.tag == tag], print_dataframe=print_dataframe)
    else:
        print_summary_table(data, print_dataframe=print_dataframe)
    summarize_graph_break(filename)"
print_summary_table;"def print_summary_table(data, print_dataframe=False):
    if print_dataframe:
        pd.options.display.max_rows = 1000
        pd.options.display.max_columns = 1000
        pd.options.display.width = 2000
        print(data)
    width = max(map(len, data.columns))
    for col in data.columns:
        try:
            if col in (""dev"", ""name"", ""batch_size"", ""tag""):
                continue
            elif col in (""pct_ops"", ""pct_time""):
                print(col.ljust(width), f""{data[col].mean():.3%}"")
            elif col in (""graphs"", ""graph_calls"", ""captured_ops"", ""total_ops""):
                print(col.ljust(width), f""{data[col].mean():.3f}"")
            elif col in (""compilation_latency""):
                print(col.ljust(width), f""mean={data[col].mean():.3f} seconds"")
            elif col in (""compression_ratio""):
                print(col.ljust(width), f""mean={data[col].mean():.3f}x"")
            elif col in (""accuracy""):
                pass_rate = (data[col] == ""pass"").mean()
                print(col.ljust(width), f""pass_rate={100 * pass_rate:.2f}%"")
            else:
                cdata = data[col]
                print(
                    col.ljust(width),
                    f""gmean={gmean(cdata):.2f}x mean={cdata.mean():.3f}x"",
                )
        except Exception:
            pass"
tensor_is_on_xla;"def tensor_is_on_xla(tensors):
    def visit(x: torch.Tensor):
        nonlocal result
        if x.device.type == ""xla"":
            result = True

    result = False
    tree_map_only(torch.Tensor, visit, tensors)
    return result"
timed;"def timed(
    model,
    model_iter_fn,
    example_inputs,
    times=1,
    return_result=False,
    collect_outputs=False,
):
    use_xla = tensor_is_on_xla(example_inputs)
    synchronize()

    if use_xla:
        xm.mark_step()
        xm.wait_device_ops()

    time_total = 0
    # Dont collect outputs to correctly measure timing
    for _ in range(times):
        # Put this call inside the loop to reset the seed for each iteration.
        # Don't include reset_rng_state() to correctly measure timing
        reset_rng_state(use_xla)
        t_iter_begin = time.perf_counter()
        result = model_iter_fn(model, example_inputs, collect_outputs=collect_outputs)

        # instead of calling sync on result_list, we should call mark_step.
        # In training case, result_list may be empty, but we want to
        # send all the pending graphs for compilation.
        if use_xla:
            # For the model running on regular torchxla (baseline), we need the
            # mark step to send the accumulated graph for compilation.
            #
            # For the model running with dynamo/torchxla bridge, in training case,
            # we need the mark step to send the optimizer graph out for
            # compilation.
            xm.mark_step()
        t_iter_end = time.perf_counter()
        time_total += t_iter_end - t_iter_begin

    t_0 = time.perf_counter()
    if use_xla:
        xm.wait_device_ops()
    synchronize()
    t_1 = time.perf_counter()
    time_total += t_1 - t_0
    return (time_total, result) if return_result else time_total"
_normalize_bench_inputs;"def _normalize_bench_inputs(example_inputs) -> tuple[tuple[Any], Mapping[str, Any]]:
    # NOTE(bowbao): For huggingface benchmark, example_inputs are formatted as dictionary,
    # and consumed like `model(**example_inputs)`.
    # For other benchmarks, example_inputs are formatted as tuple and consumed
    # like `model(*example_inputs)`.
    if isinstance(example_inputs, dict):
        return (), example_inputs
    else:
        return tuple(example_inputs), {}"
_register_dataclass_output_as_pytree;"def _register_dataclass_output_as_pytree(example_outputs) -> None:
    # NOTE(angelayi): For huggingface benchmark, some example outputs are
    # formatted as a dataclass which pytree cannot consume. So we want
    # to register the pytree implementation here
    example_outputs_flat = pytree.tree_leaves(example_outputs)
    output_dataclass_types = [
        type(out) for out in example_outputs_flat if dataclasses.is_dataclass(type(out))
    ]
    for output_type in output_dataclass_types:
        from torch._export.utils import register_dataclass_as_pytree_node

        register_dataclass_as_pytree_node(
            output_type,
            serialized_type_name=f""{output_type.__module__}.{output_type.__name__}"",
        )"
coverage_experiment;"def coverage_experiment(args, model_iter_fn, model, example_inputs):
    """"""
    Test operator/model coverage of TorchDynamo and record statistics
    taken from a profiler.  This target is mainly intended to check
    correctness.

    Writes to ./coverage.csv
    """"""
    profiler = Profiler()
    frozen_model_iter_fn = torch._dynamo.run(model_iter_fn)
    with profiler.prof:
        frozen_model_iter_fn(model, example_inputs)
    coverage_result = profiler.results()
    write_outputs(
        output_filename,
        (
            ""dev"",
            ""name"",
            ""batch_size"",
            ""graphs"",
            ""graph_calls"",
            ""captured_ops"",
            ""total_ops"",
            ""pct_ops"",
            ""pct_time"",
        ),
        [
            current_device,
            current_name,
            current_batch_size,
        ]
        + coverage_result.tocsv(),
    )
    return coverage_result"
speedup_experiment_fx2trt;"def speedup_experiment_fx2trt(args, model_iter_fn, model, example_inputs):
    """"""
    Measure speedups over eager using the trt inference backend. TRT backend is based fx graph
    generated by torch._dynamo.
    Writes to ./speedups_fx2trt.csv
    """"""
    return speedup_experiment(args, model_iter_fn, model, example_inputs)"
recompile_profiler_experiment;"def recompile_profiler_experiment(args, model_iter_fn, model, example_inputs):
    prof = torch._dynamo.utils.CompilerProfiler()
    opt_model_iter_fn = torch._dynamo.optimize(prof, nopython=args.nopython)(
        model_iter_fn
    )
    opt_model_iter_fn(model, example_inputs)
    write_outputs(
        output_filename, [""model"", ""profiler report""], [current_name, prof.report()]
    )
    met = prof.get_metrics()
    guard_failures = len(met[""guard_failures""])
    return [guard_failures]"
randomize_input;"def randomize_input(inputs):
    if isinstance(inputs, (list, tuple)):
        return type(inputs)([randomize_input(x) for x in inputs])
    elif isinstance(inputs, torch.Tensor):
        if inputs.dtype in (torch.float32, torch.float64):
            torch._dynamo.utils.counters[""randomize_input""][""times""] += 1
            return torch.randn_like(inputs)
        elif inputs.dtype == torch.int64:
            # Note: we can not simply tune integer tensors as follows
            #   `return torch.randint_like(inputs, high=inputs.max().item())`
            # This may break some invariants between tensors.
            # E.g. in embedding lookup case, one tensor is the length
            # and another is an indices tensor.
            return inputs
        else:
            raise RuntimeError(
                f""randomize_input need support tensor of type {inputs.dtype}""
            )
    else:
        raise RuntimeError(
            f""randomize_input can not handle input of type {type(inputs)}""
        )"
maybe_mark_step;"def maybe_mark_step(args):
    if args.trace_on_xla:
        xm.mark_step()"
latency_experiment;"def latency_experiment(args, model_iter_fn, model, example_inputs, mark, **kwargs):
    """"""
    Measure latency on a specific backend.
    """"""

    timings = np.zeros((args.repeat,), np.float64)
    # if we randomize the input, we should also check the result is correct
    should_randomize_input = args.randomize_input

    import contextlib

    from torch._inductor.utils import maybe_profile

    @contextlib.contextmanager
    def maybe_mark_profile(*args, **kwargs):
        prof: torch.profiler.profile = kwargs.pop(""p"", None)
        mark = kwargs.pop(""mark"", None)
        if prof:
            with torch.profiler.record_function(mark):
                yield
        else:
            yield

    times = args.iterations_per_run

    with maybe_profile(args.export_profiler_trace, **args.profile_details) as p:
        for rep in trange(args.repeat, desc=""running benchmark""):
            inputs = (
                randomize_input(copy.deepcopy(example_inputs))
                if should_randomize_input
                else example_inputs
            )
            # need call mark_step to perform the computation
            # on randomize_input. Otherwise the first call using the
            # inputs will incur high penalty then the next one.
            maybe_mark_step(args)

            with maybe_mark_profile(p=p, mark=mark):
                timings[rep], actual_output = timed(
                    model,
                    model_iter_fn,
                    inputs,
                    return_result=True,
                    times=times,
                    collect_outputs=args.collect_outputs,
                )

    if args.export_profiler_trace:
        name = args.profiler_trace_name + ""_"" + model.name
        if hasattr(args, ""rank""):
            name += f""_rank_{args.rank}""
        name += "".json""
        name = os.path.join(torch._dynamo.config.base_dir, name)
        p.export_chrome_trace(name)
    return timings"
latency_experiment_summary;"def latency_experiment_summary(suite_name, args, model, timings, **kwargs):
    median = np.median(timings, axis=0)
    speedup = median[0] / median[1]
    if args.dump_raw_metrics:
        np.save(
            f""{output_filename[:-4]}-raw_timings-{current_name}-{current_device}.npy"",
            timings,
        )

    first_headers = [""dev"", ""name"", ""batch_size""]
    first_fields = [current_device, current_name, current_batch_size]
    if ""tag"" in kwargs:
        first_headers.append(""tag"")
        first_fields.append(kwargs[""tag""])
    headers = first_headers + [""speedup"", ""abs_latency""]
    row = first_fields + [float(speedup), median[1] * 1000]
    msg = f""{speedup:.3f}x""
    if args.baseline:
        headers.extend(
            [
                ""baseline"",
                ""speedup_vs_baseline"",
            ]
        )
        df = pd.read_csv(args.baseline)
        try:
            baseline_speedup = df[df[""name""] == current_name][""speedup""].item()
            row.extend([baseline_speedup, speedup / baseline_speedup])
            msg = f""{baseline_speedup:.3f}x -> {speedup:.3f}x [{speedup / baseline_speedup:.3f}x]""
        except (KeyError, ZeroDivisionError):
            row.extend(
                [
                    0.0,
                    0.0,
                ]
            )
    if ""compilation_latency"" in kwargs:
        headers += [
            ""compilation_latency"",
            ""compression_ratio"",
            ""eager_peak_mem"",
            ""dynamo_peak_mem"",
        ]
        row.append(kwargs[""compilation_latency""])
        row.append(kwargs[""compression_ratio""])
        row.append(kwargs[""eager_peak_mem""])
        row.append(kwargs[""dynamo_peak_mem""])

    if ""cache_lookup_latency"" in kwargs:
        headers.append(""cache_lookup_latency"")
        row.append(kwargs[""cache_lookup_latency""])

    if ""dynamo_stats"" in kwargs:
        for k, v in kwargs[""dynamo_stats""].items():
            headers.append(k)
            row.append(v)
    write_outputs(
        output_filename,
        headers,
        row,
    )
    c_headers, c_data = torch._dynamo.utils.compile_times(repr=""csv"", aggregate=True)
    assert output_filename.find("".csv"") > 0, (
        f""expected output_filename to be a .csv, but got {output_filename}""
    )
    write_outputs(
        output_filename[:-4] + ""_compilation_metrics.csv"",
        first_headers + c_headers,
        first_fields + c_data,
    )

    # Hypothetically you can use this from other places, but it's currently
    # inaccessible, and when this assert fails you need to update the
    # event_name here to account for the other cases you are using this
    assert args.quantization is not None
    output_signpost(
        dict(zip(headers, row)),
        args,
        suite_name,
    )

    return msg"
speedup_experiment;"def speedup_experiment(args, model_iter_fn, model, example_inputs, **kwargs):
    """"""
    Measure speedups over eager.

    Writes to ./speedups.csv
    """"""
    # if args.dynamic_shapes:
    #     return speedup_experiment_ds(args, model_iter_fn, model, example_inputs)

    timings = np.zeros((args.repeat, 2), np.float64)
    # if we randomize the input, we should also check the result is correct
    should_randomize_input = args.randomize_input

    import contextlib

    from torch._inductor.utils import maybe_profile

    @contextlib.contextmanager
    def maybe_mark_profile(*args, **kwargs):
        prof: torch.profiler.profile = kwargs.pop(""p"", None)
        mark = kwargs.pop(""mark"", None)
        if prof:
            with torch.profiler.record_function(mark):
                yield
        else:
            yield

    times = args.iterations_per_run

    # Use higher tolerance for XLA since XLA cause numerical unstability when
    # graph size changes
    tolerance = args.xla_tolerance if args.trace_on_xla else 1e-4
    torch._dynamo.config.repro_tolerance = tolerance

    with maybe_profile(args.export_profiler_trace, **args.profile_details) as p:
        if args.export_aot_inductor:
            frozen_model_iter_fn = export_aot_inductor(
                model, example_inputs, args.inductor_compile_mode
            )
        else:
            frozen_model_iter_fn = torch._dynamo.run(model_iter_fn)

        for rep in trange(args.repeat, desc=""running benchmark""):
            inputs = (
                randomize_input(copy.deepcopy(example_inputs))
                if should_randomize_input
                else example_inputs
            )
            # need call mark_step to perform the computation
            # on randomize_input. Otherwise the first call using the
            # inputs will incur high penalty then the next one.
            maybe_mark_step(args)

            # interleave the runs to handle frequency scaling and load changes
            with maybe_mark_profile(p=p, mark=""expected""):
                timings[rep, 0], expected_output = timed(
                    model,
                    model_iter_fn,
                    inputs,
                    return_result=True,
                    times=times,
                    collect_outputs=args.collect_outputs,
                )

            # call mark_step between the 2 calls to make the comparison fair.
            maybe_mark_step(args)

            with maybe_mark_profile(p=p, mark=""actual""):
                timings[rep, 1], actual_output = timed(
                    model,
                    frozen_model_iter_fn,
                    inputs,
                    return_result=True,
                    times=times,
                    collect_outputs=args.collect_outputs,
                )

    if args.export_profiler_trace:
        name = args.profiler_trace_name + ""_"" + model.name
        if hasattr(args, ""rank""):
            name += f""_rank_{args.rank}""
        if args.export_perfdoctor and trace_handler:
            trace_handler(name, p)
        else:
            name += "".json""
            name = os.path.join(torch._dynamo.config.base_dir, name)
            p.export_chrome_trace(name)

    median = np.median(timings, axis=0)
    speedup = median[0] / median[1]
    if args.dump_raw_metrics:
        np.save(
            f""{output_filename[:-4]}-raw_timings-{current_name}-{current_device}.npy"",
            timings,
        )

    first_headers = [""dev"", ""name"", ""batch_size""]
    first_fields = [current_device, current_name, current_batch_size]
    if ""tag"" in kwargs:
        first_headers.append(""tag"")
        first_fields.append(kwargs[""tag""])
    headers = first_headers + [""speedup"", ""abs_latency""]
    row = first_fields + [float(speedup), median[1] * 1000]
    msg = f""{speedup:.3f}x""
    if args.baseline:
        headers.extend(
            [
                ""baseline"",
                ""speedup_vs_baseline"",
            ]
        )
        df = pd.read_csv(args.baseline)
        try:
            baseline_speedup = df[df[""name""] == current_name][""speedup""].item()
            row.extend([baseline_speedup, speedup / baseline_speedup])
            msg = f""{baseline_speedup:.3f}x -> {speedup:.3f}x [{speedup / baseline_speedup:.3f}x]""
        except (KeyError, ZeroDivisionError):
            row.extend(
                [
                    0.0,
                    0.0,
                ]
            )
    if ""compilation_latency"" in kwargs:
        headers += [
            ""compilation_latency"",
            ""compression_ratio"",
            ""eager_peak_mem"",
            ""dynamo_peak_mem"",
        ]
        row.append(kwargs[""compilation_latency""])
        row.append(kwargs[""compression_ratio""])
        row.append(kwargs[""eager_peak_mem""])
        row.append(kwargs[""dynamo_peak_mem""])

    if ""cache_lookup_latency"" in kwargs:
        headers.append(""cache_lookup_latency"")
        row.append(kwargs[""cache_lookup_latency""])

    if ""dynamo_stats"" in kwargs:
        for k, v in kwargs[""dynamo_stats""].items():
            headers.append(k)
            row.append(v)
    write_outputs(
        output_filename,
        headers,
        row,
    )
    c_headers, c_data = torch._dynamo.utils.compile_times(repr=""csv"", aggregate=True)
    assert output_filename.find("".csv"") > 0, (
        f""expected output_filename to be a .csv, but got {output_filename}""
    )
    write_outputs(
        output_filename[:-4] + ""_compilation_metrics.csv"",
        first_headers + c_headers,
        first_fields + c_data,
    )

    output_signpost(
        dict(zip(headers, row)),
        args,
        get_suite_from_model_iter_fn(model_iter_fn),
    )

    return msg"
speedup_experiment_ds;"def speedup_experiment_ds(args, model_iter_fn, model, example_inputs):
    """"""
    Run dynamic shapes benchmarks.

    Requires dynamic shape compatible models, which provide a list of example inputs.

    Warms up using the first input example and then iterates the inputs,
    measuring (and expecting minimal) variance between the runtime for different examples.

    """"""
    timings = np.zeros((args.repeat, len(example_inputs), 2), np.float64)

    if args.repeat > 5:
        print(
            f""\ndynamic shapes experiments are slow, consider setting --repeat less than {args.repeat}\n""
        )

    nwarmup = 4
    for rep in range(args.repeat):
        # Start each rep fresh, e.g. only warmup on example 0
        torch._dynamo.reset()
        optimized_model_iter_fn = optimize_ctx(model_iter_fn)
        for _ in range(nwarmup):
            optimized_model_iter_fn(model, example_inputs[0])

        for input_idx, inputs in enumerate(example_inputs):
            # interleave the runs to handle frequency scaling and load changes
            timings[rep, input_idx, 0] = timed(
                model, model_iter_fn, inputs, return_result=False
            )
            # different from regular speedup_experiment, we _DO_ want to allow recompilation
            timings[rep, input_idx, 1] = timed(
                model, optimized_model_iter_fn, inputs, return_result=False
            )
    medians = np.median(timings, axis=0)
    speedups = list(medians[:, 0] / medians[:, 1])
    speedups_mean = np.mean(speedups)
    speedups_median = np.median(speedups)
    speedups_var = np.var(speedups)

    # TODO this x[0] is not going to work in general but bert only has 1 input
    shapes = [x[0].shape for x in example_inputs]
    shape_keys = sorted(set(shapes))
    shape_speedups = {
        shape: [
            it[1] for it in filter(lambda it: it[0] == shape, zip(shapes, speedups))
        ]
        for shape in shape_keys
    }
    output_str = (
        f""mean: {speedups_mean:.3f}, median: {speedups_median:.3f}, var: {speedups_var:.3f}""
        + ""\nSpeedups by shape: ""
        + ""\n"".join(
            [
                f""{shape}: ""
                + "", "".join([f""{speedup: .3g}"" for speedup in shape_speedups[shape]])
                for shape in shape_keys
            ]
        )
    )
    write_outputs(
        output_filename,
        (""dev"", ""name"", ""batch_size"", ""speedup mean"", ""speedup median"", ""speedup var""),
        [
            current_device,
            current_name,
            current_batch_size,
            speedups_mean,
            speedups_median,
            speedups_var,
        ],
    )
    return output_str"
overhead_experiment;"def overhead_experiment(*args, model_iter_fn):
    """"""
    Measure overheads of TorchDynamo by running with no backend (only
    eager+FX), and reporting speedup/slowdown over eager.

    Writes to ./overheads.csv
    """"""
    return speedup_experiment(*args, model_iter_fn)"
print_fx;"def print_fx(gm, example_inputs):
    print(gm.graph)
    return gm"
print_aten_ops;"def print_aten_ops(gm, example_inputs):
    from functorch.compile import aot_module

    def trace_printer(gm, _):
        print(gm.graph)
        return gm

    return aot_module(gm, fw_compiler=trace_printer, bw_compiler=trace_printer)"
baselines;"def baselines(models, model_iter_fn, example_inputs, args):
    """"""
    Common measurement code across all baseline experiments.
    """"""
    models = list(models)
    for idx, (name, model) in enumerate(models):
        if idx == 0:
            result0 = model_iter_fn(model, example_inputs)
        elif model is not None:
            try:
                result = model_iter_fn(model, example_inputs)
                if same(result0, result):
                    continue
                print(name, ""is INCORRECT"")
            except Exception:
                log.exception(""error checking %s"", name)
            models[idx] = (name, None)
    timings = np.zeros((args.repeat, len(models)), np.float64)
    timings.fill(1.0e10)
    for rep in range(args.repeat):
        for idx, (name, model) in enumerate(models):
            if model is not None:
                try:
                    timings[rep, idx] = timed(model, model_iter_fn, example_inputs)
                except Exception:
                    pass
    pvalue = [
        ttest_ind(timings[:, 0], timings[:, i]).pvalue
        for i in range(1, timings.shape[1])
    ]
    median = np.median(timings, axis=0)
    speedup = median[0] / median[1:]
    for idx, (name, model) in enumerate(models[1:]):
        if model is None:
            speedup[idx] = 0.0
    result = "" "".join(
        [
            format_speedup(s, p, m is not None)
            for s, p, m in zip(speedup, pvalue, [m for n, m in models[1:]])
        ]
    )
    write_outputs(
        output_filename,
        (""dev"", ""name"", ""batch_size"") + tuple(n for n, m in models[1:]),
        [current_device, current_name, current_batch_size]
        + [f""{x:.4f}"" for x in speedup],
    )
    return result"
try_script;"def try_script(model, example_inputs):
    try:
        return torch.jit.script(model)
    except Exception:
        return None"
_produce_dynamic_shapes_for_export;"def _produce_dynamic_shapes_for_export(path, x):
    # mark_dynamic() is ignored for export.
    # use this to produce dynamic_shapes spec instead.
    from torch.export.dynamic_shapes import Dim

    if not isinstance(x, torch.Tensor):
        return None
    return dict.fromkeys(getattr(x, ""_dynamo_dynamic_indices"", {}), Dim.AUTO)"
export;"def export(model, example_inputs):
    from torch.export.dynamic_shapes import _combine_args, _tree_map_with_path

    example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
    example_outputs = model(*example_args, **example_kwargs)
    _register_dataclass_output_as_pytree(example_outputs)

    combined_args = _combine_args(model, example_args, example_kwargs)
    dynamic_shapes = _tree_map_with_path(
        _produce_dynamic_shapes_for_export, combined_args
    )

    # NOTE: if args.export is ever enabled for --performance mode (rather than solely
    # --accuracy), we'll need to clone the model and subtract out extra memory usage, as
    # done in AOTInductorModelCache.
    ep = torch.export.export(
        model, example_args, example_kwargs, dynamic_shapes=dynamic_shapes, strict=True
    )

    def opt_export(_, example_inputs):
        example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
        return ep.module()(*example_args, **example_kwargs)

    return opt_export"
export_aot_inductor;"def export_aot_inductor(model, example_inputs, mode):
    optimized = AOTInductorModelCache.load(model, example_inputs, mode)

    def opt_aot_inductor(_, example_inputs, collect_outputs=False):
        example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
        return optimized(*example_args, **example_kwargs)

    return opt_aot_inductor"
download_retry_decorator;"def download_retry_decorator(download_fn):
    """"""
    Decorator function for applying retry logic to a download function.

    The wrapped function will be called up to 5 times and raises an exception if the function fails each time.
    After each unsuccessful attempt, there is a delay before the next attempt, which is increased linearly with the number of tries.

    Usage:
    @download_retry_decorator
    def download_function(model_name: str):
        # download logic goes here
    """"""

    @functools.wraps(download_fn)
    def wrapper(self, *args, **kwargs) -> Any:
        tries = 0
        total_allowed_tries = MAX_DOWNLOAD_ATTEMPTS
        while tries <= total_allowed_tries:
            try:
                model = download_fn(self, *args, **kwargs)
                return model
            except Exception as e:
                tries += 1
                if tries <= total_allowed_tries:
                    wait = tries * 30
                    print(
                        f""Failed to load model: {e}. Trying again ({tries}/{total_allowed_tries}) after {wait}s""
                    )
                    time.sleep(wait)
                else:
                    raise RuntimeError(  # noqa: B904
                        f""Failed to load model '{args}' with following error(s): {str(e)}.""
                    )

    return wrapper"
read_batch_size_from_file;"def read_batch_size_from_file(args, filename, model_name):
    batch_size = None
    if os.path.exists(""benchmarks""):
        filename = os.path.join(""benchmarks"", filename)
    assert os.path.exists(filename), filename
    with open(filename) as f:
        lines = f.readlines()
        lines = [i.split("","") for i in lines if len(i.strip()) > 0]
        for val in lines:
            cur_name, b = val
            if model_name == cur_name:
                batch_size = int(b)
    if batch_size is None:
        log.warning(""Could not find batch size for %s"", model_name)
    elif batch_size == -1:
        raise RuntimeError(
            f""Batch size is unset for {model_name} in {args.batch_size_file}""
        )
    print(f""batch size: {batch_size}"")
    return batch_size"
alarm_handler;"def alarm_handler(signum, frame):
    raise TimeOutException"
exit_after;"def exit_after(s):
    """"""
    Decorator to raise TimeoutException if the fn is taking more than s seconds
    to run.
    """"""

    def outer(fn):
        def inner(*args, **kwargs):
            signal.signal(signal.SIGALRM, alarm_handler)
            signal.alarm(s)
            try:
                result = fn(*args, **kwargs)
            finally:
                signal.alarm(0)
            return result

        return inner

    return outer"
get_peak_memory;"def get_peak_memory():
    return torch.cuda.max_memory_allocated() / 10**9"
null_experiment;"def null_experiment(args, model_iter_fn, model, example_inputs):
    """"""
    A no-op experiment useful for making sure TorchBenchark alone works properly.
    """"""

    return []"
cast_to;"def cast_to(dtype, model, inputs):
    # cast model and inputs to fp16
    if dtype == torch.float16:
        model = model.half()
    else:
        model = model.to(dtype)

    inputs = tree_map(
        lambda x: x.to(dtype)
        if isinstance(x, torch.Tensor) and x.is_floating_point()
        else x,
        inputs,
    )
    return model, inputs"
cast_to_bf16;"def cast_to_bf16(model, inputs):
    return cast_to(torch.bfloat16, model, inputs)"
cast_to_fp16;"def cast_to_fp16(model, inputs):
    return cast_to(torch.float16, model, inputs)"
cast_to_fp64;"def cast_to_fp64(model, inputs):
    return cast_to(torch.float64, model, inputs)"
cast_to_fp32;"def cast_to_fp32(model, inputs):
    return cast_to(torch.float32, model, inputs)"
get_dynamo_stats;"def get_dynamo_stats():
    # TODO: consider deepcopy'ing the entire counters struct and
    # adding a helper to do subtraction on it
    return collections.Counter(
        {
            ""calls_captured"": torch._dynamo.utils.counters[""stats""][""calls_captured""],
            ""unique_graphs"": torch._dynamo.utils.counters[""stats""][""unique_graphs""],
            ""graph_breaks"": sum(torch._dynamo.utils.counters[""graph_break""].values()),
            # NB: The plus removes zero counts
            ""unique_graph_breaks"": len(+torch._dynamo.utils.counters[""graph_break""]),
            ""autograd_captures"": torch._dynamo.utils.counters[""compiled_autograd""][
                ""captures""
            ],
            ""autograd_compiles"": torch._dynamo.utils.counters[""compiled_autograd""][
                ""compiles""
            ],
            ""cudagraph_skips"": torch._dynamo.utils.counters[""inductor""][
                ""cudagraph_skips""
            ],
        }
    )"
maybe_init_distributed;"def maybe_init_distributed(should_init_distributed, rank, world_size, port=""6789""):
    try:
        if should_init_distributed:
            torch.cuda.set_device(rank)
            os.environ[""MASTER_ADDR""] = ""localhost""
            os.environ[""MASTER_PORT""] = port
            torch.distributed.init_process_group(
                ""nccl"", rank=rank, world_size=world_size
            )
        yield
    finally:
        if should_init_distributed:
            torch.distributed.destroy_process_group()"
maybe_snapshot_memory;"def maybe_snapshot_memory(should_snapshot_memory, suffix):
    # Enables Memory Snapshot tool for memory deep dives:
    # https://pytorch.org/blog/understanding-gpu-memory-1/
    try:
        if should_snapshot_memory:
            torch.cuda.memory._record_memory_history(max_entries=100000)
        yield
    finally:
        if should_snapshot_memory:
            try:
                torch.cuda.memory._dump_snapshot(
                    os.path.join(
                        torch._dynamo.config.base_dir,
                        f""{output_filename.rstrip('.csv')}_{suffix}.pickle"",
                    )
                )
            except Exception as e:
                log.error(""Failed to save memory snapshot, %s"", e)

            torch.cuda.memory._record_memory_history(enabled=None)"
help;"def help(fn):
    return fn.__doc__"
should_diff_branch;"def should_diff_branch(args):
    return args.diff_branch != diff_branch_default"
parse_args;"def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        ""--filter"", ""-k"", action=""append"", help=""filter benchmarks with regexp""
    )
    parser.add_argument(
        ""--exclude"", ""-x"", action=""append"", help=""filter benchmarks with regexp""
    )
    parser.add_argument(
        ""--exclude-exact"", action=""append"", help=""filter benchmarks with exact match""
    )
    parser.add_argument(
        ""--total-partitions"",
        type=int,
        default=1,
        choices=range(1, 16),
        help=""Total number of partitions we want to divide the benchmark suite into"",
    )
    parser.add_argument(
        ""--partition-id"",
        type=int,
        default=0,
        help=""ID of the benchmark suite partition to be run. Used to divide CI tasks"",
    )
    parser.add_argument(
        ""--devices"", ""--device"", ""-d"", action=""append"", help=""cpu, cuda or hpu""
    )
    parser.add_argument(""--device-index"", help=""CUDA device index"")
    parser.add_argument(
        ""--repeat"", ""-n"", type=int, default=30, help=""number of timing runs""
    )
    iterations_per_run_help = """"""
        Run this may iterations for each time measurement. This is mainly used for
        XLA training. We want to run multiple iterations per measurement so the
        tracing and computation for different iteartions can overlap with each
        other. This makes sure we have an accurate xla baseline.
    """"""
    parser.add_argument(
        ""--iterations-per-run"", type=int, default=1, help=iterations_per_run_help
    )
    parser.add_argument(
        ""--randomize-input"",
        action=""store_true"",
        help=""Whether to randomize the input values. Dimensions will be kept the same."",
    )
    parser.add_argument(
        ""--threads"",
        ""-t"",
        type=int,
        help=""number of threads to use for eager and inductor"",
    )
    parser.add_argument(
        ""--nopython"", action=""store_true"", help=""Turn graph breaks into errors""
    )
    parser.add_argument(
        ""--no-skip"",
        action=""store_true"",
        help=""run models that are in the global SKIP list"",
    )
    parser.add_argument(
        ""--prims-nvfuser"", action=""store_true"", help=""user prims + nvfuser backend""
    )
    parser.add_argument(
        ""--dump-raw-metrics"",
        action=""store_true"",
        help=""dump raw timing metrics from speedup experiment"",
    )
    parser.add_argument(
        ""--log-operator-inputs"",
        action=""store_true"",
        default=False,
    )
    parser.add_argument(
        ""--channels-last"",
        action=""store_true"",
        default=False,
        help=""use channels last format"",
    )
    parser.add_argument(
        ""--batch-size"", ""--batch_size"", type=int, help=""batch size for benchmarking""
    )
    parser.add_argument(
        ""--iterations"", type=int, default=2, help=""how many iterations to run""
    )
    parser.add_argument(
        ""--batch-size-file"", type=str, help=""String to load batch size from""
    )
    parser.add_argument(""--cosine"", action=""store_true"", help=""use cosine similarity"")
    parser.add_argument(
        ""--freezing"", action=""store_true"", help=""turn on freezing"", default=False
    )
    parser.add_argument(
        ""--inductor-config"",
        ""-c"",
        action=""append"",
        help=""key=value in torch._inductor.config"",
    )
    parser.add_argument(
        ""--ci"", action=""store_true"", help=""Flag to tell that its a CI run""
    )
    parser.add_argument(
        ""--dashboard"", action=""store_true"", help=""Flag to tell that its a Dashboard run""
    )
    parser.add_argument(
        ""--skip-fp64-check"", action=""store_true"", help=""skip accuracy check using fp64""
    )
    parser.add_argument(
        ""--fast"", ""-f"", action=""store_true"", help=""skip slow benchmarks""
    )
    parser.add_argument(
        ""--only"",
        help=""""""Run just one model from torchbench. Or
        specify the path and class name of the model in format like:
        --only=path:<MODEL_FILE_PATH>,class:<CLASS_NAME>

        Due to the fact that dynamo changes current working directory,
        the path should be an absolute path.

        The class should have a method get_example_inputs to return the inputs
        for the model. An example looks like
        ```
        class LinearModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(10, 10)

            def forward(self, x):
                return self.linear(x)

            def get_example_inputs(self):
                return (torch.randn(2, 10),)
        ```
    """""",
    )
    parser.add_argument(
        ""--multiprocess"",
        action=""store_true"",
        help=""Create n processes based on the number of devices (distributed use case)."",
    )
    parser.add_argument(
        ""--ddp"",
        action=""store_true"",
        help=""Wraps model in DDP before running it, and uses dynamo DDPOptmizer (graph breaks) by default."",
    )
    parser.add_argument(
        ""--fsdp"",
        action=""store_true"",
        help=""""""Wraps model in FSDP before running it.
        Doesn't recursively wrap, mainly useful for checking dynamo UnspecNNModule compatibility
    """""",
    )
    parser.add_argument(
        ""--optimize-ddp-mode"",
        type=str,
        default=""ddp_optimizer"",
        help=""Specify the DDP optimization mode -- the value of torch._dynamo.config.optimize_ddp."",
    )
    parser.add_argument(
        ""--distributed-master-port"",
        default=""6789"",
        help=""Port to bind for for torch.distributed.  Use the default unless it's conflicting with another user"",
    )
    parser.add_argument(
        ""--dynamic-shapes"",
        action=""store_true"",
        help=""Runs a dynamic shapes version of the benchmark, if available."",
    )
    parser.add_argument(
        ""--propagate-real-tensors"",
        action=""store_true"",
        help=""Capture as much data dependent as you can by unsoundly propagating real tensors"",
    )
    parser.add_argument(
        ""--dynamic-batch-only"",
        action=""store_true"",
        help=""Only assume batch dimension is dynamic.  Implies --dynamic-shapes"",
    )
    parser.add_argument(
        ""--specialize-int"", action=""store_true"", help=""Run with specialize_int=True.""
    )
    parser.add_argument(
        ""--use-eval-mode"",
        action=""store_true"",
        help=""sets model.eval() to reduce randomness"",
    )
    parser.add_argument(
        ""--skip-accuracy-check"",
        action=""store_true"",
        help=""keeps running even when accuracy fails"",
    )
    parser.add_argument(
        ""--generate-aot-autograd-stats"",
        action=""store_true"",
        help=""Generates AOT Autograd stats like how mnay graphs are sent to AOT"",
    )
    parser.add_argument(
        ""--inductor-settings"",
        action=""store_true"",
        help=""Use same settings as --inductor for baseline comparisons"",
    )
    parser.add_argument(
        ""--suppress-errors"",
        action=""store_true"",
        help=""Suppress errors instead of raising them"",
    )
    parser.add_argument(
        ""--output"",
        help=""Overrides the output filename"",
    )
    parser.add_argument(
        ""--output-directory"",
        help=""Overrides the directory to place output files."",
    )
    parser.add_argument(
        ""--disable-output"",
        action=""store_true"",
        help=""Disable writing of output files, e.g., for warm-up runs"",
    )
    parser.add_argument(
        ""--baseline"",
        help=""Compare with a prior --output"",
    )
    parser.add_argument(
        ""--part"",
        default=None,
        help=""Specify the part of the model to run."",
    )
    parser.add_argument(
        ""--export-profiler-trace"",
        action=""store_true"",
        help=""exports trace of kineto profiler"",
    )
    parser.add_argument(
        ""--profiler-trace-name"",
        ""--profiler_trace_name"",
        help=""Overwrites exported trace name"",
    )
    parser.add_argument(
        ""--profile-details"", action=""store_true"", help=""More detailed profiler trace.""
    )
    parser.add_argument(
        ""--export-perfdoctor"",
        action=""store_true"",
        help=""Export Chrome trace to perf doctor. (internal only)"",
    )
    parser.add_argument(
        ""--diff-branch"",
        default=diff_branch_default,
        help=""delta current branch against given branch."",
    )
    parser.add_argument(
        ""--tag"", default=None, help=""Specify a tag to be included in csv files.""
    )
    parser.add_argument(
        ""--explain"",
        action=""store_true"",
        help=""print some graph/op statistics during the run, similar to .explain()"",
    )
    parser.add_argument(
        ""--stats"",
        action=""store_true"",
        help=""print graph counter stats"",
    )
    parser.add_argument(
        ""--use-warm-peak-memory"",
        ""--use_warm_peak_memory"",
        action=""store_true"",
        help=""Measure peak memory using a warm run to reduce autotuning noise"",
    )
    parser.add_argument(
        ""--print-memory"",
        action=""store_true"",
        help=""print extra memory statistics"",
    )
    parser.add_argument(
        ""--print-compilation-time"",
        action=""store_true"",
        help=""print compilation latency"",
    )
    parser.add_argument(
        ""--print-dataframe-summary"",
        action=""store_true"",
        help=""print dataframe result used for calculating accuracy"",
    )
    parser.add_argument(
        ""--disable-cudagraphs"",
        action=""store_true"",
        help=""Disables cudagraphs for Inductor"",
    )
    parser.add_argument(
        ""--disable-split-reductions"",
        action=""store_true"",
        help=""Disables split reductions for Inductor"",
    )
    parser.add_argument(
        ""--disable-persistent-reductions"",
        action=""store_true"",
        help=""Disables split reductions for Inductor"",
    )
    parser.add_argument(
        ""--disable-divisible-by-16"",
        action=""store_true"",
        help=""Disables divisible by 16 hint to Triton for Inductor"",
    )
    parser.add_argument(
        ""--inductor-compile-mode"",
        default=None,
        help=""torch.compile mode argument for inductor runs."",
    )
    parser.add_argument(
        ""--print-graph-breaks"",
        action=""store_true"",
        help=""Show a warning whenever graph break"",
    )
    parser.add_argument(
        ""--log-graph-breaks"",
        action=""store_true"",
        help=""log graph breaks in a file"",
    )
    parser.add_argument(
        ""--trace-on-xla"",
        action=""store_true"",
        help=""Whether to trace the model on XLA or on eager device"",
    )
    parser.add_argument(
        ""--xla-tolerance"",
        type=float,
        default=1e-2,
        help=""XLA needs a loose tolerance to pass the correctness check"",
    )
    parser.add_argument(
        ""--collect-outputs"",
        action=""store_true"",
        help=""""""Whether to collect outputs for training. Set this to true if we
        want to verify the numerical correctness of graidents. But that may
        cause time measurement not accurate"""""",
    )
    parser.add_argument(
        ""--enable-activation-checkpointing"",
        action=""store_true"",
        help=""Enables activation checkpointing for HF models"",
    )
    parser.add_argument(""--timing"", action=""store_true"", help=""Emits phase timing"")

    parser.add_argument(
        ""--progress"",
        action=""store_true"",
        help=""Print n/k models message between each model run."",
    )

    parser.add_argument(
        ""--timeout"",
        type=int,
        default=2000,
        help=""timeout (second) for benchmarking."",
    )

    parser.add_argument(
        ""--per_process_memory_fraction"",
        type=float,
        default=1,
        help=""Set per-process GPU memory fraction (limit) for reducing usable size and reproducing OOMs"",
    )

    parser.add_argument(
        ""--no-translation-validation"",
        action=""store_true"",
        help=""Disable translation validation for accuracy builds."",
    )

    parser.add_argument(
        ""--minify"",
        action=""store_true"",
        help=""Enable minification when failure is below tolerance. Save repro script for each model."",
    )

    parser.add_argument(
        ""--compiled-autograd"",
        action=""store_true"",
        help=""Enables compiled autograd on compiled benchmark"",
    )

    parser.add_argument(
        ""--profile_dynamo_cache_lookup"",
        ""--profile-dynamo-cache-lookup"",
        action=""store_true"",
        help=""profiles TorchDynamo cache lookup"",
    )

    parser.add_argument(
        ""--snapshot-memory"",
        ""--snapshot_memory"",
        action=""store_true"",
        help=""Enables Memory Snapshot tool for memory deep dives: https://pytorch.org/blog/understanding-gpu-memory-1/"",
    )

    parser.add_argument(
        ""--retain-output"",
        action=""store_true"",
        help=""Enables appending to the already existing output file if it exists \
            instead of deleting it and creating a new one."",
    )

    group_latency = parser.add_mutually_exclusive_group()
    group_latency.add_argument(
        ""--cold-start-latency"",
        ""--cold_start_latency"",
        action=""store_true"",
        help=""Use a fresh triton cachedir when running each model, to force cold-start compile."",
    )
    group_latency.add_argument(
        ""--warm-start-latency"",
        ""--warm_start_latency"",
        action=""store_true"",
        help=""Run model(s) twice and preseve caches in between to enable a 'warm start' on the 2nd run"",
    )

    group_fuser = parser.add_mutually_exclusive_group()
    # --nvfuser is now the default, keep the option to not break scripts
    group_fuser.add_argument(""--nvfuser"", action=""store_true"", help=argparse.SUPPRESS)
    group_fuser.add_argument(""--nnc"", action=""store_true"", help=""enable NNC for GPUs"")

    group_prec = parser.add_mutually_exclusive_group()
    group_prec.add_argument(""--float16"", action=""store_true"", help=""cast model to fp16"")
    group_prec.add_argument(
        ""--bfloat16"", action=""store_true"", help=""cast model to bf16""
    )
    group_prec.add_argument(""--float32"", action=""store_true"", help=""cast model to fp32"")
    group_prec.add_argument(
        ""--amp"", action=""store_true"", help=""use automatic mixed precision""
    )
    parser.add_argument(
        ""--amp-dtype"",
        choices=(""bfloat16"", ""float16""),
        help=""the data type used with automatic mixed precision"",
    )
    group_printout = parser.add_mutually_exclusive_group()
    group_printout.add_argument(
        ""--verbose"", ""-v"", action=""store_true"", help=""enable verbose debug printouts""
    )
    group_printout.add_argument(
        ""--quiet"", ""-q"", action=""store_true"", help=""suppress debug printouts""
    )

    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        ""--coverage"", action=""store_true"", help=""(default) "" + help(coverage_experiment)
    )
    group.add_argument(
        ""--overhead"", action=""store_true"", help=help(overhead_experiment)
    )
    group.add_argument(
        ""--speedup-dynamo-ts"",
        action=""store_true"",
        help=""TorchDynamo frontend with torchscript backend"",
    )
    group.add_argument(
        ""--speedup-fx2trt"", action=""store_true"", help=help(speedup_experiment_fx2trt)
    )
    group.add_argument(
        ""--speedup-fx2trt-fp16"",
        action=""store_true"",
        help=help(speedup_experiment_fx2trt),
    )
    group.add_argument(
        ""--print-fx"",
        action=""store_true"",
        help=""Print fx traces captured from model"",
    )
    group.add_argument(
        ""--print-aten-ops"",
        action=""store_true"",
        help=""Print traces of aten ops captured by AOT autograd"",
    )
    group.add_argument(
        ""--inductor"",
        action=""store_true"",
        help=""Measure speedup with TorchInductor"",
    )
    group.add_argument(
        ""--quantization"",
        choices=[
            ""int8dynamic"",
            ""int8weightonly"",
            ""int4weightonly"",
            ""autoquant"",
            ""noquant"",
        ],
        default=None,
        help=""Measure speedup of torchao quantization with TorchInductor baseline"",
    )
    group.add_argument(
        ""--export"",
        action=""store_true"",
        help=""Measure pass rate with export"",
    )
    group.add_argument(
        ""--export-aot-inductor"",
        action=""store_true"",
        help=""Measure pass rate with Export+AOTInductor"",
    )
    group.add_argument(
        ""--xla"", action=""store_true"", help=""Compare TorchXLA to eager PyTorch""
    )
    group.add_argument(
        ""--backend"",
        choices=torch._dynamo.list_backends(exclude_tags=None),
        help=""measure speedup with a given backend"",
    )
    group.add_argument(""--nothing"", action=""store_true"", help=help(null_experiment))
    group.add_argument(
        ""--log-conv-args"",
        action=""store_true"",
        help=""Dump convolution input/weight/bias's shape/stride/dtype and other options to json"",
    )
    group.add_argument(
        ""--recompile-profiler"",
        ""--recompile_profiler"",
        action=""store_true"",
        help=""Run the dynamo recompilation profiler on each model."",
    )
    group.add_argument(
        ""--find-batch-sizes"",
        action=""store_true"",
        help=""finds the largest batch size that could fit on GPUs"",
    )

    mode_group = parser.add_mutually_exclusive_group(required=True)
    mode_group.add_argument(
        ""--accuracy"",
        action=""store_true"",
        help=""Checks accuracy with small batch size and eval mode"",
    )
    mode_group.add_argument(
        ""--performance"", action=""store_true"", help=""Measures performance speedup""
    )
    mode_group.add_argument(
        ""--tolerance"",
        action=""store_true"",
        help=""extracts the tolerance for each model with small batch size and eval mode"",
    )
    run_mode_group = parser.add_mutually_exclusive_group(required=True)
    run_mode_group.add_argument(
        ""--training"",
        action=""store_true"",
        help=""Performs training"",
    )
    run_mode_group.add_argument(
        ""--inference"", action=""store_true"", help=""Performs inference""
    )
    return parser.parse_args(args)"
process_entry;"def process_entry(rank, runner, original_dir, args):
    args.rank = rank
    with maybe_init_distributed(
        args.init_distributed,
        rank=rank,
        world_size=args.world_size,
        port=args.distributed_master_port,
    ):
        return run(runner, args, original_dir)"
maybe_fresh_cache;"def maybe_fresh_cache(args):
    cache_dir_assigned = ""TORCHINDUCTOR_CACHE_DIR"" in os.environ
    if not cache_dir_assigned and (
        args.cold_start_latency or args.warm_start_latency or args.ci
    ):
        return fresh_inductor_cache()
    else:
        return contextlib.nullcontext()"
write_csv_when_exception;"def write_csv_when_exception(args, name: str, status: str, device=None):
    print(status)
    placeholder_batch_size = 0
    devices = [device] if device is not None else args.devices
    if args.accuracy:
        headers = [""dev"", ""name"", ""batch_size"", ""accuracy""]
        rows = [[device, name, placeholder_batch_size, status] for device in devices]
    elif args.performance:
        headers = [""dev"", ""name"", ""batch_size"", ""speedup"", ""abs_latency""]
        rows = [[device, name, placeholder_batch_size, 0.0, 0.0] for device in devices]
    else:
        headers = []
        rows = [[device, name, placeholder_batch_size, 0.0] for device in devices]

    for row in rows:
        write_outputs(output_filename, headers, row)"
log_operator_inputs;"def log_operator_inputs(model, example_inputs, model_iter_fn, name, args):
    mode = ""training"" if args.training else ""eval""
    output = os.path.join(os.path.dirname(args.output), f""{name}_{mode}.txt"")

    # TODO - add option for coalescing inputs over multiple runs
    if os.path.exists(output):
        print(f""Skipping {name}, {output} already exists"")
        return

    print(f""Running {name}"")
    try:
        from .microbenchmarks.operator_inp_utils import OperatorInputsMode
    except ImportError:
        from microbenchmarks.operator_inp_utils import OperatorInputsMode

    operator_mode = OperatorInputsMode()
    fake_tensor_mode = FakeTensorMode()

    with torch._subclasses.fake_tensor.FakeCopyMode(fake_tensor_mode):
        model_fake = copy.deepcopy(model)
        example_inputs_fake = copy.deepcopy(example_inputs)
    try:
        with fake_tensor_mode, operator_mode:
            model_iter_fn(model_fake, example_inputs_fake, collect_outputs=False)
    except Exception as e:
        print(f""{name} failed to run with fake tensors, trying real. Exception: {e}"")
        operator_mode = OperatorInputsMode()
        try:
            with operator_mode:
                model_iter_fn(model, example_inputs, collect_outputs=False)
        except Exception as e2:
            print(f""{name} failed to run with real. Exception: {e2}"")
            raise

    print(f""Writing output to {output}"")
    operator_mode.log_to_file(output)"
flatten;"def flatten(lst):
        for item in lst:
            if isinstance(item, list):
                yield from flatten(item)
            else:
                yield item"
maybe_list_to_set;"def maybe_list_to_set(obj):
        if isinstance(obj, dict):
            return {k: maybe_list_to_set(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return set(flatten(obj))
        return obj"
deterministic_torch_manual_seed;"def deterministic_torch_manual_seed(*args, **kwargs):
        from torch._C import default_generator

        seed = 1337
        if HAS_CUDA:
            import torch.cuda

            if not torch.cuda._is_in_bad_fork():
                torch.cuda.manual_seed_all(seed)
        if HAS_XPU:
            import torch.xpu

            if not torch.xpu._is_in_bad_fork():
                torch.xpu.manual_seed_all(seed)
        return default_generator.manual_seed(seed)"
reset_counters;"def reset_counters(cls):
        for k, v in torch._dynamo.utils.counters.items():
            cls.totals[k].update(v)
        ok = torch._dynamo.utils.counters[""frames""][""ok""]
        total = torch._dynamo.utils.counters[""frames""][""total""]
        torch._dynamo.utils.counters.clear()
        return ok, total"
print_summary;"def print_summary(cls):
        for k, v in sorted(cls.totals.items()):
            lines = ""\n  "".join(map(str, v.most_common(50)))
            print(f""STATS {k}\n  {lines}"")"
aot_summary;"def aot_summary(cls):
        return [cls.totals[""aot_autograd""][""total""], cls.totals[""aot_autograd""][""ok""]]"
maybe_mark_profile;"def maybe_mark_profile(*args, **kwargs):
        prof: torch.profiler.profile = kwargs.pop(""p"", None)
        mark = kwargs.pop(""mark"", None)
        if prof:
            with torch.profiler.record_function(mark):
                yield
        else:
            yield"
maybe_mark_profile;"def maybe_mark_profile(*args, **kwargs):
        prof: torch.profiler.profile = kwargs.pop(""p"", None)
        mark = kwargs.pop(""mark"", None)
        if prof:
            with torch.profiler.record_function(mark):
                yield
        else:
            yield"
trace_printer;"def trace_printer(gm, _):
        print(gm.graph)
        return gm"
load;"def load(cls, model, example_inputs, mode):
        import torch._inductor
        from torch.export.dynamic_shapes import _combine_args, _tree_map_with_path

        key = weakref.ref(model)
        if key not in cls.cache:
            # Register the output dataclass to pytree
            example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
            with torch.no_grad():
                # copy.deepcopy is required to prevent any surprising side-effect,
                # see https://github.com/pytorch/pytorch/issues/113029
                # This will cause memory stats to be overshadowed by this eager run.
                # To fix that, memory stats will be reset later.
                example_outputs = copy.deepcopy(model)(*example_args, **example_kwargs)

            if pytree.is_namedtuple_instance(example_outputs):
                typ = type(example_outputs)
                pytree._register_namedtuple(
                    typ,
                    serialized_type_name=f""{typ.__module__}.{typ.__name__}"",
                )
            else:
                _register_dataclass_output_as_pytree(example_outputs)

            combined_args = _combine_args(model, example_args, example_kwargs)
            dynamic_shapes = _tree_map_with_path(
                _produce_dynamic_shapes_for_export, combined_args
            )

            # delete example_outputs and reset memory stats here
            del example_outputs
            if current_device == ""cuda"":
                empty_gpu_cache(current_device)
                torch.cuda.reset_peak_memory_stats()
                pre_clone_memory_used = torch.cuda.max_memory_allocated()
            elif current_device == ""hpu"":
                torch.hpu.reset_peak_memory_stats()
                pre_clone_memory_used = torch.hpu.max_memory_allocated()

            # Clone the model pre-exporting.  This prevents scenarios observed in a few
            # models, where the forward pass modifies model state while exporting, and
            # FakeTensors are thus saved as model data members.  This invalidates model
            # reuse in eager mode, so it's safest to export a model clone.
            model_clone = copy.deepcopy(model)

            # Since CPU doesn't monitor max memory allocation, anything measuring peak
            # memory will miss our transient model clone on CPU anyway.
            #
            # The justification for tracking this value (in order to remove it from the
            # AOTInductor memory measurements) is that normal usage of AOTInductor would
            # not clone the model, since the eager model would be unused post-export.
            clone_memory_used = 0.0
            if current_device == ""cuda"":
                clone_memory_used = (
                    torch.cuda.max_memory_allocated() - pre_clone_memory_used
                ) / 1e9
            elif current_device == ""hpu"":
                clone_memory_used = (
                    torch.hpu.max_memory_allocated() - pre_clone_memory_used
                ) / 1e9

            inductor_configs = {}
            if mode == ""max-autotune"":
                inductor_configs[""max_autotune""] = True
            ep = torch.export.export(
                model_clone,
                example_args,
                example_kwargs,
                dynamic_shapes=dynamic_shapes,
                strict=False,
            )
            with torch.no_grad():
                package_path = torch._inductor.aoti_compile_and_package(
                    ep, inductor_configs=inductor_configs
                )  # type: ignore[arg-type]

            cls.cache[key] = (
                torch._inductor.aoti_load_package(package_path),
                clone_memory_used,
            )

        return cls.cache[key][0]"
get_excess_memory;"def get_excess_memory(cls, model) -> float:
        return cls.cache.get(weakref.ref(model), (None, 0.0))[1]"
opt_export;"def opt_export(_, example_inputs):
        example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
        return ep.module()(*example_args, **example_kwargs)"
opt_aot_inductor;"def opt_aot_inductor(_, example_inputs, collect_outputs=False):
        example_args, example_kwargs = _normalize_bench_inputs(example_inputs)
        return optimized(*example_args, **example_kwargs)"
skip_models;"def skip_models(self):
        return set()"
skip_models_for_cuda;"def skip_models_for_cuda(self):
        return set()"
skip_models_for_cpu;"def skip_models_for_cpu(self):
        return set()"
skip_models_for_freezing_cpu;"def skip_models_for_freezing_cpu(self):
        return set()"
skip_models_for_freezing_cuda;"def skip_models_for_freezing_cuda(self):
        return set()"
slow_models;"def slow_models(self):
        return set()"
very_slow_models;"def very_slow_models(self):
        return set()"
non_deterministic_models;"def non_deterministic_models(self):
        return set()"
fp32_only_models;"def fp32_only_models(self):
        return set()"
force_amp_for_fp16_bf16_models;"def force_amp_for_fp16_bf16_models(self):
        return set()"
force_fp16_for_bf16_models;"def force_fp16_for_bf16_models(self):
        return set()"
skip_not_suitable_for_training_models;"def skip_not_suitable_for_training_models(self):
        return set()"
failing_torchinductor_models;"def failing_torchinductor_models(self):
        return set()"
failing_fx2trt_models;"def failing_fx2trt_models(self):
        return set()"
skip_accuracy_checks_large_models_dashboard;"def skip_accuracy_checks_large_models_dashboard(self):
        return set()"
skip_accuracy_check_as_eager_non_deterministic;"def skip_accuracy_check_as_eager_non_deterministic(self):
        return set()"
skip_multiprocess_models;"def skip_multiprocess_models(self):
        return set()"
skip_models_due_to_control_flow;"def skip_models_due_to_control_flow(self):
        return set()"
skip_models_due_to_export_not_supported;"def skip_models_due_to_export_not_supported(self):
        return set()"
disable_cudagraph_models;"def disable_cudagraph_models(self):
        return set()"
guard_on_nn_module_models;"def guard_on_nn_module_models(self):
        return set()"
inline_inbuilt_nn_modules_models;"def inline_inbuilt_nn_modules_models(self):
        return set()"
get_tolerance_and_cosine_flag;"def get_tolerance_and_cosine_flag(self, is_training, current_device, name):
        raise NotImplementedError"
use_larger_multiplier_for_smaller_tensor;"def use_larger_multiplier_for_smaller_tensor(self, name):
        return False"
deepcopy_model;"def deepcopy_model(self, model):
        return copy.deepcopy(model)"
cast_based_on_args;"def cast_based_on_args(self, model, example_inputs):
        if self.args.float32 or self.args.only in self.fp32_only_models:
            if not self.args.float32:
                log.warning(""Model %s supports float32 only"", self.args.only)
            model, example_inputs = cast_to_fp32(model, example_inputs)
        elif self.args.float16:
            if self.args.only in self.force_amp_for_fp16_bf16_models:
                log.warning(
                    ""Model %s does not support float16, running with amp instead"",
                    self.args.only,
                )
                self.args.amp = True
                self.setup_amp()
            else:
                model, example_inputs = cast_to_fp16(model, example_inputs)
        elif self.args.bfloat16:
            if self.args.only in self.force_amp_for_fp16_bf16_models:
                log.warning(
                    ""Model %s does not support bfloat16, running with amp instead"",
                    self.args.only,
                )
                self.args.amp = True
                self.setup_amp()
            elif self.args.only in self.force_fp16_for_bf16_models:
                log.warning(
                    ""Model %s does not support bfloat16, running with float16 instead"",
                    self.args.only,
                )
                model, example_inputs = cast_to_fp16(model, example_inputs)
            else:
                model, example_inputs = cast_to_bf16(model, example_inputs)

        return model, example_inputs"
validate_model;"def validate_model(self, model, example_inputs):
        """"""
        Runs the eager model with example inputs to ensure that eager passes.
        """"""
        model = self.deepcopy_model(model)
        example_inputs = clone_inputs(example_inputs)
        model, example_inputs = self.cast_based_on_args(model, example_inputs)
        try:
            self.model_iter_fn(model, example_inputs)
        except Exception as e:
            raise RuntimeError(""Eager run failed"") from e"
maybe_cast;"def maybe_cast(self, model, example_inputs):
        model, example_inputs = self.cast_based_on_args(model, example_inputs)
        return model, example_inputs"
decay_batch_exp;"def decay_batch_exp(self, batch_size, factor=0.5, divisor=2):
        out_batch_size = batch_size * factor
        if out_batch_size > divisor:
            out_batch_size = (out_batch_size + 1) // divisor * divisor
        else:
            out_batch_size = batch_size - 1
        return max(0, int(out_batch_size))"
batch_size_finder;"def batch_size_finder(self, device, model_name, initial_batch_size=1024):
        batch_size = initial_batch_size
        while batch_size >= 1:
            empty_gpu_cache(current_device)
            try:
                device, name, model, example_inputs, _ = self.load_model(
                    device,
                    model_name,
                    batch_size,
                )
                self.model_iter_fn(model, example_inputs)
                return batch_size
            except RuntimeError as e:
                error_str = str(e)
                if ""channels_last"" in error_str:
                    break
            batch_size = self.decay_batch_exp(batch_size)
        return 1"
run_n_iterations;"def run_n_iterations(self, mod, inputs, model_iter_fn):
        n = self.args.iterations
        for _ in range(n - 1):
            model_iter_fn(mod, inputs, collect_outputs=False)
        return model_iter_fn(mod, inputs, collect_outputs=True)"
optimizer_zero_grad;"def optimizer_zero_grad(self, mod):
        if self.optimizer is not None:
            self.optimizer.zero_grad(True)
        else:
            mod.zero_grad(True)"
optimizer_step;"def optimizer_step(self):
        if self.optimizer is not None:
            self.optimizer.step()"
get_benchmark_indices;"def get_benchmark_indices(self, length):
        start = self._args.partition_id * (length // self._args.total_partitions)
        end = (
            (self._args.partition_id + 1) * (length // self._args.total_partitions)
            if self._args.partition_id < self._args.total_partitions - 1
            else length
        )
        return start, end"
get_fsdp_auto_wrap_policy;"def get_fsdp_auto_wrap_policy(self, model_name: str):
        from diffusers.models.transformer_2d import Transformer2DModel
        from torchbenchmark.models.nanogpt.model import Block
        from transformers.models.llama.modeling_llama import LlamaDecoderLayer
        from transformers.models.t5.modeling_t5 import T5Block
        from transformers.models.whisper.modeling_whisper import WhisperEncoderLayer

        from torch.distributed.fsdp.wrap import (
            ModuleWrapPolicy,
            size_based_auto_wrap_policy,
        )

        # handcrafted wrap policy
        MODEL_FSDP_WRAP = {
            ""stable_diffusion_unet"": (Transformer2DModel,),
            ""hf_T5"": (T5Block,),
            ""hf_T5_base"": (T5Block,),
            ""hf_T5_large"": (T5Block,),
            ""hf_Whisper"": (WhisperEncoderLayer,),
            ""llama_v2_7b_16h"": (LlamaDecoderLayer,),
            ""nanogpt"": (Block,),
        }

        if model_name not in MODEL_FSDP_WRAP:
            # default to using wrap policy based on module size
            return functools.partial(
                size_based_auto_wrap_policy, recurse=True, min_num_params=int(1e5)
            )

        return ModuleWrapPolicy(MODEL_FSDP_WRAP[model_name])"
deepcopy_and_maybe_parallelize;"def deepcopy_and_maybe_parallelize(self, model):
        model = self.deepcopy_model(model)
        if self.args.ddp:
            assert torch.distributed.is_available(), (
                ""Can't use DDP without a distributed enabled build""
            )
            from torch.nn.parallel import DistributedDataParallel as DDP

            model = DDP(model, find_unused_parameters=True)
        elif self.args.fsdp:
            assert torch.distributed.is_available(), (
                ""Can't use FSDP without a distributed enabled build""
            )
            from torch.distributed.fsdp import (
                FullyShardedDataParallel as FSDP,
                MixedPrecision,
            )

            if self.args.float16:
                dtype = torch.float16
            elif self.args.bfloat16:
                dtype = torch.bfloat16
            else:
                dtype = torch.float32

            mp_policy = MixedPrecision(
                param_dtype=dtype,
                # Gradient communication precision.
                reduce_dtype=dtype,
                # Buffer precision.
                buffer_dtype=dtype,
            )

            model = FSDP(
                model,
                use_orig_params=True,
                device_id=torch.cuda.current_device()
                if self.args.devices[-1] == ""cuda""
                else None,
                mixed_precision=mp_policy,
                limit_all_gathers=True,
                auto_wrap_policy=self.get_fsdp_auto_wrap_policy(self.args.only),
            )
        return model"
check_accuracy;"def check_accuracy(
        self, name, model, example_inputs, optimize_ctx, experiment, tag
    ):
        """"""
        Checks accuracy.
        1) Collect the outputs with fp64 datatype. This is useful for error checking.
        2) Checks if eager itself has variations.
        """"""
        start_stats = get_dynamo_stats()

        def record_status(accuracy_status, dynamo_start_stats):
            """"""
            Records the status in the csv file
            """"""
            if current_name in self.non_deterministic_models:
                if accuracy_status in (
                    ""pass"",
                    ""eager_two_runs_differ"",
                    ""fail_accuracy"",
                ):
                    accuracy_status = ""pass""

            headers = [""dev"", ""name"", ""batch_size"", ""accuracy""]
            fields = [current_device, current_name, current_batch_size, accuracy_status]

            if tag is not None:
                headers.insert(3, ""tag"")
                fields.insert(3, tag)

            o_headers = list(headers)
            o_fields = list(fields)

            dynamo_stats = get_dynamo_stats()
            dynamo_stats.subtract(dynamo_start_stats)
            for k, v in dynamo_stats.items():
                headers.append(k)
                fields.append(v)

            total_wall_time = output_signpost(
                dict(zip(o_headers, o_fields)),
                self.args,
                self.suite_name,
            )
            headers.append(""compilation_latency"")
            fields.append(total_wall_time)
            write_outputs(output_filename, headers, fields)

            if self.args.print_compilation_time:
                print(f""Compilation time (from dynamo_timed): {total_wall_time}"")

            return accuracy_status

        if name in self.skip_accuracy_checks_large_models_dashboard:
            return record_status(""pass_due_to_skip"", dynamo_start_stats=start_stats)

        # Skip all accuracy check for the torchao backend
        if self.args.backend == ""torchao"":
            return record_status(""pass_due_to_skip"", dynamo_start_stats=start_stats)

        with self.pick_grad(name, self.args.training):
            # Collect the fp64 reference outputs to be used later for accuracy checking.
            fp64_outputs = None
            model_fp64 = None
            inputs_fp64 = None
            try:
                model_fp64, inputs_fp64 = cast_to_fp64(
                    self.deepcopy_and_maybe_parallelize(model),
                    clone_inputs(example_inputs),
                )
                self.init_optimizer(name, current_device, model_fp64.parameters())
                fp64_outputs = self.run_n_iterations(
                    model_fp64, inputs_fp64, self.model_iter_fn
                )
                fp64_outputs = tree_map(
                    lambda x: x.to(torch.float64)
                    if isinstance(x, torch.Tensor) and x.is_floating_point()
                    else x,
                    fp64_outputs,
                )
            except Exception:
                log.warning(
                    ""fp64 golden ref were not generated for %s. Setting accuracy check to cosine"",
                    name,
                )
                self.args.cosine = True
                fp64_outputs = None
            finally:
                del model_fp64, inputs_fp64
                empty_gpu_cache(current_device)

            tolerance, cos_similarity = self.get_tolerance_and_cosine_flag(
                self.args.training, current_device, name
            )

            # Cast the model to float16/float32 as necessary
            model, example_inputs = self.maybe_cast(model, example_inputs)
            accuracy_status = ""pass""

            # Get results of native pytorch
            reset_rng_state()
            model_copy = None
            try:
                model_copy = self.deepcopy_and_maybe_parallelize(model)
                self.init_optimizer(name, current_device, model_copy.parameters())
                correct_result = self.run_n_iterations(
                    model_copy, clone_inputs(example_inputs), self.model_iter_fn
                )
            except Exception as e:
                accuracy_status = (
                    ""eager_1st_run_OOM""
                    if isinstance(e, torch.cuda.OutOfMemoryError)
                    else ""eager_1st_run_fail""
                )
                log.exception("""")
                return record_status(accuracy_status, dynamo_start_stats=start_stats)
            finally:
                del model_copy
                empty_gpu_cache(current_device)

            # Rerun native pytorch
            reset_rng_state()
            model_copy = None
            try:
                model_copy = self.deepcopy_and_maybe_parallelize(model)
                self.init_optimizer(name, current_device, model_copy.parameters())
                correct_rerun_result = self.run_n_iterations(
                    model_copy, clone_inputs(example_inputs), self.model_iter_fn
                )
            except Exception as e:
                accuracy_status = (
                    ""eager_2nd_run_OOM""
                    if isinstance(e, torch.cuda.OutOfMemoryError)
                    else ""eager_2nd_run_fail""
                )
                log.exception("""")
                return record_status(accuracy_status, dynamo_start_stats=start_stats)
            finally:
                del model_copy
                empty_gpu_cache(current_device)

            # Two eager runs should have exactly same result
            is_same = True
            try:
                if (
                    name not in self.skip_accuracy_check_as_eager_non_deterministic
                    and not same(
                        correct_result,
                        correct_rerun_result,
                        fp64_ref=None,
                        cos_similarity=False,
                        tol=0,
                        equal_nan=self.equal_nan,
                        use_larger_multiplier_for_smaller_tensor=self.use_larger_multiplier_for_smaller_tensor(
                            name
                        ),
                    )
                ):
                    is_same = False
            except Exception:
                # Sometimes torch.allclose may throw RuntimeError
                is_same = False

            if not is_same:
                accuracy_status = ""eager_two_runs_differ""
                return record_status(accuracy_status, dynamo_start_stats=start_stats)

            correct_rerun_result = None

            # Run with Dynamo
            reset_rng_state()
            torch._dynamo.reset()
            torch._dynamo.utils.counters.clear()
            model_copy = None
            try:
                model_copy = self.deepcopy_and_maybe_parallelize(model)
                self.init_optimizer(name, current_device, model_copy.parameters())
                if self.args.export or self.args.export_aot_inductor:
                    # apply export on module directly
                    # no need for n iterations
                    # the logic should be the same to self.model_iter_fn (forward_pass)
                    with self.autocast(**self.autocast_arg):
                        optimized_model_iter_fn = optimize_ctx(
                            model_copy, example_inputs
                        )
                        new_result = optimized_model_iter_fn(model_copy, example_inputs)
                else:
                    optimized_model_iter_fn = optimize_ctx(self.model_iter_fn)
                    new_result = self.run_n_iterations(
                        model_copy, example_inputs, optimized_model_iter_fn
                    )
            except Exception as e:
                log.exception("""")
                print(
                    ""TorchDynamo optimized model failed to run because of following error""
                )
                accuracy_status = (
                    ""OOM""
                    if isinstance(e, torch.cuda.OutOfMemoryError)
                    else ""fail_to_run""
                )
                return record_status(accuracy_status, dynamo_start_stats=start_stats)
            finally:
                del model_copy

            if name in self.skip_accuracy_check_as_eager_non_deterministic:
                return record_status(""pass_due_to_skip"", dynamo_start_stats=start_stats)

            force_max_multiplier = False
            if (
                self.args.freezing
                and self.args.bfloat16
                and torch._dynamo.utils.counters[""inductor""][""binary_folding_conv""] > 0
            ):
                force_max_multiplier = True

            try:
                if self.args.training and self.args.amp:
                    if process_fn := self.get_output_amp_train_process_func.get(
                        name, None
                    ):
                        correct_result = process_fn(correct_result)
                        new_result = process_fn(new_result)
                        fp64_outputs = process_fn(fp64_outputs)

                if not same(
                    correct_result,
                    new_result,
                    fp64_outputs,
                    equal_nan=self.equal_nan,
                    use_larger_multiplier_for_smaller_tensor=self.use_larger_multiplier_for_smaller_tensor(
                        name
                    ),
                    cos_similarity=cos_similarity,
                    tol=tolerance,
                    force_max_multiplier=force_max_multiplier,
                ):
                    is_same = False
            except Exception:
                # Sometimes torch.allclose may throw RuntimeError
                is_same = False

            if not is_same:
                if self.args.skip_accuracy_check:
                    accuracy_status = ""pass_due_to_skip""
                else:
                    accuracy_status = ""fail_accuracy""
                return record_status(accuracy_status, dynamo_start_stats=start_stats)

        return record_status(accuracy_status, dynamo_start_stats=start_stats)"
check_tolerance;"def check_tolerance(
        self, name, model, example_inputs, optimize_ctx, base_device=""cpu""
    ):
        """"""
        Checks tolerance based on https://pytorch.org/docs/stable/generated/torch.allclose.html.
        """"""
        tolerance_status = ""pass""
        if name in self.skip_accuracy_checks_large_models_dashboard:
            tolerance_status = ""pass_due_to_skip""
            return tolerance_status
        # Cast the model to float16/float32 as necessary
        model, example_inputs = self.maybe_cast(model, example_inputs)

        with self.pick_grad(name, self.args.training):
            # Get results of native pytorch
            reset_rng_state()
            model_copy = copy.deepcopy(model)
            model_copy = model_copy.to(base_device)
            example_inputs_copy = copy.deepcopy(example_inputs)
            example_inputs_copy = tree_map(
                lambda x: x.to(base_device), example_inputs_copy
            )
            self.init_optimizer(name, base_device, model_copy.parameters())
            correct_result = self.run_n_iterations(
                model_copy, example_inputs_copy, self.model_iter_fn
            )

            # Run with Dynamo
            # Sometime CI fails with random triton compilation failure which will be skipped for now
            # TODO: revisit this after switching to new Triton runtime
            reset_rng_state()
            torch._dynamo.reset()
            try:
                self.init_optimizer(name, current_device, model.parameters())
                optimized_model_iter_fn = optimize_ctx(self.model_iter_fn)
                new_result = self.run_n_iterations(
                    model_copy, example_inputs, optimized_model_iter_fn
                )
            except Exception:
                log.exception("""")
                print(
                    ""TorchDynamo optimized model failed to run because of following error""
                )
                return ""fail_to_run""

            def dump_max_mean_values(tol, ref, res):
                if isinstance(ref, (list, tuple, torch.nn.ParameterList, torch.Size)):
                    for refi, resi in zip(ref, res):
                        dump_max_mean_values(tol, refi, resi)
                elif isinstance(ref, dict):
                    for k in ref.keys():
                        dump_max_mean_values(tol, ref[k], res[k])
                elif isinstance(ref, torch.Tensor):
                    res = res.to(base_device)
                    t = torch.abs(ref - res) / (1 + torch.abs(ref))
                    tol.append(t.flatten().to(torch.float32))
                return tol

            tol = []
            dump_max_mean_values(tol, correct_result, new_result)
            tol = torch.cat(tol)
            tol = torch.tensor(tol)
            max = torch.max(tol)
            mean = torch.mean(tol)
            div = torch.std(tol)
            headers = [""dev"", ""name"", ""batch_size"", ""max"", ""mean"", ""std""]
            fields = [
                current_device,
                current_name,
                current_batch_size,
                max.item(),
                mean.item(),
                div.item(),
            ]
            write_outputs(output_filename, headers, fields)
        return tolerance_status"
run_performance_test_non_alternate;"def run_performance_test_non_alternate(
        self, name, model, example_inputs, optimize_ctx, experiment, tag=None
    ):
        ""Run performance test in non-alternately.""
        assert experiment.func is latency_experiment, (
            ""Must run with latency_experiment.""
        )

        def warmup(fn, model, example_inputs, mode, niters=10):
            peak_mem = 0
            start_stats = get_dynamo_stats()
            try:
                if current_device == ""cuda"":
                    torch.cuda.reset_peak_memory_stats()
                    empty_gpu_cache(current_device)
                elif current_device == ""hpu"":
                    torch.hpu.reset_peak_memory_stats()
                t0 = time.perf_counter()
                for _ in range(niters):
                    fn(model, example_inputs)
                t1 = time.perf_counter()
                latency = t1 - t0
                if current_device == ""cuda"":
                    peak_mem = get_peak_memory()
                elif current_device == ""hpu"":
                    peak_mem = torch.hpu.max_memory_allocated() / 10**9
                elif current_device == ""cpu"":
                    total = psutil.virtual_memory().total
                    percentage = psutil.Process(os.getpid()).memory_percent()
                    peak_mem = percentage * total / 10**9
            except Exception:
                log.exception(""Backend %s failed in warmup()"", mode)
                write_csv_when_exception(
                    self.args, current_name, ""warmup_failed"", current_device
                )
                output_signpost({}, self.args, self.suite_name, error=""warmup_failed"")
                return sys.exit(-1)
            dynamo_stats = get_dynamo_stats()
            dynamo_stats.subtract(start_stats)
            return latency, peak_mem, dynamo_stats

        # Cast the model to float16/float32 as necessary
        model, example_inputs = self.maybe_cast(model, example_inputs)

        # Use distributed wrapping as necessary
        model = self.deepcopy_and_maybe_parallelize(model)

        self.init_optimizer(name, current_device, model.parameters())

        # The self.autocast context is needed for the model we export with aot_compile,
        # similar to what we do in the check_accuracy function
        ctx = (
            self.autocast(**self.autocast_arg)
            if self.args.export_aot_inductor
            else contextlib.nullcontext()
        )

        with self.pick_grad(name, self.args.training), ctx:
            ok, total = Stats.reset_counters()
            experiment_kwargs = {}
            if tag is not None:
                experiment_kwargs[""tag""] = tag
            results = []

            with maybe_snapshot_memory(
                self.args.snapshot_memory, f""eager_{self.args.only}""
            ):
                eager_latency, eager_peak_mem, _ = warmup(
                    self.model_iter_fn, model, example_inputs, ""eager""
                )
                if self.args.use_warm_peak_memory:
                    _, eager_peak_mem, _ = warmup(
                        self.model_iter_fn, model, example_inputs, ""eager"", niters=1
                    )

            baseline_timings = experiment(
                model, example_inputs, mark=""expected"", **experiment_kwargs
            )

            if self.args.export_aot_inductor:
                optimized_model_iter_fn = optimize_ctx
            else:
                optimized_model_iter_fn = optimize_ctx(self.model_iter_fn)

            with maybe_snapshot_memory(
                self.args.snapshot_memory, f""compiled_{self.args.only}""
            ):
                dynamo_latency, dynamo_peak_mem, dynamo_stats = warmup(
                    optimized_model_iter_fn, model, example_inputs, ""dynamo""
                )
                if self.args.use_warm_peak_memory:
                    _, dynamo_peak_mem, _ = warmup(
                        optimized_model_iter_fn,
                        model,
                        example_inputs,
                        ""dynamo"",
                        niters=1,
                    )
                # If we use warm peak memory, the AOT model loading transient memory
                # won't be present on the warm measurement.  We only have to account for
                # it when using cold memory.
                elif self.args.export_aot_inductor:
                    dynamo_peak_mem -= AOTInductorModelCache.get_excess_memory(model)

            if self.args.profile_dynamo_cache_lookup:
                with torch.profiler.profile(
                    activities=[torch.profiler.ProfilerActivity.CPU]
                ) as prof:
                    warmup(optimized_model_iter_fn, model, example_inputs, ""dynamo"")

                events = list(
                    filter(
                        lambda event: ""TorchDynamo Cache Lookup"" in event.key,
                        prof.key_averages(),
                    )
                )
                dynamo_cache_lookup_latency = events[0].self_cpu_time_total

            compilation_time = dynamo_latency - eager_latency
            compression_ratio = (
                eager_peak_mem / dynamo_peak_mem if dynamo_peak_mem else 0.0
            )
            if self.args.print_memory:
                print(
                    f""memory: eager: {eager_peak_mem:.2f} GB, ""
                    f""dynamo: {dynamo_peak_mem:.2f} GB, ""
                    f""ratio: {compression_ratio:.2f}""
                )

            if self.args.print_compilation_time:
                print(f""Compilation time: {compilation_time:.2f}"")

            if experiment.func is speedup_experiment:
                experiment_kwargs[""compilation_latency""] = compilation_time
                experiment_kwargs[""compression_ratio""] = compression_ratio
                experiment_kwargs[""eager_peak_mem""] = eager_peak_mem
                experiment_kwargs[""dynamo_peak_mem""] = dynamo_peak_mem
                experiment_kwargs[""dynamo_stats""] = dynamo_stats
                if self.args.profile_dynamo_cache_lookup:
                    experiment_kwargs[""cache_lookup_latency""] = (
                        dynamo_cache_lookup_latency
                    )

            backend_timings = experiment(
                model, example_inputs, mark=""expected"", **experiment_kwargs
            )
            timings = np.stack((baseline_timings, backend_timings), axis=1)
            result_summary = latency_experiment_summary(
                self.suite_name, self.args, model, timings, **experiment_kwargs
            )
            if not hasattr(model, name):
                model.name = name
            results.append(result_summary)
            return "" "".join(map(str, results))"
run_performance_test;"def run_performance_test(
        self, name, model, example_inputs, optimize_ctx, experiment, tag=None
    ):
        if self.args.xla:
            with self.pick_grad(name, self.args.training):
                return experiment(*self.maybe_cast(model, example_inputs))

        def warmup(fn, model, example_inputs, mode, niters=5):
            peak_mem = 0
            start_stats = get_dynamo_stats()
            try:
                if current_device == ""cuda"":
                    torch.cuda.reset_peak_memory_stats()
                    empty_gpu_cache(current_device)
                elif current_device == ""hpu"":
                    torch.hpu.reset_peak_memory_stats()
                t0 = time.perf_counter()
                for _ in range(niters):
                    fn(model, example_inputs)
                t1 = time.perf_counter()
                latency = t1 - t0
                if current_device == ""cuda"":
                    peak_mem = get_peak_memory()
                elif current_device == ""hpu"":
                    peak_mem = torch.hpu.max_memory_allocated() / 10**9
                elif current_device == ""cpu"":
                    total = psutil.virtual_memory().total
                    percentage = psutil.Process(os.getpid()).memory_percent()
                    peak_mem = percentage * total / 10**9
            except Exception:
                log.exception(""Backend %s failed in warmup()"", mode)
                write_csv_when_exception(
                    self.args, current_name, ""warmup_failed"", current_device
                )
                output_signpost({}, self.args, self.suite_name, error=""warmup_failed"")
                return sys.exit(-1)
            dynamo_stats = get_dynamo_stats()
            dynamo_stats.subtract(start_stats)
            return latency, peak_mem, dynamo_stats

        # Cast the model to float16/float32 as necessary
        model, example_inputs = self.maybe_cast(model, example_inputs)

        # Use distributed wrapping as necessary
        model = self.deepcopy_and_maybe_parallelize(model)

        self.init_optimizer(name, current_device, model.parameters())

        # The self.autocast context is needed for the model we export with aot_compile,
        # similar to what we do in the check_accuracy function
        ctx = (
            self.autocast(**self.autocast_arg)
            if self.args.export_aot_inductor
            else contextlib.nullcontext()
        )

        with self.pick_grad(name, self.args.training), ctx:
            ok, total = Stats.reset_counters()
            experiment_kwargs = {}
            if tag is not None:
                experiment_kwargs[""tag""] = tag
            results = []
            with maybe_snapshot_memory(
                self.args.snapshot_memory, f""eager_{self.args.only}""
            ):
                eager_latency, eager_peak_mem, _ = warmup(
                    self.model_iter_fn, copy.deepcopy(model), example_inputs, ""eager""
                )
                if self.args.use_warm_peak_memory:
                    _, eager_peak_mem, _ = warmup(
                        self.model_iter_fn,
                        copy.deepcopy(model),
                        example_inputs,
                        ""eager"",
                        niters=1,
                    )

            if self.args.export_aot_inductor:
                optimized_model_iter_fn = optimize_ctx
            else:
                optimized_model_iter_fn = optimize_ctx(self.model_iter_fn)

            with maybe_snapshot_memory(
                self.args.snapshot_memory, f""compiled_{self.args.only}""
            ):
                dynamo_latency, dynamo_peak_mem, dynamo_stats = warmup(
                    optimized_model_iter_fn, model, example_inputs, ""dynamo""
                )
                if self.args.use_warm_peak_memory:
                    _, dynamo_peak_mem, _ = warmup(
                        optimized_model_iter_fn,
                        model,
                        example_inputs,
                        ""dynamo"",
                        niters=1,
                    )
                # If we use warm peak memory, the AOT model loading transient memory
                # won't be present on the warm measurement.  We only have to account for
                # it when using cold memory.
                elif self.args.export_aot_inductor:
                    dynamo_peak_mem -= AOTInductorModelCache.get_excess_memory(model)

            if self.args.profile_dynamo_cache_lookup:
                with torch.profiler.profile(
                    activities=[torch.profiler.ProfilerActivity.CPU]
                ) as prof:
                    warmup(optimized_model_iter_fn, model, example_inputs, ""dynamo"")

                events = list(
                    filter(
                        lambda event: ""TorchDynamo Cache Lookup"" in event.key,
                        prof.key_averages(),
                    )
                )
                dynamo_cache_lookup_latency = events[0].self_cpu_time_total

            compilation_time = dynamo_latency - eager_latency
            compression_ratio = (
                eager_peak_mem / dynamo_peak_mem if dynamo_peak_mem else 0.0
            )
            if self.args.print_memory:
                print(
                    f""memory: eager: {eager_peak_mem:.2f} GB, ""
                    f""dynamo: {dynamo_peak_mem:.2f} GB, ""
                    f""ratio: {compression_ratio:.2f}""
                )

            if self.args.print_compilation_time:
                print(f""Compilation time: {compilation_time:.2f}"")

            if experiment.func is speedup_experiment:
                experiment_kwargs[""compilation_latency""] = compilation_time
                experiment_kwargs[""compression_ratio""] = compression_ratio
                experiment_kwargs[""eager_peak_mem""] = eager_peak_mem
                experiment_kwargs[""dynamo_peak_mem""] = dynamo_peak_mem
                experiment_kwargs[""dynamo_stats""] = dynamo_stats
                if self.args.profile_dynamo_cache_lookup:
                    experiment_kwargs[""cache_lookup_latency""] = (
                        dynamo_cache_lookup_latency
                    )

            if experiment.func is coverage_experiment:
                ok, total = Stats.reset_counters()
                results = []
                # run with torch._dynamo few times to populate the cache
                for _ in range(3):
                    optimized_model_iter_fn(model, example_inputs)
                _, frames_second_pass = Stats.reset_counters()  # should be 0
                if frames_second_pass > 0:
                    optimized_model_iter_fn(model, example_inputs)
                    _, frames_third_pass = Stats.reset_counters()  # should be 0
                else:
                    frames_third_pass = 0

                results.append(
                    f""{ok:3}/{total:3} +{frames_third_pass} frames {compilation_time:3.0f}s""
                )

            if not hasattr(model, name):
                model.name = name
            results.append(experiment(model, example_inputs, **experiment_kwargs))
            return "" "".join(map(str, results))"
minify_model;"def minify_model(
        self,
        name,
        model,
        example_inputs,
        optimize_ctx,
        experiment,
        tag,
    ):
        log.info(""Minifying %s..."", name)
        os.environ[""TORCH_COMPILE_DEBUG""] = ""1""
        os.environ[""TORCHDYNAMO_REPRO_AFTER""] = ""dynamo""
        os.environ[""TORCHDYNAMO_REPRO_LEVEL""] = ""4""

        self.check_accuracy(name, model, example_inputs, optimize_ctx, experiment, tag)

        if self.args.output_directory:
            repro_dir = self.args.output_directory
        else:
            repro_dir = torch._dynamo.config.base_dir

        try:
            shutil.move(""repro.py"", f""{repro_dir}/{name}_repro.py"")
        except OSError:
            log.error(""Could not find repro script for model %s"", name)
        else:
            log.info(
                ""Repro script for model %s with minified graph saved to %s"",
                name,
                repro_dir,
            )"
maybe_preserve_compile_debug;"def maybe_preserve_compile_debug(self, name, status):
        if (
            name in CI_PRESERVE_COMPILE_DEBUG
            and status in CI_PRESERVE_COMPILE_DEBUG[name]
        ):
            src_dir = torch._dynamo.utils.get_debug_dir()
            if os.path.isdir(src_dir):
                dbg_dir = os.path.join(
                    os.getcwd(), ""test"", ""debug"", ""torch_compile_debug""
                )
                dst_dir = os.path.join(dbg_dir, os.path.basename(src_dir))
                try:
                    os.makedirs(dbg_dir, exist_ok=True)
                    os.rename(src_dir, dst_dir)
                    log.warning(""Moved %s to %s"", src_dir, dst_dir)
                except OSError:
                    log.exception(""Failed to preserve %s"", src_dir)"
run_one_model;"def run_one_model(
        self,
        name,
        model,
        example_inputs,
        optimize_ctx,
        experiment,
        explain=False,
        tag=None,
    ):
        mode = ""train"" if self.args.training else ""eval""
        msg = f""{current_device:4} {mode:5} {current_name:34} ""
        if tag:
            msg += f"" {tag:26}""
        print(msg, flush=True)

        start_stats = get_dynamo_stats()

        if self.args.accuracy:
            status = self.check_accuracy(
                name, model, example_inputs, optimize_ctx, experiment, tag
            )
            print(status)
            if status == ""fail_accuracy"" and self.args.minify:
                self.minify_model(
                    name, model, example_inputs, optimize_ctx, experiment, tag
                )
        elif self.args.tolerance:
            status = self.check_tolerance(name, model, example_inputs, optimize_ctx)
            print(status)
        elif self.args.performance:
            if self.args.backend == ""torchao"":
                status = self.run_performance_test_non_alternate(
                    name, model, example_inputs, optimize_ctx, experiment, tag
                )
            else:
                status = self.run_performance_test(
                    name, model, example_inputs, optimize_ctx, experiment, tag
                )
            print(status)
        empty_gpu_cache(current_device)

        self.maybe_preserve_compile_debug(name, status)

        if self.args.timing:
            from torch._dynamo.utils import op_count, print_time_report
            from torch.utils._stats import simple_call_counter

            print_time_report()
            stats = ""STATS: ""
            stats = stats + "" | "".join(
                itertools.chain(
                    [f""call_* op count: {op_count}""],
                    (f""{key}:{value}"" for key, value in simple_call_counter.items()),
                )
            )
            print(stats)
        stats = get_dynamo_stats()
        stats.subtract(start_stats)

        if explain:
            print(
                f""Dynamo produced {stats['unique_graphs']} graphs ""
                f""covering {stats['calls_captured']} ops with ""
                f""{stats['graph_breaks']} graph breaks ({stats['unique_graph_breaks']} unique)""
            )

        if explain or self.args.log_graph_breaks or self.args.print_graph_breaks:
            filename = f""{output_filename.rstrip('.csv')}_graph_breaks.csv""

            def add_double_quotes(x):
                # Delimiter because reason could have comma
                return f'""{x}""'

            for graph_break in graph_break_reasons:
                reason = add_double_quotes(graph_break.reason)
                user_stack = add_double_quotes(
                    "", "".join([str(x) for x in graph_break.user_stack])
                )
                write_outputs(
                    filename,
                    [""model"", ""reason"", ""user_stack""],
                    [current_name, reason, user_stack],
                )

        if self.args.stats:
            Stats.print_summary()"
record_status;"def record_status(accuracy_status, dynamo_start_stats):
            """"""
            Records the status in the csv file
            """"""
            if current_name in self.non_deterministic_models:
                if accuracy_status in (
                    ""pass"",
                    ""eager_two_runs_differ"",
                    ""fail_accuracy"",
                ):
                    accuracy_status = ""pass""

            headers = [""dev"", ""name"", ""batch_size"", ""accuracy""]
            fields = [current_device, current_name, current_batch_size, accuracy_status]

            if tag is not None:
                headers.insert(3, ""tag"")
                fields.insert(3, tag)

            o_headers = list(headers)
            o_fields = list(fields)

            dynamo_stats = get_dynamo_stats()
            dynamo_stats.subtract(dynamo_start_stats)
            for k, v in dynamo_stats.items():
                headers.append(k)
                fields.append(v)

            total_wall_time = output_signpost(
                dict(zip(o_headers, o_fields)),
                self.args,
                self.suite_name,
            )
            headers.append(""compilation_latency"")
            fields.append(total_wall_time)
            write_outputs(output_filename, headers, fields)

            if self.args.print_compilation_time:
                print(f""Compilation time (from dynamo_timed): {total_wall_time}"")

            return accuracy_status"
dump_max_mean_values;"def dump_max_mean_values(tol, ref, res):
                if isinstance(ref, (list, tuple, torch.nn.ParameterList, torch.Size)):
                    for refi, resi in zip(ref, res):
                        dump_max_mean_values(tol, refi, resi)
                elif isinstance(ref, dict):
                    for k in ref.keys():
                        dump_max_mean_values(tol, ref[k], res[k])
                elif isinstance(ref, torch.Tensor):
                    res = res.to(base_device)
                    t = torch.abs(ref - res) / (1 + torch.abs(ref))
                    tol.append(t.flatten().to(torch.float32))
                return tol"
add_double_quotes;"def add_double_quotes(x):
                # Delimiter because reason could have comma
                return f'""{x}""'"
detect_and_mark_batch;"def detect_and_mark_batch(t):
                nonlocal marked
                for i, s in enumerate(t.size()):
                    if s == batch_size:
                        torch._dynamo.mark_dynamic(t, i)
                        marked = True
                        break"
model_iter_fn_and_mark_step;"def model_iter_fn_and_mark_step(*args, **kwargs):
                torch.compiler.cudagraph_mark_step_begin()
                model_iter_fn(*args, **kwargs)"
torchviz_model;"def torchviz_model(args, model, inputs, rank):
    from torchviz import make_dot

    outputs = model(*inputs)
    loss = reduce_to_scalar_loss(outputs)
    parameter_names = dict(model.named_parameters())
    dot = make_dot(loss, params=parameter_names, show_attrs=True, show_saved=True)
    if rank == 0:
        dot.render(""torchviz.dot"")"
profile_model;"def profile_model(args, model, inputs, rank):
    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
        for i in range(args.repeat):
            with record_function(""Forward""):
                outputs = model(*inputs)
                loss = reduce_to_scalar_loss(outputs)
            with record_function(""Backward""):
                loss.backward()
    if rank == 0:
        prof.export_chrome_trace(args.trace_file)"
run_model;"def run_model(args, model, inputs, key):
    rank = int(os.getenv(""RANK"", 0))
    world_size = int(os.getenv(""WORLD_SIZE"", 1))
    # result_q = []

    setup(rank, world_size)
    if args.device == ""cuda"":
        # needed for FSDP
        torch.cuda.set_device(rank)

    dev_rank = f""{args.device}:{rank}""
    model = model.to(dev_rank)

    def move_tensor(maybe_tensor):
        if torch.is_tensor(maybe_tensor):
            return maybe_tensor.to(dev_rank)
        return maybe_tensor

    inputs = pytree.tree_map(move_tensor, inputs)

    if args.fsdp:
        model = apply_fsdp(
            args,
            model,
            use_checkpointing=args.fsdp_checkpoint,
            use_wrap_policy=args.fsdp_wrap,
        )
    elif args.ddp:
        model = DDP(model)

    if args.verbose:
        print(model)

    if args.dynamo:
        dynamo.reset()
        if args.verbose:
            dynamo.config.verbose = True
            dynamo.config.log_level = logging.DEBUG
        if args.dynamo_no_optimize_ddp:
            dynamo.config.optimize_ddp = False
        if args.dynamo == ""inductor"" and args.fsdp:
            torch._inductor.config.triton.cudagraphs = False
            log.warning(""disabling inductor cudagraphs for compatibility with FSDP"")

        def print_compile(gm, ex):
            print(
                f""print_compile:\n{str(gm.graph)}\n-----------------------------------------""
            )
            return gm

        dynamo_ctx = dynamo.optimize(
            print_compile if args.dynamo == ""print"" else args.dynamo
        )
        model = dynamo_ctx(model)

    # warmup
    _ = timed(model, model_iter_fn, inputs, times=3, return_result=False)
    t_total = timed(
        model, model_iter_fn, inputs, times=args.repeat, return_result=False
    )
    if args.torchviz:
        torchviz_model(args, model, inputs, rank)
    if args.profile:
        profile_model(args, model, inputs, rank)

    cleanup()
    return t_total"
move_tensor;"def move_tensor(maybe_tensor):
        if torch.is_tensor(maybe_tensor):
            return maybe_tensor.to(dev_rank)
        return maybe_tensor"
print_compile;"def print_compile(gm, ex):
            print(
                f""print_compile:\n{str(gm.graph)}\n-----------------------------------------""
            )
            return gm"
model_iter_fn;"def model_iter_fn(model, example_inputs, collect_outputs=False):
    outputs = model(*example_inputs)
    loss = reduce_to_scalar_loss(outputs)
    loss.backward()
    if collect_outputs:
        return outputs"
get_model;"def get_model(args):
    if args.torchbench_model:
        setup_torchbench_cwd()
        module = importlib.import_module(
            f""torchbenchmark.models.{args.torchbench_model}""
        )
        benchmark_cls = getattr(module, ""Model"", None)
        bm = benchmark_cls(test=""train"", device=args.device, batch_size=args.batch_size)
        model, inputs = bm.get_module()
    elif args.toy_model:
        model = ToyModel()
        inputs = (torch.randn(20, 10),)
    else:
        raise argparse.ArgumentError(
            args.torchbench_model, message=""Must specify a model""
        )

    return model, inputs"
fsdp_checkpointing_base;"def fsdp_checkpointing_base(model, blocks):
    """"""apply activation checkpointing to model
    returns None as model is updated directly
    """"""
    non_reentrant_wrapper = functools.partial(
        checkpoint_wrapper,
        offload_to_cpu=False,
        checkpoint_impl=CheckpointImpl.NO_REENTRANT,
    )

    def check_fn(submodule):
        return isinstance(submodule, blocks)

    apply_activation_checkpointing(
        model, checkpoint_wrapper_fn=non_reentrant_wrapper, check_fn=check_fn
    )"
apply_fsdp;"def apply_fsdp(args, model, use_checkpointing=False, use_wrap_policy=True):
    wrap_policy = None
    blocks = MODEL_FSDP_WRAP[
        ""toy_model"" if model.__class__ is ToyModel else args.torchbench_model
    ]
    if use_wrap_policy:
        wrap_policy = ModuleWrapPolicy(blocks)

    model = FSDP(model, auto_wrap_policy=wrap_policy, use_orig_params=True)
    if use_checkpointing:
        fsdp_checkpointing_base(model, blocks)
    return model"
process_hf_reformer_output;"def process_hf_reformer_output(out):
    assert isinstance(out, list)
    # second output is unstable
    return [elem for i, elem in enumerate(out) if i != 1]"
get_module_cls_by_model_name;"def get_module_cls_by_model_name(model_cls_name):
    _module_by_model_name = {
        ""Speech2Text2Decoder"": ""transformers.models.speech_to_text_2.modeling_speech_to_text_2"",
        ""TrOCRDecoder"": ""transformers.models.trocr.modeling_trocr"",
    }
    module_name = _module_by_model_name.get(model_cls_name, ""transformers"")
    module = importlib.import_module(module_name)
    return getattr(module, model_cls_name)"
get_sequence_length;"def get_sequence_length(model_cls, model_name):
    if model_name.startswith((""Blenderbot"",)):
        seq_length = 128
    elif model_name.startswith((""GPT2"", ""Bart"", ""T5"", ""PLBart"", ""MBart"")):
        seq_length = 1024
    elif model_name in (""AllenaiLongformerBase"", ""BigBird""):
        seq_length = 1024
    elif model_name.startswith(""OPT""):
        seq_length = 2048
    elif ""Reformer"" in model_name:
        seq_length = 4096
    elif model_name.startswith(
        (
            ""Albert"",
            ""Deberta"",
            ""Layout"",
            ""Electra"",
            ""XLNet"",
            ""MegatronBert"",
            ""Bert"",
            ""Roberta"",
        )
    ) or model_name in (""DistillGPT2"", ""GoogleFnet"", ""YituTechConvBert"", ""CamemBert""):
        seq_length = 512
    elif model_name in (""TrOCRForCausalLM""):
        seq_length = 256
    elif model_name.startswith(""MobileBert""):
        seq_length = 128
    elif model_name.startswith(""Wav2Vec2""):
        # If too short, will fail with something like
        # ValueError: `mask_length` has to be smaller than `sequence_length`,
        # but got `mask_length`: 10 and `sequence_length`: 9`
        seq_length = 10000  # NB: a more realistic size is 155136
    else:
        log.info(
            f""Sequence Length not defined for {model_name}. Choosing 128 arbitrarily""  # noqa: G004
        )
        seq_length = 128
    return seq_length"
generate_inputs_for_model;"def generate_inputs_for_model(
    model_cls, model, model_name, bs, device, include_loss_args=False
):
    # TODO - Check if following values are representative
    num_choices = 3
    num_visual_features = 42
    seq_length = get_sequence_length(model_cls, model_name)
    vocab_size = model.config.vocab_size

    if model_name.startswith(""Wav2Vec2""):
        # TODO: If we add more input_values style models, try to work this
        # into the overall control flow
        target_length = 100
        return {
            ""input_values"": torch.randn((bs, seq_length), device=device),
            # Added because that's what the example training script has
            ""attention_mask"": rand_int_tensor(device, 0, 2, (bs, seq_length)),
            ""labels"": rand_int_tensor(device, 0, vocab_size, (bs, target_length)),
        }

    if model_name.endswith(""MultipleChoice""):
        input = rand_int_tensor(device, 0, vocab_size, (bs, num_choices, seq_length))
    elif model_name.startswith(""Roberta""):
        input = rand_int_tensor(device, 0, 1, (bs, seq_length))
    else:
        input = rand_int_tensor(device, 0, vocab_size, (bs, seq_length))

    if ""Bart"" in model_name:
        input[:, -1] = model.config.eos_token_id

    input_dict = {""input_ids"": input}

    if model_name.startswith((""T5"", ""M2M100"", ""MT5"")) or model_cls in [
        BlenderbotModel,
        BlenderbotSmallModel,
        BlenderbotForConditionalGeneration,
        BlenderbotSmallForConditionalGeneration,
        PegasusModel,
        PegasusForConditionalGeneration,
        MarianModel,
        MarianMTModel,
    ]:
        input_dict[""decoder_input_ids""] = input

    if model_name.startswith(""Lxmert""):
        visual_feat_dim, visual_pos_dim = (
            model.config.visual_feat_dim,
            model.config.visual_pos_dim,
        )
        input_dict[""visual_feats""] = torch.randn(
            bs, num_visual_features, visual_feat_dim
        )
        input_dict[""visual_pos""] = torch.randn(bs, num_visual_features, visual_pos_dim)

    if include_loss_args:
        if model_name.endswith(""PreTraining""):
            if model_cls in [ElectraForPreTraining, LxmertForPreTraining]:
                input_dict[""labels""] = rand_int_tensor(device, 0, 1, (bs, seq_length))
            else:
                label_name = (
                    ""sentence_order_label""
                    if model_cls in [AlbertForPreTraining]
                    else ""next_sentence_label""
                )
                input_dict[""labels""] = (
                    rand_int_tensor(device, 0, vocab_size, (bs, seq_length)),
                )
                input_dict[label_name] = rand_int_tensor(device, 0, 1, (bs,))
        elif model_name.endswith(""QuestionAnswering""):
            input_dict[""start_positions""] = rand_int_tensor(
                device, 0, seq_length, (bs,)
            )
            input_dict[""end_positions""] = rand_int_tensor(device, 0, seq_length, (bs,))
        elif model_name.endswith(
            (""MaskedLM"", ""HeadModel"", ""CausalLM"", ""DoubleHeadsModel"")
        ):
            input_dict[""labels""] = rand_int_tensor(
                device, 0, vocab_size, (bs, seq_length)
            )
        elif model_name.endswith(""TokenClassification""):
            input_dict[""labels""] = rand_int_tensor(
                device, 0, model.config.num_labels - 1, (bs, seq_length)
            )
        elif model_name.endswith(""MultipleChoice""):
            input_dict[""labels""] = rand_int_tensor(device, 0, num_choices, (bs,))
        elif model_name.endswith(""SequenceClassification""):
            input_dict[""labels""] = rand_int_tensor(
                device, 0, model.config.num_labels - 1, (bs,)
            )
        elif model_name.endswith(""NextSentencePrediction""):
            input_dict[""labels""] = rand_int_tensor(device, 0, 1, (bs,))
        elif model_name.endswith(""ForConditionalGeneration""):
            input_dict[""labels""] = rand_int_tensor(
                device, 0, vocab_size - 1, (bs, seq_length)
            )
        elif model_name in EXTRA_MODELS:
            input_dict[""labels""] = rand_int_tensor(
                device, 0, vocab_size, (bs, seq_length)
            )
        else:
            raise NotImplementedError(
                f""Class {model_name} unsupported for training test ""
            )

    return input_dict"
rand_int_tensor;"def rand_int_tensor(device, low, high, shape):
    return torch.randint(
        low,
        high,
        shape,
        device=device,
        dtype=torch.int64,
        requires_grad=False,
    )"
refresh_model_names_and_batch_sizes;"def refresh_model_names_and_batch_sizes():
    """"""
    This function reads the HF Fx tracer supported models and finds the largest
    batch size that could fit on the GPU with PyTorch eager.

    The resulting data is written in huggingface_models_list.txt.

    Note - We only need to run this function if we believe that HF Fx tracer now
    supports more models.
    """"""
    import transformers.utils.fx as hf_fx

    family = {}
    lm_seen = set()
    family_seen = set()
    for cls_name in hf_fx._SUPPORTED_MODELS:
        if ""For"" not in cls_name:
            continue

        model_cls = get_module_cls_by_model_name(cls_name)

        # TODO: AttributeError: '*Config' object has no attribute 'vocab_size'
        if model_cls in [
            CLIPModel,
            CLIPVisionModel,
            # SwinForImageClassification,
            # SwinForImageClassification,
            # SwinForMaskedImageModeling,
            # SwinModel,
            ViTForImageClassification,
            ViTForMaskedImageModeling,
            ViTModel,
        ]:
            continue

        # TODO: AssertionError: Padding_idx must be within num_embeddings
        if model_cls in [MarianForCausalLM, MarianMTModel, MarianModel]:
            continue

        # TODO: ""model is not supported yet"" from HFTracer
        if model_cls in [HubertForSequenceClassification]:
            continue

        # TODO: shape mismatch in loss calculation
        if model_cls in [LxmertForQuestionAnswering]:
            continue

        family_name = cls_name.split(""For"")[0]
        if family_name not in family:
            family[family_name] = []
        if cls_name.endswith((""MaskedLM"", ""CausalLM"")) and family_name not in lm_seen:
            family[family_name].append(cls_name)
            lm_seen.add(family_name)
        elif (
            cls_name.endswith(
                (""SequenceClassification"", ""ConditionalGeneration"", ""QuestionAnswering"")
            )
            and family_name not in family_seen
        ):
            family[family_name].append(cls_name)
            family_seen.add(family_name)
        elif cls_name.endswith(""ImageClassification""):
            family[family_name].append(cls_name)

    chosen_models = set()
    for members in family.values():
        chosen_models.update(set(members))

    # Add the EXTRA_MODELS
    chosen_models.update(set(EXTRA_MODELS.keys()))

    for model_name in sorted(chosen_models):
        try:
            subprocess.check_call(
                [sys.executable]
                + sys.argv
                + [""--find-batch-sizes""]
                + [f""--only={model_name}""]
                + [f""--output={MODELS_FILENAME}""]
            )
        except subprocess.SubprocessError:
            log.warning(f""Failed to find suitable batch size for {model_name}"")"
huggingface_main;"def huggingface_main():
    # Code to refresh model names and batch sizes
    # if ""--find-batch-sizes"" not in sys.argv:
    #     refresh_model_names_and_batch_sizes()
    logging.basicConfig(level=logging.WARNING)
    warnings.filterwarnings(""ignore"")
    main(HuggingfaceRunner())"
skip_models;"def skip_models(self):
        return self._skip[""all""]"
skip_models_for_cpu;"def skip_models_for_cpu(self):
        return self._skip[""device""][""cpu""]"
fp32_only_models;"def fp32_only_models(self):
        return self._config[""only_fp32""]"
skip_models_due_to_control_flow;"def skip_models_due_to_control_flow(self):
        return self._skip[""control_flow""]"
use_larger_multiplier_for_smaller_tensor;"def use_larger_multiplier_for_smaller_tensor(self, name):
        return name in [
            ""ElectraForQuestionAnswering"",
            ""MegatronBertForQuestionAnswering"",
        ]"
_get_model_cls_and_config;"def _get_model_cls_and_config(self, model_name):
        if model_name not in EXTRA_MODELS:
            model_cls = get_module_cls_by_model_name(model_name)
            config_cls = model_cls.config_class
            config = config_cls()

            # NB: some models need a pad token defined to handle BS > 1
            if (
                model_cls
                in [
                    GPT2ForSequenceClassification,
                    GPTNeoForSequenceClassification,
                    GPTJForSequenceClassification,
                ]
                or model_cls.__name__.startswith(""Roberta"")
                or model_cls.__name__.startswith(""Marian"")
            ):
                config.pad_token_id = 0

        else:
            config, model_cls = EXTRA_MODELS[model_name]

        return model_cls, config"
_download_model;"def _download_model(self, model_name):
        model_cls, config = self._get_model_cls_and_config(model_name)
        if ""auto"" in model_cls.__module__:
            # Handle auto classes
            model = model_cls.from_config(config)
        else:
            model = model_cls(config)
        return model"
load_model;"def load_model(
        self,
        device,
        model_name,
        batch_size=None,
        extra_args=None,
    ):
        is_training = self.args.training
        use_eval_mode = self.args.use_eval_mode
        dtype = torch.float32
        reset_rng_state()
        model_cls, config = self._get_model_cls_and_config(model_name)
        model = self._download_model(model_name)
        model = model.to(device, dtype=dtype)
        if self.args.enable_activation_checkpointing:
            model.gradient_checkpointing_enable()
        if model_name in BATCH_SIZE_KNOWN_MODELS:
            batch_size_default = BATCH_SIZE_KNOWN_MODELS[model_name]
        elif batch_size is None:
            batch_size_default = 16
            log.info(
                f""Batch size not specified for {model_name}. Setting batch_size=16""  # noqa: G004
            )

        if batch_size is None:
            batch_size = batch_size_default
            batch_size_divisors = self._config[""batch_size""][""divisors""]
            if model_name in batch_size_divisors:
                batch_size = max(int(batch_size / batch_size_divisors[model_name]), 1)
                log.info(
                    f""Running smaller batch size={batch_size} for {model_name}, orig batch_size={batch_size_default}""  # noqa: G004
                )

        example_inputs = generate_inputs_for_model(
            model_cls, model, model_name, batch_size, device, include_loss_args=True
        )

        # So we can check for correct gradients without eliminating the dropout computation
        for attr in dir(config):
            if ""drop"" in attr and isinstance(getattr(config, attr), float):
                setattr(config, attr, 1e-30)

        if (
            is_training
            and not use_eval_mode
            and not (
                self.args.accuracy and model_name in self._config[""only_inference""]
            )
        ):
            model.train()
        else:
            model.eval()

        self.validate_model(model, example_inputs)
        return device, model_name, model, example_inputs, batch_size"
iter_model_names;"def iter_model_names(self, args):
        model_names = list(BATCH_SIZE_KNOWN_MODELS.keys()) + list(EXTRA_MODELS.keys())
        model_names = set(model_names)
        model_names = sorted(model_names)

        start, end = self.get_benchmark_indices(len(model_names))
        for index, model_name in enumerate(model_names):
            if index < start or index >= end:
                continue
            if (
                not re.search(""|"".join(args.filter), model_name, re.IGNORECASE)
                or re.search(""|"".join(args.exclude), model_name, re.IGNORECASE)
                or model_name in args.exclude_exact
                or model_name in self.skip_models
            ):
                continue
            yield model_name"
skip_accuracy_checks_large_models_dashboard;"def skip_accuracy_checks_large_models_dashboard(self):
        if self.args.dashboard or self.args.accuracy:
            return self._accuracy[""skip""][""large_models""]
        return set()"
get_output_amp_train_process_func;"def get_output_amp_train_process_func(self):
        return {}"
pick_grad;"def pick_grad(self, name, is_training):
        if is_training:
            return torch.enable_grad()
        else:
            return torch.no_grad()"
get_tolerance_and_cosine_flag;"def get_tolerance_and_cosine_flag(self, is_training, current_device, name):
        cosine = self.args.cosine
        if is_training:
            from torch._inductor import config as inductor_config

            if (name in self._config[""tolerance""][""higher_training""]) or (
                inductor_config.max_autotune
                and name in self._config[""tolerance""][""higher_max_autotune_training""]
            ):
                return 2e-2, cosine
            else:
                return 1e-2, cosine
        else:
            if (
                current_device == ""cpu""
                and name in self._config[""tolerance""][""higher_inference_cpu""]
            ):
                return 5e-3, cosine
            if name in self._config[""tolerance""][""higher_inference""]:
                return 4e-3, cosine
        return 1e-3, cosine"
forward_and_backward_pass;"def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
        cloned_inputs = clone_inputs(inputs)
        self.optimizer_zero_grad(mod)
        with self.autocast(**self.autocast_arg):
            pred = mod(**cloned_inputs)
            loss = self.compute_loss(pred)
        self.grad_scaler.scale(loss).backward()
        self.optimizer_step()
        if collect_outputs:
            return collect_results(mod, None, loss, cloned_inputs)
        return None"
longest_common_prefix;"def longest_common_prefix(strs):
    shortest_str = min(strs, key=len)
    for i, char in enumerate(shortest_str):
        for other in strs:
            if other[i] != char:
                return shortest_str[:i]
    return """""
normalize_file;"def normalize_file(f):
    if ""site-packages/"" in f:
        return f.split(""site-packages/"", 2)[1]
    else:
        return os.path.relpath(f)"
flag_speedup;"def flag_speedup(x):
    return x < 0.95"
flag_compilation_latency;"def flag_compilation_latency(x):
    return x > 120"
flag_compression_ratio;"def flag_compression_ratio(x):
    return x < 0.9"
flag_accuracy;"def flag_accuracy(x):
    return ""pass"" not in x"
percentage;"def percentage(part, whole, decimals=2):
    if whole == 0:
        return 0
    return round(100 * float(part) / float(whole), decimals)"
parse_args;"def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--devices"", action=""append"", help=""cpu or cuda"")
    parser.add_argument(""--dtypes"", action=""append"", help=""float16/float32/amp"")
    parser.add_argument(""--suites"", action=""append"", help=""huggingface/torchbench/timm"")
    parser.add_argument(
        ""--compilers"",
        action=""append"",
        help=f""For --inference, options are {INFERENCE_COMPILERS}. For --training, options are {TRAINING_COMPILERS}"",
    )

    parser.add_argument(
        ""--flag-compilers"",
        action=""append"",
        help=""List of compilers to flag issues. Same format as --compilers."",
    )
    parser.add_argument(
        ""--quick"", action=""store_true"", help=""Just runs one model. Helps in debugging""
    )
    parser.add_argument(
        ""--output-dir"",
        help=""Choose the output directory to save the logs"",
        default=DEFAULT_OUTPUT_DIR,
    )
    parser.add_argument(
        ""--keep-output-dir"",
        action=""store_true"",
        help=""Do not cleanup the output directory before running"",
    )

    # Choose either generation of commands, pretty parsing or e2e runs
    group = parser.add_mutually_exclusive_group(required=False)
    group.add_argument(
        ""--print-run-commands"",
        ""--print_run_commands"",
        action=""store_true"",
        help=""Generate commands and saves them to run.sh"",
    )
    group.add_argument(
        ""--visualize-logs"",
        ""--visualize_logs"",
        action=""store_true"",
        help=""Pretty print the log files and draw graphs"",
    )
    group.add_argument(
        ""--run"",
        action=""store_true"",
        default=True,
        help=""Generate commands, run and parses the files"",
    )

    parser.add_argument(
        ""--log-operator-inputs"",
        action=""store_true"",
        default=False,
        help=""Log operator inputs"",
    )
    parser.add_argument(
        ""--include-slowdowns"",
        ""--include_slowdowns"",
        action=""store_true"",
        default=False,
        help=""Include slowdowns in geomean performance speedup report. By default, slowdowns are ignored. ""
        ""This is because one can always use eager if compile is not speeding things up"",
    )

    parser.add_argument(
        ""--extra-args"", default="""", help=""Append commandline with these args""
    )

    # Choose either inference or training
    group_mode = parser.add_mutually_exclusive_group(required=True)
    group_mode.add_argument(
        ""--inference"", action=""store_true"", help=""Only run inference related tasks""
    )
    group_mode.add_argument(
        ""--training"", action=""store_true"", help=""Only run training related tasks""
    )

    parser.add_argument(
        ""--base-sha"",
        help=""commit id for the tested pytorch"",
    )
    parser.add_argument(
        ""--total-partitions"",
        type=int,
        help=""Total number of partitions, to be passed to the actual benchmark script"",
    )
    parser.add_argument(
        ""--partition-id"",
        type=int,
        help=""ID of partition, to be passed to the actual benchmark script"",
    )

    parser.add_argument(
        ""--update-dashboard"",
        action=""store_true"",
        default=False,
        help=""Updates to dashboard"",
    )
    parser.add_argument(
        ""--no-graphs"",
        action=""store_true"",
        default=False,
        help=""Do not genenerate and upload metric graphs"",
    )
    parser.add_argument(
        ""--no-update-archive"",
        action=""store_true"",
        default=False,
        help=""Do not update lookup.csv or the log archive"",
    )
    parser.add_argument(
        ""--no-gh-comment"",
        action=""store_true"",
        default=False,
        help=""Do not write a comment to github"",
    )
    parser.add_argument(
        ""--no-detect-regressions"",
        action=""store_true"",
        default=False,
        help=""Do not compare to previous runs for regressions or metric graphs."",
    )
    parser.add_argument(
        ""--update-dashboard-test"",
        action=""store_true"",
        default=False,
        help=""does all of --no-graphs, --no-update-archive, and --no-gh-comment"",
    )
    parser.add_argument(
        ""--dashboard-image-uploader"",
        default=DASHBOARD_DEFAULTS[""dashboard_image_uploader""],
        help=""Image uploader command"",
    )
    parser.add_argument(
        ""--dashboard-archive-path"",
        default=DASHBOARD_DEFAULTS[""dashboard_archive_path""],
        help=""Archived directory path"",
    )
    parser.add_argument(
        ""--archive-name"",
        help=""Directory name under dashboard-archive-path to copy output-dir to. ""
        ""If not provided, a generated name is used."",
    )
    parser.add_argument(
        ""--dashboard-gh-cli-path"",
        default=DASHBOARD_DEFAULTS[""dashboard_gh_cli_path""],
        help=""Github CLI path"",
    )
    parser.add_argument(
        ""--batch-size"",
        ""--batch_size"",
        type=int,
        default=None,
        help=""batch size for benchmarking"",
    )
    parser.add_argument(
        ""--threads"",
        ""-t"",
        type=int,
        default=None,
        help=""number of threads to use for eager and inductor."",
    )
    launcher_group = parser.add_argument_group(""CPU Launcher Parameters"")
    launcher_group.add_argument(
        ""--enable-cpu-launcher"",
        ""--enable_cpu_launcher"",
        action=""store_true"",
        default=False,
        help=""Use torch.backends.xeon.run_cpu to get the peak performance on Intel(R) Xeon(R) Scalable Processors."",
    )
    launcher_group.add_argument(
        ""--cpu-launcher-args"",
        ""--cpu_launcher_args"",
        type=str,
        default="""",
        help=""Provide the args of torch.backends.xeon.run_cpu. ""
        ""To look up what optional arguments this launcher offers: python -m torch.backends.xeon.run_cpu --help"",
    )
    parser.add_argument(
        ""--no-cold-start-latency"",
        action=""store_true"",
        default=False,
        help=""Do not include --cold-start-latency on inductor benchmarks"",
    )
    parser.add_argument(
        ""--inductor-compile-mode"",
        default=None,
        help=""torch.compile mode argument for inductor runs."",
    )
    args = parser.parse_args()
    return args"
get_mode;"def get_mode(args):
    if args.inference:
        return ""inference""
    return ""training"""
get_skip_tests;"def get_skip_tests(suite, device, is_training: bool):
    """"""
    Generate -x seperated string to skip the unusual setup training tests
    """"""
    skip_tests = set()
    original_dir = abspath(os.getcwd())
    module = importlib.import_module(suite)
    os.chdir(original_dir)

    if suite == ""torchbench"":
        skip_tests.update(module.TorchBenchmarkRunner().skip_models)
        if is_training:
            skip_tests.update(
                module.TorchBenchmarkRunner().skip_not_suitable_for_training_models
            )
        if device == ""cpu"":
            skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cpu)
        elif device == ""cuda"":
            skip_tests.update(module.TorchBenchmarkRunner().skip_models_for_cuda)

    skip_tests = (f""-x {name}"" for name in skip_tests)
    skip_str = "" "".join(skip_tests)
    return skip_str"
generate_csv_name;"def generate_csv_name(args, dtype, suite, device, compiler, testing):
    mode = get_mode(args)
    return f""{compiler}_{suite}_{dtype}_{mode}_{device}_{testing}.csv"""
generate_commands;"def generate_commands(args, dtypes, suites, devices, compilers, output_dir):
    mode = get_mode(args)
    suites_str = ""_"".join(suites)
    devices_str = ""_"".join(devices)
    dtypes_str = ""_"".join(dtypes)
    compilers_str = ""_"".join(compilers)
    generated_file = (
        f""run_{mode}_{devices_str}_{dtypes_str}_{suites_str}_{compilers_str}.sh""
    )
    with open(generated_file, ""w"") as runfile:
        lines = []

        lines.append(""#!/bin/bash"")
        lines.append(""set -x"")
        lines.append(""# Setup the output directory"")
        if not args.keep_output_dir:
            lines.append(f""rm -rf {output_dir}"")
        # It's ok if the output directory already exists
        lines.append(f""mkdir -p {output_dir}"")
        lines.append("""")

        for testing in [""performance"", ""accuracy""]:
            for iter in itertools.product(suites, devices, dtypes):
                suite, device, dtype = iter
                lines.append(
                    f""# Commands for {suite} for device={device}, dtype={dtype} for {mode} and for {testing} testing""
                )
                info = TABLE[mode]
                for compiler in compilers:
                    base_cmd = info[compiler]
                    output_filename = f""{output_dir}/{generate_csv_name(args, dtype, suite, device, compiler, testing)}""
                    launcher_cmd = ""python""
                    if args.enable_cpu_launcher:
                        launcher_cmd = f""python -m torch.backends.xeon.run_cpu {args.cpu_launcher_args}""
                    cmd = f""{launcher_cmd} benchmarks/dynamo/{suite}.py --{testing} --{dtype} -d{device} --output={output_filename}""
                    cmd = f""{cmd} {base_cmd} {args.extra_args} --dashboard""
                    skip_tests_str = get_skip_tests(suite, device, args.training)
                    cmd = f""{cmd} {skip_tests_str}""

                    if args.log_operator_inputs:
                        cmd = f""{cmd} --log-operator-inputs""

                    if args.quick:
                        filters = DEFAULTS[""quick""][suite]
                        cmd = f""{cmd} {filters}""

                    if (
                        compiler
                        in (
                            ""inductor"",
                            ""inductor_no_cudagraphs"",
                        )
                        and not args.no_cold_start_latency
                    ):
                        cmd = f""{cmd} --cold-start-latency""

                    if args.batch_size is not None:
                        cmd = f""{cmd} --batch-size {args.batch_size}""

                    if args.threads is not None:
                        cmd = f""{cmd} --threads {args.threads}""

                    if args.total_partitions is not None:
                        cmd = f""{cmd} --total-partitions {args.total_partitions}""

                    if args.partition_id is not None:
                        cmd = f""{cmd} --partition-id {args.partition_id}""

                    if args.inductor_compile_mode is not None:
                        cmd = f""{cmd} --inductor-compile-mode {args.inductor_compile_mode}""
                    lines.append(cmd)
                lines.append("""")
        runfile.writelines([line + ""\n"" for line in lines])
    return generated_file"
generate_dropdown_comment;"def generate_dropdown_comment(title, body):
    str_io = io.StringIO()
    str_io.write(f""{title}\n"")
    str_io.write(""<details>\n"")
    str_io.write(""<summary>see more</summary>\n"")
    str_io.write(f""{body}"")
    str_io.write(""\n"")
    str_io.write(""</details>\n\n"")
    return str_io.getvalue()"
build_summary;"def build_summary(args):
    out_io = io.StringIO()

    def print_commit_hash(path, name):
        if args.base_sha is not None:
            if name == ""pytorch"":
                out_io.write(f""{name} commit: {args.base_sha}\n"")
        elif exists(path):
            import git

            repo = git.Repo(path, search_parent_directories=True)
            sha = repo.head.object.hexsha
            date = repo.head.object.committed_datetime
            out_io.write(f""{name} commit: {sha}\n"")
            out_io.write(f""{name} commit date: {date}\n"")
        else:
            out_io.write(f""{name} Absent\n"")

    def env_var(name):
        if name in os.environ:
            out_io.write(f""{name} = {os.environ[name]}\n"")
        else:
            out_io.write(f""{name} = {None}\n"")

    out_io.write(""\n"")
    out_io.write(""### Run name ###\n"")
    out_io.write(get_archive_name(args, args.dtypes[0]))
    out_io.write(""\n"")

    out_io.write(""\n"")
    out_io.write(""### Commit hashes ###\n"")
    print_commit_hash(""../pytorch"", ""pytorch"")
    print_commit_hash(""../torchbenchmark"", ""torchbench"")

    out_io.write(""\n"")
    out_io.write(""### TorchDynamo config flags ###\n"")
    for key in dir(torch._dynamo.config):
        val = getattr(torch._dynamo.config, key)
        if not key.startswith(""__"") and isinstance(val, bool):
            out_io.write(f""torch._dynamo.config.{key} = {val}\n"")

    out_io.write(""\n"")
    out_io.write(""### Torch version ###\n"")
    out_io.write(f""torch: {torch.__version__}\n"")

    out_io.write(""\n"")
    out_io.write(""### Environment variables ###\n"")
    env_var(""TORCH_CUDA_ARCH_LIST"")
    env_var(""CUDA_HOME"")
    env_var(""USE_LLVM"")

    if ""cuda"" in args.devices:
        out_io.write(""\n"")
        out_io.write(""### GPU details ###\n"")
        out_io.write(f""CUDNN VERSION: {torch.backends.cudnn.version()}\n"")
        out_io.write(f""Number CUDA Devices: {torch.cuda.device_count()}\n"")
        out_io.write(f""Device Name: {torch.cuda.get_device_name(0)}\n"")
        out_io.write(
            f""Device Memory [GB]: {torch.cuda.get_device_properties(0).total_memory / 1e9}\n""
        )

    title = ""## Build Summary""
    comment = generate_dropdown_comment(title, out_io.getvalue())
    with open(f""{output_dir}/gh_build_summary.txt"", ""w"") as gh_fh:
        gh_fh.write(comment)"
archive_data;"def archive_data(archive_name):
    if archive_name is not None:
        prefix_match = re.search(r""\w+(?=_performance)"", archive_name)
        if prefix_match is not None:
            prefix = prefix_match.group(0)
        else:
            prefix = """"
        day_match = re.search(r""day_(\d+)_"", archive_name)
        if day_match is not None:
            day = day_match.group(1)
        else:
            day = ""000""
    else:
        now = datetime.now(tz=timezone(timedelta(hours=-8)))
        day = now.strftime(""%j"")
        prefix = now.strftime(f""day_{day}_%d_%m_%y"")
    return day, prefix"
default_archive_name;"def default_archive_name(dtype):
    _, prefix = archive_data(None)
    return f""{prefix}_performance_{dtype}_{randint(100, 999)}"""
get_archive_name;"def get_archive_name(args, dtype):
    return (
        default_archive_name(dtype) if args.archive_name is None else args.archive_name
    )"
archive;"def archive(src_dir, dest_dir_prefix, archive_name, dtype):
    if archive_name is None:
        archive_name = default_archive_name(dtype)
    # Copy the folder to archived location
    dest = os.path.join(dest_dir_prefix, archive_name)
    shutil.copytree(src_dir, dest, dirs_exist_ok=True)
    print(f""copied contents of {src_dir} to {dest}"")"
get_metric_title;"def get_metric_title(metric):
    if metric == ""speedup"":
        return ""Performance speedup""
    elif metric == ""accuracy"":
        return ""Accuracy""
    elif metric == ""compilation_latency"":
        return ""Compilation latency (sec)""
    elif metric == ""compression_ratio"":
        return ""Peak Memory Compression Ratio""
    elif metric == ""abs_latency"":
        return ""Absolute latency (ms)""
    raise RuntimeError(""unknown metric"")"
parse_logs;"def parse_logs(args, dtypes, suites, devices, compilers, flag_compilers, output_dir):
    mode = get_mode(args)
    build_summary(args)
    include_slowdowns = args.include_slowdowns

    parser_class = ParsePerformanceLogs
    parser = parser_class(
        suites,
        devices,
        dtypes,
        compilers,
        flag_compilers,
        mode,
        output_dir,
        include_slowdowns,
    )
    parser.gen_summary_files()
    return"
get_date;"def get_date(log_info):
    return datetime.strptime(f""{log_info.day}"", ""%j"").strftime(""%m-%d"")"
find_last_2_with_filenames;"def find_last_2_with_filenames(lookup_file, dashboard_archive_path, dtype, filenames):
    df = pd.read_csv(lookup_file, names=(""day"", ""mode"", ""prec"", ""path""))
    df = df[df[""mode""] == ""performance""]
    df = df[df[""prec""] == dtype]
    df = df[::-1]
    last2 = []
    for path in df[""path""]:
        output_dir = os.path.join(dashboard_archive_path, path)
        fullpaths = [
            os.path.join(dashboard_archive_path, path, name) for name in filenames
        ]
        if all(os.path.exists(fullpath) for fullpath in fullpaths):
            last2.append(output_dir)
        if len(last2) >= 2:
            return last2
    return None"
print_commit_hash;"def print_commit_hash(path, name):
        if args.base_sha is not None:
            if name == ""pytorch"":
                out_io.write(f""{name} commit: {args.base_sha}\n"")
        elif exists(path):
            import git

            repo = git.Repo(path, search_parent_directories=True)
            sha = repo.head.object.hexsha
            date = repo.head.object.committed_datetime
            out_io.write(f""{name} commit: {sha}\n"")
            out_io.write(f""{name} commit date: {date}\n"")
        else:
            out_io.write(f""{name} Absent\n"")"
has_header;"def has_header(self, output_filename):
        header_present = False
        with open(output_filename) as f:
            line = f.readline()
            if ""dev"" in line:
                header_present = True
        return header_present"
plot_graph;"def plot_graph(self, df, title):
        labels = df.columns.values.tolist()
        labels = labels[3:]
        df.plot(
            x=""name"",
            y=labels,
            kind=""bar"",
            width=0.65,
            title=title,
            ylabel=""Speedup over eager"",
            xlabel="""",
            grid=True,
            figsize=(max(len(df.index) / 4, 5), 10),
            edgecolor=""black"",
        )
        plt.tight_layout()
        plt.savefig(f""{self.output_dir}/{title}.png"")"
read_csv;"def read_csv(self, output_filename):
        if self.has_header(output_filename):
            return pd.read_csv(output_filename)
        else:
            return pd.read_csv(
                output_filename,
                names=[
                    ""dev"",
                    ""name"",
                    ""batch_size"",
                    ""speedup"",
                    ""abs_latency"",
                    ""compilation_latency"",
                    ""compression_ratio"",
                ],
                header=None,
                engine=""python"",
            )"
clean_batch_sizes;"def clean_batch_sizes(self, frames):
        # Clean up batch sizes when its 0
        if len(frames) == 1:
            return frames
        batch_sizes = frames[0][""batch_size""].to_list()
        for frame in frames[1:]:
            frame_batch_sizes = frame[""batch_size""].to_list()
            for idx, (batch_a, batch_b) in enumerate(
                zip(batch_sizes, frame_batch_sizes)
            ):
                assert batch_a == batch_b or batch_a == 0 or batch_b == 0, (
                    f""a={batch_a}, b={batch_b}""
                )
                batch_sizes[idx] = max(batch_a, batch_b)
        for frame in frames:
            frame[""batch_size""] = batch_sizes
        return frames"
extract_df;"def extract_df(self, metric, testing):
        for iter in itertools.product(self.suites, self.devices, self.dtypes):
            suite, device, dtype = iter
            frames = []
            for compiler in self.compilers:
                output_filename = f""{self.output_dir}/{compiler}_{suite}_{dtype}_{self.mode}_{device}_{testing}.csv""
                df = self.read_csv(output_filename)
                if metric not in df:
                    df.insert(len(df.columns), metric, np.nan)
                df = df[[""dev"", ""name"", ""batch_size"", metric]]
                df.rename(columns={metric: compiler}, inplace=True)
                df[""batch_size""] = df[""batch_size""].astype(int)
                frames.append(df)

            # Merge the results
            frames = self.clean_batch_sizes(frames)
            if len(self.compilers) == 1:
                df = frames[0]
            else:
                # Merge data frames
                df = pd.merge(frames[0], frames[1], on=[""dev"", ""name"", ""batch_size""])
                for idx in range(2, len(frames)):
                    df = pd.merge(df, frames[idx], on=[""dev"", ""name"", ""batch_size""])

            if testing == ""performance"":
                for compiler in self.compilers:
                    df[compiler] = pd.to_numeric(df[compiler], errors=""coerce"").fillna(
                        0
                    )

            df_copy = df.copy()
            df_copy = df_copy.sort_values(
                by=list(reversed(self.compilers)), ascending=False
            )
            if ""inductor"" in self.compilers:
                df_copy = df_copy.sort_values(by=""inductor"", ascending=False)
            self.untouched_parsed_frames[suite][metric] = df_copy

            if testing == ""performance"":
                df_accuracy = self.parsed_frames[suite][""accuracy""]
                perf_rows = []
                for model_name in df[""name""]:
                    perf_row = df[df[""name""] == model_name].copy()
                    acc_row = df_accuracy[df_accuracy[""name""] == model_name]
                    for compiler in self.compilers:
                        if not perf_row.empty:
                            if acc_row.empty:
                                perf_row[compiler] = 0.0
                            elif acc_row[compiler].iloc[0] in (
                                ""model_fail_to_load"",
                                ""eager_fail_to_run"",
                            ):
                                perf_row = pd.DataFrame()
                            elif acc_row[compiler].iloc[0] not in (
                                ""pass"",
                                ""pass_due_to_skip"",
                            ):
                                perf_row[compiler] = 0.0
                    if not perf_row.empty:
                        perf_rows.append(perf_row)
                df = pd.concat(perf_rows)
            df = df.sort_values(by=list(reversed(self.compilers)), ascending=False)

            if ""inductor"" in self.compilers:
                df = df.sort_values(by=""inductor"", ascending=False)
            self.parsed_frames[suite][metric] = df"
get_passing_entries;"def get_passing_entries(self, compiler, df):
        return df[compiler][df[compiler] > 0]"
comp_time;"def comp_time(self, compiler, df):
        df = self.get_passing_entries(compiler, df)
        # df = df.sort_values(by=compiler, ascending=False)[compiler][: self.bottom_k]
        if df.empty:
            return ""0.0""

        return f""{df.mean():.2f}"""
exec_summary_df;"def exec_summary_df(self, fn, metric):
        """"""
        Generate a table with passrate and geomean perf
        """"""
        cols = {}
        cols[""Compiler""] = self.compilers
        for suite in self.suites:
            df = self.parsed_frames[suite][metric]
            # speedups = [self.geomean(compiler, df) for compiler in self.compilers]
            speedups = [fn(compiler, df) for compiler in self.compilers]
            col = pd.Series(data=speedups, index=self.compilers)
            cols[suite] = col
        df = pd.DataFrame(cols)
        df = df.fillna(0)
        df.to_csv(os.path.join(self.output_dir, f""{fn.__name__}.csv""))
        return df"
exec_summary_text;"def exec_summary_text(self, caption, fn, metric):
        df = self.exec_summary_df(fn, metric)
        tabform = tabulate(df, headers=""keys"", tablefmt=""pretty"", showindex=""never"")

        str_io = io.StringIO()
        str_io.write(f""{caption}"")
        str_io.write(""~~~\n"")
        str_io.write(f""{tabform}\n"")
        str_io.write(""~~~\n"")
        return str_io.getvalue()"
generate_executive_summary;"def generate_executive_summary(self):
        machine = ""A100 GPUs""
        if ""cpu"" in self.devices:
            get_machine_cmd = ""lscpu| grep 'Model name' | awk -F':' '{print $2}'""
            machine = subprocess.getstatusoutput(get_machine_cmd)[1].strip()
        description = (
            ""We evaluate different backends ""
            ""across three benchmark suites - torchbench, huggingface and timm. We run ""
            ""these experiments on ""
            + machine
            + "". Each experiment runs one iteration of forward pass ""
            ""and backward pass for training and forward pass only for inference. ""
            ""For accuracy, we check the numerical correctness of forward pass outputs and gradients ""
            ""by comparing with native pytorch. We measure speedup ""
            ""by normalizing against the performance of native pytorch. We report mean ""
            ""compilation latency numbers and peak memory footprint reduction ratio. \n\n""
            ""Caveats\n""
            ""1) Batch size has been reduced to workaround OOM errors. Work is in progress to ""
            ""reduce peak memory footprint.\n""
            ""2) Experiments do not cover dynamic shapes.\n""
            ""3) Experimental setup does not have optimizer.\n\n""
        )
        comment = generate_dropdown_comment("""", description)
        str_io = io.StringIO()
        str_io.write(""\n"")
        str_io.write(""## Executive Summary ##\n"")
        str_io.write(comment)

        speedup_caption = ""Geometric mean speedup \n""
        speedup_summary = self.exec_summary_text(
            speedup_caption, self.geomean, ""speedup""
        )

        passrate_caption = ""Passrate\n""
        passrate_summary = self.exec_summary_text(
            passrate_caption, self.passrate, ""speedup""
        )

        comp_time_caption = ""Mean compilation time (seconds)\n""
        comp_time_summary = self.exec_summary_text(
            comp_time_caption, self.comp_time, ""compilation_latency""
        )

        peak_memory_caption = (
            ""Peak memory footprint compression ratio (higher is better)\n""
        )
        peak_memory_summary = self.exec_summary_text(
            peak_memory_caption, self.memory, ""compression_ratio""
        )

        str_io.write(
            ""To measure performance, compilation latency and memory footprint reduction, ""
            ""we remove the models that fail accuracy checks.\n\n""
        )
        str_io.write(passrate_summary)
        str_io.write(speedup_summary)
        str_io.write(comp_time_summary)
        str_io.write(peak_memory_summary)
        self.executive_summary = str_io.getvalue()"
flag_bad_entries;"def flag_bad_entries(self, suite, metric, flag_fn):
        df = self.untouched_parsed_frames[suite][metric]
        df = df.drop(""dev"", axis=1)
        df = df.rename(columns={""batch_size"": ""bs""})
        # apply flag_fn elementwise to flag_compilers columns,
        # if one element fails, the entire row is flagged
        flag = np.logical_or.reduce(
            df[self.flag_compilers].applymap(flag_fn),
            axis=1,
        )
        df = df[flag]
        df = df.assign(suite=suite)
        return df.reindex(columns=[""suite"", ""name""] + self.flag_compilers)"
generate_warnings;"def generate_warnings(self):
        title = ""## Warnings ##""
        body = (
            ""We flag models where:\n\n""
            "" - accuracy fails\n""
            "" - speedup < 0.95x (NOTE: 0.0 speedup typically signifies a failure in the performance test)\n""
            "" - compilation latency > 120 sec.\n""
            "" - compression ratio < 0.9\n""
            ""\n""
        )
        for metric in [
            ""accuracy"",
            ""speedup"",
            ""compilation_latency"",
            ""compression_ratio"",
        ]:
            dfs = []
            for suite in self.suites:
                dfs.append(self.flag_bad_entries(suite, metric, FLAG_FNS[metric]))
            df = pd.concat(dfs, axis=0)
            if df.empty:
                continue
            tabform = tabulate(df, headers=""keys"", tablefmt=""pretty"", showindex=""never"")
            str_io = io.StringIO()
            str_io.write(""\n"")
            str_io.write(get_metric_title(metric) + "" warnings\n"")
            str_io.write(""~~~\n"")
            str_io.write(f""{tabform}\n"")
            str_io.write(""~~~\n"")
            body += str_io.getvalue()

        comment = generate_dropdown_comment(title, body)
        return comment"
prepare_message;"def prepare_message(self, suite):
        title = f""## {suite} suite with {self.dtypes[0]} precision ##""
        body = """"
        for metric in [
            ""speedup"",
            ""accuracy"",
            ""compilation_latency"",
            ""compression_ratio"",
            ""abs_latency"",
        ]:
            df = self.untouched_parsed_frames[suite][metric]
            df = df.drop(""dev"", axis=1)
            df = df.rename(columns={""batch_size"": ""bs""})
            tabform = tabulate(df, headers=""keys"", tablefmt=""pretty"", showindex=""never"")
            str_io = io.StringIO()
            str_io.write(""\n"")
            str_io.write(get_metric_title(metric) + ""\n"")
            str_io.write(""~~~\n"")
            str_io.write(f""{tabform}\n"")
            str_io.write(""~~~\n"")
            body += str_io.getvalue()

        comment = generate_dropdown_comment(title, body)
        return comment"
gen_summary_files;"def gen_summary_files(self):
        self.generate_executive_summary()
        for suite in self.suites:
            self.plot_graph(
                self.untouched_parsed_frames[suite][""speedup""],
                f""{suite}_{self.dtypes[0]}"",
            )

        with open(f""{self.output_dir}/gh_title.txt"", ""w"") as gh_fh:
            str_io = io.StringIO()
            str_io.write(""\n"")
            str_io.write(f""# Performance Dashboard for {self.dtypes[0]} precision ##\n"")
            str_io.write(""\n"")
            gh_fh.write(str_io.getvalue())

        with open(f""{self.output_dir}/gh_executive_summary.txt"", ""w"") as gh_fh:
            gh_fh.write(self.executive_summary)

        with open(f""{self.output_dir}/gh_warnings.txt"", ""w"") as gh_fh:
            warnings_body = self.generate_warnings()
            gh_fh.write(warnings_body)

        str_io = io.StringIO()
        for suite in self.suites:
            str_io.write(self.prepare_message(suite))
        str_io.write(""\n"")
        with open(f""{self.output_dir}/gh_{self.mode}.txt"", ""w"") as gh_fh:
            gh_fh.write(str_io.getvalue())"
generate_diff;"def generate_diff(self, last2, filename, caption):
        df_cur, df_prev = (pd.read_csv(os.path.join(path, filename)) for path in last2)
        df_merge = df_cur.merge(df_prev, on=""Compiler"", suffixes=(""_cur"", ""_prev""))
        data = {col: [] for col in (""compiler"", ""suite"", ""prev_value"", ""cur_value"")}
        for _, row in df_merge.iterrows():
            if row[""Compiler""] in self.args.flag_compilers:
                for suite in self.args.suites:
                    if suite + ""_prev"" not in row or suite + ""_cur"" not in row:
                        continue
                    data[""compiler""].append(row[""Compiler""])
                    data[""suite""].append(suite)
                    data[""prev_value""].append(row[suite + ""_prev""])
                    data[""cur_value""].append(row[suite + ""_cur""])

        df = pd.DataFrame(data)
        tabform = tabulate(df, headers=""keys"", tablefmt=""pretty"", showindex=""never"")
        str_io = io.StringIO()
        str_io.write(""\n"")
        str_io.write(f""{caption}\n"")
        str_io.write(""~~~\n"")
        str_io.write(f""{tabform}\n"")
        str_io.write(""~~~\n"")
        return str_io.getvalue()"
generate_comment;"def generate_comment(self):
        title = ""## Summary Statistics Diff ##\n""
        body = (
            ""For each relevant compiler, we compare the summary statistics ""
            ""for the most 2 recent reports that actually run the compiler.\n\n""
        )
        dtype = self.args.dtypes[0]
        last2 = find_last_2_with_filenames(
            self.lookup_file,
            self.args.dashboard_archive_path,
            dtype,
            [""geomean.csv"", ""passrate.csv""],
        )

        if last2 is None:
            body += ""Could not find most 2 recent reports.\n\n""
        else:
            for state, path in zip((""Current"", ""Previous""), last2):
                body += f""{state} report name: {path}\n\n""
            body += self.generate_diff(last2, ""passrate.csv"", ""Passrate diff"")
            body += self.generate_diff(
                last2, ""geomean.csv"", ""Geometric mean speedup diff""
            )

        comment = generate_dropdown_comment(title, body)

        with open(f""{self.args.output_dir}/gh_summary_diff.txt"", ""w"") as gh_fh:
            gh_fh.write(comment)"
generate_comment;"def generate_comment(self):
        title = ""## Recent Regressions ##\n""
        body = (
            ""For each relevant compiler, we compare the most recent 2 reports ""
            ""(that actually run the compiler) to find previously unflagged ""
            ""models that are now flagged as problematic (according to the ""
            ""'Warnings' section).\n\n""
        )
        dtype = self.args.dtypes[0]
        device = self.args.devices[0]
        for suite in self.args.suites:
            body += f""### Regressions for {suite} ###\n""
            last2 = {}

            for compiler in self.args.flag_compilers:
                filenames = [
                    generate_csv_name(
                        self.args, dtype, suite, device, compiler, testing
                    )
                    for testing in [""performance"", ""accuracy""]
                ]
                compiler_last2 = find_last_2_with_filenames(
                    self.lookup_file, self.args.dashboard_archive_path, dtype, filenames
                )
                if compiler_last2 is not None:
                    last2[compiler] = [
                        ParsePerformanceLogs(
                            [suite],
                            [device],
                            [dtype],
                            [compiler],
                            [compiler],
                            get_mode(self.args),
                            output_dir,
                        )
                        for output_dir in compiler_last2
                    ]
                    for state, path in zip((""Current"", ""Previous""), compiler_last2):
                        body += (
                            f""{state} report name (compiler: {compiler}, ""
                            f""suite: {suite}): {path}\n\n""
                        )

            regressions_present = False
            for metric in [
                ""accuracy"",
                ""speedup"",
                ""compilation_latency"",
                ""compression_ratio"",
            ]:
                dfs = []
                for compiler in self.args.flag_compilers:
                    if last2[compiler] is None:
                        continue

                    df_cur, df_prev = (
                        last2[compiler][i].untouched_parsed_frames[suite][metric]
                        for i in (0, 1)
                    )
                    df_merge = df_cur.merge(
                        df_prev, on=""name"", suffixes=(""_cur"", ""_prev"")
                    )
                    flag_fn = FLAG_FNS[metric]
                    flag = np.logical_and(
                        df_merge[compiler + ""_prev""].apply(
                            lambda x: not pd.isna(x) and not flag_fn(x)
                        ),
                        df_merge[compiler + ""_cur""].apply(
                            lambda x: not pd.isna(x) and flag_fn(x)
                        ),
                    )
                    df_bad = df_merge[flag]
                    dfs.append(
                        pd.DataFrame(
                            data={
                                ""compiler"": compiler,
                                ""name"": df_bad[""name""],
                                ""prev_status"": df_bad[compiler + ""_prev""],
                                ""cur_status"": df_bad[compiler + ""_cur""],
                            }
                        )
                    )

                if not dfs:
                    continue
                df = pd.concat(dfs, axis=0)
                if df.empty:
                    continue
                regressions_present = True
                tabform = tabulate(
                    df, headers=""keys"", tablefmt=""pretty"", showindex=""never""
                )
                str_io = io.StringIO()
                str_io.write(""\n"")
                str_io.write(f""{get_metric_title(metric)} regressions\n"")
                str_io.write(""~~~\n"")
                str_io.write(f""{tabform}\n"")
                str_io.write(""~~~\n"")
                body += str_io.getvalue()

            if not regressions_present:
                body += ""No regressions found.\n""

        comment = generate_dropdown_comment(title, body)

        with open(f""{self.args.output_dir}/gh_metric_regression.txt"", ""w"") as gh_fh:
            gh_fh.write(comment)"
find_last_k;"def find_last_k(self):
        """"""
        Find the last k pairs of (day number, log_path)
        """"""
        dtype = self.args.dtypes[0]
        df = pd.read_csv(self.lookup_file, names=(""day"", ""mode"", ""prec"", ""path""))
        df = df[df[""mode""] == ""performance""]
        df = df[df[""prec""] == dtype]
        log_infos = []
        for day, path in zip(df[""day""], df[""path""]):
            log_infos.append(LogInfo(day, path))

        assert len(log_infos) >= self.k
        log_infos = log_infos[len(log_infos) - self.k :]
        return log_infos"
generate_comment;"def generate_comment(self):
        title = ""## Metrics over time ##\n""
        str_io = io.StringIO()
        if not self.args.update_dashboard_test and not self.args.no_graphs:
            for name in glob.glob(self.args.output_dir + ""/*over_time.png""):
                output = (
                    subprocess.check_output([self.args.dashboard_image_uploader, name])
                    .decode(""ascii"")
                    .rstrip()
                )
                str_io.write(f""\n{name} : ![]({output})\n"")
        comment = generate_dropdown_comment(title, str_io.getvalue())

        with open(f""{self.args.output_dir}/gh_regression.txt"", ""w"") as gh_fh:
            gh_fh.write(comment)"
update_lookup_file;"def update_lookup_file(self):
        dtype = self.args.dtypes[0]
        day, _ = archive_data(self.args.archive_name)
        target_dir = get_archive_name(self.args, dtype)
        # Update lookup csv the folder to arhived logs
        subprocess.check_call(
            f'echo ""{day},performance,{dtype},{target_dir}"" >> {self.lookup_file}',
            shell=True,
        )"
upload_graphs;"def upload_graphs(self):
        title = ""## Performance graphs ##\n""
        str_io = io.StringIO()
        if not self.args.update_dashboard_test and not self.args.no_graphs:
            for name in glob.glob(self.output_dir + ""/*png""):
                if ""over_time"" not in name:
                    output = (
                        subprocess.check_output(
                            [self.args.dashboard_image_uploader, name]
                        )
                        .decode(""ascii"")
                        .rstrip()
                    )
                    str_io.write(f""\n{name} : ![]({output})\n"")
        comment = generate_dropdown_comment(title, str_io.getvalue())

        with open(f""{self.output_dir}/gh_graphs.txt"", ""w"") as gh_fh:
            gh_fh.write(comment)"
gen_comment;"def gen_comment(self):
        files = [
            ""gh_title.txt"",
            ""gh_executive_summary.txt"",
            ""gh_summary_diff.txt"",
            ""gh_warnings.txt"",
            ""gh_regression.txt"",
            ""gh_metric_regression.txt"",
            ""gh_training.txt"" if self.args.training else ""gh_inference.txt"",
            ""gh_graphs.txt"",
            ""gh_build_summary.txt"",
        ]
        all_lines = []
        for f in files:
            try:
                with open(os.path.join(self.output_dir, f)) as fh:
                    all_lines.extend(fh.readlines())
            except FileNotFoundError:
                pass

        return ""\n"".join([x.rstrip() for x in all_lines])"
comment_on_gh;"def comment_on_gh(self, comment):
        """"""
        Send a commment to dashboard
        """"""
        with tempfile.NamedTemporaryFile(mode=""w"", delete=False) as f:
            f.write(comment)
            filename = f.name

        issue_number = ""93794""
        if self.args.dtypes[0] == ""float32"":
            issue_number = ""93518""

        subprocess.check_call(
            [
                self.args.dashboard_gh_cli_path,
                ""issue"",
                ""comment"",
                ""--repo=https://github.com/pytorch/pytorch.git"",
                issue_number,
                ""-F"",
                filename,
            ]
        )

        os.remove(filename)"
find_csv_files;"def find_csv_files(path, perf_compare):
    """"""
    Recursively search for all CSV files in directory and subdirectories whose
    name contains a target string.
    """"""

    def is_csv(f):
        if perf_compare:
            regex = r""training_(torchbench|huggingface|timm_models)\.csv""
            return re.match(regex, f) is not None
        else:
            return f.endswith(""_performance.csv"")

    csv_files = []
    for root, dirs, files in os.walk(path):
        for file in files:
            if is_csv(file):
                csv_files.append(os.path.join(root, file))
    return csv_files"
is_csv;"def is_csv(f):
        if perf_compare:
            regex = r""training_(torchbench|huggingface|timm_models)\.csv""
            return re.match(regex, f) is not None
        else:
            return f.endswith(""_performance.csv"")"
is_asan_or_tsan;"def is_asan_or_tsan():
        return False"
pip_install;"def pip_install(package):
    subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])"
refresh_model_names;"def refresh_model_names():
    import glob

    from timm.models import list_models

    def read_models_from_docs():
        models = set()
        # TODO - set the path to pytorch-image-models repo
        for fn in glob.glob(""../pytorch-image-models/docs/models/*.md""):
            with open(fn) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if not line.startswith(""model = timm.create_model(""):
                        continue

                    model = line.split(""'"")[1]
                    # print(model)
                    models.add(model)
        return models

    def get_family_name(name):
        known_families = [
            ""darknet"",
            ""densenet"",
            ""dla"",
            ""dpn"",
            ""ecaresnet"",
            ""halo"",
            ""regnet"",
            ""efficientnet"",
            ""deit"",
            ""mobilevit"",
            ""mnasnet"",
            ""convnext"",
            ""resnet"",
            ""resnest"",
            ""resnext"",
            ""selecsls"",
            ""vgg"",
            ""xception"",
        ]

        for known_family in known_families:
            if known_family in name:
                return known_family

        if name.startswith(""gluon_""):
            return ""gluon_"" + name.split(""_"")[1]
        return name.split(""_"")[0]

    def populate_family(models):
        family = {}
        for model_name in models:
            family_name = get_family_name(model_name)
            if family_name not in family:
                family[family_name] = []
            family[family_name].append(model_name)
        return family

    docs_models = read_models_from_docs()
    all_models = list_models(pretrained=True, exclude_filters=[""*in21k""])

    all_models_family = populate_family(all_models)
    docs_models_family = populate_family(docs_models)

    for key in docs_models_family:
        del all_models_family[key]

    chosen_models = set()
    chosen_models.update(value[0] for value in docs_models_family.values())

    chosen_models.update(value[0] for key, value in all_models_family.items())

    filename = ""timm_models_list.txt""
    if os.path.exists(""benchmarks""):
        filename = ""benchmarks/"" + filename
    with open(filename, ""w"") as fw:
        for model_name in sorted(chosen_models):
            fw.write(model_name + ""\n"")"
timm_main;"def timm_main():
    logging.basicConfig(level=logging.WARNING)
    warnings.filterwarnings(""ignore"")
    main(TimmRunner())"
read_models_from_docs;"def read_models_from_docs():
        models = set()
        # TODO - set the path to pytorch-image-models repo
        for fn in glob.glob(""../pytorch-image-models/docs/models/*.md""):
            with open(fn) as f:
                while True:
                    line = f.readline()
                    if not line:
                        break
                    if not line.startswith(""model = timm.create_model(""):
                        continue

                    model = line.split(""'"")[1]
                    # print(model)
                    models.add(model)
        return models"
get_family_name;"def get_family_name(name):
        known_families = [
            ""darknet"",
            ""densenet"",
            ""dla"",
            ""dpn"",
            ""ecaresnet"",
            ""halo"",
            ""regnet"",
            ""efficientnet"",
            ""deit"",
            ""mobilevit"",
            ""mnasnet"",
            ""convnext"",
            ""resnet"",
            ""resnest"",
            ""resnext"",
            ""selecsls"",
            ""vgg"",
            ""xception"",
        ]

        for known_family in known_families:
            if known_family in name:
                return known_family

        if name.startswith(""gluon_""):
            return ""gluon_"" + name.split(""_"")[1]
        return name.split(""_"")[0]"
populate_family;"def populate_family(models):
        family = {}
        for model_name in models:
            family_name = get_family_name(model_name)
            if family_name not in family:
                family[family_name] = []
            family[family_name].append(model_name)
        return family"
skip_models;"def skip_models(self):
        return self._skip[""all""]"
force_amp_for_fp16_bf16_models;"def force_amp_for_fp16_bf16_models(self):
        return FORCE_AMP_FOR_FP16_BF16_MODELS"
force_fp16_for_bf16_models;"def force_fp16_for_bf16_models(self):
        return set()"
get_output_amp_train_process_func;"def get_output_amp_train_process_func(self):
        return {}"
skip_accuracy_check_as_eager_non_deterministic;"def skip_accuracy_check_as_eager_non_deterministic(self):
        if self.args.accuracy and self.args.training:
            return SKIP_ACCURACY_CHECK_AS_EAGER_NON_DETERMINISTIC_MODELS
        return set()"
guard_on_nn_module_models;"def guard_on_nn_module_models(self):
        return {
            ""convit_base"",
        }"
inline_inbuilt_nn_modules_models;"def inline_inbuilt_nn_modules_models(self):
        return {
            ""lcnet_050"",
        }"
_download_model;"def _download_model(self, model_name):
        model = create_model(
            model_name,
            in_chans=3,
            scriptable=False,
            num_classes=None,
            drop_rate=0.0,
            drop_path_rate=None,
            drop_block_rate=None,
            pretrained=True,
        )
        return model"
load_model;"def load_model(
        self,
        device,
        model_name,
        batch_size=None,
        extra_args=None,
    ):
        if self.args.enable_activation_checkpointing:
            raise NotImplementedError(
                ""Activation checkpointing not implemented for Timm models""
            )

        is_training = self.args.training
        use_eval_mode = self.args.use_eval_mode

        channels_last = self._args.channels_last
        model = self._download_model(model_name)

        if model is None:
            raise RuntimeError(f""Failed to load model '{model_name}'"")
        model.to(
            device=device,
            memory_format=torch.channels_last if channels_last else None,
        )

        self.num_classes = model.num_classes

        data_config = resolve_data_config(
            vars(self._args) if timmversion >= ""0.8.0"" else self._args,
            model=model,
            use_test_size=not is_training,
        )
        input_size = data_config[""input_size""]
        recorded_batch_size = TIMM_MODELS[model_name]

        if model_name in BATCH_SIZE_DIVISORS:
            recorded_batch_size = max(
                int(recorded_batch_size / BATCH_SIZE_DIVISORS[model_name]), 1
            )
        batch_size = batch_size or recorded_batch_size

        torch.manual_seed(1337)
        input_tensor = torch.randint(
            256, size=(batch_size,) + input_size, device=device
        ).to(dtype=torch.float32)
        mean = torch.mean(input_tensor)
        std_dev = torch.std(input_tensor)
        example_inputs = (input_tensor - mean) / std_dev

        if channels_last:
            example_inputs = example_inputs.contiguous(
                memory_format=torch.channels_last
            )
        example_inputs = [
            example_inputs,
        ]
        self.target = self._gen_target(batch_size, device)

        self.loss = torch.nn.CrossEntropyLoss().to(device)

        if model_name in SCALED_COMPUTE_LOSS:
            self.compute_loss = self.scaled_compute_loss

        if is_training and not use_eval_mode:
            model.train()
        else:
            model.eval()

        self.validate_model(model, example_inputs)

        return device, model_name, model, example_inputs, batch_size"
iter_model_names;"def iter_model_names(self, args):
        # for model_name in list_models(pretrained=True, exclude_filters=[""*in21k""]):
        model_names = sorted(TIMM_MODELS.keys())
        start, end = self.get_benchmark_indices(len(model_names))
        for index, model_name in enumerate(model_names):
            if index < start or index >= end:
                continue
            if (
                not re.search(""|"".join(args.filter), model_name, re.IGNORECASE)
                or re.search(""|"".join(args.exclude), model_name, re.IGNORECASE)
                or model_name in args.exclude_exact
                or model_name in self.skip_models
            ):
                continue

            yield model_name"
pick_grad;"def pick_grad(self, name, is_training):
        if is_training:
            return torch.enable_grad()
        else:
            return torch.no_grad()"
use_larger_multiplier_for_smaller_tensor;"def use_larger_multiplier_for_smaller_tensor(self, name):
        return name in REQUIRE_LARGER_MULTIPLIER_FOR_SMALLER_TENSOR"
get_tolerance_and_cosine_flag;"def get_tolerance_and_cosine_flag(self, is_training, current_device, name):
        cosine = self.args.cosine
        tolerance = 1e-3

        if self.args.freezing and name in REQUIRE_HIGHER_TOLERANCE_FOR_FREEZING:
            # the conv-batchnorm fusion used under freezing may cause relatively
            # large numerical difference. We need are larger tolerance.
            # Check https://github.com/pytorch/pytorch/issues/120545 for context
            tolerance = 8 * 1e-2

        if is_training:
            from torch._inductor import config as inductor_config

            if name in REQUIRE_EVEN_HIGHER_TOLERANCE or (
                inductor_config.max_autotune
                and name in REQUIRE_EVEN_HIGHER_TOLERANCE_MAX_AUTOTUNE
            ):
                tolerance = 8 * 1e-2
            elif name in REQUIRE_HIGHER_TOLERANCE or (
                self.args.amp and name in REQUIRE_HIGHER_TOLERANCE_AMP
            ):
                tolerance = 4 * 1e-2
            else:
                tolerance = 1e-2
        return tolerance, cosine"
_gen_target;"def _gen_target(self, batch_size, device):
        return torch.empty((batch_size,) + (), device=device, dtype=torch.long).random_(
            self.num_classes
        )"
compute_loss;"def compute_loss(self, pred):
        # High loss values make gradient checking harder, as small changes in
        # accumulation order upsets accuracy checks.
        return reduce_to_scalar_loss(pred)"
scaled_compute_loss;"def scaled_compute_loss(self, pred):
        # Loss values need zoom out further.
        return reduce_to_scalar_loss(pred) / 1000.0"
forward_pass;"def forward_pass(self, mod, inputs, collect_outputs=True):
        with self.autocast(**self.autocast_arg):
            return mod(*inputs)"
forward_and_backward_pass;"def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
        cloned_inputs = clone_inputs(inputs)
        self.optimizer_zero_grad(mod)
        with self.autocast(**self.autocast_arg):
            pred = mod(*cloned_inputs)
            if isinstance(pred, tuple):
                pred = pred[0]
            loss = self.compute_loss(pred)
        self.grad_scaler.scale(loss).backward()
        self.optimizer_step()
        if collect_outputs:
            return collect_results(mod, None, loss, cloned_inputs)
        return None"
setup_baseline;"def setup_baseline():
    from torchao.quantization.utils import recommended_inductor_config_setter

    recommended_inductor_config_setter()
    torch._dynamo.config.automatic_dynamic_shapes = False
    torch._dynamo.config.recompile_limit = 10000"
torchao_optimize_ctx;"def torchao_optimize_ctx(quantization: str):
    from torchao.quantization.quant_api import (
        autoquant,
        int4_weight_only,
        int8_dynamic_activation_int8_weight,
        int8_weight_only,
        quantize_,
    )
    from torchao.utils import unwrap_tensor_subclass

    def inner(model_iter_fn: Callable):
        def _torchao_apply(module: torch.nn.Module, example_inputs: Any):
            if getattr(module, ""_quantized"", None) is None:
                if quantization == ""int8dynamic"":
                    quantize_(
                        module,
                        int8_dynamic_activation_int8_weight(),
                        set_inductor_config=False,
                    )
                elif quantization == ""int8weightonly"":
                    quantize_(module, int8_weight_only(), set_inductor_config=False)
                elif quantization == ""int4weightonly"":
                    quantize_(module, int4_weight_only(), set_inductor_config=False)
                if quantization == ""autoquant"":
                    autoquant(module, error_on_unseen=False, set_inductor_config=False)
                    if isinstance(example_inputs, dict):
                        module(**example_inputs)
                    else:
                        module(*example_inputs)
                    from torchao.quantization.autoquant import AUTOQUANT_CACHE

                    if len(AUTOQUANT_CACHE) == 0:
                        raise Exception(  # noqa: TRY002
                            ""NotAutoquantizable""
                            f""Found no autoquantizable layers in model {type(module)}, stopping autoquantized run""
                        )
                else:
                    unwrap_tensor_subclass(module)
                setattr(module, ""_quantized"", True)  # noqa: B010
            model_iter_fn(module, example_inputs)

        return _torchao_apply

    return inner"
_torchao_apply;"def _torchao_apply(module: torch.nn.Module, example_inputs: Any):
            if getattr(module, ""_quantized"", None) is None:
                if quantization == ""int8dynamic"":
                    quantize_(
                        module,
                        int8_dynamic_activation_int8_weight(),
                        set_inductor_config=False,
                    )
                elif quantization == ""int8weightonly"":
                    quantize_(module, int8_weight_only(), set_inductor_config=False)
                elif quantization == ""int4weightonly"":
                    quantize_(module, int4_weight_only(), set_inductor_config=False)
                if quantization == ""autoquant"":
                    autoquant(module, error_on_unseen=False, set_inductor_config=False)
                    if isinstance(example_inputs, dict):
                        module(**example_inputs)
                    else:
                        module(*example_inputs)
                    from torchao.quantization.autoquant import AUTOQUANT_CACHE

                    if len(AUTOQUANT_CACHE) == 0:
                        raise Exception(  # noqa: TRY002
                            ""NotAutoquantizable""
                            f""Found no autoquantizable layers in model {type(module)}, stopping autoquantized run""
                        )
                else:
                    unwrap_tensor_subclass(module)
                setattr(module, ""_quantized"", True)  # noqa: B010
            model_iter_fn(module, example_inputs)"
_reassign_parameters;"def _reassign_parameters(model):
    # torch_geometric models register parameter as tensors due to
    # https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/dense/linear.py#L158-L168
    # Since it is unusual thing to do, we just reassign them to parameters
    def state_dict_hook(module, destination, prefix, local_metadata):
        for name, param in module.named_parameters():
            if isinstance(destination[name], torch.Tensor) and not isinstance(
                destination[name], torch.nn.Parameter
            ):
                destination[name] = torch.nn.Parameter(destination[name])

    model._register_state_dict_hook(state_dict_hook)"
setup_torchbench_cwd;"def setup_torchbench_cwd():
    original_dir = abspath(os.getcwd())

    os.environ[""KALDI_ROOT""] = ""/tmp""  # avoids some spam
    for torchbench_dir in (
        ""./torchbenchmark"",
        ""../torchbenchmark"",
        ""../torchbench"",
        ""../benchmark"",
        ""../../torchbenchmark"",
        ""../../torchbench"",
        ""../../benchmark"",
        ""../../../torchbenchmark"",
        ""../../../torchbench"",
        ""../../../benchmark"",
    ):
        if exists(torchbench_dir):
            break

    if exists(torchbench_dir):
        torchbench_dir = abspath(torchbench_dir)
        os.chdir(torchbench_dir)
        sys.path.append(torchbench_dir)

    return original_dir"
process_hf_reformer_output;"def process_hf_reformer_output(out):
    assert isinstance(out, list)
    # second output is unstable
    return [elem for i, elem in enumerate(out) if i != 1]"
process_hf_whisper_output;"def process_hf_whisper_output(out):
    out_ret = []
    for i, elem in enumerate(out):
        if i == 0:
            if elem is not None:
                assert isinstance(elem, dict)
                out_ret.append({k: v for k, v in elem.items() if k != ""logits""})
        elif i != 1:
            out_ret.append(elem)

    return out_ret"
torchbench_main;"def torchbench_main():
    original_dir = setup_torchbench_cwd()
    logging.basicConfig(level=logging.WARNING)
    warnings.filterwarnings(""ignore"")
    main(TorchBenchmarkRunner(), original_dir)"
state_dict_hook;"def state_dict_hook(module, destination, prefix, local_metadata):
        for name, param in module.named_parameters():
            if isinstance(destination[name], torch.Tensor) and not isinstance(
                destination[name], torch.nn.Parameter
            ):
                destination[name] = torch.nn.Parameter(destination[name])"
_require_larger_multiplier_for_smaller_tensor;"def _require_larger_multiplier_for_smaller_tensor(self):
        return self._config[""require_larger_multiplier_for_smaller_tensor""]"
skip_models;"def skip_models(self):
        return self._skip[""all""]"
skip_models_for_cpu;"def skip_models_for_cpu(self):
        return self._skip[""device""][""cpu""]"
skip_models_for_cuda;"def skip_models_for_cuda(self):
        return self._skip[""device""][""cuda""]"
skip_models_for_freezing_cuda;"def skip_models_for_freezing_cuda(self):
        return self._skip[""freezing""][""cuda""]"
disable_cudagraph_models;"def disable_cudagraph_models(self):
        return self._config[""disable_cudagraph""]"
skip_models_for_freezing_cpu;"def skip_models_for_freezing_cpu(self):
        return self._skip[""freezing""][""cpu""]"
slow_models;"def slow_models(self):
        return self._config[""slow""]"
very_slow_models;"def very_slow_models(self):
        return self._config[""very_slow""]"
non_deterministic_models;"def non_deterministic_models(self):
        return self._config[""non_deterministic""]"
get_output_amp_train_process_func;"def get_output_amp_train_process_func(self):
        return process_train_model_output"
skip_not_suitable_for_training_models;"def skip_not_suitable_for_training_models(self):
        return self._skip[""test""][""training""]"
failing_fx2trt_models;"def failing_fx2trt_models(self):
        return self._config[""trt_not_yet_working""]"
force_amp_for_fp16_bf16_models;"def force_amp_for_fp16_bf16_models(self):
        return self._config[""dtype""][""force_amp_for_fp16_bf16_models""]"
force_fp16_for_bf16_models;"def force_fp16_for_bf16_models(self):
        return self._config[""dtype""][""force_fp16_for_bf16_models""]"
skip_accuracy_checks_large_models_dashboard;"def skip_accuracy_checks_large_models_dashboard(self):
        if self.args.dashboard or self.args.accuracy:
            return self._accuracy[""skip""][""large_models""]
        return set()"
skip_accuracy_check_as_eager_non_deterministic;"def skip_accuracy_check_as_eager_non_deterministic(self):
        if self.args.accuracy and self.args.training:
            return self._accuracy[""skip""][""eager_not_deterministic""]
        return set()"
skip_multiprocess_models;"def skip_multiprocess_models(self):
        return self._skip[""multiprocess""]"
skip_models_due_to_control_flow;"def skip_models_due_to_control_flow(self):
        return self._skip[""control_flow""]"
skip_models_due_to_export_not_supported;"def skip_models_due_to_export_not_supported(self):
        return self._skip[""export_not_supported""]"
guard_on_nn_module_models;"def guard_on_nn_module_models(self):
        return {
            ""vision_maskrcnn"",
        }"
inline_inbuilt_nn_modules_models;"def inline_inbuilt_nn_modules_models(self):
        return {
            ""basic_gnn_edgecnn"",
            ""drq"",
            ""hf_Reformer"",
            ""DALLE2_pytorch"",
            ""hf_BigBird"",
            ""detectron2_maskrcnn_r_50_fpn"",
            ""detectron2_maskrcnn_r_101_fpn"",
            ""vision_maskrcnn"",
            ""doctr_reco_predictor"",
            ""hf_T5_generate"",
        }"
load_model;"def load_model(
        self,
        device,
        model_name,
        batch_size=None,
        part=None,
        extra_args=None,
    ):
        if self.args.enable_activation_checkpointing:
            raise NotImplementedError(
                ""Activation checkpointing not implemented for Torchbench models""
            )
        is_training = self.args.training
        use_eval_mode = self.args.use_eval_mode
        candidates = [
            f""torchbenchmark.models.{model_name}"",
            f""torchbenchmark.canary_models.{model_name}"",
            f""torchbenchmark.models.fb.{model_name}"",
        ]
        for c in candidates:
            try:
                module = importlib.import_module(c)
                break
            except ModuleNotFoundError as e:
                if e.name != c:
                    raise
        else:
            raise ImportError(f""could not import any of {candidates}"")
        benchmark_cls = getattr(module, ""Model"", None)
        if benchmark_cls is None:
            raise NotImplementedError(f""{model_name}.Model is None"")

        if not hasattr(benchmark_cls, ""name""):
            benchmark_cls.name = model_name

        cant_change_batch_size = (
            not getattr(benchmark_cls, ""ALLOW_CUSTOMIZE_BSIZE"", True)
            or model_name in self._config[""dont_change_batch_size""]
        )
        if cant_change_batch_size:
            batch_size = None
        if (
            batch_size is None
            and is_training
            and model_name in self._batch_size[""training""]
        ):
            batch_size = self._batch_size[""training""][model_name]
        elif (
            batch_size is None
            and not is_training
            and model_name in self._batch_size[""inference""]
        ):
            batch_size = self._batch_size[""inference""][model_name]

        # Control the memory footprint for few models
        if self.args.accuracy and model_name in self._accuracy[""max_batch_size""]:
            batch_size = min(batch_size, self._accuracy[""max_batch_size""][model_name])

        # workaround ""RuntimeError: not allowed to set torch.backends.cudnn flags""
        torch.backends.__allow_nonbracketed_mutation_flag = True
        if extra_args is None:
            extra_args = []
        if part:
            extra_args += [""--part"", part]

        # sam_fast only runs with amp
        if model_name == ""sam_fast"":
            self.args.amp = True
            self.setup_amp()

        if model_name == ""vision_maskrcnn"" and is_training:
            # Output of vision_maskrcnn model is a list of bounding boxes,
            # sorted on the basis of their scores. This makes accuracy
            # comparison hard with torch.compile. torch.compile can cause minor
            # divergences in the output because of how fusion works for amp in
            # TorchInductor compared to eager.  Therefore, instead of looking at
            # all the bounding boxes, we compare only top 4.
            model_kwargs = {""box_detections_per_img"": 4}
            benchmark = benchmark_cls(
                test=""train"",
                device=device,
                batch_size=batch_size,
                extra_args=extra_args,
                model_kwargs=model_kwargs,
            )
            use_eval_mode = True
        elif is_training:
            benchmark = benchmark_cls(
                test=""train"",
                device=device,
                batch_size=batch_size,
                extra_args=extra_args,
            )
        else:
            benchmark = benchmark_cls(
                test=""eval"",
                device=device,
                batch_size=batch_size,
                extra_args=extra_args,
            )
        model, example_inputs = benchmark.get_module()
        if model_name in [
            ""basic_gnn_edgecnn"",
            ""basic_gnn_gcn"",
            ""basic_gnn_sage"",
            ""basic_gnn_gin"",
        ]:
            _reassign_parameters(model)

        # Models that must be in train mode while training
        if is_training and (
            not use_eval_mode or model_name in self._config[""only_training""]
        ):
            model.train()
        else:
            model.eval()
        gc.collect()
        batch_size = benchmark.batch_size
        if model_name == ""torchrec_dlrm"":
            batch_namedtuple = namedtuple(
                ""Batch"", ""dense_features sparse_features labels""
            )
            example_inputs = tuple(
                batch_namedtuple(
                    dense_features=batch.dense_features,
                    sparse_features=batch.sparse_features,
                    labels=batch.labels,
                )
                for batch in example_inputs
            )
        # Torchbench has quite different setup for yolov3, so directly passing
        # the right example_inputs
        if model_name == ""yolov3"":
            example_inputs = (torch.rand(batch_size, 3, 384, 512).to(device),)
        # See https://github.com/pytorch/benchmark/issues/1561
        if model_name == ""maml_omniglot"":
            batch_size = 5
            assert example_inputs[0].shape[0] == batch_size
        if model_name == ""vision_maskrcnn"":
            batch_size = 1
        # global current_name, current_device
        # current_device = device
        # current_name = benchmark.name

        if self.args.trace_on_xla:
            # work around for: https://github.com/pytorch/xla/issues/4174
            import torch_xla  # noqa: F401
        self.validate_model(model, example_inputs)
        return device, benchmark.name, model, example_inputs, batch_size"
iter_model_names;"def iter_model_names(self, args):
        from torchbenchmark import _list_canary_model_paths, _list_model_paths

        models = _list_model_paths()
        models += [
            f
            for f in _list_canary_model_paths()
            if os.path.basename(f) in self._config[""canary_models""]
        ]
        models.sort()

        start, end = self.get_benchmark_indices(len(models))
        for index, model_path in enumerate(models):
            if index < start or index >= end:
                continue

            model_name = os.path.basename(model_path)
            if (
                not re.search(""|"".join(args.filter), model_name, re.IGNORECASE)
                or re.search(""|"".join(args.exclude), model_name, re.IGNORECASE)
                or model_name in args.exclude_exact
                or model_name in self.skip_models
            ):
                continue

            yield model_name"
pick_grad;"def pick_grad(self, name, is_training):
        if is_training or name in (""maml"",):
            return torch.enable_grad()
        else:
            return torch.no_grad()"
use_larger_multiplier_for_smaller_tensor;"def use_larger_multiplier_for_smaller_tensor(self, name):
        return name in self._require_larger_multiplier_for_smaller_tensor"
get_tolerance_and_cosine_flag;"def get_tolerance_and_cosine_flag(self, is_training, current_device, name):
        tolerance = 1e-4
        cosine = self.args.cosine
        # Increase the tolerance for torch allclose
        if self.args.float16 or self.args.amp:
            if self.args.freezing and (freezing := self._tolerance[""freezing""]):
                higher_fp16 = freezing.get(""higher_fp16"", None)
                even_higher = freezing.get(""even_higher"", None)
                if higher_fp16 and name in higher_fp16:
                    return 1e-2, cosine
                elif even_higher and name in even_higher:
                    return 8 * 1e-2, cosine
            if name in self._tolerance[""higher_fp16""]:
                return 1e-2, cosine
            elif name in self._tolerance[""even_higher""]:
                return 8 * 1e-2, cosine
            return 1e-3, cosine

        if self.args.bfloat16:
            if name in self._tolerance[""higher_bf16""]:
                return 1e-2, cosine

        if is_training and (current_device == ""cuda"" or current_device == ""xpu""):
            tolerance = 1e-3
            if name in self._tolerance[""cosine""]:
                cosine = True
            elif name in self._tolerance[""higher""]:
                tolerance = 1e-3
            elif name in self._tolerance[""even_higher""]:
                tolerance = 8 * 1e-2
        return tolerance, cosine"
compute_loss;"def compute_loss(self, pred):
        return reduce_to_scalar_loss(pred)"
forward_pass;"def forward_pass(self, mod, inputs, collect_outputs=True):
        with self.autocast(**self.autocast_arg):
            if isinstance(inputs, dict):
                return mod(**inputs)
            else:
                return mod(*inputs)"
forward_and_backward_pass;"def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
        cloned_inputs = clone_inputs(inputs)
        self.optimizer_zero_grad(mod)
        with self.autocast(**self.autocast_arg):
            if isinstance(cloned_inputs, dict):
                pred = mod(**cloned_inputs)
            else:
                pred = mod(*cloned_inputs)
            loss = self.compute_loss(pred)
        self.grad_scaler.scale(loss).backward()
        self.optimizer_step()
        if collect_outputs:
            return collect_results(mod, None, loss, cloned_inputs)
        return None"
data_processing;"def data_processing(num_samples, batch_size):
    dataset = load_dataset(""yelp_review_full"")
    tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")

    def tokenize_function(examples):
        return tokenizer(examples[""text""], padding=""max_length"", truncation=True)

    tokenized_datasets = dataset.map(tokenize_function, batched=True)

    tokenized_datasets = tokenized_datasets.remove_columns([""text""])
    tokenized_datasets = tokenized_datasets.rename_column(""label"", ""labels"")
    tokenized_datasets.set_format(""torch"")

    small_train_dataset = tokenized_datasets[""train""].select(range(num_samples))
    small_eval_dataset = tokenized_datasets[""test""].select(range(num_samples))

    train_dataloader = DataLoader(small_train_dataset, batch_size=batch_size)
    eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)

    return train_dataloader, eval_dataloader"
training_iter_fn;"def training_iter_fn(batch, model, optimizer):
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    return loss"
model_training_evaluation;"def model_training_evaluation(
    backend, train_dataloader, eval_dataloader, model, optimizer, num_epochs, evaluation
):
    model.to(device)
    model.train()
    loss_history = []
    if not backend:
        # Run with native Pytorch
        opt_training_iter_fn = training_iter_fn
    else:
        # Support backends: eager, aot_eager, aot_nvfuser and inductor
        opt_training_iter_fn = torch._dynamo.optimize(backend)(training_iter_fn)
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, batch in enumerate(train_dataloader, 0):
            batch = {k: v.to(device) for k, v in batch.items()}
            loss = opt_training_iter_fn(batch, model, optimizer)
            running_loss += loss.item()
            if i % 100 == 99:
                loss_history.append(running_loss / 100)
                running_loss = 0.0

    if evaluation:
        metric = load_metric(""accuracy"")
        model.eval()
        if not backend:
            opt_model = model
        else:
            opt_model = torch._dynamo.optimize(backend)(model)
        for batch in eval_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            with torch.no_grad():
                outputs = opt_model(**batch)

            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            metric.add_batch(predictions=predictions, references=batch[""labels""])

        return loss_history, metric.compute()
    else:
        return loss_history, None"
check_loss;"def check_loss(ref_loss, res_loss):
    assert len(ref_loss) == len(res_loss)
    length = len(ref_loss)
    x = min(length, 10)
    return sum(res_loss[-x:]) / 10 <= sum(ref_loss[-x:]) / 10 + 0.1"
parse_args;"def parse_args():
    parser = argparse.ArgumentParser(
        description=""TorchDynamo end to end training/evaluation benchmark""
    )
    parser.add_argument(
        ""--epochs"", type=int, default=10, help=""number of epochs to train (default: 10)""
    )
    parser.add_argument(
        ""--num-samples"",
        type=int,
        default=1000,
        help=""number of samples to train/eval (default: 1000)"",
    )
    parser.add_argument(
        ""--batch-size"",
        type=int,
        default=8,
        help=""input batch size for training (default: 8)"",
    )
    parser.add_argument(
        ""--lr"", type=float, default=5e-5, help=""learning rate (default: 5e-5)""
    )
    parser.add_argument(
        ""--backend"",
        choices=torch._dynamo.list_backends(exclude_tags=None),
        default=""inductor"",
        help=""train/evaluate model with a given backend (default: inductor)"",
    )
    parser.add_argument(
        ""--optimizer"",
        default=""Adam"",
        help=""train model using a given optimizer (default: Adam)"",
    )
    parser.add_argument(
        ""--evaluation"",
        action=""store_true"",
        help=""running evaluation after model training"",
    )
    args = parser.parse_args()
    return args"
tokenize_function;"def tokenize_function(examples):
        return tokenizer(examples[""text""], padding=""max_length"", truncation=True)"
query_job_sha;"def query_job_sha(repo, sha):
    params = {
        ""queryVariables"": {""sha"": sha, ""repo"": repo},
    }
    # If you are a Meta employee, go to P1679979893 to get the id and secret.
    # Otherwise, ask a Meta employee give you the id and secret.
    KEY_ID = os.environ[""CH_KEY_ID""]
    KEY_SECRET = os.environ[""CH_KEY_SECRET""]

    r = requests.post(
        url=ARTIFACTS_QUERY_URL,
        data=json.dumps(params),
        headers={""Content-Type"": ""application/json""},
        auth=(KEY_ID, KEY_SECRET),
    )
    return r.json()[""data""]"
parse_job_name;"def parse_job_name(job_str):
    return (part.strip() for part in job_str.split(""/""))"
parse_test_str;"def parse_test_str(test_str):
    return (part.strip() for part in test_str[6:].strip("")"").split("",""))"
get_artifacts_urls;"def get_artifacts_urls(results, suites):
    urls = {}
    for r in results:
        if (
            r[""workflowName""] in (""inductor"", ""inductor-periodic"")
            and ""test"" in r[""jobName""]
            and ""build"" not in r[""jobName""]
            and ""runner-determinator"" not in r[""jobName""]
            and ""unit-test"" not in r[""jobName""]
        ):
            *_, test_str = parse_job_name(r[""jobName""])
            suite, shard_id, num_shards, machine, *_ = parse_test_str(test_str)
            workflowId = r[""workflowId""]
            id = r[""id""]
            runAttempt = r[""runAttempt""]

            if suite in suites:
                artifact_filename = f""test-reports-test-{suite}-{shard_id}-{num_shards}-{machine}_{id}.zip""
                s3_url = f""{S3_BASE_URL}/{repo}/{workflowId}/{runAttempt}/artifact/{artifact_filename}""
                urls[(suite, int(shard_id))] = s3_url
                print(f""{suite} {shard_id}, {num_shards}: {s3_url}"")
    return urls"
normalize_suite_filename;"def normalize_suite_filename(suite_name):
    strs = suite_name.split(""_"")
    subsuite = strs[-1]
    if ""timm"" in subsuite:
        subsuite = subsuite.replace(""timm"", ""timm_models"")

    return subsuite"
download_artifacts_and_extract_csvs;"def download_artifacts_and_extract_csvs(urls):
    dataframes = {}
    for (suite, shard), url in urls.items():
        try:
            resp = urlopen(url)
            subsuite = normalize_suite_filename(suite)
            artifact = ZipFile(BytesIO(resp.read()))
            for phase in (""training"", ""inference""):
                name = f""test/test-reports/{phase}_{subsuite}.csv""
                try:
                    df = pd.read_csv(artifact.open(name))
                    df[""graph_breaks""] = df[""graph_breaks""].fillna(0).astype(int)
                    prev_df = dataframes.get((suite, phase), None)
                    dataframes[(suite, phase)] = (
                        pd.concat([prev_df, df]) if prev_df is not None else df
                    )
                except KeyError:
                    print(
                        f""Warning: Unable to find {name} in artifacts file from {url}, continuing""
                    )
        except urllib.error.HTTPError:
            print(f""Unable to download {url}, perhaps the CI job isn't finished?"")

    return dataframes"
write_filtered_csvs;"def write_filtered_csvs(root_path, dataframes):
    for (suite, phase), df in dataframes.items():
        out_fn = os.path.join(root_path, f""{suite}_{phase}.csv"")
        df.to_csv(out_fn, index=False, columns=[""name"", ""accuracy"", ""graph_breaks""])
        apply_lints(out_fn)"
parse_log_file;"def parse_log_file(file_path):
    with open(file_path) as f:
        logs = json.load(f)

    occurrence_count = {}
    benchmark_logs = {}

    # Parse the logs
    for entry in logs:
        if ""invoke"" in entry:
            shape = entry[""invoke""]
            if shape not in occurrence_count:
                occurrence_count[shape] = 0
            occurrence_count[shape] += 1
        else:
            for shape, timings in entry.items():
                if shape not in benchmark_logs:
                    benchmark_logs[shape] = []
                benchmark_logs[shape].extend(timings)

    return occurrence_count, benchmark_logs"
optimize_templates;"def optimize_templates(N, occurrence_count, benchmark_logs, verbose=False):
    # Set of all possible Triton templates keyed by their attributes
    triton_templates = set()
    for timings in benchmark_logs.values():
        for timing in timings:
            if timing[""type""] == ""triton"":
                triton_templates.add(
                    (
                        timing[""BLOCK_M""],
                        timing[""BLOCK_N""],
                        timing[""BLOCK_K""],
                        timing[""num_stages""],
                        timing[""num_warps""],
                    )
                )

    # Print the initial data
    if verbose:
        print(""Occurrence Count:"", occurrence_count)
        print(""Triton Templates:"", triton_templates)

    # Create a dictionary to store template selection variables
    template_vars = {
        template: pulp.LpVariable(f""Template_{template}"", 0, 1, pulp.LpBinary)
        for template in triton_templates
    }

    # Variables to select specific timing option for each shape
    selection_vars = {
        (shape, ""cublas""): pulp.LpVariable(
            f""Select_{shape}_cublas"", 0, 1, pulp.LpBinary
        )
        for shape in occurrence_count
    }
    for shape in occurrence_count:
        for template in triton_templates:
            selection_vars[(shape, template)] = pulp.LpVariable(
                f""Select_{shape}_{template}"", 0, 1, pulp.LpBinary
            )

    # Variables for the total time for each shape
    min_time_vars = pulp.LpVariable.dicts(
        ""MinTime"", occurrence_count.keys(), 0, None, pulp.LpContinuous
    )

    # Define the problem
    prob = pulp.LpProblem(""MatrixMultiplicationOptimization"", pulp.LpMinimize)

    # Objective: Minimize the weighted total time
    prob += pulp.lpSum(
        [occurrence_count[shape] * min_time_vars[shape] for shape in occurrence_count]
    )

    # Constraints to select exactly N templates
    prob += pulp.lpSum([template_vars[template] for template in triton_templates]) == N

    # Store triton options per shape for debugging
    triton_options_per_shape = {}

    # Constraints for the total time for each shape
    for shape in occurrence_count:
        # Get cuBLAS time
        cublas_times = [
            timing[""time""]
            for timing in benchmark_logs[shape]
            if timing[""type""] == ""cublas""
        ]
        min_cublas_time = min(cublas_times)

        # Collect Triton options
        triton_options = []
        for template in triton_templates:
            triton_times = [
                timing[""time""]
                for timing in benchmark_logs[shape]
                if timing[""type""] == ""triton""
                and (
                    timing[""BLOCK_M""],
                    timing[""BLOCK_N""],
                    timing[""BLOCK_K""],
                    timing[""num_stages""],
                    timing[""num_warps""],
                )
                == template
            ]
            if triton_times:
                min_triton_time = min(triton_times)
                triton_options.append((min_triton_time, template))

        # Save triton options for debugging
        triton_options_per_shape[shape] = triton_options

        # Ensure exactly one timing option is selected for each shape
        prob += (
            pulp.lpSum(
                [selection_vars[(shape, ""cublas"")]]
                + [
                    selection_vars[(shape, template)]
                    for triton_time, template in triton_options
                ]
            )
            == 1
        )

        # Ensure min_time_vars[shape] matches the selected timing option
        prob += min_time_vars[shape] == (
            selection_vars[(shape, ""cublas"")] * min_cublas_time
            + pulp.lpSum(
                [
                    selection_vars[(shape, template)] * triton_time
                    for triton_time, template in triton_options
                ]
            )
        )

        # Ensure Triton templates can only be selected if they are included in the N allowed templates
        for triton_time, template in triton_options:
            prob += selection_vars[(shape, template)] <= template_vars[template]

    # Print the constraints
    if verbose:
        print(""Constraints:"")
        for constraint in prob.constraints.values():
            print(constraint)

    # Solve the problem with suppressed output
    prob.solve(pulp.PULP_CBC_CMD(msg=False))

    # Output the selected templates and their configurations
    selected_templates = [
        template
        for template in triton_templates
        if pulp.value(template_vars[template]) == 1
    ]
    total_time = sum(
        pulp.value(min_time_vars[shape]) * occurrence_count[shape]
        for shape in occurrence_count
    )

    # Print the values of the decision variables after solving
    if verbose:
        print(""Decision Variable Values:"")
        for var in prob.variables():
            print(f""{var.name} = {var.varValue}"")

    # # Debugging information
    if verbose:
        for shape in occurrence_count:
            print(f""Shape: {shape}"")
            print(f""  Min Time: {pulp.value(min_time_vars[shape])}"")
            print(f""  Occurrences: {occurrence_count[shape]}"")
            print(
                f""  Min CuBLAS Time: {min_cublas_time} Selected: {pulp.value(selection_vars[(shape, 'cublas')])}""
            )
            for triton_time, template in triton_options_per_shape[shape]:
                print(
                    f""  Triton Template: {template} Time: {triton_time} Selected: {pulp.value(selection_vars[(shape, template)])}""
                )

    return selected_templates, total_time"
time_with_torch_timer;"def time_with_torch_timer(fn, args, kwargs=None, iters=100):
    kwargs = kwargs or {}
    env = {""args"": args, ""kwargs"": kwargs, ""fn"": fn}
    fn_call = ""fn(*args, **kwargs)""

    # Measure end-to-end time
    timer = Timer(stmt=f""{fn_call}"", globals=env)
    tt = timer.timeit(iters)

    return tt"
mm;"def mm(a, b, bias):
        y = torch.mm(a, b)
        return y"
compute_speedups;"def compute_speedups(args, models, example_inputs):
    expected = models[0](*example_inputs)
    for model in models[1:]:
        actual = model(*example_inputs)
        assert same(actual, expected), expected[0] - actual[0]

    timings = np.zeros((args.repeat, len(models)), np.float64)
    for rep in range(args.repeat):
        # interleave the runs to handle frequency scaling and load changes
        for m, model in enumerate(models):
            timings[rep, m] = timed(model, example_inputs)
    median = np.median(timings, axis=0)
    return (median[0] / median[1:]).tolist()"
microbenchmark;"def microbenchmark(args, model, example_inputs):
    compiled_fn = compile_fx(torch.fx.symbolic_trace(model), example_inputs)
    cudagraphs_eager = cudagraphs_inner(model, example_inputs, copy_outputs=False)
    cudagraphs_jit = cudagraphs_inner(
        torch.jit.trace(model, example_inputs), example_inputs, copy_outputs=False
    )
    return compute_speedups(
        args,
        [cudagraphs_eager, cudagraphs_jit, compiled_fn],
        example_inputs,
    )"
maybe_record_function;"def maybe_record_function(name):
    return torch.profiler.record_function(name) if profile_enabled else nullcontext()"
compute_speedups;"def compute_speedups(
    operator, models, example_inputs, repeats, accuracy_checking=False, device=""cuda""
):
    expected = models[0](*example_inputs)
    if accuracy_checking:
        for model in models[1:]:
            actual = model(*example_inputs)
            # change to assert later
            try:
                same(actual, expected, cos_similarity=True, equal_nan=True)
            except AssertionError as e:
                print(e)
                print(f""Accuracy check failed: {operator}"")
                print((expected[0] - actual[0]).abs().max())

    timings = np.zeros((repeats, len(models)), np.float64)
    for rep in range(repeats):
        with maybe_record_function(f""rep_{rep}""):
            # interleave the runs to handle frequency scaling and load changes
            for m, model in enumerate(models):
                with maybe_record_function(f""model_{m}""):
                    if device == ""cuda"":
                        model(*example_inputs)

                        # benchmarker.benchmark_gpu() clears L2 cache to hide the latency of CPU launch time
                        # along with cuda synchronization
                        timings[rep, m] = benchmarker.benchmark_gpu(
                            lambda: model(*example_inputs)
                        )
                    else:
                        from torch._inductor.utils import timed

                        timings[rep, m] = timed(model, example_inputs)
    return np.median(timings, axis=0)"
strip_overloads;"def strip_overloads(gm):
    """"""
    Modifies the target of graph nodes in :attr:`gm` to strip overloads.
    Args:
        gm(fx.GraphModule): The input Fx graph module to be modified
    """"""
    for node in gm.graph.nodes:
        if isinstance(node.target, torch._ops.OpOverload):
            node.target = node.target.overloadpacket
    gm.recompile()"
convert_to_jit;"def convert_to_jit(gm, gm_args):
    strip_overloads(gm)
    try:
        return torch.jit.script(gm)
    except Exception:
        pass
    return torch.jit.trace(gm, gm_args)"
to_channels_last;"def to_channels_last(ten):
    return ten if ten.ndim != 4 else ten.to(memory_format=torch.channels_last)"
microbenchmark;"def microbenchmark(
    operator,
    args,
    kwargs,
    accuracy_checking,
    repeats,
    inductor_configs,
    measure_nvfuser,
    device,
):
    gm, gm_args = gen_gm_and_inputs(operator, args, kwargs)
    torch.jit._builtins._register_builtin(
        torch.ops.aten.convolution_backward.default, ""aten::convolution_backward""
    )
    compiled = [gm]
    for config in inductor_configs:
        t = -time.perf_counter()
        compiled.append(compile_fx(gm, gm_args, config_patches=config))
        t += time.perf_counter()
        if t > 10:
            print(f""slow compile inductor {t:.1f}s {config}"")

    if measure_nvfuser:
        g = convert_to_jit(gm, gm_args)
        cudagraphs_jit = cudagraphs_inner(
            g, gm_args, copy_outputs=False, copy_inputs=False
        )
        compiled += [cudagraphs_jit]
    if accuracy_checking:
        repeats = 1

    medians = compute_speedups(
        operator, compiled, gm_args, repeats, accuracy_checking, device
    )
    return medians"
skip_operator;"def skip_operator(operator):
    nyi_strings = (
        ""aten.gather.default"",
        ""nll_loss"",
        ""aten.index"",
        ""aten.scatter_"",
        ""masked_fill_.Scalar"",
    )

    if any(nyi_string in str(operator) for nyi_string in nyi_strings):
        # maybe disable aten.native_layer_norm.default
        # TODO - inputs cannot be randomly initialized, causes cyda failures
        print(f""Skipping {operator}, input generator nyi"")
        return True

    # not covered by other non-compute operator heuristics
    if operator == torch.ops.aten._unsafe_view.default:
        print(f""Skipping {operator}, non compute operator"")
        return True

    # some of inductor registered to the OpOverload, some registered to OpOverloadPacket
    op_impls = [operator]
    if isinstance(operator, torch._ops.OpOverload):
        op_impls.append(operator.overloadpacket)

    # TODO - skip benchmarking fallbacks. for some ops we have both lowerings and fallbacks
    # so its not clear just from operator what will be lowered.

    if all(op not in decompositions and op not in lowerings for op in op_impls):
        print(f""Skipping {operator}, no inductor impl"")
        return True

    if ""convolution"" in str(operator):
        return True

    return False"
truncate_inp;"def truncate_inp(arg):
    if arg in dtype_abbrs:
        return dtype_abbrs[arg]
    elif isinstance(arg, torch.device):
        return arg.type
    else:
        return arg"
serialize_sparse_tensor;"def serialize_sparse_tensor(e):
    if isinstance(e, torch._subclasses.FakeTensor):
        return FuncCallWrapper(""ST"", list(e.shape), e.dtype, e.layout, e.is_coalesced())
    else:
        return FuncCallWrapper(
            ""ST"", list(e.shape), e.dtype, e.layout, e.is_coalesced(), e._nnz()
        )"
deserialize_sparse_tensor;"def deserialize_sparse_tensor(size, dtype, layout, is_coalesced, nnz=None):
    raise NotImplementedError"
deserialize_tensor;"def deserialize_tensor(size, dtype, stride=None):
    if stride is not None:
        out = torch.empty_strided(size, stride, dtype=dtype)
    else:
        out = torch.empty(size, dtype=dtype)
    try:
        out.copy_(make_tensor(size, dtype=dtype, device=""cpu""))
    except Exception as e:
        print(e)
        return out
    return out"
serialize_tensor;"def serialize_tensor(e):
    if not e.is_contiguous():
        return FuncCallWrapper(""T"", list(e.shape), e.dtype, stride=e.stride())
    else:
        return FuncCallWrapper(""T"", list(e.shape), e.dtype)"
serialize_torch_args;"def serialize_torch_args(e):
    if isinstance(e, torch.Tensor):
        if e.is_sparse:
            return serialize_sparse_tensor(e)
        return serialize_tensor(e)
    else:
        return truncate_inp(e)"
contains_tensor;"def contains_tensor(elems):
    for elem in pytree.tree_leaves(elems):
        if isinstance(elem, torch.Tensor):
            return True
    return False"
skip_args;"def skip_args(elems):
    for i in pytree.tree_leaves(elems):
        # only shows up in constructors and ops like that
        if isinstance(i, (torch.memory_format, torch.storage.UntypedStorage)):
            return True
    return False"
contains_tensor_types;"def contains_tensor_types(type):
    return type.isSubtypeOf(tensor_type) or any(
        contains_tensor_types(e) for e in type.containedTypes()
    )"
non_compute_operator;"def non_compute_operator(op):
    schema = op._schema

    # skip constructors
    if not any(contains_tensor_types(arg.type) for arg in schema.arguments):
        return True
    if ""_like"" in op.name():
        return True

    # allow in place writes
    if schema.is_mutable:
        return False

    tensor_inps = [arg for arg in schema.arguments if arg.type is tensor_type]
    tensor_outputs = [ret for ret in schema.returns if ret.type is tensor_type]

    # skip aliasing unless there are multiple outputs
    if len(tensor_outputs) != 1:
        return False

    for inp in tensor_inps:
        if inp.alias_info and tensor_outputs[0].alias_info:
            if inp.alias_info.before_set.intersection(
                tensor_outputs[0].alias_info.after_set
            ):
                return True

    return False"
map_to_device;"def map_to_device(e, device):
    if isinstance(e, torch.Tensor):
        return e.to(device)
    elif isinstance(e, torch.device):
        return device
    elif isinstance(e, str):
        if e == ""cuda"" or e == ""cpu"":
            return device.type
    else:
        return e"
map_to_dtype;"def map_to_dtype(e, dtype):
    if isinstance(e, torch.Tensor) and e.is_floating_point():
        return e.to(dtype)
    elif isinstance(e, torch.dtype):
        return dtype
    else:
        return e"
deserialize_args;"def deserialize_args(inps):
    inps = inps.strip().strip(""'"")
    global_vals = {
        ""T"": deserialize_tensor,
        ""ST"": deserialize_sparse_tensor,
        ""th"": torch,
        ""inf"": math.inf,
        ""torch"": torch,
        **dtype_abbrs_parsing,
    }
    # f strings introduce quotations we dont want
    for key in dtype_abbrs_parsing:
        inps = inps.replace(f""'{key}'"", key)
    return eval(inps.strip().strip(""'"").strip('""'), global_vals)"
get_inputs_for_operator;"def get_inputs_for_operator(
        self, operator, dtype=None, device=""cuda""
    ) -> Generator[tuple[Iterable[Any], dict[str, Any]], None, None]:
        assert str(operator) in self.operator_db, (
            f""Could not find {operator}, must provide overload""
        )

        if ""embedding"" in str(operator):
            log.warning(""Embedding inputs NYI, input data cannot be randomized"")
            yield
            return

        # line[1] represents number of times these inputs occured, ignored for now
        for line in self.operator_db[str(operator)].items():
            inps = line[0]

            args, kwargs = deserialize_args(inps)

            # Backwards require some inputs to be float16 and some to be float32
            # So we record on half and upcast to float when specified
            if dtype and dtype != torch.float16:
                to_dtype = partial(map_to_dtype, dtype=dtype)
                args, kwargs = tree_map(to_dtype, (args, kwargs))

            if device:
                to_device = partial(map_to_device, device=torch.device(device))
                args, kwargs = tree_map(to_device, (args, kwargs))

            yield args, kwargs"
get_all_ops;"def get_all_ops(self):
        for key in self.operator_db.keys():
            try:
                op = eval(key)
            except AttributeError as ae:
                log.warning(""Evaluating an op name into an OpOverload: %s"", ae)
                continue
            yield op"
get_call_frequency;"def get_call_frequency(self, op):
        assert str(op) in self.operator_db, (
            f""Could not find {op}, must provide overload""
        )

        count = 0
        for counter in self.operator_db[str(op)].values():
            count += counter
        return count"
merge;"def merge(self, other):
        for operator, counter_dict in other.operator_db.items():
            for inps, cnt in counter_dict.items():
                self.operator_db[operator][inps] += cnt"
get_timm_loader;"def get_timm_loader():
        return OperatorInputsLoader._load_directory(TIMM_DIR)"
get_huggingface_loader;"def get_huggingface_loader():
        return OperatorInputsLoader._load_directory(HF_DIR)"
get_torchbench_loader;"def get_torchbench_loader():
        return OperatorInputsLoader._load_directory(TORCHBENCH_DIR)"
_load_directory;"def _load_directory(inp_dir):
        assert os.path.isdir(inp_dir), inp_dir
        union = None
        for inp in os.listdir(inp_dir):
            if inp[-4:] != "".txt"":
                continue
            path = os.path.join(inp_dir, inp)
            if union is None:
                union = OperatorInputsLoader(path)
            else:
                union.merge(OperatorInputsLoader(path))
        return union"
to_channels_last;"def to_channels_last(x):
    assert x.dim() == 4

    # NCHW -> NHWC
    stride_order = [3, 0, 2, 1]
    y = x.clone().as_strided(
        x.shape,
        ir.FlexibleLayout.stride_ordered(x.shape, stride_order),
    )
    y.copy_(x)
    assert torch.allclose(x, y)
    return y"
bench_conv;"def bench_conv(with_stack=True):
    x = torch.rand(256, 3, 224, 224).cuda()
    weight = torch.rand(64, 3, 7, 7).cuda()

    x_chan = to_channels_last(x)
    weight_chan = to_channels_last(weight)
    kwargs = {
        ""stride"": [2, 2],
        ""padding"": [3, 3],
        ""dilation"": [1, 1],
        ""transposed"": False,
        ""output_padding"": [0, 0],
        ""groups"": 1,
    }

    def baseline_fn():
        return torch.convolution(x, weight, bias=None, **kwargs)

    def test_fn():
        return torch.convolution(x_chan, weight_chan, bias=None, **kwargs)

    # warmup
    baseline_fn()
    test_fn()

    torch.cuda.synchronize()
    with torch.profiler.profile(with_stack=with_stack) as p:
        baseline_out = baseline_fn()
        test_out = test_fn()
        torch.cuda.synchronize()

    p.export_chrome_trace(""/tmp/chrome.json"")
    assert torch.allclose(baseline_out, test_out, atol=1e-3, rtol=1e-3), (
        baseline_out[0][0][0][:32],
        test_out[0][0][0][:32],
    )

    baseline_ms = benchmarker.benchmark_gpu(baseline_fn, rep=40)
    test_ms = benchmarker.benchmark_gpu(test_fn, rep=40)
    print(f""baseline {baseline_ms} test {test_ms} speedup {baseline_ms / test_ms:.3f}x"")"
with_iterations;"def with_iterations(self, value: int) -> Self:
        self._num_iterations = value
        return self"
enable_instruction_count;"def enable_instruction_count(self) -> Self:
        self._enable_instruction_count = True
        return self"
enable_compile_time_instruction_count;"def enable_compile_time_instruction_count(self) -> Self:
        self._enable_compile_time_instruction_count = True
        return self"
_count_instructions;"def _count_instructions(self) -> int:
        print(f""collecting instruction count for {self.name()}"")
        results = []
        for i in range(self._num_iterations):
            self._prepare()
            id = i_counter.start()
            self._work()
            count = i_counter.end(id)
            print(f""instruction count for iteration {i} is {count}"")
            results.append(count)
        return min(results)"
_count_compile_time_instructions;"def _count_compile_time_instructions(self) -> int:
        gc.disable()

        try:
            print(f""collecting compile time instruction count for {self.name()}"")
            config.record_compile_time_instruction_count = True

            results = []
            for i in range(self._num_iterations):
                self._prepare()
                gc.collect()
                # CompileTimeInstructionCounter.record is only called on convert_frame._compile_inner
                # hence this will only count instruction count spent in compile_inner.
                CompileTimeInstructionCounter.clear()
                self._work()
                count = CompileTimeInstructionCounter.value()
                if count == 0:
                    raise RuntimeError(
                        ""compile time instruction count is 0, please check your benchmarks""
                    )
                print(f""compile time instruction count for iteration {i} is {count}"")
                results.append(count)

            config.record_compile_time_instruction_count = False
            return min(results)
        finally:
            gc.enable()"
_write_to_json;"def _write_to_json(self, output_dir: str) -> None:
        """"""
        Write the result into JSON format, so that it can be uploaded to the benchmark database
        to be displayed on OSS dashboard. The JSON format is defined at
        https://github.com/pytorch/pytorch/wiki/How-to-integrate-with-PyTorch-OSS-benchmark-database
        """"""
        records = []
        for entry in self.results:
            metric_name = entry[1]
            value = entry[2]

            if not metric_name or value is None:
                continue

            records.append(
                {
                    ""benchmark"": {
                        ""name"": ""pr_time_benchmarks"",
                        ""mode"": self.mode(),
                        ""extra_info"": {
                            ""is_dynamic"": self.is_dynamic(),
                            ""device"": self.device(),
                            ""description"": self.description(),
                        },
                    },
                    ""model"": {
                        ""name"": self.name(),
                        ""type"": self.category(),
                        ""backend"": self.backend(),
                    },
                    ""metric"": {
                        ""name"": metric_name,
                        ""benchmark_values"": [value],
                    },
                }
            )

        with open(os.path.join(output_dir, f""{self.name()}.json""), ""w"") as f:
            json.dump(records, f)"
append_results;"def append_results(self, path: str) -> None:
        with open(path, ""a"", newline="""") as csvfile:
            # Create a writer object
            writer = csv.writer(csvfile)
            # Write the data to the CSV file
            for entry in self.results:
                writer.writerow(entry)

        # TODO (huydhn) This requires the path to write to, so it needs to be in the same place
        # as the CSV writer for now
        self._write_to_json(os.path.dirname(os.path.abspath(path)))"
collect_all;"def collect_all(self) -> Self:
        self._prepare_once()
        self.results = []
        if (
            self._enable_instruction_count
            and self._enable_compile_time_instruction_count
        ):
            raise RuntimeError(
                ""not supported until we update the logger, both logs to the same field now""
            )

        if self._enable_instruction_count:
            r = self._count_instructions()
            self.results.append((self.name(), ""instruction_count"", r))
            if log_to_scuba:
                scribe_log_torch_benchmark_compile_time(
                    name=self.name(),
                    instruction_count=r,
                )
        if self._enable_compile_time_instruction_count:
            r = self._count_compile_time_instructions()

            self.results.append(
                (
                    self.name(),
                    ""compile_time_instruction_count"",
                    r,
                )
            )
            if log_to_scuba:
                # TODO add a new field compile_time_instruction_count to the logger.
                scribe_log_torch_benchmark_compile_time(
                    name=self.name(),
                    instruction_count=r,
                )
        return self"
fn;"def fn(x):
    return x + 1"
fn1;"def fn1(x):
    return fn(x)"
fn2;"def fn2(x):
    return fn1(x)"
fn3;"def fn3(x):
    return fn2(x)"
fn4;"def fn4(x):
    return fn3(x)"
fn5;"def fn5(x):
    return fn4(x)"
fn6;"def fn6(x):
    return fn5(x)"
fn7;"def fn7(x):
    return fn6(x)"
fn8;"def fn8(x):
    return fn7(x)"
fn9;"def fn9(x):
    return fn8(x)"
lstm_backward_setup;"def lstm_backward_setup(lstm_outputs, seed=None):
    hx, _ = lstm_outputs
    return simple_backward_setup(hx, seed)"
simple_backward_setup;"def simple_backward_setup(output, seed=None):
    assert isinstance(output, torch.Tensor)
    if seed:
        torch.manual_seed(seed)
    grad_output = torch.randn_like(output)
    return output, grad_output"
simple_backward;"def simple_backward(output, grad_output, **kwargs):
    return output.backward(grad_output, **kwargs)"
pytorch_lstm_creator;"def pytorch_lstm_creator(**kwargs):
    input, hidden, _, module = lstm_inputs(return_module=True, **kwargs)
    return ModelDef(
        inputs=[input, hidden],
        params=flatten_list(module.all_weights),
        forward=module,
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lstm_creator;"def lstm_creator(script=True, **kwargs):
    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)
    inputs = [input, hidden] + params[0]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=lstm_factory(lstm_cell, script),
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lnlstm_creator;"def lnlstm_creator(script=True, decompose_layernorm=False, **kwargs):
    assert script is True
    from .custom_lstms import script_lnlstm

    input_size = kwargs[""inputSize""]
    hidden_size = kwargs[""hiddenSize""]
    seq_len = kwargs[""seqLength""]
    batch_size = kwargs[""miniBatch""]
    ge = script_lnlstm(
        input_size, hidden_size, 1, decompose_layernorm=decompose_layernorm
    ).cuda()

    input = torch.randn(seq_len, batch_size, input_size, device=""cuda"")
    states = [
        (
            torch.randn(batch_size, hidden_size, device=""cuda""),
            torch.randn(batch_size, hidden_size, device=""cuda""),
        )
    ]

    return ModelDef(
        inputs=[input, states],
        params=ge.parameters(),
        forward=ge,
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
dropoutlstm_creator;"def dropoutlstm_creator(script=True, **kwargs):
    assert script is True
    from .custom_lstms import LSTMState, script_lstm

    input_size = kwargs[""inputSize""]
    hidden_size = kwargs[""hiddenSize""]
    seq_len = kwargs[""seqLength""]
    batch_size = kwargs[""miniBatch""]
    num_layers = kwargs[""numLayers""]
    ge = script_lstm(input_size, hidden_size, num_layers, dropout=True).cuda()

    input = torch.randn(seq_len, batch_size, input_size, device=""cuda"")
    states = [
        LSTMState(
            torch.randn(batch_size, hidden_size, device=""cuda""),
            torch.randn(batch_size, hidden_size, device=""cuda""),
        )
        for _ in range(num_layers)
    ]
    return ModelDef(
        inputs=[input, states],
        params=ge.parameters(),
        forward=ge,
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lstm_premul_creator;"def lstm_premul_creator(script=True, **kwargs):
    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)
    inputs = [input, hidden] + params[0]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=lstm_factory_premul(premul_lstm_cell, script),
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lstm_premul_bias_creator;"def lstm_premul_bias_creator(script=True, **kwargs):
    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)
    inputs = [input, hidden] + params[0]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=lstm_factory_premul_bias(premul_lstm_cell_no_bias, script),
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lstm_simple_creator;"def lstm_simple_creator(script=True, **kwargs):
    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)
    inputs = [input] + [h[0] for h in hidden] + params[0]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=lstm_factory_simple(flat_lstm_cell, script),
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
lstm_multilayer_creator;"def lstm_multilayer_creator(script=True, **kwargs):
    input, hidden, params, _ = lstm_inputs(return_module=False, **kwargs)
    inputs = [input, hidden, flatten_list(params)]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=lstm_factory_multilayer(lstm_cell, script),
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
imagenet_cnn_creator;"def imagenet_cnn_creator(arch, jit=True):
    def creator(device=""cuda"", **kwargs):
        model = arch().to(device)
        x = torch.randn(32, 3, 224, 224, device=device)
        if jit:
            model = torch.jit.trace(model, x)
        return ModelDef(
            inputs=(x,),
            params=list(model.parameters()),
            forward=model,
            backward_setup=simple_backward_setup,
            backward=simple_backward,
        )

    return creator"
varlen_lstm_inputs;"def varlen_lstm_inputs(
    minlen=30,
    maxlen=100,
    numLayers=1,
    inputSize=512,
    hiddenSize=512,
    miniBatch=64,
    return_module=False,
    device=""cuda"",
    seed=None,
    **kwargs,
):
    if seed is not None:
        torch.manual_seed(seed)
    lengths = torch.randint(
        low=minlen, high=maxlen, size=[miniBatch], dtype=torch.long, device=device
    )
    x = [torch.randn(length, inputSize, device=device) for length in lengths]
    hx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)
    cx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)
    lstm = torch.nn.LSTM(inputSize, hiddenSize, numLayers).to(device)

    if return_module:
        return x, lengths, (hx, cx), lstm.all_weights, lstm
    else:
        # NB: lstm.all_weights format:
        # wih, whh, bih, bhh = lstm.all_weights[layer]
        return x, lengths, (hx, cx), lstm.all_weights, None"
varlen_lstm_backward_setup;"def varlen_lstm_backward_setup(forward_output, seed=None):
    if seed:
        torch.manual_seed(seed)
    rnn_utils = torch.nn.utils.rnn
    sequences = forward_output[0]
    padded = rnn_utils.pad_sequence(sequences)
    grad = torch.randn_like(padded)
    return padded, grad"
varlen_pytorch_lstm_creator;"def varlen_pytorch_lstm_creator(**kwargs):
    rnn_utils = torch.nn.utils.rnn
    sequences, _, hidden, _, module = varlen_lstm_inputs(return_module=True, **kwargs)

    def forward(sequences, hidden):
        packed = rnn_utils.pack_sequence(sequences, enforce_sorted=False)
        out, new_hidden = module(packed, hidden)
        padded, lengths = rnn_utils.pad_packed_sequence(out)
        # XXX: It's more efficient to store the output in its padded form,
        # but that might not be conducive to loss computation.
        # Un-padding the output also makes the backward pass 2x slower...
        # return [padded[:lengths[i], i, :] for i in range(lengths.size(0))]
        return padded, new_hidden

    return ModelDef(
        inputs=[sequences, hidden],
        params=flatten_list(module.all_weights),
        forward=forward,
        backward_setup=lstm_backward_setup,
        backward=simple_backward,
    )"
varlen_lstm_factory;"def varlen_lstm_factory(cell, script):
    def dynamic_rnn(
        sequences: list[Tensor],
        hiddens: tuple[Tensor, Tensor],
        wih: Tensor,
        whh: Tensor,
        bih: Tensor,
        bhh: Tensor,
    ) -> tuple[list[Tensor], tuple[list[Tensor], list[Tensor]]]:
        hx, cx = hiddens
        hxs = hx.unbind(1)
        cxs = cx.unbind(1)
        # List of: (output, hx, cx)
        outputs = []
        hx_outs = []
        cx_outs = []

        for batch in range(len(sequences)):
            output = []
            hy, cy = hxs[batch], cxs[batch]
            inputs = sequences[batch].unbind(0)

            for seq_idx in range(len(inputs)):
                hy, cy = cell(
                    inputs[seq_idx].unsqueeze(0), (hy, cy), wih, whh, bih, bhh
                )
                output += [hy]
            outputs += [torch.stack(output)]
            hx_outs += [hy.unsqueeze(0)]
            cx_outs += [cy.unsqueeze(0)]

        return outputs, (hx_outs, cx_outs)

    if script:
        cell = torch.jit.script(cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
varlen_lstm_creator;"def varlen_lstm_creator(script=False, **kwargs):
    sequences, _, hidden, params, _ = varlen_lstm_inputs(return_module=False, **kwargs)
    inputs = [sequences, hidden] + params[0]
    return ModelDef(
        inputs=inputs,
        params=flatten_list(params),
        forward=varlen_lstm_factory(lstm_cell, script),
        backward_setup=varlen_lstm_backward_setup,
        backward=simple_backward,
    )"
layernorm_pytorch_lstm_creator;"def layernorm_pytorch_lstm_creator(**kwargs):
    input, hidden, _, module = lstm_inputs(return_module=True, **kwargs)
    batch_size = kwargs[""miniBatch""]
    hidden_size = kwargs[""hiddenSize""]
    ln_i = torch.nn.LayerNorm(4 * hidden_size).cuda()
    ln_h = torch.nn.LayerNorm(4 * hidden_size).cuda()
    ln_c = torch.nn.LayerNorm(hidden_size).cuda()
    ln_input1 = torch.randn(batch_size, 4 * hidden_size, device=""cuda"")

    def forward(input, hidden):
        out, new_hidden = module(input, hidden)
        # plus (seq_len * three laynorm cell computation) to mimic the lower bound of
        # Layernorm cudnn LSTM in the forward pass
        seq_len = len(input.unbind(0))
        hy, cy = new_hidden
        for i in range(seq_len):
            ln_i(ln_input1)
            ln_h(ln_input1)
            cy = ln_c(cy)

        return out, (hy, cy)

    return ModelDef(
        inputs=[input, hidden],
        params=flatten_list(module.all_weights),
        forward=forward,
        backward_setup=lstm_backward_setup,
        backward=None,
    )"
stack_weights;"def stack_weights(weights):
    def unzip_columns(mat):
        assert isinstance(mat, list)
        assert isinstance(mat[0], list)
        layers = len(mat)
        columns = len(mat[0])
        return [[mat[layer][col] for layer in range(layers)] for col in range(columns)]

    # XXX: script fns have problems indexing multidim lists, so we try to
    # avoid them by stacking tensors
    all_weights = weights
    packed_weights = [torch.stack(param) for param in unzip_columns(all_weights)]
    return packed_weights"
lstm_inputs;"def lstm_inputs(
    seqLength=100,
    numLayers=1,
    inputSize=512,
    hiddenSize=512,
    miniBatch=64,
    dropout=0.0,
    return_module=False,
    device=""cuda"",
    seed=None,
):
    if seed is not None:
        torch.manual_seed(seed)
    x = torch.randn(seqLength, miniBatch, inputSize, device=device)
    hx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)
    cx = torch.randn(numLayers, miniBatch, hiddenSize, device=device)
    lstm = torch.nn.LSTM(inputSize, hiddenSize, numLayers, dropout=dropout)
    if ""cuda"" in device:
        lstm = lstm.cuda()

    if return_module:
        return x, (hx, cx), lstm.all_weights, lstm
    else:
        # NB: lstm.all_weights format:
        # wih, whh, bih, bhh = lstm.all_weights[layer]
        return x, (hx, cx), lstm.all_weights, None"
lstm_factory;"def lstm_factory(cell, script):
    def dynamic_rnn(
        input: Tensor,
        hidden: tuple[Tensor, Tensor],
        wih: Tensor,
        whh: Tensor,
        bih: Tensor,
        bhh: Tensor,
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:
        hx, cx = hidden
        outputs = []
        inputs = input.unbind(0)
        hy, cy = hx[0], cx[0]
        for seq_idx in range(len(inputs)):
            hy, cy = cell(inputs[seq_idx], (hy, cy), wih, whh, bih, bhh)
            outputs += [hy]
        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))

    if script:
        cell = torch.jit.script(cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
lstm_factory_premul;"def lstm_factory_premul(premul_cell, script):
    def dynamic_rnn(
        input: Tensor,
        hidden: tuple[Tensor, Tensor],
        wih: Tensor,
        whh: Tensor,
        bih: Tensor,
        bhh: Tensor,
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:
        hx, cx = hidden
        outputs = []
        inputs = torch.matmul(input, wih.t()).unbind(0)
        hy, cy = hx[0], cx[0]
        for seq_idx in range(len(inputs)):
            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), whh, bih, bhh)
            outputs += [hy]
        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))

    if script:
        premul_cell = torch.jit.script(premul_cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
lstm_factory_premul_bias;"def lstm_factory_premul_bias(premul_cell, script):
    def dynamic_rnn(
        input: Tensor,
        hidden: tuple[Tensor, Tensor],
        wih: Tensor,
        whh: Tensor,
        bih: Tensor,
        bhh: Tensor,
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:
        hx, cx = hidden
        outputs = []
        inpSize = input.size()
        # add bias for all timesteps instead of going step-by-step, results in a single reduction kernel in the backward
        # FIXME matmul(x,y) + bias currently goes through jit AD, and backward formula in AD is not optimized for this
        # case. Workaround with mm and views.
        inpSize = input.size()
        inputs = torch.mm(input.view(-1, inpSize[2]), wih.t()) + bih
        inputs = inputs.view(inpSize[0], inpSize[1], -1).unbind(0)
        hy, cy = hx[0], cx[0]
        for seq_idx in range(len(inputs)):
            hy, cy = premul_cell(inputs[seq_idx], (hy, cy), whh, bhh)
            outputs += [hy]
        return torch.stack(outputs), (hy.unsqueeze(0), cy.unsqueeze(0))

    if script:
        premul_cell = torch.jit.script(premul_cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
lstm_factory_simple;"def lstm_factory_simple(cell, script):
    def dynamic_rnn(input, hx, cx, wih, whh, bih, bhh):
        hy = hx  # for scoping
        cy = cx  # for scoping
        inputs = input.unbind(0)
        for seq_idx in range(len(inputs)):
            hy, cy = cell(inputs[seq_idx], hy, cy, wih, whh, bih, bhh)
        return hy, cy

    if script:
        cell = torch.jit.script(cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
lstm_factory_multilayer;"def lstm_factory_multilayer(cell, script):
    def dynamic_rnn(
        input: Tensor, hidden: tuple[Tensor, Tensor], params: list[Tensor]
    ) -> tuple[Tensor, tuple[Tensor, Tensor]]:
        params_stride = 4  # NB: this assumes that biases are there
        hx, cx = hidden
        hy, cy = hidden  # for scoping...
        inputs, outputs = input.unbind(0), []
        for layer in range(hx.size(0)):
            hy = hx[layer]
            cy = cx[layer]
            base_idx = layer * params_stride
            wih = params[base_idx]
            whh = params[base_idx + 1]
            bih = params[base_idx + 2]
            bhh = params[base_idx + 3]
            for seq_idx in range(len(inputs)):
                hy, cy = cell(inputs[seq_idx], (hy, cy), wih, whh, bih, bhh)
                outputs += [hy]
            inputs, outputs = outputs, []
        return torch.stack(inputs), (hy.unsqueeze(0), cy.unsqueeze(0))

    if script:
        cell = torch.jit.script(cell)
        dynamic_rnn = torch.jit.script(dynamic_rnn)

    return dynamic_rnn"
dynamically_quantize_per_channel;"def dynamically_quantize_per_channel(x, quant_min, quant_max, target_dtype):
    # assumes symmetric quantization
    # assumes axis == 0
    # assumes dense memory format
    # TODO(future): relax ^ as needed

    # default setup for affine quantization of activations
    eps = torch.finfo(torch.float32).eps

    # get min and max
    min_val, max_val = torch.aminmax(x, dim=1)

    # calculate scales and zero_points based on min and max
    # reference: https://fburl.com/code/srbiybme
    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))
    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))
    device = min_val_neg.device

    # reference: https://fburl.com/code/4wll53rk
    max_val_pos = torch.max(-min_val_neg, max_val_pos)
    scales = max_val_pos / (float(quant_max - quant_min) / 2)
    # ensure scales is the same dtype as the original tensor
    scales = torch.clamp(scales, min=eps).to(x.dtype)
    zero_points = torch.zeros(min_val_neg.size(), dtype=torch.int64, device=device)

    # quantize based on qmin/qmax/scales/zp
    # reference: https://www.internalfb.com/code/fbsource/[8edc275012b1]/fbcode/caffe2/torch/ao/quantization/fx/_decomposed.py?lines=63
    x_div = x / scales.unsqueeze(-1)
    x_round = torch.round(x_div)
    x_zp = x_round + zero_points.unsqueeze(-1)
    quant = torch.clamp(x_zp, quant_min, quant_max).to(target_dtype)

    return quant, scales, zero_points"
replace_linear_weight_only_int8_per_channel;"def replace_linear_weight_only_int8_per_channel(module):
    for name, child in module.named_children():
        if isinstance(child, nn.Linear):
            setattr(
                module,
                name,
                WeightOnlyInt8Linear(child.in_features, child.out_features),
            )
        else:
            replace_linear_weight_only_int8_per_channel(child)"
create_quantized_state_dict;"def create_quantized_state_dict(self):
        cur_state_dict = self.mod.state_dict()
        for fqn, mod in self.mod.named_modules():
            if isinstance(mod, torch.nn.Linear):
                int8_weight, scales, _ = dynamically_quantize_per_channel(
                    mod.weight.float(), -128, 127, torch.int8
                )
                cur_state_dict[f""{fqn}.weight""] = int8_weight.to(""cpu"")
                cur_state_dict[f""{fqn}.scales""] = scales.to(mod.weight.dtype).to(""cpu"")

        return cur_state_dict"
convert_for_runtime;"def convert_for_runtime(self):
        replace_linear_weight_only_int8_per_channel(self.mod)
        return self.mod"
benchmark_torch_function_in_microseconds;"def benchmark_torch_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    return do_bench(lambda: func(*args, **kwargs)) * 1e3"
get_inputs;"def get_inputs(
    config: ExperimentGroupConfig,
) -> tuple[torch.Tensor, ...]:
    op_name = config.op_name
    M, N, K = config.shape
    batch_size = config.batch_size
    dtype = config.dtype
    device = torch.device(""cuda"")

    if op_name == ""mm"":
        A = torch.randn(M, K, dtype=dtype, device=device)
        B = torch.randn(N, K, dtype=dtype, device=device).t()
        return A, B
    elif op_name == ""addmm"":
        A = torch.randn(M, K, dtype=dtype, device=device)
        B = torch.randn(N, K, dtype=dtype, device=device).t()
        C = torch.randn(N, dtype=dtype, device=device)
        return C, A, B
    elif op_name == ""bmm"":
        A = torch.randn(batch_size, M, K, dtype=dtype, device=device)
        B = torch.randn(batch_size, N, K, dtype=dtype, device=device).permute(0, 2, 1)
        return A, B
    else:
        raise ValueError(f""Unknown op {op_name}"")"
run_single_experiment_group;"def run_single_experiment_group(
    group_config: ExperimentGroupConfig,
) -> list[ExperimentResults]:
    inputs = get_inputs(group_config)
    op = getattr(torch, group_config.op_name)

    results = []

    for config in group_config.experiments:
        torch._dynamo.reset()
        torch._inductor.utils.clear_inductor_caches()
        compiled_op = torch.compile(op, fullgraph=True, options=config.to_options())

        start_time = time.perf_counter()
        try:
            _ = compiled_op(*inputs)
        except Exception as e:
            import traceback

            log.warning(
                f""Benchmark config {config.name()} failed: {e}, ""  # noqa: G004
                f""traceback: {traceback.format_exc()}""
            )
            results.append(
                ExperimentResults(
                    name=config.name(),
                    forward_time=float(""inf""),
                    compilation_time=float(""inf""),
                )
            )
            continue
        compilation_time = time.perf_counter() - start_time

        forward_time = benchmark_torch_function_in_microseconds(
            compiled_op,
            *inputs,
        )

        results.append(
            ExperimentResults(
                name=config.name(),
                forward_time=forward_time,
                compilation_time=compilation_time,
            )
        )

    return results"
generate_experiment_groups;"def generate_experiment_groups(
    op_names: list[str],
    shapes: list[tuple[int, int, int]],
    dtypes: list[torch.dtype],
    enable_persistent_tma_matmuls: list[bool],
    cutlass_instantiation_levels: list[str],
    batch_sizes: list[int],
) -> list[ExperimentGroupConfig]:
    groups = []
    for (
        op_name,
        shape,
        dtype,
        batch_size,
    ) in itertools.product(op_names, shapes, dtypes, batch_sizes):
        group = ExperimentGroupConfig(
            op_name=op_name,
            shape=shape,
            dtype=dtype,
            batch_size=batch_size,
        )
        experiments = generate_experiment_configs(
            enable_persistent_tma_matmuls, cutlass_instantiation_levels
        )
        group.experiments.extend(experiments)
        groups.append(group)

    return groups"
generate_experiment_configs;"def generate_experiment_configs(
    enable_persistent_tma_matmuls: list[bool], cutlass_instantiation_levels: list[str]
) -> list[ExperimentConfig]:
    configs = []

    # add aten configs
    configs.append(
        AtenExperimentConfig(
            max_autotune_gemm_backends=""ATEN"",
        )
    )

    # add triton configs
    for enable_persistent_tma_matmul in enable_persistent_tma_matmuls:
        configs.append(
            TritonExperimentConfig(
                max_autotune_gemm_backends=""TRITON"",
                enable_persistent_tma_matmul=enable_persistent_tma_matmul,
            )
        )

    # add cutlass configs
    for cutlass_instantiation_level in cutlass_instantiation_levels:
        configs.append(
            CutlassExperimentConfig(
                max_autotune_gemm_backends=""CUTLASS"",
                cutlass_instantiation_level=cutlass_instantiation_level,
            )
        )

    return configs"
calculate_table_data;"def calculate_table_data(results: list[ExperimentResults]) -> dict:
    table_data = defaultdict(list)
    aten_perf: Optional[float] = None

    for experiment_result in results:
        for key, value in experiment_result.asdict().items():
            assert key in UNITS, f""Unknown key {key}""
            table_data[key + UNITS[key]].append(value)

        if experiment_result.name == ""aten"":
            aten_perf = experiment_result.forward_time
            table_data[PERF_OVER_ATEN_STR].append(""NA"")
        elif aten_perf is not None:
            perf_over_aten = (
                (experiment_result.forward_time - aten_perf) / aten_perf * 100
            )
            table_data[PERF_OVER_ATEN_STR].append(perf_over_aten)
        else:
            # fallback in case aten is not in experiment group
            table_data[PERF_OVER_ATEN_STR].append(""NA"")

    return table_data"
get_printable_results;"def get_printable_results(experiment_groups: list[ExperimentGroup]) -> list[str]:
    edge_over_aten = defaultdict(list)
    output = []

    for experiment_group in experiment_groups:
        group_config_name = experiment_group.config.name()
        output.append(f""\nExperiment group: {group_config_name}"")

        table_data = calculate_table_data(experiment_group.results)
        for name, edge in zip(table_data[""name""], table_data[PERF_OVER_ATEN_STR]):
            edge_over_aten[name].append(edge)
        output.append(
            tabulate(table_data, headers=""keys"", tablefmt=""pretty"", floatfmt="".3f"")
        )

    if ""aten"" in edge_over_aten:
        output.append(""\nAverage edge over aten (max(-edge, 0), higher is better):"")
        for name in edge_over_aten:
            if name != ""aten"":
                values = [
                    max(-v, 0.0)
                    for v in edge_over_aten[name]
                    if v != float(""inf"") and v != ""NA""
                ]
                valid_count = len(values)
                average_edge = sum(values) / valid_count if values else ""No valid data""
                output.append(
                    f""{name}: {average_edge} (from {valid_count} valid values)""
                )
        output.append(""\n"")

    return ""\n"".join(output)"
trace_handler;"def trace_handler(prof):
                prof.export_chrome_trace(""trace.json"")"
__post_init__;"def __post_init__(self) -> None:
        for field in dataclasses.fields(self):
            assert field.type == str
            value: str = getattr(self, field.name)
            object.__setattr__(self, field.name, textwrap.dedent(value))"
init_from_stmts;"def init_from_stmts(
        cls,
        py_stmt: Optional[str] = None,
        cpp_stmt: Optional[str] = None,
        # Generic constructor arguments
        setup: GroupedSetup = GroupedSetup(),
        signature: Optional[str] = None,
        torchscript: bool = False,
        autograd: bool = False,
        num_threads: Union[int, tuple[int, ...]] = 1,
    ) -> ""GroupedBenchmark"":
        """"""Create a set of benchmarks from free-form statements.

        This method of benchmark definition is analogous to Timer use, where
        we simply execute the provided stmts.
        """"""
        if py_stmt is not None:
            py_stmt = textwrap.dedent(py_stmt)

        if cpp_stmt is not None:
            cpp_stmt = textwrap.dedent(cpp_stmt)

        signature_args, signature_output = cls._parse_signature(signature)
        py_model_setup = (
            cls._model_from_py_stmt(
                py_stmt=py_stmt,
                signature_args=signature_args,
                signature_output=signature_output,
            )
            if torchscript
            else None
        )

        return cls(
            py_fwd_stmt=py_stmt,
            cpp_fwd_stmt=cpp_stmt,
            py_model_setup=py_model_setup,
            cpp_model_setup=None,
            inferred_model_setup=True,
            setup=setup,
            signature_args=signature_args,
            signature_output=signature_output,
            torchscript=torchscript,
            autograd=autograd,
            num_threads=(num_threads,) if isinstance(num_threads, int) else num_threads,
        )"
init_from_model;"def init_from_model(
        cls,
        py_model_setup: Optional[str] = None,
        cpp_model_setup: Optional[str] = None,
        # Generic constructor arguments
        setup: GroupedSetup = GroupedSetup(),
        signature: Optional[str] = None,
        torchscript: bool = False,
        autograd: bool = False,
        num_threads: Union[int, tuple[int, ...]] = 1,
    ) -> ""GroupedBenchmark"":
        """"""Create a set of benchmarks using torch.nn Modules.

        This method of benchmark creation takes setup code, and then calls
        a model rather than a free form block of code. As a result, there are
        two additional requirements compared to `init_from_stmts`:
          - `signature` must be provided.
          - A model (named ""model"") must be defined, either with `model = ...`
            or `def model(...): ...` in Python or `auto model = ...` in C++.
        """"""
        signature_args, signature_output = cls._parse_signature(signature)
        if signature_args is None:
            raise ValueError(
                ""signature is needed when initializing from model definitions.""
            )

        return cls(
            *cls._make_model_invocation(
                signature_args, signature_output, RuntimeMode.EAGER
            ),
            py_model_setup=py_model_setup,
            cpp_model_setup=cpp_model_setup,
            inferred_model_setup=False,
            setup=setup,
            signature_args=signature_args,
            signature_output=signature_output,
            torchscript=torchscript,
            autograd=autograd,
            num_threads=(num_threads,) if isinstance(num_threads, int) else num_threads,
        )"
init_from_variants;"def init_from_variants(
        cls,
        py_block: str = """",
        cpp_block: str = """",
        num_threads: Union[int, tuple[int, ...]] = 1,
    ) -> dict[Union[tuple[str, ...], Optional[str]], ""GroupedBenchmark""]:
        py_cases, py_setup, py_global_setup = cls._parse_variants(
            py_block, Language.PYTHON
        )
        cpp_cases, cpp_setup, cpp_global_setup = cls._parse_variants(
            cpp_block, Language.CPP
        )

        assert not py_global_setup
        setup = GroupedSetup(
            py_setup=py_setup,
            cpp_setup=cpp_setup,
            global_setup=cpp_global_setup,
        )

        # NB: The key is actually `Tuple[str, ...]`, however MyPy gets confused
        #     and we use the superset `Union[Tuple[str, ...], Optional[str]` to
        #     match the expected signature.
        variants: dict[Union[tuple[str, ...], Optional[str]], GroupedBenchmark] = {}

        seen_labels: set[str] = set()
        for label in it.chain(py_cases.keys(), cpp_cases.keys()):
            if label in seen_labels:
                continue
            seen_labels.add(label)

            py_lines = py_cases.get(label, [])
            cpp_lines = cpp_cases.get(label, [])

            n_lines = max(len(py_lines), len(cpp_lines))
            py_lines += [""""] * (n_lines - len(py_lines))
            cpp_lines += [""""] * (n_lines - len(cpp_lines))
            lines = [
                (py_stmt, cpp_stmt)
                for py_stmt, cpp_stmt in zip(py_lines, cpp_lines)
                if py_stmt or cpp_stmt
            ]

            for i, (py_stmt, cpp_stmt) in enumerate(lines):
                case = (f""Case: {i:>2}"",) if len(lines) > 1 else ()
                variants[(label,) + case] = GroupedBenchmark.init_from_stmts(
                    py_stmt=py_stmt or None,
                    cpp_stmt=cpp_stmt or None,
                    setup=setup,
                    num_threads=num_threads,
                )

        return variants"
__post_init__;"def __post_init__(self) -> None:
        if self.autograd and self.signature_output is None:
            raise ValueError(
                ""An output variable must be specified when `autograd=True`.""
            )

        if self.py_model_setup and ""model"" not in self.py_model_setup:
            raise ValueError(
                ""`py_model_setup` appears to be missing `model` definition.""
            )

        if self.cpp_model_setup and ""model"" not in self.cpp_model_setup:
            raise ValueError(
                ""`cpp_model_setup` appears to be missing `model` definition.""
            )"
_parse_signature;"def _parse_signature(
        signature: Optional[str],
    ) -> tuple[Optional[tuple[str, ...]], Optional[str]]:
        if signature is None:
            return None, None

        match = re.search(r""^f\((.*)\) -> (.*)$"", signature)
        if match is None:
            raise ValueError(f""Invalid signature: `{signature}`"")

        args: tuple[str, ...] = tuple(match.groups()[0].split("", ""))
        output: str = match.groups()[1].strip()

        if "","" in output:
            raise ValueError(
                f""Multiple return values are not currently allowed: `{output}`""
            )

        if output == ""None"":
            return args, None

        return args, output"
_model_from_py_stmt;"def _model_from_py_stmt(
        py_stmt: Optional[str],
        signature_args: Optional[tuple[str, ...]],
        signature_output: Optional[str],
    ) -> str:
        if py_stmt is None:
            raise ValueError(""`py_stmt` must be defined in order to derive a model."")

        if signature_args is None:
            raise ValueError(""signature is needed in order to derive a model."")

        return textwrap.dedent(
            f""""""\
            def model({"", "".join(signature_args)}):
            {{stmt_str}}
                return {signature_output}
        """"""
        ).format(stmt_str=textwrap.indent(py_stmt, "" "" * 4))"
_make_model_invocation;"def _make_model_invocation(
        signature_args: tuple[str, ...],
        signature_output: Optional[str],
        runtime: RuntimeMode,
    ) -> tuple[str, str]:
        py_prefix, cpp_prefix = """", """"
        if signature_output is not None:
            py_prefix = f""{signature_output} = ""
            cpp_prefix = f""auto {signature_output} = ""

        if runtime == RuntimeMode.EAGER:
            model_name = ""model""
            cpp_invocation = (
                f""{cpp_prefix}{model_name}->forward({', '.join(signature_args)});""
            )

        else:
            assert runtime == RuntimeMode.JIT
            model_name = ""jit_model""
            cpp_invocation = textwrap.dedent(
                f""""""\
                std::vector<torch::jit::IValue> ivalue_inputs({{
                    {"", "".join([f""torch::jit::IValue({a})"" for a in signature_args])}
                }});
                {cpp_prefix}{model_name}.forward(ivalue_inputs);
            """"""
            )

        # NB:
        #   In python we invoke __call__, however C++ doesn't have an analogous
        #   method so we invoke `forward` instead. This means that Python
        #   is doing extra work (e.g. checking hooks) compared to C++; however
        #   because this is the default user experience that's acceptable.
        py_invocation = f""{py_prefix}{model_name}({', '.join(signature_args)})""

        return py_invocation, cpp_invocation"
_parse_variants;"def _parse_variants(
        block: str, language: Language
    ) -> tuple[dict[str, list[str]], str, str]:
        block = textwrap.dedent(block).strip()
        comment = ""#"" if language == Language.PYTHON else ""//""
        label_pattern = f""{comment} @(.+)$""
        label = """"

        lines_by_label: dict[str, list[str]] = {""SETUP"": [], ""GLOBAL_SETUP"": []}
        for line in block.splitlines(keepends=False):
            match = re.search(label_pattern, line.strip())
            if match:
                label = match.groups()[0]
                if label.replace("" "", ""_"").upper() in (""SETUP"", ""GLOBAL_SETUP""):
                    label = label.replace("" "", ""_"").upper()
                continue

            lines_by_label.setdefault(label, [])
            if line.startswith(comment):
                line = """"
            lines_by_label[label].append(line)

        setup = ""\n"".join(lines_by_label.pop(""SETUP""))
        global_setup = ""\n"".join(lines_by_label.pop(""GLOBAL_SETUP""))

        return lines_by_label, setup, global_setup"
_generate_torchscript_file;"def _generate_torchscript_file(model_src: str, name: str) -> Optional[str]:
    """"""Returns the path a saved model if one can be constructed from `spec`.

    Because TorchScript requires actual source code in order to script a
    model, we can't simply `eval` an appropriate model string. Instead, we
    must write the correct source to a temporary Python file and then import
    the TorchScript model from that temporary file.

    `model_src` must contain `jit_model = ...`, which `materialize` will supply.
    """"""
    # Double check.
    assert ""jit_model = "" in model_src, f""Missing jit_model definition:\n{model_src}""

    # `torch.utils.benchmark.Timer` will automatically import torch, so we
    # need to match that convention.
    model_src = f""import torch\n{model_src}""

    model_root = os.path.join(get_temp_dir(), ""TorchScript_models"")
    os.makedirs(model_root, exist_ok=True)
    module_path = os.path.join(model_root, f""torchscript_{name}.py"")
    artifact_path = os.path.join(model_root, f""torchscript_{name}.pt"")

    if os.path.exists(module_path):
        # The uuid in `name` should protect against this, but it doesn't hurt
        # to confirm.
        raise ValueError(f""File {module_path} already exists."")

    with open(module_path, ""w"") as f:
        f.write(model_src)

    # Import magic to actually load our function.
    module_spec = importlib.util.spec_from_file_location(
        f""torchscript__{name}"", module_path
    )
    assert module_spec is not None
    module = importlib.util.module_from_spec(module_spec)
    loader = module_spec.loader
    assert loader is not None

    loader.exec_module(module)

    # And again, the type checker has no way of knowing that this line is valid.
    jit_model = module.jit_model  # type: ignore[attr-defined]
    assert isinstance(jit_model, (torch.jit.ScriptFunction, torch.jit.ScriptModule)), (
        f""Expected ScriptFunction or ScriptModule, got: {type(jit_model)}""
    )
    jit_model.save(artifact_path)  # type: ignore[call-arg]

    # Cleanup now that we have the actual serialized model.
    os.remove(module_path)
    return artifact_path"
_get_stmt;"def _get_stmt(
    benchmark: GroupedBenchmark,
    runtime: RuntimeMode,
    autograd: AutogradMode,
    language: Language,
) -> Optional[str]:
    """"""Specialize a GroupedBenchmark for a particular configuration.""""""
    is_python = language == Language.PYTHON

    # During GroupedBenchmark construction, py_fwd_stmt and cpp_fwd_stmt are
    # set to the eager invocation. So in the RuntimeMode.EAGER case we can
    # simply reuse them. For the RuntimeMode.JIT case, we need to generate
    # an appropriate `jit_model(...)` invocation.
    if runtime == RuntimeMode.EAGER:
        stmts = (benchmark.py_fwd_stmt, benchmark.cpp_fwd_stmt)

    else:
        assert runtime == RuntimeMode.JIT
        assert benchmark.signature_args is not None
        stmts = GroupedBenchmark._make_model_invocation(
            benchmark.signature_args, benchmark.signature_output, RuntimeMode.JIT
        )

    stmt = stmts[0 if is_python else 1]

    if autograd == AutogradMode.FORWARD_BACKWARD and stmt is not None:
        assert benchmark.signature_output is not None
        backward = (
            f""{benchmark.signature_output}""
            # In C++ we have to get the Tensor out of the IValue to call `.backward()`
            f""{'.toTensor()' if runtime == RuntimeMode.JIT and language == Language.CPP else ''}""
            f"".backward(){';' if language == Language.CPP else ''}""
        )
        stmt = f""{stmt}\n{backward}""
    return stmt"
_get_setup;"def _get_setup(
    benchmark: GroupedBenchmark,
    runtime: RuntimeMode,
    language: Language,
    stmt: str,
    model_path: Optional[str],
) -> str:
    """"""Specialize a GroupedBenchmark for a particular configuration.

    Setup requires two extra pieces of information:
      1) The benchmark stmt. This is needed to warm up the model and avoid
         measuring lazy initialization.
      2) The model path so we can load it during the benchmark.

    These are only used when `runtime == RuntimeMode.JIT`.
    """"""

    # By the time we get here, details about how to set up a model have already
    # been determined by GroupedBenchmark. (Or set to None if appropriate.) We
    # simply need to collect and package the code blocks.
    if language == Language.PYTHON:
        setup = benchmark.setup.py_setup
        model_setup = benchmark.py_model_setup
    else:
        assert language == Language.CPP
        setup = benchmark.setup.cpp_setup
        model_setup = benchmark.cpp_model_setup

    if runtime == RuntimeMode.EAGER:
        return ""\n"".join([setup, model_setup or """"])

    assert runtime == RuntimeMode.JIT
    assert model_path is not None

    # We template `""{model_path}""`, so quotes would break model loading. The
    # model path is generated within the benchmark, so this is just an
    # abundance of caution rather than something that is expected in practice.
    assert '""' not in model_path

    # `stmt` may contain newlines, so we can't use f-strings. Instead we need
    # to generate templates so that dedent works properly.
    if language == Language.PYTHON:
        setup_template: str = textwrap.dedent(
            f""""""
            jit_model = torch.jit.load(""{model_path}"")

            # Warmup `jit_model`
            for _ in range(3):
            {{stmt}}
        """"""
        )

    else:
        assert language == Language.CPP
        setup_template = textwrap.dedent(
            f""""""
            const std::string fpath = ""{model_path}"";
            auto jit_model = torch::jit::load(fpath);

            // Warmup `jit_model`
            for (int i = 0; i < 3; i++) {{{{
            {{stmt}}
            }}}}
        """"""
        )

    model_load = setup_template.format(stmt=textwrap.indent(stmt, "" "" * 4))
    return ""\n"".join([setup, model_load])"
_update_active_jobs;"def _update_active_jobs(self) -> None:
        active_jobs: list[InProgress] = []
        for job in self._active_jobs:
            self._currently_processed = job.work_order
            if not job.check_finished():
                active_jobs.append(job)
                continue

            result: Union[WorkerOutput, WorkerFailure] = job.result
            if isinstance(result, WorkerOutput):
                self._results[job.work_order] = result
                assert job.cpu_list is not None
                self._core_pool.release(job.cpu_list)
                self._durations[job.work_order] = job.duration

            else:
                assert isinstance(result, WorkerFailure)
                raise WorkerFailed(cmd=job.proc.cmd, wrapped_trace=result.failure_trace)
        self._currently_processed = None
        self._active_jobs.clear()
        self._active_jobs.extend(active_jobs)"
_enqueue_new_jobs;"def _enqueue_new_jobs(self) -> None:
        work_queue: list[WorkOrder] = []
        for i, work_order in enumerate(self._work_queue):
            self._currently_processed = work_order
            cpu_list = self._core_pool.reserve(work_order.timer_args.num_threads)

            if cpu_list is None:
                work_queue.append(work_order)
            else:
                self._active_jobs.append(InProgress(work_order, cpu_list))

                # Stagger creation. This helps with contention.
                time.sleep(0.5)
        self._currently_processed = None
        self._work_queue.clear()
        self._work_queue.extend(work_queue)"
_print_progress;"def _print_progress(self) -> None:
        fraction = f""{len(self._results)} / {len(self._work_items)}""
        elapsed = f""{time.time() - self._start_time:.0f} seconds""
        if len(self._results) < 5:
            eta = ""Unknown""
        else:
            remaining = len(self._work_items) - len(self._results)
            iters_remaining = math.ceil(remaining / self._core_pool._num_cores)
            mean_time = sum(self._durations.values()) / len(self._durations)
            eta_minutes = math.ceil(iters_remaining * mean_time / 60)
            eta = f""~{eta_minutes:.0f} minute{'s' if eta_minutes > 1 else ''}""
        print(f""\r{fraction} ({elapsed}), ETA: {eta}"", end="""")"
_force_shutdown;"def _force_shutdown(self, verbose: bool = False) -> None:
        """"""Try to interrupt jobs, and kill if need be.
        We would prefer to softly terminate jobs so that they have a chance to
        clean up before shutting down.
        """"""
        for job in self._active_jobs:
            job.proc.interrupt()

        if verbose and self._currently_processed is not None:
            print(
                textwrap.dedent(
                    f""""""
                Failed when processing the following Job:
                  Label:      {self._currently_processed.label}
                  AutoLabels: {self._currently_processed.autolabels}
                  Source cmd: {self._currently_processed.source_cmd}
            """"""
                ).strip()
                + ""\n""
            )

        if self._active_jobs:
            time.sleep(0.5)

        remaining_jobs = [j for j in self._active_jobs if j.proc.poll() is None]
        if remaining_jobs:
            print(
                f""SIGINT sent to {len(self._active_jobs)} jobs, ""
                f""{len(remaining_jobs)} have not yet exited.\n""
                ""Entering short cleanup loop, after which stragglers will ""
                ""be forcibly terminated.""
            )

            for _ in range(5):
                time.sleep(2.0)
                remaining_jobs = [j for j in remaining_jobs if j.proc.poll() is None]
                if remaining_jobs:
                    print(f""{len(remaining_jobs)} still remain."")
                else:
                    print(""All remaining jobs have gracefully terminated."")
                    return

            print(f""{len(remaining_jobs)} jobs refused to exit. Forcibly terminating."")
            for j in remaining_jobs:
                j.proc.terminate()"
_canary_import;"def _canary_import(self) -> None:
        """"""Make sure we can import torch before launching a slew of workers.""""""
        source_cmds: set[str] = set()
        for w in self._work_items:
            if w.source_cmd is not None:
                source_cmds.add(f""{w.source_cmd} && "")

        for source_cmd in source_cmds or {""""}:
            cmd = f'{source_cmd}{PYTHON_CMD} -c ""import torch""'
            proc = subprocess.run(
                cmd,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                encoding=""utf-8"",
                executable=SHELL,
            )

            if proc.returncode:
                raise ImportError(
                    f""Failed to import torch in subprocess: {cmd}\n{proc.stdout}""
                )"
_print_header;"def _print_header(self):
        DASH_LINE = ""-"" * 40
        print(
            f""# {DASH_LINE}\n""
            ""# PyTorch/Caffe2 Operator Micro-benchmarks\n""
            f""# {DASH_LINE}\n""
            f""# Tag : {self.args.tag_filter}\n""
        )
        if self.args.list_tests:
            print(""# List of tests:"")
        elif self.args.list_ops:
            print(""# List of Operators to run:"")
            self.printed_ops_list = set()
            if self.args.operators:
                print(f""# {self.args.operators}"")"
_print_perf_result;"def _print_perf_result(self, reported_run_time_us, test_case):
        if self.args.report_aibench:
            # Output for AIBench
            # Print out per iteration execution time instead of avg time
            return
            test_name = ""_"".join([test_case.framework, test_case.test_config.test_name])
            for run in range(self.num_runs):
                print(
                    f""{test_case.framework}Observer ""
                    + json.dumps(
                        {
                            ""type"": test_name,
                            ""metric"": ""latency"",
                            ""unit"": ""us"",
                            ""value"": str(reported_run_time_us[run]),
                        }
                    )
                )
        else:
            print(f""# Mode: {'JIT' if self.use_jit else 'Eager'}"")
            print(
                f""# Name: {test_case.test_config.test_name}\n# Input: {test_case.test_config.input_config}""
            )

            mode = ""Backward"" if test_case.test_config.run_backward else ""Forward""
            if self.num_runs > 1:
                for run in range(self.num_runs):
                    print(
                        f""Run: {run}, {mode} Execution Time (us) : {reported_run_time_us[run]:.3f}""
                    )
                print()
            else:
                print(f""{mode} Execution Time (us) : {reported_run_time_us[0]:.3f}\n"")"
_perf_result_to_dict;"def _perf_result_to_dict(self, reported_run_time_us, test_case):
        """"""This function is the parallel of _print_perf_result, which instead of
        writing information to terminal, returns a dictionary.
        """"""
        if self.args.report_aibench:
            return {}
        out = {
            ""test_name"": test_case.test_config.test_name,
            ""input_config"": test_case.test_config.input_config,
            ""mode"": ""JIT"" if self.use_jit else ""Eager"",
            ""run"": ""Backward"" if test_case.test_config.run_backward else ""Forward"",
            ""latency"": round(reported_run_time_us[0], 3),
            ""latency unit"": ""us"",
        }

        # parsing test_case.test_config.input_config, adding it as entries to the 'out' dictionary
        # input: 'M: 1, N: 1, K: 1, device: cpu'
        # output: {'M':'1', 'N':'1', 'K':'1', 'device': 'cpu'}
        # splitting the string on unnested commas
        def split(s):
            open_to_close = {""{"": ""}"", ""("": "")"", ""["": ""]""}
            break_idxs = [-1]
            curr_brackets = []
            for i, c in enumerate(s):
                if c in open_to_close.keys():
                    curr_brackets.append(c)
                elif c in open_to_close.values():
                    assert curr_brackets and open_to_close[curr_brackets[-1]] == c, (
                        ""ERROR: not able to parse the string!""
                    )
                    curr_brackets.pop()
                elif c == "","" and (not curr_brackets):
                    break_idxs.append(i)
            break_idxs.append(len(s))
            out = []
            for i in range(len(break_idxs) - 1):
                start, end = break_idxs[i], break_idxs[i + 1]
                out.append(s[start + 1 : end])
            return out

        key_vals = split(
            test_case.test_config.input_config
        )  # 'M: [(32, 16), (64, 32)], ZPB: 2' -> ['M: [(32, 16), (64, 32)]', 'ZPB: 2']
        key_vals = [
            (key.strip(), value.strip())
            for key, value in map(lambda str: str.split("":""), key_vals)  # noqa: C417
        ]  # ['M: (32, 16)', 'ZPB: 2'] -> [('M', '(32, 16)'), ('ZPB', '2')]
        out.update(key_vals)

        return out"
_predict_num_iter_needed;"def _predict_num_iter_needed(self, i):
        return i * self.multiplier"
_iteration_result_is_significant;"def _iteration_result_is_significant(
        self, iters, run_time_sec, curr_test_total_time, has_explicit_iteration_count
    ):
        """"""This function decides whether the measured time can be reported based on the
        following conditions: 1) the number of iterations is larger than the max_iters.
        2) the execution time is larger than the predefined minimum_time
        3) the execution time is larger than user defined minimum_time
        """"""
        return (
            iters > self.max_iters
            or run_time_sec > self.predefined_minimum_secs
            or has_explicit_iteration_count
        ) and curr_test_total_time > self.args.min_time_per_test"
_launch_backward;"def _launch_backward(self, test_case, iters, print_per_iter=False):
        """"""This function runs forward path of an op to get an output. Then the backward path is executed
        and the execution time is reported
        """"""
        test_case.run_forward(num_runs=1, print_per_iter=False, cuda_sync=False)
        test_case._output_mean()
        backward_time = timeit.timeit(
            functools.partial(test_case.run_backward, iters, print_per_iter), number=1
        )
        return backward_time"
lstm_backward_s+764:769etup;"def _measure_time(self, launch_test, test_case, iters, print_per_iter):
        """"""
        This function execute the operator for <iters> iterations then look at the time.
        If it's not significant, the number of iterations will be increased before rerun.
        The execution stops when the time becomes significant.
        """"""
        curr_test_total_time = 0
        time_trace = []
        while True:
            run_time_sec = launch_test(test_case, iters, print_per_iter)
            curr_test_total_time += run_time_sec
            # Analyze time after each run to decide if the result is stable
            results_are_significant = self._iteration_result_is_significant(
                iters,
                run_time_sec,
                curr_test_total_time,
                self.has_explicit_iteration_count,
            )

            report_run_time = 1e6 * run_time_sec / iters
            time_trace.append(report_run_time)
            # Print out the time spent in each epoch in ms
            if self.args.report_aibench:
                mode = ""JIT"" if self.use_jit else ""Eager""
                test_name = ""_"".join(
                    [test_case.framework, test_case.test_config.test_name, mode]
                )
                print(
                    ""PyTorchObserver ""
                    + json.dumps(
                        {
                            ""type"": test_name,
                            ""metric"": ""latency"",
                            ""unit"": ""ms"",
                            ""value"": str(report_run_time / 1e3),
                        }
                    )
                )
            if results_are_significant:
                break

            # Re-estimate the hopefully-sufficient
            # iteration count, and run the benchmark again...
            iters = self._predict_num_iter_needed(iters)
        reported_run_time_us = np.percentile(np.array(time_trace), 50)
        return reported_run_time_us"
_check_keep;"def _check_keep(self, test_flag, cmd_flag):
        return cmd_flag is None or test_flag == cmd_flag"
_check_operator_first_char;"def _check_operator_first_char(self, test_flag, cmd_flag):
        return cmd_flag is None or test_flag[:1].lower() in cmd_flag"
_check_keep_list;"def _check_keep_list(self, test_flag, cmd_flag_list):
        return cmd_flag_list is None or any(
            test_flag == cmd_flag for cmd_flag in cmd_flag_list
        )"
_check_skip;"def _check_skip(self, test_module, cmd_flag):
        return cmd_flag is None or (test_module not in cmd_flag)"
_keep_test;"def _keep_test(self, test_case):
        # TODO: consider regex matching for test filtering.
        # Currently, this is a sub-string matching.
        op_test_config = test_case.test_config

        operators = (
            benchmark_utils.process_arg_list(self.args.operators)
            if self.args.operators
            else None
        )

        # Filter framework, operator, test_name, tag, forward_only
        return (
            self._check_keep(op_test_config.test_name, self.args.test_name)
            and self._check_keep_list(test_case.op_bench.module_name(), operators)
            and self._check_skip(test_case.op_bench.module_name(), SKIP_OP_LISTS)
            and self._check_operator_first_char(
                test_case.op_bench.module_name(), self.operator_range
            )
            and (
                self.args.tag_filter == ""all""
                or self._check_keep(op_test_config.tag, self.args.tag_filter)
            )
            and (
                not self.args.forward_only
                or op_test_config.run_backward != self.args.forward_only
            )
            and (
                self.args.device == ""None""
                or ""device"" not in test_case.test_config.input_config
                or self.args.device in op_test_config.test_name
            )
        )"
_print_test_case_info;"def _print_test_case_info(self, test_case):
        # Print out the test name and skip the real execution
        if self.args.list_tests:
            print(f""# {test_case.test_config.test_name}"")
            return True
        elif self.args.list_ops:
            if self.args.operators is None:
                op_name = test_case.op_bench.module_name()

                if op_name not in self.printed_ops_list:
                    print(f""# {op_name}"")
                    self.printed_ops_list.add(op_name)
            return True

        return False"
create_pytorch_op_test_case;"def create_pytorch_op_test_case(op_bench, test_config):
    """"""This method is used to generate est. func_name is a global unique
    string. For PyTorch add operator with M=8, N=2, K=1, tag = long, here
    are the values for the members in test_case:
    op.module_name: add
    framework: PyTorch
    test_config: TestConfig(test_name='add_M8_N2_K1', input_config='M: 8, N: 2, K: 1',
        tag='long', run_backward=False)
    func_name: addPyTorchTestConfig(test_name='add_M8_N2_K1', input_config='M: 8, N: 2, K: 1',
                                    tag='long', run_backward=False)
    """"""
    test_case = PyTorchOperatorTestCase(op_bench, test_config)
    test_config = test_case.test_config
    op = test_case.op_bench
    func_name = f""{op.module_name()}{test_case.framework}{str(test_config)}""
    return (func_name, test_case)"
_set_backward_test;"def _set_backward_test(self, is_backward):
        self._is_backward = is_backward"
extract_inputs_tuple;"def extract_inputs_tuple(self):
        self.inputs_tuple = tuple(self.inputs.values())"
get_inputs;"def get_inputs(self):
        # Need to convert the inputs to tuple outside of JIT so that
        # JIT can infer the size of the inputs.
        return self.inputs_tuple"
forward_impl;"def forward_impl(self):
        # This is to supply the inputs to the forward function which
        # will be called in both the eager and JIT mode of local runs
        return self.forward(*self.get_inputs())"
forward_consume;"def forward_consume(self, iters: int):
        #  _consume is used to avoid the dead-code-elimination optimization
        for _ in range(iters):
            torch.ops.operator_benchmark._consume(self.forward_impl())"
module_name;"def module_name(self):
        """"""this is used to label the operator being benchmarked""""""
        if self.user_given_name:
            return self.user_given_name
        return self.__class__.__name__"
set_module_name;"def set_module_name(self, name):
        self.user_given_name = name"
_generate_jit_forward_graph;"def _generate_jit_forward_graph(self):
        """"""generate a graph for the forward function via scripting""""""
        scripted_op_bench = torch.jit.script(self.op_bench)
        return scripted_op_bench.forward_consume"
run_jit_forward;"def run_jit_forward(self, num_runs, print_per_iter=False, cuda_sync=False):
        """"""Run the forward path of an op with JIT mode""""""
        if self._jit_forward_graph is None:
            self._jit_forward_graph = self._generate_jit_forward_graph()
        self._jit_forward_graph(num_runs)"
_print_per_iter;"def _print_per_iter(self):
        # print last 50 values
        length = min(len(self.time_series), 50)
        for i in range(length):
            print(
                ""PyTorchObserver ""
                + json.dumps(
                    {
                        ""type"": self.test_config.test_name,
                        ""metric"": ""latency"",
                        ""unit"": ""ms"",
                        ""value"": str(self.time_series[length - i - 1]),
                    }
                )
            )"
run_forward;"def run_forward(self, num_runs, print_per_iter, cuda_sync):
        """"""Run the forward path of an op with eager mode""""""
        if print_per_iter:
            for _ in range(num_runs):
                start_time = time.time()
                self.output = self.op_bench.forward_impl()
                if cuda_sync:
                    torch.cuda.synchronize(torch.cuda.current_device())
                end_time = time.time()
                self.time_series.append((end_time - start_time) * 1e3)
        else:
            for _ in range(num_runs):
                self.output = self.op_bench.forward_impl()
            if cuda_sync:
                torch.cuda.synchronize(torch.cuda.current_device())"
_output_mean;"def _output_mean(self):
        """"""TODO (mingzhe): it is not necessary to sum up everything by myself,
        torch.autograd.backward do take a gradient tensor. By default, it
        is the same shape as your output tensor, with all 1s.
        Mathematically, it is the same as if the output is summed together.
        So we should be able to get ride of this method.
        dummy function for gradient calculation
        """"""
        self.mean = self.output.mean()"
run_backward;"def run_backward(self, num_runs, print_per_iter=False):
        """"""Run the backward path of an op in many iterations""""""
        # TODO: can we use JIT here to reduce python overhead?
        for _ in range(num_runs):
            self.mean.backward(retain_graph=True)"
generate_pt_test;"def generate_pt_test(configs, pt_bench_op):
    """"""This function creates PyTorch op test based on the given operator""""""
    _register_test(configs, pt_bench_op, create_pytorch_op_test_case, False)"
generate_pt_gradient_test;"def generate_pt_gradient_test(configs, pt_bench_op):
    """"""This function creates PyTorch op test based on the given operator""""""
    _register_test(configs, pt_bench_op, create_pytorch_op_test_case, True)"
generate_pt_tests_from_op_list;"def generate_pt_tests_from_op_list(ops_list, configs, pt_bench_op):
    """"""This function creates pt op tests one by one from a list of dictionaries.
    ops_list is a list of dictionary. Each dictionary includes
    the name of the operator and the math operation. Here is an example of using this API:
    unary_ops_configs = op_bench.config_list(
        attrs=[...],
        attr_names=[""M"", ""N""],
    )
    unary_ops_list = op_bench.op_list(
        attr_names=[""op_name"", ""op_func""],
        attrs=[
            [""abs"", torch.abs],
        ],
    )
    class UnaryOpBenchmark(op_bench.TorchBenchmarkBase):
        def init(self, M, N, op_name, op_func):
            ...
        def forward(self):
            ...
    op_bench.generate_pt_tests_from_op_list(unary_ops_list, unary_ops_configs, UnaryOpBenchmark)
    """"""
    for op in ops_list:
        _register_test(configs, pt_bench_op, create_pytorch_op_test_case, False, op)"
generate_pt_gradient_tests_from_op_list;"def generate_pt_gradient_tests_from_op_list(ops_list, configs, pt_bench_op):
    for op in ops_list:
        _register_test(configs, pt_bench_op, create_pytorch_op_test_case, True, op)"
shape_to_string;"def shape_to_string(shape):
    return "", "".join([str(x) for x in shape])"
numpy_random;"def numpy_random(dtype, *shapes):
    """"""Return a random numpy tensor of the provided dtype.
    Args:
        shapes: int or a sequence of ints to defining the shapes of the tensor
        dtype: use the dtypes from numpy
            (https://numpy.org/doc/stable/user/basics.types.html)
    Return:
        numpy tensor of dtype
    """"""
    # TODO: consider more complex/custom dynamic ranges for
    # comprehensive test coverage.
    return np.random.rand(*shapes).astype(dtype)"
set_omp_threads;"def set_omp_threads(num_threads):
    existing_value = os.environ.get(""OMP_NUM_THREADS"", """")
    if existing_value != """":
        print(
            f""Overwriting existing OMP_NUM_THREADS value: {existing_value}; Setting it to {num_threads}.""
        )
    os.environ[""OMP_NUM_THREADS""] = str(num_threads)"
set_mkl_threads;"def set_mkl_threads(num_threads):
    existing_value = os.environ.get(""MKL_NUM_THREADS"", """")
    if existing_value != """":
        print(
            f""Overwriting existing MKL_NUM_THREADS value: {existing_value}; Setting it to {num_threads}.""
        )
    os.environ[""MKL_NUM_THREADS""] = str(num_threads)"
cross_product;"def cross_product(*inputs):
    """"""
    Return a list of cartesian product of input iterables.
    For example, cross_product(A, B) returns ((x,y) for x in A for y in B).
    """"""
    return list(itertools.product(*inputs))"
get_n_rand_nums;"def get_n_rand_nums(min_val, max_val, n):
    random.seed((1 << 32) - 1)
    return random.sample(range(min_val, max_val), n)"
generate_configs;"def generate_configs(**configs):
    """"""
    Given configs from users, we want to generate different combinations of
    those configs
    For example, given M = ((1, 2), N = (4, 5)) and sample_func being cross_product,
    we will generate (({'M': 1}, {'N' : 4}),
                      ({'M': 1}, {'N' : 5}),
                      ({'M': 2}, {'N' : 4}),
                      ({'M': 2}, {'N' : 5}))
    """"""
    assert ""sample_func"" in configs, ""Missing sample_func to generate configs""
    result = []
    for key, values in configs.items():
        if key == ""sample_func"":
            continue
        tmp_result = []
        for value in values:
            tmp_result.append({key: value})
        result.append(tmp_result)

    results = configs[""sample_func""](*result)
    return results"
cross_product_configs;"def cross_product_configs(**configs):
    """"""
    Given configs from users, we want to generate different combinations of
    those configs
    For example, given M = ((1, 2), N = (4, 5)),
    we will generate (({'M': 1}, {'N' : 4}),
                      ({'M': 1}, {'N' : 5}),
                      ({'M': 2}, {'N' : 4}),
                      ({'M': 2}, {'N' : 5}))
    """"""
    _validate(configs)
    configs_attrs_list = []
    for key, values in configs.items():
        tmp_results = [{key: value} for value in values]
        configs_attrs_list.append(tmp_results)

    # TODO(mingzhe0908) remove the conversion to list.
    # itertools.product produces an iterator that produces element on the fly
    # while converting to a list produces everything at the same time.
    generated_configs = list(itertools.product(*configs_attrs_list))
    return generated_configs"
config_list;"def config_list(**configs):
    """"""Generate configs based on the list of input shapes.
    This function will take input shapes specified in a list from user. Besides
    that, all other parameters will be cross producted first and each of the
    generated list will be merged with the input shapes list.

    Reserved Args:
        attr_names(reserved): a list of names for input shapes.
        attrs(reserved): a list of values for each input shape.
        corss_product: a dictionary of attributes which will be
                       cross producted with the input shapes.
        tags(reserved): a tag used to filter inputs.

    Here is an example:
    attrs = [
        [1, 2],
        [4, 5],
    ],
    attr_names = ['M', 'N'],
    cross_product_configs={
        'device': ['cpu', 'cuda'],
    },

    we will generate [[{'M': 1}, {'N' : 2}, {'device' : 'cpu'}],
                      [{'M': 1}, {'N' : 2}, {'device' : 'cuda'}],
                      [{'M': 4}, {'N' : 5}, {'device' : 'cpu'}],
                      [{'M': 4}, {'N' : 5}, {'device' : 'cuda'}]]
    """"""
    generated_configs = []
    reserved_names = [""attrs"", ""attr_names"", ""tags""]
    if any(attr not in configs for attr in reserved_names):
        raise ValueError(""Missing attrs in configs"")

    _validate(configs)

    cross_configs = None
    if ""cross_product_configs"" in configs:
        cross_configs = cross_product_configs(**configs[""cross_product_configs""])

    for inputs in configs[""attrs""]:
        tmp_result = [
            {configs[""attr_names""][i]: input_value}
            for i, input_value in enumerate(inputs)
        ]
        # TODO(mingzhe0908):
        # If multiple 'tags' were provided, do they get concat?
        # If a config has both ['short', 'medium'], it should match
        # both 'short' and 'medium' tag-filter?
        tmp_result.append({""tags"": ""_"".join(configs[""tags""])})
        if cross_configs:
            generated_configs += [tmp_result + list(config) for config in cross_configs]
        else:
            generated_configs.append(tmp_result)

    return generated_configs"
random_sample_configs;"def random_sample_configs(**configs):
    """"""
    This function randomly sample <total_samples> values from the given inputs based on
    their weights.
    Here is an example showing what are the expected inputs and outputs from this function:
    M = [1, 2],
    N = [4, 5],
    K = [7, 8],
    probs = attr_probs(
        M = [0.7, 0.2],
        N = [0.5, 0.2],
        K = [0.6, 0.2],
    ),
    total_samples=10,
    this function will generate
    [
        [{'K': 7}, {'M': 1}, {'N': 4}],
        [{'K': 7}, {'M': 2}, {'N': 5}],
        [{'K': 8}, {'M': 2}, {'N': 4}],
        ...
    ]
    Note:
    The probs is optional. Without them, it implies everything is 1. The probs doesn't
    have to reflect the actual normalized probability, the implementation will
    normalize it.
    TODO (mingzhe09088):
    (1):  a lambda that accepts or rejects a config as a sample. For example: for matmul
    with M, N, and K, this function could get rid of (M * N * K > 1e8) to filter out
    very slow benchmarks.
    (2): Make sure each sample is unique. If the number of samples are larger than the
    total combinations, just return the cross product. Otherwise, if the number of samples
    is close to the number of cross-products, it is numerical safer to generate the list
    that you don't want, and remove them.
    """"""
    if ""probs"" not in configs:
        raise ValueError(
            ""probs is missing. Consider adding probs or using other config functions""
        )

    configs_attrs_list = []
    randomsample = RandomSample(configs)
    for i in range(configs[""total_samples""]):
        tmp_attr_list = randomsample.get_one_set_of_inputs()
        tmp_attr_list.append({""tags"": ""_"".join(configs[""tags""])})
        configs_attrs_list.append(tmp_attr_list)
    return configs_attrs_list"
get_operator_range;"def get_operator_range(chars_range):
    """"""Generates the characters from chars_range inclusive.""""""
    if chars_range == ""None"" or chars_range is None:
        return None

    if all(item not in chars_range for item in ["","", ""-""]):
        raise ValueError(
            ""The correct format for operator_range is ""
            ""<start>-<end>, or <point>, <start>-<end>""
        )

    ops_start_chars_set = set()
    ranges = chars_range.split("","")
    for item in ranges:
        if len(item) == 1:
            ops_start_chars_set.add(item.lower())
            continue
        start, end = item.split(""-"")
        ops_start_chars_set.update(
            chr(c).lower() for c in range(ord(start), ord(end) + 1)
        )
    return ops_start_chars_set"
process_arg_list;"def process_arg_list(arg_list):
    if arg_list == ""None"":
        return None

    return [fr.strip() for fr in arg_list.split("","") if len(fr.strip()) > 0]"
_distribution_func;"def _distribution_func(self, key, weights):
        """"""this is a cumulative distribution function used for random sampling inputs""""""
        if key in self.saved_cum_distribution:
            return self.saved_cum_distribution[key]

        total = sum(weights)
        result = []
        cumsum = 0
        for w in weights:
            cumsum += w
            result.append(cumsum / total)
        self.saved_cum_distribution[key] = result
        return result"
_random_sample;"def _random_sample(self, key, values, weights):
        """"""given values and weights, this function randomly sample values based their weights""""""
        # TODO(mingzhe09088): cache the results to avoid recalculation overhead
        assert len(values) == len(weights)
        _distribution_func_vals = self._distribution_func(key, weights)
        x = random.random()
        idx = bisect.bisect(_distribution_func_vals, x)

        assert idx <= len(values), ""Wrong index value is returned""
        # Due to numerical property, the last value in cumsum could be slightly
        # smaller than 1, and lead to the (index == len(values)).
        if idx == len(values):
            idx -= 1
        return values[idx]"
get_one_set_of_inputs;"def get_one_set_of_inputs(self):
        tmp_attr_list = []
        for key, values in self.configs.items():
            if key in _reserved_keywords:
                continue
            value = self._random_sample(key, values, self.configs[""probs""][str(key)])
            tmp_results = {key: value}
            tmp_attr_list.append(tmp_results)
        return tmp_attr_list"
generate_data_for_repeat;"def generate_data_for_repeat():
    input_tensors = [torch.randn(*input_shape) for input_shape in input_shapes]
    total_num_elements = 0
    for input_tensor, repeat in zip(input_tensors, repeats):
        total_num_elements += input_tensor.numel()
        total_num_elements += input_tensor.numel() * np.prod(repeat)
    return input_tensors, (total_num_elements * DTYPE_TO_BYTES[""float""])"
cudnn_benchmark_configs;"def cudnn_benchmark_configs(configs):
        return [(*config, dict(cudnn=False)) for config in configs]"
set_global_threads;"def set_global_threads(num_threads):
        os.environ[""OMP_NUM_THREADS""] = str(num_threads)
        os.environ[""MKL_NUM_THREADS""] = str(num_threads)
        os.environ[""TVM_NUM_THREADS""] = str(num_threads)
        os.environ[""NNC_NUM_THREADS""] = str(num_threads)"
run_default_configs;"def run_default_configs(bench_cls, allow_skip=True):
        for mode, device, dtype, config in itertools.product(
            modes, devices, datatypes, bench_cls.default_configs()
        ):
            bench = bench_cls(mode, device, dtype, *config)
            bench.output_type = args.output
            bench.jit_mode = args.jit_mode
            if not bench.is_supported():
                if allow_skip:
                    continue
                else:
                    raise ValueError(
                        f""attempted to run an unsupported benchmark: {bench.desc()}""
                    )
            bench.run(args)"
run_with_input_iter;"def run_with_input_iter(bench_cls, input_iter, allow_skip=True):
        tensor_dim_specs = input_iter.split("","")
        tensor_dim_specs = [dim.split("":"") for dim in tensor_dim_specs]

        configs = []
        for start, stop, inc in tensor_dim_specs:
            dim_list = []
            if inc == ""pow2"":
                curr = int(start)
                while curr <= int(stop):
                    dim_list.append(curr)
                    curr <<= 1
            elif inc == ""pow2+1"":
                curr = int(start)
                while curr <= int(stop):
                    dim_list.append(curr)
                    curr -= 1
                    curr <<= 1
                    curr += 1
            else:
                dim_list = list(range(int(start), int(stop) + int(inc), int(inc)))
            configs.append(dim_list)
        configs = itertools.product(*configs)

        for mode, device, dtype, config in itertools.product(
            modes, devices, datatypes, list(configs)
        ):
            bench = bench_cls(mode, device, dtype, *config)
            bench.output_type = args.output
            bench.jit_mode = args.jit_mode
            if not bench.is_supported():
                if allow_skip:
                    continue
                else:
                    raise ValueError(
                        f""attempted to run an unsupported benchmark: {bench.desc()}""
                    )
            bench.run(args)"
benchmark_torch_function_in_microseconds;"def benchmark_torch_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    # warmup
    for _ in range(5):
        func(*args, **kwargs)
    t0 = benchmark.Timer(
        stmt=""func(*args, **kwargs)"",
        globals={""args"": args, ""kwargs"": kwargs, ""func"": func},
    )
    return t0.adaptive_autorange(min_run_time=0.1).median * 1e6"
generate_inputs;"def generate_inputs(
    batch_size, q_sequence_length, kv_sequence_length, embed_dim, dtype, device
):
    q_shape = (batch_size, q_sequence_length, embed_dim)
    kv_shape = (batch_size, kv_sequence_length, embed_dim)

    make_q = partial(torch.rand, q_shape, device=device, dtype=dtype)
    make_kv = partial(torch.rand, kv_shape, device=device, dtype=dtype)
    return make_q(), make_kv(), make_kv()"
run_single_experiment;"def run_single_experiment(config: ExperimentConfig) -> ExperimentResults:
    device = torch.device(""cuda"")
    composite_mha = CompositeMHA(
        config.num_heads, config.embed_dim, device, config.dtype
    )
    composite_mha.reset_parameters()
    query, key, value = generate_inputs(
        config.batch_size,
        config.q_seq_len,
        config.k_seq_len,
        config.embed_dim,
        config.dtype,
        device,
    )
    attn_mask = CausalBias(
        CausalVariant.LOWER_RIGHT, config.q_seq_len, config.k_seq_len
    )
    attn_mask_tensor = attn_mask._materialize(device)

    materialized_mask_time = benchmark_torch_function_in_microseconds(
        composite_mha, query, key, value, attn_mask_tensor
    )
    attn_mask_subclass_time = benchmark_torch_function_in_microseconds(
        composite_mha, query, key, value, attn_mask
    )
    torch.testing.assert_close(
        composite_mha(query, key, value, attn_mask_tensor),
        composite_mha(query, key, value, attn_mask),
    )

    return ExperimentResults(
        materialized_mask_time=materialized_mask_time,
        attn_mask_subclass_time=attn_mask_subclass_time,
    )"
generate_experiment_configs;"def generate_experiment_configs() -> list[ExperimentConfig]:
    batch_sizes = [1, 8, 16, 128]
    num_heads = [16, 32]
    q_kv_seq_lens = [(128, 256), (256, 416), (512, 4097), (1024, 2048), (1, 2048)]
    embed_dims = [2048, 4096]
    dtypes = [
        torch.bfloat16,
    ]
    all_configs = []
    for bsz, heads, (q_seq_len, kv_seq_len), embed_dim, dtype in itertools.product(
        batch_sizes, num_heads, q_kv_seq_lens, embed_dims, dtypes
    ):
        all_configs.append(
            ExperimentConfig(
                batch_size=bsz,
                num_heads=heads,
                q_seq_len=q_seq_len,
                k_seq_len=kv_seq_len,
                embed_dim=embed_dim,
                dtype=dtype,
            )
        )

    return all_configs"
calculate_speedup;"def calculate_speedup(results: ExperimentResults) -> float:
    return results.materialized_mask_time / results.attn_mask_subclass_time"
print_results;"def print_results(results: list[Experiment]):
    # Calculate speedups
    speedups = [calculate_speedup(r.results) for r in results]

    # Find indices of max and min speedups
    max_speedup_index = np.argmax(speedups)
    min_speedup_index = np.argmin(speedups)

    # Get the config dictionaries
    max_config_dict = results[max_speedup_index].config.asdict()
    min_config_dict = results[min_speedup_index].config.asdict()

    # Create table data
    table_data = [
        {
            ""Type"": ""Average"",
            ""Speedup"": np.mean(speedups),
            **dict.fromkeys(max_config_dict),
        },
        {""Type"": ""Max"", ""Speedup"": speedups[max_speedup_index], **max_config_dict},
        {""Type"": ""Min"", ""Speedup"": speedups[min_speedup_index], **min_config_dict},
    ]

    # Print table
    print(tabulate(table_data, headers=""keys"", tablefmt=""pretty""))"
get_entries;"def get_entries(self) -> list:
        return [
            f""{self.materialized_mask_time:2f}"",
            f""{self.attn_mask_subclass_time:2f}"",
        ]"
reset_parameters;"def reset_parameters(self):
        nn.init.xavier_uniform_(self.q_proj_weight)
        nn.init.xavier_uniform_(self.k_proj_weight)
        nn.init.xavier_uniform_(self.v_proj_weight)
        nn.init.constant_(self.out_proj, 0.0)"
benchmark_torch_function;"def benchmark_torch_function(iters, f, *args, **kwargs):
    f(*args, **kwargs)
    f(*args, **kwargs)
    torch.cuda.synchronize()
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    start_event.record()
    for _ in range(iters):
        f(*args, **kwargs)
    end_event.record()
    torch.cuda.synchronize()
    # elapsed_time has a resolution of 0.5 microseconds:
    # but returns milliseconds, so we need to multiply it to increase resolution
    return start_event.elapsed_time(end_event) * 1000 / iters, *f(*args, **kwargs)"
benchmark_torch_function_in_microseconds;"def benchmark_torch_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    # warmup
    for _ in range(5):
        func(*args, **kwargs)
    return benchmarker.benchmark_gpu(lambda: func(*args, **kwargs)) * 1e3"
generate_inputs;"def generate_inputs(
    batch_size: int,
    q_heads: int,
    q_sequence_length: int,
    kv_heads: int,
    kv_sequence_length: int,
    head_dim: int,
    dtype: torch.dtype,
    device: torch.device,
    requires_grad: bool,
    nested_tensors: bool = False,
):
    torch.manual_seed(0)
    q_shape = (batch_size, q_sequence_length, q_heads * head_dim)
    kv_shape = (batch_size, kv_sequence_length, kv_heads * head_dim)

    assert q_heads % kv_heads == 0

    make_q = partial(
        torch.rand, q_shape, device=device, dtype=dtype, requires_grad=requires_grad
    )
    make_kv = partial(
        torch.rand, kv_shape, device=device, dtype=dtype, requires_grad=requires_grad
    )

    if nested_tensors:
        query = (
            make_q()
            .view(1, q_sequence_length * batch_size, q_heads, head_dim)
            .transpose(1, 2)
        )
        key = (
            make_kv()
            .view(1, batch_size * kv_sequence_length, kv_heads, head_dim)
            .transpose(1, 2)
        )
        value = (
            make_kv()
            .view(1, batch_size * kv_sequence_length, kv_heads, head_dim)
            .transpose(1, 2)
        )
    else:
        query = (
            make_q()
            .view(batch_size, q_sequence_length, q_heads, head_dim)
            .transpose(1, 2)
        )
        key = (
            make_kv()
            .view(batch_size, kv_sequence_length, kv_heads, head_dim)
            .transpose(1, 2)
        )
        value = (
            make_kv()
            .view(batch_size, kv_sequence_length, kv_heads, head_dim)
            .transpose(1, 2)
        )
    return query, key, value"
generate_jagged_inputs;"def generate_jagged_inputs(
    shape: tuple[int],
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    offsets: torch.Tensor,
):
    B, Hq, M, Hkv, N, D = shape

    def offsets_to_lengths(
        offsets: torch.Tensor, device: Union[str, torch.device]
    ) -> torch.tensor:
        """"""Converts a list of offsets to a list of lengths. Reverse op of attn_gym.masks.document_mask.length_to_offsets

        Args:
            offsets: A 1D tensor of offsets
            device: The device to place the output tensor on
        """"""
        lengths = offsets[1:] - offsets[:-1]
        return lengths

    flatten_q = query.transpose(1, 2).flatten(start_dim=0, end_dim=1)
    flatten_k = key.transpose(1, 2).flatten(start_dim=0, end_dim=1)
    flatten_v = value.transpose(1, 2).flatten(start_dim=0, end_dim=1)

    q_list = [
        flatten_q[offsets[i] : offsets[i + 1]].clone().detach().to(query.dtype)
        for i in range(len(offsets) - 1)
    ]
    q = torch.nested.as_nested_tensor(q_list, device=query.device)

    k_list = [
        flatten_k[offsets[i] : offsets[i + 1]].clone().detach().to(key.dtype)
        for i in range(len(offsets) - 1)
    ]
    k = torch.nested.as_nested_tensor(k_list, device=key.device)
    v_list = [
        flatten_v[offsets[i] : offsets[i + 1]].clone().detach().to(value.dtype)
        for i in range(len(offsets) - 1)
    ]
    v = torch.nested.as_nested_tensor(v_list, device=value.device)

    return q, k, v"
query_key_value_clones;"def query_key_value_clones(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    dtype: torch.dtype = None,
):
    """"""Clones the query, key, and value tensors and moves them to the specified dtype.""""""
    if dtype is None:
        dtype = query.dtype
    query_ref = query.clone().detach().to(dtype).requires_grad_(query.requires_grad)
    key_ref = key.clone().detach().to(dtype).requires_grad_(key.requires_grad)
    value_ref = value.clone().detach().to(dtype).requires_grad_(value.requires_grad)
    return query_ref, key_ref, value_ref"
run_single_backend_sdpa;"def run_single_backend_sdpa(
    config: ExperimentConfig,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    out_compile: torch.Tensor,
    score_mod: Callable | None,
    block_mask: BlockMask | None,
    mask_kwargs,
    backend: str,
) -> ExperimentResults:
    backend_context = get_backend_context(backend)
    with backend_context:
        _device = torch.device(""cuda"")
        eager_sdpa = generate_eager_sdpa(
            config.attn_type, config.shape, config.dtype, block_mask, score_mod
        )

        if config.attn_type == ""document_mask"":
            q_eager, k_eager, v_eager = generate_jagged_inputs(
                config.shape, query, key, value, **mask_kwargs
            )
            q_eager = q_eager.transpose(1, 2).requires_grad_(query.requires_grad)
            k_eager = k_eager.transpose(1, 2).requires_grad_(key.requires_grad)
            v_eager = v_eager.transpose(1, 2).requires_grad_(value.requires_grad)
        else:
            q_eager, k_eager, v_eager = query_key_value_clones(query, key, value)

        if eager_sdpa:
            try:
                out_eager = eager_sdpa(query=q_eager, key=k_eager, value=v_eager)
            except RuntimeError as e:
                print(
                    f""[SKIP] SDPA Backend {backend} for shape {config.shape}. \n\t\t\tError encountered: {e} ""
                )
                return ExperimentResults(
                    fwd_time=float(""nan""),
                    bwd_time=float(""nan"") if config.calculate_bwd_time else None,
                )
            if config.attn_type in [""document_mask""]:
                flatten_o_eager = torch.cat(torch.unbind(out_eager.transpose(1, 2)))
                flatten_o_compile = out_compile.transpose(1, 2).flatten(
                    start_dim=0, end_dim=1
                )
                torch.testing.assert_close(
                    flatten_o_eager, flatten_o_compile, atol=1e-2, rtol=1e-2
                )
            elif not (
                config.attn_type in [""rel"", ""alibi""]
                and config.dtype in [torch.float16, torch.bfloat16]
            ):  # rel has accuracy issue with 16bit floats
                torch.testing.assert_close(out_eager, out_compile, atol=1e-2, rtol=1e-2)

        if eager_sdpa:
            forward_eager_time = benchmark_torch_function_in_microseconds(
                eager_sdpa, query=q_eager, key=k_eager, value=v_eager
            )
        else:
            forward_eager_time = float(""nan"")

        if config.calculate_bwd_time:
            # TODO: debug backward pass for njt
            if eager_sdpa and not config.attn_type == ""document_mask"":
                dOut = torch.randn_like(out_eager.transpose(1, 2)).transpose(1, 2)
                backward_eager_time = benchmark_torch_function_in_microseconds(
                    out_eager.backward, dOut, retain_graph=True
                )
            else:
                backward_eager_time = float(""nan"")

            return ExperimentResults(
                fwd_time=forward_eager_time,
                bwd_time=backward_eager_time,
            )
        else:
            return ExperimentResults(
                fwd_time=forward_eager_time,
                bwd_time=None,
            )"
run_single_backend_FA;"def run_single_backend_FA(
    config: ExperimentConfig,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    out_compile: torch.Tensor,
    score_mod: Callable | None,
    block_mask: BlockMask | None,
    mask_kwargs,
    backend: str,
) -> ExperimentResults:
    assert backend in [""fav2"", ""fav3"", ""fakv""]
    # Generate callable for specific backend.
    if backend in [""fav2"", ""fav3""]:
        FA = generate_FA_callable(
            config.attn_type, config.shape, config.dtype, backend, **mask_kwargs
        )
    elif backend == ""fakv"":
        FA = generate_FD_callable(config.attn_type, config.shape, config.dtype)

    q_FA, k_FA, v_FA = query_key_value_clones(query, key, value)
    q_FA, k_FA, v_FA = q_FA.transpose(1, 2), k_FA.transpose(1, 2), v_FA.transpose(1, 2)
    if config.attn_type == ""document_mask"":
        q_FA = q_FA.flatten(start_dim=0, end_dim=1)
        k_FA = k_FA.flatten(start_dim=0, end_dim=1)
        v_FA = v_FA.flatten(start_dim=0, end_dim=1)

    if FA:
        out_FA = FA(q=q_FA, k=k_FA, v=v_FA)
        if config.attn_type in [""document_mask""]:
            out_FA_updated = out_FA[None, :, :, :]
        else:
            out_FA_updated = out_FA

        if not (
            config.attn_type in [""rel"", ""alibi""]
            and config.dtype in [torch.float16, torch.bfloat16]
        ):
            torch.testing.assert_close(
                out_FA_updated, out_compile.transpose(1, 2), atol=1e-2, rtol=1e-2
            )

    if FA:
        forward_FA_time = benchmark_torch_function_in_microseconds(
            FA, q=q_FA, k=k_FA, v=v_FA
        )
    else:
        forward_FA_time = float(""nan"")

    if config.calculate_bwd_time:
        if FA:
            dOut = torch.randn_like(out_FA)
            backward_FA_time = benchmark_torch_function_in_microseconds(
                out_FA.backward, dOut, retain_graph=True
            )
        else:
            backward_FA_time = float(""nan"")

    return ExperimentResults(
        fwd_time=forward_FA_time,
        bwd_time=backward_FA_time if config.calculate_bwd_time else None,
    )"
run_single_experiment;"def run_single_experiment(
    config: ExperimentConfig,
    dynamic=False,
    max_autotune=False,
) -> dict[str, ExperimentResults]:
    device = torch.device(""cuda"")
    batch_size, q_heads, q_seq_len, kv_heads, kv_seq_len, head_dim = config.shape
    query, key, value = generate_inputs(
        batch_size,
        q_heads,
        q_seq_len,
        kv_heads,
        kv_seq_len,
        head_dim,
        config.dtype,
        device,
        requires_grad=config.calculate_bwd_time,
        nested_tensors=config.attn_type == ""document_mask"",
    )
    score_mod = generate_score_mod(config.attn_type, config.shape)
    block_mask, mask_kwargs = generate_block_mask(config.attn_type, config.shape)
    kernel_options = get_kernel_options(config.attn_type, config.shape)

    if max_autotune:
        compiled_sdpa = torch.compile(
            flex_attention, dynamic=dynamic, mode=""max-autotune-no-cudagraphs""
        )
    else:
        compiled_sdpa = torch.compile(flex_attention, dynamic=dynamic)

    out_compile = compiled_sdpa(
        query=query,
        key=key,
        value=value,
        score_mod=score_mod,
        block_mask=block_mask,
        enable_gqa=True,
        kernel_options=kernel_options,
    )

    forward_compiled_time = benchmark_torch_function_in_microseconds(
        compiled_sdpa,
        query,
        key,
        value,
        score_mod=score_mod,
        block_mask=block_mask,
        enable_gqa=True,
        kernel_options=kernel_options,
    )

    results = {}
    for backend in config.backends:
        if backend in [""fav2"", ""fav3"", ""fakv""]:
            results[backend] = run_single_backend_FA(
                config,
                query,
                key,
                value,
                out_compile,
                score_mod,
                block_mask,
                mask_kwargs,
                backend,
            )
        else:  # sdpa
            results[backend] = run_single_backend_sdpa(
                config,
                query,
                key,
                value,
                out_compile,
                score_mod,
                block_mask,
                mask_kwargs,
                backend,
            )

    if config.calculate_bwd_time:
        dOut = torch.randn_like(out_compile)
        backward_compile_time = benchmark_torch_function_in_microseconds(
            out_compile.backward, dOut, retain_graph=True
        )
    sparsity = block_mask.sparsity() / 100.0 if block_mask is not None else 0.0
    sparsity = sparsity if config.attn_type != ""document_mask"" else 0.5

    results[""compiled""] = ExperimentResults(
        fwd_time=forward_compiled_time,
        bwd_time=backward_compile_time if config.calculate_bwd_time else None,
        sparsity=sparsity,
    )

    return results"
calculate_speedup;"def calculate_speedup(
    results: ExperimentResults, baseline_results: ExperimentResults, type: str
) -> float:
    if type == ""fwd"":
        return baseline_results.fwd_time / results.fwd_time
    elif type == ""bwd"":
        assert results.bwd_time is not None
        return baseline_results.bwd_time / results.bwd_time
    else:
        raise ValueError(f""Invalid type {type}"")"
calculate_bandwidth;"def calculate_bandwidth(
    config: ExperimentConfig, results: ExperimentResults, type: str
) -> float:
    B, Hq, M, Hkv, N, D = config.shape
    sparsity = results.sparsity if M == 1 else 0.0
    if type == ""fwd"":
        batch_size, q_heads, q_seq_len, kv_heads, kv_seq_len, head_dim = config.shape
        query_size = (
            batch_size
            * q_heads
            * q_seq_len
            * head_dim
            * torch.finfo(config.dtype).bits
            / 8
        )
        kv_size = (
            batch_size
            * kv_heads
            * kv_seq_len
            * head_dim
            * torch.finfo(config.dtype).bits
            / 8
            * 2
        )
        output_size = query_size
        total_size = (
            query_size + kv_size * (1 - sparsity) + output_size
        ) / 1e9  # In GB
        time_in_seconds = results.fwd_time / 1e6
        return total_size / time_in_seconds / 1e3
    else:
        raise ValueError(f""Invalid type {type}"")"
calculate_tflops;"def calculate_tflops(config: ExperimentConfig, results: ExperimentResults) -> float:
    (B, Hq, M, Hkv, N, D) = config.shape
    qk_flops = M * N * D * 2
    softmax_flops = M * N * 2  # Not counting online softmax overhead
    o_flops = M * D * N * 2
    # Not counting split k overhead
    total_flops = B * Hq * (qk_flops + softmax_flops + o_flops) * (1 - results.sparsity)
    return total_flops / results.fwd_time / 1e6"
get_average_speedups;"def get_average_speedups(results: list[Experiment], type: str, backend: str):
    # Calculate speedups
    speedups = [
        calculate_speedup(r.results[""compiled""], r.results[backend], type)
        for r in results
    ]

    # Find indices of max and min speedups
    max_speedup_index = np.nanargmax(speedups)
    min_speedup_index = np.nanargmin(speedups)

    # Get the config dictionaries
    max_config_dict = results[max_speedup_index].config.asdict()
    min_config_dict = results[min_speedup_index].config.asdict()

    # Create table data
    table_data = [
        {
            ""Type"": ""Average"",
            ""Speedup"": np.nanmean(speedups),
            **dict.fromkeys(max_config_dict),
        },
        {""Type"": ""Max"", ""Speedup"": speedups[max_speedup_index], **max_config_dict},
        {""Type"": ""Min"", ""Speedup"": speedups[min_speedup_index], **min_config_dict},
    ]

    return table_data"
print_results;"def print_results(results: list[Experiment], save_path: Optional[str] = None):
    table_data = defaultdict(list)
    for experiment in results:
        backends = experiment.config.backends + [""compiled""]
        for key, value in experiment.asdict().items():
            if key in backends:
                if value.fwd_time:
                    table_data[f""fwd_{key}""].append(float(value.fwd_time))
                if value.bwd_time:
                    table_data[f""bwd_{key}""].append(float(value.bwd_time))
            else:
                table_data[key].append(value)

    # Calculate speedups
    for backend in results[0].config.backends:
        fwd_speedups = [
            calculate_speedup(r.results[""compiled""], r.results[backend], type=""fwd"")
            for r in results
        ]
        table_data[f""fwd_{backend}_speedup""] = fwd_speedups

    if results[0].config.calculate_bwd_time:
        for backend in results[0].config.backends:
            bwd_speedups = [
                calculate_speedup(r.results[""compiled""], r.results[backend], type=""bwd"")
                for r in results
            ]
            table_data[f""bwd_{backend}_speedup""] = bwd_speedups

    # Calculate mem + computational throughput
    if results[0].config.cal_bandwidth:
        fwd_bandwidth = [
            calculate_bandwidth(r.config, r.results[""compiled""], type=""fwd"")
            for r in results
        ]
        table_data[""fwd_mem_bw (TB/s)""] = fwd_bandwidth
        fwd_tflops = [
            calculate_tflops(r.config, r.results[""compiled""]) for r in results
        ]
        table_data[""TFlops/s""] = fwd_tflops

    print(tabulate(table_data, headers=""keys"", tablefmt=""github"", floatfmt="".3f""))

    for backend in results[0].config.backends:
        if np.isnan(table_data[f""fwd_{backend}_speedup""]).all():
            continue
        print(""\n"")
        print(f""FWD Speedups vs. {backend}"".center(125, ""=""))
        print(""\n"")
        average_data = get_average_speedups(results, type=""fwd"", backend=backend)
        print(tabulate(average_data, headers=""keys"", tablefmt=""github"", floatfmt="".3f""))

        if results[0].config.calculate_bwd_time:
            print(""\n"")
            print(f""BWD Speedups vs. {backend}"".center(125, ""=""))
            print(""\n"")
            average_data = get_average_speedups(results, type=""bwd"", backend=backend)
            print(
                tabulate(
                    average_data, headers=""keys"", tablefmt=""github"", floatfmt="".3f""
                )
            )

    if save_path is not None:
        with open(save_path, ""w"", newline="""") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=table_data.keys())
            writer.writeheader()
            for i in range(len(next(iter(table_data.values())))):
                row = {k: v[i] for k, v in table_data.items()}
                writer.writerow(row)
        print(f""\nResults saved to {save_path}"")"
generate_score_mod;"def generate_score_mod(attn_type: str, shape: tuple[int]) -> Callable | None:
    B, Hq, M, Hkv, N, D = shape
    is_decoding = M == 1
    from attn_gym.mods import generate_alibi_bias, generate_tanh_softcap

    def relative_bias(score, b, h, m, n):
        return score + (m - n)

    def head_bias(score, b, h, m, n):
        return score + 2 * h

    function_dict = {
        ""noop"": None,
        ""causal"": None,
        ""rel"": relative_bias,
        ""head_bias"": head_bias,
        ""alibi"": generate_alibi_bias(Hq),
        ""sliding_window"": None,
        ""document_mask"": None,
        ""prefix_lm"": None,
        ""softcap"": generate_tanh_softcap(softcap_value, approx=True),
    }

    score_mod = function_dict[attn_type]
    is_decoding = M == 1
    if is_decoding and score_mod:
        offset = torch.tensor(N // 2).to(""cuda"")

        def score_mod_w_offset(score, b, h, m, n):
            return score_mod(score, b, h, m + offset, n)

        new_score_mod = score_mod_w_offset
    else:
        new_score_mod = score_mod

    return new_score_mod"
generate_block_mask;"def generate_block_mask(attn_type: str, shape: tuple[int]):
    B, Hq, M, Hkv, N, D = shape
    is_decoding = M == 1

    def causal(b, h, m, n):
        return m >= n

    def gen_offset(off):
        def offset(b, h, m, n):
            return m + off >= n

        return offset

    from attn_gym.masks import (
        generate_doc_mask_mod,
        generate_prefix_lm_mask,
        generate_sliding_window,
    )
    from attn_gym.masks.document_mask import length_to_offsets

    def generate_random_lengths(total_length, num_documents):
        # Initialize all lengths to 1 to ensure each document has at least one token
        lengths = [1] * num_documents
        remaining_length = total_length - num_documents

        # Randomly distribute the remaining length
        for _ in range(remaining_length):
            index = random.randint(0, num_documents - 1)
            lengths[index] += 1
        return lengths

    mask_mod_kwargs = {}

    assert attn_type != ""document_mask"" or not is_decoding
    if attn_type == ""document_mask"":
        random.seed(0)
        lengths = generate_random_lengths(N * B, B)
        mask_mod_kwargs = dict(offsets=length_to_offsets(lengths, ""cuda""))

    mask_mod_dict = {
        ""noop"": None,
        ""causal"": causal,
        ""rel"": None,
        ""head_bias"": None,
        ""alibi"": causal,
        ""sliding_window"": generate_sliding_window(sliding_window_size),
        ""document_mask"": partial(generate_doc_mask_mod, mask_mod=causal),
        ""prefix_lm"": generate_prefix_lm_mask(prefix_length),
        ""softcap"": causal,
    }

    mask_mod = mask_mod_dict[attn_type]

    if mask_mod_kwargs:
        mask_mod = mask_mod(**mask_mod_kwargs)

    if is_decoding and mask_mod:
        cached_seq_len = torch.tensor(N // 2).to(""cuda"")

        def decoding_w_cached_seq_len(b, h, m, n):
            return mask_mod(b, h, m + cached_seq_len, n)

        new_mask_mod = decoding_w_cached_seq_len
    else:
        new_mask_mod = mask_mod

    mask_shape = (1, 1, M, N) if attn_type != ""document_mask"" else (1, 1, M * B, N * B)
    compiled_block_mask = torch.compile(create_block_mask)
    if new_mask_mod:
        block_mask = compiled_block_mask(new_mask_mod, *mask_shape, ""cuda"")
    else:
        block_mask = compiled_block_mask(noop_mask, *mask_shape, ""cuda"")
    return block_mask, mask_mod_kwargs"
get_kernel_options;"def get_kernel_options(attn_type: str, shape: tuple[int]):
    B, Hq, M, Hkv, N, D = shape
    is_decoding = M == 1
    kernel_opt_training_dict = {
        ""noop"": None,
        ""causal"": None,
        ""rel"": None,
        ""head_bias"": None,
        ""alibi"": None,
        ""sliding_window"": None,
        ""document_mask"": {
            ""BLOCK_N"": 32,
            ""BLOCK_M"": 128,
            ""fwd_num_warps"": 8,
            ""fwd_num_stages"": 4,
            ""BLOCK_M1"": 64,
            ""BLOCK_N1"": 64,
            ""BLOCK_M2"": 64,
            ""BLOCK_N2"": 64,
        }
        if torch.cuda.get_device_capability() >= (8, 0) and D <= 128
        else None,
        ""prefix_lm"": None,
        ""softcap"": None,
    }

    def get_default_split_k(B: int, H: int, Mk: int) -> int:
        num_SM = torch.cuda.get_device_properties(""cuda"").multi_processor_count
        """"""Heuristic for the number of splits from xformer""""""
        bh = max(B * H, 1)  # NOTE: Handle B*h=0 case
        split_k = num_SM // bh * 2  # Each SM should at least get one block.
        split_k = max(split_k, 1)

        return split_k

    kernel_opt_decoding_dict = {
        ""noop"": None,
        ""causal"": {""SPLIT_KV"": get_default_split_k(B, Hkv, N) * 2},
        ""rel"": None,
        ""head_bias"": None,
        ""alibi"": {""SPLIT_KV"": get_default_split_k(B, Hkv, N) * 2},
        ""sliding_window"": None,
        ""document_mask"": None,
        ""prefix_lm"": None,
        ""softcap"": {""SPLIT_KV"": get_default_split_k(B, Hkv, N) * 2},
    }

    return (
        kernel_opt_decoding_dict[attn_type]
        if is_decoding
        else kernel_opt_training_dict[attn_type]
    )"
get_backend_context;"def get_backend_context(backend: str):
    """"""
    Returns a context manager for the specified backend.
    Args:
        backend (str): The name of the backend to use.
                       Valid options are 'fav2', 'cudnn', 'math', 'efficient', 'fav3', 'fakv', 'og-eager'.
    Returns:
        A context manager for the specified backend.
    Raises:
        ValueError: If an invalid backend is specified.
    """"""
    backends = {
        ""fav2"": nullcontext(),
        ""cudnn"": sdpa_kernel(SDPBackend.CUDNN_ATTENTION),
        ""math"": sdpa_kernel(SDPBackend.MATH),
        ""efficient"": sdpa_kernel(SDPBackend.EFFICIENT_ATTENTION),
        ""fav3"": nullcontext(),
        ""fakv"": nullcontext(),
        ""og-eager"": nullcontext(),
    }

    if backend not in backends:
        raise ValueError(
            f""Unknown backend: {backend}. Valid options are: {', '.join(backends.keys())}""
        )

    return backends[backend]"
generate_FA_callable;"def generate_FA_callable(
    attn_type: str, shape: tuple[int], dtype: torch.dtype, backend: str, **kwargs
) -> Callable | None:
    if dtype not in [torch.float16, torch.bfloat16]:
        return None
    if backend == ""fav2"":
        try:
            from flash_attn import flash_attn_func, flash_attn_varlen_func
        except ImportError:
            print(
                ""Flash attention 2 is not installed. Please install it to run fav2 backend. ""
            )
            raise
    elif backend == ""fav3"":
        try:
            from flash_attn.flash_attn_interface import (
                flash_attn_func,
                flash_attn_varlen_func,
            )
        except ImportError:
            print(
                ""Flash attention 3 is not installed. Please install it to run fav3 backend. ""
            )
            raise
    else:
        print(""Unknown backend "" + backend)
        return None

    B, Hq, M, Hkv, N, D = shape

    FA_kwargs = {}
    if attn_type == ""alibi"":
        h = torch.arange(Hq, dtype=torch.float32, device=""cuda"")
        alibi_slopes = torch.exp2(-((h + 1) * 8.0 / Hq))
        FA_kwargs = dict(alibi_slopes=alibi_slopes)
    elif attn_type == ""document_mask"":
        FA_kwargs[""cu_seqlens_q""] = kwargs[""offsets""].to(torch.int32)
        FA_kwargs[""cu_seqlens_k""] = kwargs[""offsets""].to(torch.int32)

        def offsets_to_lengths(
            offsets: torch.Tensor, device: Union[str, torch.device]
        ) -> torch.tensor:
            lengths = offsets[1:] - offsets[:-1]
            return lengths

        lengths = offsets_to_lengths(kwargs[""offsets""], ""cpu"")
        max_length = torch.max(lengths)
        FA_kwargs[""max_seqlen_q""] = max_length
        FA_kwargs[""max_seqlen_k""] = max_length

    FA_dict = {
        ""noop"": partial(flash_attn_func, causal=False),
        ""causal"": partial(flash_attn_func, causal=True),
        ""rel"": None,
        ""head_bias"": None,
        ""alibi"": partial(flash_attn_func, causal=True, **FA_kwargs),
        ""sliding_window"": partial(
            flash_attn_func, window_size=(sliding_window_size, 0), causal=True
        ),
        ""document_mask"": partial(flash_attn_varlen_func, causal=True, **FA_kwargs),
        ""prefix_lm"": None,
        ""softcap"": partial(flash_attn_func, softcap=softcap_value, causal=True),
    }

    return FA_dict[attn_type]"
generate_FD_callable;"def generate_FD_callable(
    attn_type: str, shape: tuple[int], dtype: torch.dtype
) -> Callable | None:
    if dtype not in [torch.float16, torch.bfloat16]:
        return None
    try:
        from flash_attn import flash_attn_with_kvcache
    except ImportError:
        print(
            ""Flash attention 2 is not installed. Please install it to run fakv backend. ""
        )
        raise

    B, Hq, M, Hkv, N, D = shape

    assert M == 1

    def flash_attn_with_kvcache_renamed(q, k, v, **kwargs):
        return flash_attn_with_kvcache(q, k_cache=k, v_cache=v, **kwargs)

    FA_kwargs = {}
    if attn_type == ""alibi"":
        h = torch.arange(Hq, dtype=torch.float32, device=""cuda"")
        alibi_slopes = torch.exp2(-((h + 1) * 8.0 / Hq))
        FA_kwargs = dict(alibi_slopes=alibi_slopes)

    FD_dict = {
        ""noop"": partial(flash_attn_with_kvcache_renamed, causal=False),
        ""causal"": partial(flash_attn_with_kvcache_renamed, cache_seqlens=N // 2),
        ""rel"": None,
        ""head_bias"": None,
        ""alibi"": partial(
            flash_attn_with_kvcache_renamed, cache_seqlens=N // 2, **FA_kwargs
        ),
        ""sliding_window"": partial(
            flash_attn_with_kvcache_renamed,
            cache_seqlens=N // 2,
            window_size=(sliding_window_size, 0),
        ),
        ""document_mask"": None,
        ""prefix_lm"": None,
        ""softcap"": partial(flash_attn_with_kvcache_renamed, softcap=softcap_value),
    }

    return FD_dict[attn_type]"
generate_attn_mask_linear_score_mod;"def generate_attn_mask_linear_score_mod(
    shape: tuple[int], block_mask: BlockMask, score_mod: Callable, dtype: torch.dtype
):
    B, Hq, M, N = shape
    if block_mask is None and score_mod is None:
        return None
    b = torch.arange(B, dtype=int, device=""cuda"")
    h = torch.arange(Hq, dtype=int, device=""cuda"")
    m = torch.arange(M, dtype=int, device=""cuda"")
    n = torch.arange(N, dtype=int, device=""cuda"")

    score = torch.zeros(B, Hq, M, N, dtype=dtype, device=""cuda"")
    bias = score_mod(
        score,
        b[:, None, None, None],
        h[None, :, None, None],
        m[None, None, :, None],
        n[None, None, None, :],
    )
    bool_mask = create_mask(block_mask.mask_mod, B, Hq, M, N, device=""cuda"")
    attn_mask = bias.masked_fill(bool_mask.logical_not(), float(""-inf""))
    return attn_mask.to(dtype)"
generate_eager_sdpa;"def generate_eager_sdpa(
    attn_type: str,
    shape: tuple[int],
    dtype: torch.dtype,
    block_mask: BlockMask,
    score_mod: Callable | None = None,
    **kwargs,
) -> Callable | None:
    B, Hq, M, Hkv, N, D = shape
    is_decoding = M == 1
    if attn_type == ""sliding_window"" or attn_type == ""prefix_lm"":
        attn_mask = create_mask(block_mask.mask_mod, 1, 1, M, N, device=""cuda"")
    elif attn_type == ""rel"":
        attn_mask = generate_attn_mask_linear_score_mod(
            [1, 1, M, N], block_mask, score_mod, dtype
        )
    elif attn_type == ""head_bias"":
        h = torch.arange(Hq, dtype=int, device=""cuda"")
        attn_mask = (2 * h[None, :, None, None]).broadcast_to(1, Hq, M, N).to(dtype)
    elif attn_type == ""alibi"":
        attn_mask = generate_attn_mask_linear_score_mod(
            [1, Hq, M, N], block_mask, score_mod, dtype
        )
    else:
        attn_mask = None

    sdpa_dict = {
        ""noop"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""causal"": partial(
            F.scaled_dot_product_attention, is_causal=True, enable_gqa=(Hq != Hkv)
        ),
        ""rel"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""head_bias"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""alibi"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""sliding_window"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""document_mask"": partial(
            F.scaled_dot_product_attention, is_causal=True, enable_gqa=(Hq != Hkv)
        )
        if Hq == Hkv
        else None,
        ""prefix_lm"": partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        ),
        ""softcap"": None,
    }

    if is_decoding and attn_type == ""causal"":
        attn_mask = create_mask(block_mask.mask_mod, 1, 1, M, N, device=""cuda"")
        sdpa_dict[""causal""] = partial(
            F.scaled_dot_product_attention, is_causal=False, enable_gqa=(Hq != Hkv)
        )

    return (
        partial(sdpa_dict[attn_type], attn_mask=attn_mask)
        if sdpa_dict[attn_type]
        else None
    )"
generate_experiment_configs;"def generate_experiment_configs(
    calculate_bwd: bool,
    dtype: torch.dtype,
    batch_sizes: list[int],
    num_heads: list[tuple[int, int]],
    seq_lens: list[int],
    head_dims: list[int],
    score_mods_str: list[str],
    decoding: bool,
    kv_cache_size: list[int],
    cal_bandwidth: bool,
    backends: list[str],
) -> list[ExperimentConfig]:
    assert not (calculate_bwd and decoding), ""Decoding does not support backward""

    if decoding:
        q_kv_seq_lens = [(1, i) for i in seq_lens]  # only testing query length == 1
    else:
        q_kv_seq_lens = [(i, i) for i in seq_lens]  # only testing q_len == kv_len
    dtypes = [dtype]

    all_configs = []
    for (
        bsz,
        (q_heads, kv_heads),
        (q_seq_len, kv_seq_len),
        head_dim,
        attn_type,
        dtype,
    ) in itertools.product(
        kv_cache_size if kv_cache_size else batch_sizes,
        num_heads,
        q_kv_seq_lens,
        head_dims,
        score_mods_str,
        dtypes,
    ):
        if kv_cache_size:
            head_size_bytes = torch.finfo(dtype).bits / 8 * head_dim
            bsz = int(
                (bsz * 1024 * 1024) // (kv_heads * kv_seq_len * head_size_bytes * 2)
            )
            if bsz <= 0:
                continue

        assert q_heads % kv_heads == 0

        all_configs.append(
            ExperimentConfig(
                shape=(bsz, q_heads, q_seq_len, kv_heads, kv_seq_len, head_dim),
                attn_type=attn_type,
                dtype=dtype,
                calculate_bwd_time=calculate_bwd,
                cal_bandwidth=cal_bandwidth,
                backends=backends,
            )
        )

    return all_configs"
heads_input_type;"def heads_input_type(s):
    try:
        hq, hkv = map(int, s.split("",""))
        return hq, hkv
    except Exception as e:
        raise argparse.ArgumentTypeError(""Heads must be Hq,Hkv"") from e"
offsets_to_lengths;"def offsets_to_lengths(
        offsets: torch.Tensor, device: Union[str, torch.device]
    ) -> torch.tensor:
        """"""Converts a list of offsets to a list of lengths. Reverse op of attn_gym.masks.document_mask.length_to_offsets

        Args:
            offsets: A 1D tensor of offsets
            device: The device to place the output tensor on
        """"""
        lengths = offsets[1:] - offsets[:-1]
        return lengths"
relative_bias;"def relative_bias(score, b, h, m, n):
        return score + (m - n)"
generate_random_lengths;"def generate_random_lengths(total_length, num_documents):
        # Initialize all lengths to 1 to ensure each document has at least one token
        lengths = [1] * num_documents
        remaining_length = total_length - num_documents

        # Randomly distribute the remaining length
        for _ in range(remaining_length):
            index = random.randint(0, num_documents - 1)
            lengths[index] += 1
        return lengths"
get_default_split_k;"def get_default_split_k(B: int, H: int, Mk: int) -> int:
        num_SM = torch.cuda.get_device_properties(""cuda"").multi_processor_count
        """"""Heuristic for the number of splits from xformer""""""
        bh = max(B * H, 1)  # NOTE: Handle B*h=0 case
        split_k = num_SM // bh * 2  # Each SM should at least get one block.
        split_k = max(split_k, 1)

        return split_k"
flash_attn_with_kvcache_renamed;"def flash_attn_with_kvcache_renamed(q, k, v, **kwargs):
        return flash_attn_with_kvcache(q, k_cache=k, v_cache=v, **kwargs)"
score_mod_w_offset;"def score_mod_w_offset(score, b, h, m, n):
            return score_mod(score, b, h, m + offset, n)"
offset;"def offset(b, h, m, n):
            return m + off >= n"
decoding_w_cached_seq_len;"def decoding_w_cached_seq_len(b, h, m, n):
            return mask_mod(b, h, m + cached_seq_len, n)"
offsets_to_lengths;"def offsets_to_lengths(
            offsets: torch.Tensor, device: Union[str, torch.device]
        ) -> torch.tensor:
            lengths = offsets[1:] - offsets[:-1]
            return lengths"
build_composite_mha_from_nn_mha;"def build_composite_mha_from_nn_mha(pt):
    assert pt._qkv_same_embed_dim
    in_proj_weight = pt.in_proj_weight
    assert in_proj_weight is not None
    assert pt.batch_first
    return CompositeMHA(pt.num_heads, pt.in_proj_weight, pt.in_proj_bias, pt.out_proj)"
generate_rand_batch;"def generate_rand_batch(
    batch_size,
    max_sequence_len,
    embed_dimension,
    pad_percentage=None,
    dtype=torch.float16,
    device=""cuda"",
):
    if not pad_percentage:
        return (
            torch.randn(
                batch_size,
                max_sequence_len,
                embed_dimension,
                dtype=dtype,
                device=device,
            ),
            None,
        )
    # Really slow but should work
    seq_len_list = [
        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))
        for _ in range(batch_size)
    ]
    # Make random ele max length
    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len
    # print(f""Theoretical padding: {pad_percentage} actual: {1 - (sum(seq_len_list) / (batch_size * max_sequence_len))}"")
    return (
        torch.nested.nested_tensor(
            [
                torch.randn(seq_len, embed_dimension, dtype=dtype, device=device)
                for seq_len in seq_len_list
            ]
        ),
        seq_len_list,
    )"
benchmark_torch_function_in_microseconds;"def benchmark_torch_function_in_microseconds(f, *args, **kwargs):
    t0 = benchmark.Timer(
        stmt=""f(*args, **kwargs)"", globals={""args"": args, ""kwargs"": kwargs, ""f"": f}
    )
    return t0.blocked_autorange().mean * 1e6"
assert_close_tensors;"def assert_close_tensors(tensor_a, tensor_b):
    # First order sanity check. Not a replacement for rigorous tests.
    if tensor_a.is_nested and tensor_b.is_nested:
        for a, b in zip(tensor_a.unbind(), tensor_b.unbind()):
            assert torch.allclose(a, b, atol=1e-2, rtol=1e-2)
    else:
        assert torch.allclose(tensor_a, tensor_b, atol=1e-3, rtol=1e-3)"
run_single_experiment;"def run_single_experiment(config: ExperimentConfig) -> ExperimentResults:
    with sdp_kernel(
        enable_math=config.enable_math,
        enable_flash=config.enable_flash,
        enable_mem_efficient=config.enable_mem_efficient,
        enable_cudnn=config.enable_cudnn,
    ):
        dropout_p = 0.0
        mask = None

        nn_mha = torch.nn.MultiheadAttention(
            embed_dim=config.embed_dimension,
            num_heads=config.num_heads,
            batch_first=True,
            dropout=dropout_p,
        )
        nn_mha = nn_mha.eval().to(""cuda"", config.dtype)
        composite_mha = build_composite_mha_from_nn_mha(nn_mha)
        qkv, lengths = generate_rand_batch(
            config.batch_size,
            config.max_sequence_len,
            config.embed_dimension,
            config.pad_percentage,
            config.dtype,
        )
        nn_mha_output, _ = nn_mha(qkv, qkv, qkv, mask)
        composite_mha_output, _ = composite_mha(qkv, qkv, qkv, mask)

        # First order sanity check
        assert_close_tensors(nn_mha_output, composite_mha_output)

        nn_mha_time = benchmark_torch_function_in_microseconds(
            nn_mha, qkv, qkv, qkv, mask
        )
        composite_mha_time = benchmark_torch_function_in_microseconds(
            composite_mha, qkv, qkv, qkv, mask
        )

        # TorchDynamo will error on NestedTensors
        if config.pad_percentage is None:
            compiled_nn_mha = torch.compile(nn_mha)
            compiled_composite_mha = torch.compile(composite_mha)

            compiled_nn_mha_time = benchmark_torch_function_in_microseconds(
                compiled_nn_mha, qkv, qkv, qkv, mask
            )

            compiled_composite_mha_time = benchmark_torch_function_in_microseconds(
                compiled_composite_mha,
                qkv,
                qkv,
                qkv,
                mask,
            )
        else:
            compiled_nn_mha_time = None
            compiled_composite_mha_time = None

        results = ExperimentResults(
            nn_mha_time,
            compiled_nn_mha_time,
            composite_mha_time,
            compiled_composite_mha_time,
        )
        return Experiment(config, results)"
generate_experiments;"def generate_experiments(
    batch_sizes, num_heads, max_seq_lens, embed_dims, dtypes, pad_percentages
) -> list[ExperimentConfig]:
    configs = []
    for bsz, n_heads, seq_len, embed_dim, dtype, padding in itertools.product(
        batch_sizes, num_heads, max_seq_lens, embed_dims, dtypes, pad_percentages
    ):
        configs.append(
            ExperimentConfig(
                batch_size=bsz,
                num_heads=n_heads,
                max_sequence_len=seq_len,
                embed_dimension=embed_dim,
                dtype=dtype,
                pad_percentage=padding,
                enable_math=False,
                enable_flash=True,
                enable_mem_efficient=True,
                enable_cudnn=True,
            )
        )
    return configs"
get_entries;"def get_entries(self) -> list:
        return [
            self.batch_size,
            self.num_heads,
            self.max_sequence_len,
            self.embed_dimension,
            self.dtype,
            self.pad_percentage,
            self.enable_math,
            self.enable_flash,
            self.enable_mem_efficient,
            self.enable_cudnn,
        ]"
get_entry_names;"def get_entry_names(cls) -> list[str]:
        return [
            ""batch_size"",
            ""num_heads"",
            ""max_sequence_len"",
            ""embed_dimension"",
            ""dtype"",
            ""pad_percentage"",
            ""enable_math"",
            ""enable_flash"",
            ""enable_mem_efficient"",
            ""enable_cudnn"",
        ]"
benchmark_torch_function_in_microseconds;"def benchmark_torch_function_in_microseconds(func: Callable, *args, **kwargs) -> float:
    # warmup
    for _ in range(5):
        func(*args, **kwargs)
    t0 = benchmark.Timer(
        stmt=""func(*args, **kwargs)"",
        globals={""args"": args, ""kwargs"": kwargs, ""func"": func},
    )
    return t0.adaptive_autorange(min_run_time=0.1).median * 1e6"
calculate_tflops;"def calculate_tflops(
    config: ExperimentConfig,
    time_us: float,
    is_backward: bool = False,
    sparsity: float = 0.0,
) -> float:
    """"""
    Calculate TFLOPS for scaled dot product attention.

    Parameters:
    - config: The experiment configuration
    - time_us: The execution time in microseconds
    - is_backward: Whether to calculate for backward pass (includes gradient computation)
    - sparsity: Sparsity factor between 0.0 and 1.0, where 0.0 means no sparsity and 1.0 means fully sparse

    Returns:
    - TFLOPS value
    """"""
    B = config.batch_size
    H = config.num_heads
    M = config.q_seq_len
    N = config.kv_seq_len
    D = config.head_dim

    # Calculate density factor (1.0 - sparsity)
    density = 1.0 - sparsity

    # Forward pass FLOPs
    qk_flops = (
        M * N * D * 2
    )  # Q*K^T matmul: (M,D) @ (D,N) with 2 FLOPs per multiply-add
    softmax_flops = M * N * 2  # Softmax operations (exp and div)
    av_flops = (
        M * N * D * 2
    )  # Attention @ V: (M,N) @ (N,D) with 2 FLOPs per multiply-add

    total_flops = B * H * (qk_flops + softmax_flops + av_flops)

    # Apply density factor to account for sparsity
    total_flops *= density

    # For backward pass flash uses 2.5x more flops will use this
    if is_backward:
        total_flops *= 2.5

    # Convert to TFLOPS: flops / (time_us * 1e-6) / 1e12
    tflops = total_flops / (time_us * 1e-6) / 1e12

    return tflops"
get_input;"def get_input(
    config: ExperimentConfig,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    q = torch.randn(
        (config.batch_size, config.num_heads, config.q_seq_len, config.head_dim),
        dtype=config.dtype,
        device=config.device,
        requires_grad=True,
    )
    k = torch.randn(
        (config.batch_size, config.num_heads, config.kv_seq_len, config.head_dim),
        dtype=config.dtype,
        device=config.device,
        requires_grad=True,
    )
    v = torch.randn(
        (config.batch_size, config.num_heads, config.kv_seq_len, config.head_dim),
        dtype=config.dtype,
        device=config.device,
        requires_grad=True,
    )
    return q, k, v"
run_single_experiment;"def run_single_experiment(config: ExperimentConfig) -> ExperimentResults:
    q, k, v = get_input(config)
    is_causal = config.is_causal
    context = (
        sdpa_kernel(config.backend) if config.backend is not None else nullcontext()
    )
    with context:
        forward_time = benchmark_cuda_function_in_microseconds(
            scaled_dot_product_attention,
            q,
            k,
            v,
            is_causal=is_causal,
            attn_mask=None,
        )
        out_torch = scaled_dot_product_attention(
            q, k, v, is_causal=is_causal, attn_mask=None
        )
        dOut = torch.randn_like(out_torch)
        backward_time = benchmark_cuda_function_in_microseconds(
            out_torch.backward, dOut, retain_graph=True
        )

    # Calculate TFLOPS for forward and backward passes
    sparsity = 0.5 if is_causal else 0.0
    forward_tflops = calculate_tflops(config, forward_time, sparsity=sparsity)
    backward_tflops = calculate_tflops(
        config, backward_time, is_backward=True, sparsity=sparsity
    )

    return ExperimentResults(
        forward_time=forward_time,
        backward_time=backward_time,
        forward_tflops=forward_tflops,
        backward_tflops=backward_tflops,
    )"
print_results;"def print_results(experiments: list[Experiment]):
    table_data = defaultdict(list)
    for experiment in experiments:
        for key, value in experiment.asdict().items():
            table_data[key].append(value)
    del table_data[""device""]
    if table_data[""backend""][0] is None:
        del table_data[""backend""]
    print(tabulate(table_data, headers=""keys"", tablefmt=""pretty"", floatfmt="".3f""))"
write_results_to_csv;"def write_results_to_csv(
    experiments: list[Experiment], output_dir: str = ""benchmark_results""
):
    """"""
    Write experiment results to a CSV file in the specified directory.
    The filename includes a timestamp for uniqueness.
    """"""
    import csv
    import os
    from datetime import datetime

    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)

    # Generate filename with timestamp
    timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
    filename = os.path.join(output_dir, f""benchmark_results_{timestamp}.csv"")

    # Get all fields from the first experiment
    if not experiments:
        return

    fieldnames = list(experiments[0].asdict().keys())
    if ""device"" in fieldnames:
        fieldnames.remove(""device"")  # Remove device field as it's always cuda

    # Write results to CSV
    with open(filename, ""w"", newline="""") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for experiment in experiments:
            row = experiment.asdict()
            if ""device"" in row:
                del row[""device""]  # Remove device field
            writer.writerow(row)

    print(f""Results written to: {filename}"")"
generate_experiment_configs;"def generate_experiment_configs() -> list[ExperimentConfig]:
    batch_sizes = [1, 8, 16]
    num_heads = [16]
    q_kv_seq_lens = [(128, 128), (256, 256), (512, 512), (1024, 1024), (8192, 8192)]
    embed_dims = [2048]
    backends = [None]  # If set to None, all backends are enabled
    dtypes = [
        torch.bfloat16,
    ]
    is_causal = [True, False]
    all_configs = []
    for (
        bsz,
        heads,
        (q_seq_len, kv_seq_len),
        embed_dim,
        causal,
        dtype,
        backend,
    ) in itertools.product(
        batch_sizes, num_heads, q_kv_seq_lens, embed_dims, is_causal, dtypes, backends
    ):
        all_configs.append(
            ExperimentConfig(
                batch_size=bsz,
                num_heads=heads,
                q_seq_len=q_seq_len,
                kv_seq_len=kv_seq_len,
                embed_dim=embed_dim,
                is_causal=causal,
                dtype=dtype,
                backend=backend,
            )
        )

    return all_configs"
compute_output;"def compute_output(num_unrolls, InType, is_main):
        code = []

        pred = ""svAll"" if is_main else ""pg""
        if InType == ""float"":
            for i in range(num_unrolls):
                code.append(f""        output = svmla_x({pred}, output, svld1(svAll, &ip{i}[k]), wgt{i});"")
        elif InType == ""at::Half"":
            for i in range(num_unrolls):
                code.append(f""        auto input{i} = svcvt_f32_x({pred}, svreinterpret_f16(\n""
                f""          svld1uh_u32({pred}, reinterpret_cast<const uint16_t*>(&ip{i}[k]))));"")
            for i in range(num_unrolls):
                code.append(f""        output = svmla_x({pred}, output, input{i}, wgt{i});"")
        elif InType == ""at::BFloat16"":
            for i in range(num_unrolls):
                code.append(f""        auto input{i} = svreinterpret_f32(svlsl_x({pred},\n""
                f""          svld1uh_u32({pred}, reinterpret_cast<const uint16_t*>(&ip{i}[k])), 16));"")
            for i in range(num_unrolls):
                code.append(f""        output = svmla_x({pred}, output, input{i}, wgt{i});"")
        elif InType == ""uint8_t"":
            code.append(f""        output = svadd_x({pred}, output, bio);"")
            for i in range(num_unrolls):
                code.append(f""        auto input{i} = svcvt_f32_x({pred}, svld1ub_u32({pred}, &ip{i}[k]));"")
            for i in range(num_unrolls):
                code.append(f""        output = svmla_x({pred}, output, input{i}, wgt{i});"")
        else:
            raise ValueError(f'Unknown datatype ""{InType}""')

        return code"
linkcode_resolve;"def linkcode_resolve(domain, info):
    if domain != ""py"":
        return None
    if not info[""module""]:
        return None

    try:
        module = __import__(info[""module""], fromlist=[""""])
        obj = module
        for part in info[""fullname""].split("".""):
            obj = getattr(obj, part)
        # Get the source file and line number
        obj = inspect.unwrap(obj)
        fn = inspect.getsourcefile(obj)
        source, lineno = inspect.getsourcelines(obj)
    except Exception:
        return None

    # Determine the tag based on the torch_version
    if RELEASE:
        version_parts = torch_version.split(
            "".""
        )  # For release versions, format as ""vX.Y.Z"" for correct path in repo
        patch_version = (
            version_parts[2].split(""+"")[0].split(""a"")[0]
        )  # assuming a0 always comes after release version in versions.txt
        version_path = f""v{version_parts[0]}.{version_parts[1]}.{patch_version}""
    else:
        version_path = torch.version.git_version
    fn = os.path.relpath(fn, start=os.path.dirname(torch.__file__))
    return (
        f""https://github.com/pytorch/pytorch/blob/{version_path}/torch/{fn}#L{lineno}""
    )"
coverage_post_process;"def coverage_post_process(app, exception):
    if exception is not None:
        return

    # Only run this test for the coverage build
    if not isinstance(app.builder, CoverageBuilder):
        return

    if not torch.distributed.is_available():
        raise RuntimeError(
            ""The coverage tool cannot run with a version ""
            ""of PyTorch that was built with USE_DISTRIBUTED=0 ""
            ""as this module's API changes.""
        )

    # These are all the modules that have ""automodule"" in an rst file
    # These modules are the ones for which coverage is checked
    # Here, we make sure that no module is missing from that list
    modules = app.env.domaindata[""py""][""modules""]

    # We go through all the torch submodules and make sure they are
    # properly tested
    missing = set()

    def is_not_internal(modname):
        split_name = modname.split(""."")
        for name in split_name:
            if name[0] == ""_"":
                return False
        return True

    # The walk function does not return the top module
    if ""torch"" not in modules:
        missing.add(""torch"")

    for _, modname, ispkg in pkgutil.walk_packages(
        path=torch.__path__, prefix=torch.__name__ + "".""
    ):
        if is_not_internal(modname):
            if modname not in modules and modname not in ignore_duplicated_modules:
                missing.add(modname)

    output = []

    if missing:
        mods = "", "".join(missing)
        output.append(
            f""\nYou added the following module(s) to the PyTorch namespace '{mods}' ""
            ""but they have no corresponding entry in a doc .rst file. You should ""
            ""either make sure that the .rst file that contains the module's documentation ""
            ""properly contains either '.. automodule:: mod_name' (if you do not want ""
            ""the paragraph added by the automodule, you can simply use '.. py:module:: mod_name') ""
            "" or make the module private (by appending an '_' at the beginning of its name).""
        )

    # The output file is hard-coded by the coverage tool
    # Our CI is setup to fail if any line is added to this file
    output_file = path.join(app.outdir, ""python.txt"")

    if output:
        with open(output_file, ""a"") as f:
            for o in output:
                f.write(o)"
process_docstring;"def process_docstring(app, what_, name, obj, options, lines):
    """"""
    Custom process to transform docstring lines Remove ""Ignore"" blocks

    Args:
        app (sphinx.application.Sphinx): the Sphinx application object

        what (str):
            the type of the object which the docstring belongs to (one of
            ""module"", ""class"", ""exception"", ""function"", ""method"", ""attribute"")

        name (str): the fully qualified name of the object

        obj: the object itself

        options: the options given to the directive: an object with
            attributes inherited_members, undoc_members, show_inheritance
            and noindex that are true if the flag option of same name was
            given to the auto directive

        lines (List[str]): the lines of the docstring, see above

    References:
        https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html
    """"""
    import re

    remove_directives = [
        # Remove all xdoctest directives
        re.compile(r""\s*>>>\s*#\s*x?doctest:\s*.*""),
        re.compile(r""\s*>>>\s*#\s*x?doc:\s*.*""),
    ]
    filtered_lines = [
        line for line in lines if not any(pat.match(line) for pat in remove_directives)
    ]
    # Modify the lines inplace
    lines[:] = filtered_lines

    # make sure there is a blank line at the end
    if lines and lines[-1].strip():
        lines.append("""")"
hide_edit_button_for_pages;"def hide_edit_button_for_pages(app, pagename, templatename, context, doctree):
    if pagename.startswith(""generated/""):
        context[""theme_use_edit_page_button""] = False"
patched_make_field;"def patched_make_field(self, types, domain, items, **kw):
    # `kw` catches `env=None` needed for newer sphinx while maintaining
    #  backwards compatibility when passed along further down!

    # type: (List, unicode, Tuple) -> nodes.field
    def handle_item(fieldarg, content):
        par = nodes.paragraph()
        par += addnodes.literal_strong("""", fieldarg)  # Patch: this line added
        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,
        #                           addnodes.literal_strong))
        if fieldarg in types:
            par += nodes.Text("" ("")
            # NOTE: using .pop() here to prevent a single type node to be
            # inserted twice into the doctree, which leads to
            # inconsistencies later when references are resolved
            fieldtype = types.pop(fieldarg)
            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):
                typename = fieldtype[0].astext()
                builtin_types = [""int"", ""long"", ""float"", ""bool"", ""type""]
                for builtin_type in builtin_types:
                    pattern = rf""(?<![\w.]){builtin_type}(?![\w.])""
                    repl = f""python:{builtin_type}""
                    typename = re.sub(pattern, repl, typename)
                par.extend(
                    self.make_xrefs(
                        self.typerolename,
                        domain,
                        typename,
                        addnodes.literal_emphasis,
                        **kw,
                    )
                )
            else:
                par += fieldtype
            par += nodes.Text("")"")
        par += nodes.Text("" -- "")
        par += content
        return par

    fieldname = nodes.field_name("""", self.label)
    if len(items) == 1 and self.can_collapse:
        fieldarg, content = items[0]
        bodynode = handle_item(fieldarg, content)
    else:
        bodynode = self.list_type()
        for fieldarg, content in items:
            bodynode += nodes.list_item("""", handle_item(fieldarg, content))
    fieldbody = nodes.field_body("""", bodynode)
    return nodes.field("""", fieldname, fieldbody)"
is_not_internal;"def is_not_internal(modname):
        split_name = modname.split(""."")
        for name in split_name:
            if name[0] == ""_"":
                return False
        return True"
visit_reference;"def visit_reference(self, node):
        if ""refuri"" in node and ""generated"" in node.get(""refuri""):
            ref = node.get(""refuri"")
            ref_anchor = ref.split(""#"")
            if len(ref_anchor) > 1:
                # Only add the id if the node href and the text match,
                # i.e. the href is ""torch.flip#torch.flip"" and the content is
                # ""torch.flip"" or ""flip"" since that is a signal the node refers
                # to autogenerated content
                anchor = ref_anchor[1]
                txt = node.parent.astext()
                if txt == anchor or txt == anchor.split(""."")[-1]:
                    self.body.append(f'<p id=""{ref_anchor[1]}""/>')
        return old_call(self, node)"
handle_item;"def handle_item(fieldarg, content):
        par = nodes.paragraph()
        par += addnodes.literal_strong("""", fieldarg)  # Patch: this line added
        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,
        #                           addnodes.literal_strong))
        if fieldarg in types:
            par += nodes.Text("" ("")
            # NOTE: using .pop() here to prevent a single type node to be
            # inserted twice into the doctree, which leads to
            # inconsistencies later when references are resolved
            fieldtype = types.pop(fieldarg)
            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):
                typename = fieldtype[0].astext()
                builtin_types = [""int"", ""long"", ""float"", ""bool"", ""type""]
                for builtin_type in builtin_types:
                    pattern = rf""(?<![\w.]){builtin_type}(?![\w.])""
                    repl = f""python:{builtin_type}""
                    typename = re.sub(pattern, repl, typename)
                par.extend(
                    self.make_xrefs(
                        self.typerolename,
                        domain,
                        typename,
                        addnodes.literal_emphasis,
                        **kw,
                    )
                )
            else:
                par += fieldtype
            par += nodes.Text("")"")
        par += nodes.Text("" -- "")
        par += content
        return par"
_sort_key_func;"def _sort_key_func(entry):
        pattern = entry[""pattern""]
        while isinstance(pattern, tuple):
            pattern = pattern[-1]

        pattern = remove_boolean_dispatch_from_name(pattern)
        if not isinstance(pattern, str):
            # methods are already strings
            pattern = torch.typename(pattern)

        # we want
        #
        #   torch.nn.modules.pooling.AdaptiveAvgPool1d
        #
        # and
        #
        #   torch._VariableFunctionsClass.adaptive_avg_pool1d
        #
        # to be next to each other, so convert to all lower case
        # and remove the underscores, and compare the last part
        # of the string
        pattern_str_normalized = pattern.lower().replace(""_"", """")
        key = pattern_str_normalized.split(""."")[-1]
        return key"
generate_example_rst;"def generate_example_rst(example_case: ExportCase):
    """"""
    Generates the .rst files for all the examples in db/examples/
    """"""

    model = example_case.model

    tags = "", "".join(f"":doc:`{tag} <{tag}>`"" for tag in example_case.tags)

    source_file = (
        inspect.getfile(model.__class__)
        if isinstance(model, torch.nn.Module)
        else inspect.getfile(model)
    )
    with open(source_file) as file:
        source_code = file.read()
    source_code = source_code.replace(""\n"", ""\n    "")
    splitted_source_code = re.split(r""@export_rewrite_case.*\n"", source_code)

    assert len(splitted_source_code) in {
        1,
        2,
    }, f""more than one @export_rewrite_case decorator in {source_code}""

    more_arguments = """"
    if example_case.example_kwargs:
        more_arguments += "", example_kwargs""
    if example_case.dynamic_shapes:
        more_arguments += "", dynamic_shapes=dynamic_shapes""

    # Generate contents of the .rst file
    title = f""{example_case.name}""
    doc_contents = f""""""{title}
{""^"" * (len(title))}

.. note::

    Tags: {tags}

    Support Level: {example_case.support_level.name}

Original source code:

.. code-block:: python

    {splitted_source_code[0]}

    torch.export.export(model, example_args{more_arguments})

Result:

.. code-block::

""""""

    # Get resulting graph from dynamo trace
    try:
        exported_program = export(
            model,
            example_case.example_args,
            example_case.example_kwargs,
            dynamic_shapes=example_case.dynamic_shapes,
            strict=True,
        )
        graph_output = str(exported_program)
        graph_output = re.sub(r""        # File(.|\n)*?\n"", """", graph_output)
        graph_output = graph_output.replace(""\n"", ""\n    "")
        output = f""    {graph_output}""
    except torchdynamo.exc.Unsupported as e:
        output = ""    Unsupported: "" + str(e).split(""\n"")[0]
    except AssertionError as e:
        output = ""    AssertionError: "" + str(e).split(""\n"")[0]
    except RuntimeError as e:
        output = ""    RuntimeError: "" + str(e).split(""\n"")[0]

    doc_contents += output + ""\n""

    if len(splitted_source_code) == 2:
        doc_contents += f""""""\n
You can rewrite the example above to something like the following:

.. code-block:: python

{splitted_source_code[1]}

""""""

    return doc_contents"
generate_index_rst;"def generate_index_rst(example_cases, tag_to_modules, support_level_to_modules):
    """"""
    Generates the index.rst file
    """"""

    support_contents = """"
    for k, v in support_level_to_modules.items():
        support_level = k.name.lower().replace(""_"", "" "").title()
        module_contents = ""\n\n"".join(v)
        support_contents += f""""""
{support_level}
{""-"" * (len(support_level))}

{module_contents}
""""""

    tag_names = ""\n    "".join(t for t in tag_to_modules.keys())

    with open(os.path.join(PWD, ""blurb.txt"")) as file:
        blurb = file.read()

    # Generate contents of the .rst file
    doc_contents = f"""""".. _torch.export_db:

ExportDB
========

{blurb}

.. toctree::
    :maxdepth: 1
    :caption: Tags

    {tag_names}

{support_contents}
""""""

    with open(os.path.join(EXPORTDB_SOURCE, ""index.rst""), ""w"") as f:
        f.write(doc_contents)"
generate_tag_rst;"def generate_tag_rst(tag_to_modules):
    """"""
    For each tag that shows up in each ExportCase.tag, generate an .rst file
    containing all the examples that have that tag.
    """"""

    for tag, modules_rst in tag_to_modules.items():
        doc_contents = f""{tag}\n{'=' * (len(tag) + 4)}\n""
        full_modules_rst = ""\n\n"".join(modules_rst)
        full_modules_rst = re.sub(
            r""={3,}"", lambda match: ""-"" * len(match.group()), full_modules_rst
        )
        doc_contents += full_modules_rst

        with open(os.path.join(EXPORTDB_SOURCE, f""{tag}.rst""), ""w"") as f:
            f.write(doc_contents)"
generate_rst;"def generate_rst():
    if not os.path.exists(EXPORTDB_SOURCE):
        os.makedirs(EXPORTDB_SOURCE)

    example_cases = all_examples()
    tag_to_modules = {}
    support_level_to_modules = {}
    for example_case in example_cases.values():
        doc_contents = generate_example_rst(example_case)

        for tag in example_case.tags:
            tag_to_modules.setdefault(tag, []).append(doc_contents)

        support_level_to_modules.setdefault(example_case.support_level, []).append(
            doc_contents
        )

    generate_tag_rst(tag_to_modules)
    generate_index_rst(example_cases, tag_to_modules, support_level_to_modules)"
get_model_name;"def get_model_name(filename):
    """"""
    Get model name from a file in format {model_name}_chrome_trace_*.json
    """"""
    _, tail = os.path.split(filename)
    modelname = tail[: tail.find(""_chrome_trace"")]
    return modelname"
get_total_length;"def get_total_length(run_times_df, modelname):
    return float(run_times_df[run_times_df[""name""] == modelname][""runtime""])"
nnc_addnorm;"def nnc_addnorm(a, b, mean, std):
    return (a + b - mean) / std"
eager_addnorm;"def eager_addnorm(a, b, mean, std):
    return (a + b - mean) / std"
inplace_addnorm;"def inplace_addnorm(a, b, mean, std, out):
    out = torch.add(a, b, out=out)
    torch.sub(out, mean, out=out)
    torch.div(out, std, out=out)
    return out"
maybe_synced;"def maybe_synced(fn):
    if CUDA:
        synchronize = torch.cuda.synchronize
        synchronize()  # warmup

        def _fn():
            result = fn()
            synchronize()
            return result

        return _fn
    return fn"
benchmark_loop;"def benchmark_loop(setup):
    result = np.zeros((REPEAT, len(SIZES), 2), dtype=np.float64)
    for s, n in enumerate(SIZES):
        nnc, aten = setup(n)
        nnc = maybe_synced(nnc)
        aten = maybe_synced(aten)

        for r in range(result.shape[0]):
            result[r, s, 0] = timeit.timeit(nnc, number=NUMBER[s])
            result[r, s, 1] = timeit.timeit(aten, number=NUMBER[s])

    result = np.median(result, axis=0)
    assert result.shape == (len(SIZES), 2)
    result = result[:, 1] / result[:, 0]
    print(result)
    return result"
inplace_setup;"def inplace_setup(n):
        a, b = make_args(n)
        result_aten = torch.clone(a)
        result_nnc = torch.clone(a)
        nnc(result_nnc, b, out=result_nnc)
        aten(result_aten, b, out=result_aten)
        torch.testing.assert_close(result_aten, result_nnc)
        return (lambda: nnc(a, b, out=a), lambda: aten(a, b, out=a))"
out_setup;"def out_setup(n):
        args = make_args(n)
        result_aten = out(n)
        result_nnc = out(n)
        aten(*args, out=result_aten)
        nnc(*args, out=result_nnc)
        torch.testing.assert_close(result_aten, result_nnc)
        result = out(n)
        return (lambda: nnc(*args, out=result), lambda: aten(*args, out=result))"
backwards_setup;"def backwards_setup(n):
        args = make_args(n)
        (grad_var,) = (a for a in args if a.requires_grad)
        aten(*args).sum().backward()
        correct = grad_var.grad.clone()
        grad_var.grad.zero_()
        nnc(*args).sum().backward()
        torch.testing.assert_close(correct, grad_var.grad)
        return (
            lambda: nnc(*args).sum().backward(),
            lambda: aten(*args).sum().backward(),
        )"
compute_loss;"def compute_loss(weights, image, target):
    images = image.unsqueeze(0)
    targets = target.unsqueeze(0)
    output = func_model(weights, images)
    loss = criterion(output, targets)
    return loss"
functorch_per_sample_grad;"def functorch_per_sample_grad():
    compute_grad = grad(compute_loss)
    compute_per_sample_grad = vmap(compute_grad, (None, 0, 0))

    start = time.time()
    result = compute_per_sample_grad(weights, images, targets)
    torch.cuda.synchronize()
    end = time.time()

    return result, end - start"
opacus_per_sample_grad;"def opacus_per_sample_grad():
    start = time.time()
    output = model_opacus(images)
    loss = criterion(output, targets)
    loss.backward()
    torch.cuda.synchronize()
    end = time.time()
    expected = [p.grad_sample for p in model_opacus.parameters()]
    for p in model_opacus.parameters():
        delattr(p, ""grad_sample"")
        p.grad = None
    return expected, end - start"
medium_channels_last;"def medium_channels_last():
    return (
        rand(32, 3, 224, 224).to(memory_format=torch.channels_last),
        rand(32, 3, 224, 224).to(memory_format=torch.channels_last),
    )"
medium_broadcast;"def medium_broadcast():
    return (rand(32, 12, 64, 64), rand(64))"
medium_broadcast_channels_last;"def medium_broadcast_channels_last():
    return (rand(32, 3, 223, 223).to(memory_format=torch.channels_last), rand(3, 1, 1))"
large_transpose;"def large_transpose():
    return (rand(8192, 8192).transpose(0, 1), rand(8192, 8192).transpose(0, 1))"
large_channels_last;"def large_channels_last():
    return (
        rand(32, 32, 256, 256).to(memory_format=torch.channels_last),
        rand(32, 32, 256, 256).to(memory_format=torch.channels_last),
    )"
pathological_broadcast;"def pathological_broadcast():
    return (rand(1, 32, 32, 2), rand(1024, 1, 1, 2))"
native_hardswish;"def native_hardswish(a):
    return torch._C._nn.hardswish(a)"
_bind_dims_to_size;"def _bind_dims_to_size(lhs_size, rhs, lhs_debug):
    from . import DimensionMismatchError

    not_bound = tuple((i, r) for i, r in enumerate(rhs) if not r.is_bound)
    if len(not_bound) == 1:
        idx, d = not_bound[0]
        rhs_so_far = prod(r.size for r in rhs if r.is_bound)
        if lhs_size % rhs_so_far != 0:
            rhs_s = tuple(""?"" if not r.is_bound else str(r.size) for r in rhs)
            raise DimensionMismatchError(
                f""inferred dimension does not evenly fit into larger dimension: {lhs_size} vs {rhs_s}""
            )
        new_size = lhs_size // rhs_so_far
        d.size = new_size
    elif len(not_bound) > 1:
        rhs_s = tuple(""?"" if not r.is_bound else str(r.size) for r in rhs)
        raise DimensionMismatchError(
            f""cannot infer the size of two dimensions at once: {rhs} with sizes {rhs_s}""
        )
    else:
        rhs_size = prod(r.size for r in rhs)
        if lhs_size != rhs_size:
            raise DimensionMismatchError(
                f""Dimension sizes to do not match ({lhs_size} != {rhs_size}) when matching {lhs_debug} to {rhs}""
            )"
_tensor_levels;"def _tensor_levels(inp):
    from . import _Tensor

    if isinstance(inp, _Tensor):
        return inp._tensor, llist(inp._levels), inp._has_device
    else:
        return inp, llist(range(-inp.ndim, 0)), True"
_match_levels;"def _match_levels(v, from_levels, to_levels):
    view = []
    permute = []
    requires_view = False
    size = v.size()
    for t in to_levels:
        try:
            idx = from_levels.index(t)
            permute.append(idx)
            view.append(size[idx])
        except ValueError:
            view.append(1)
            requires_view = True
    if permute != list(range(len(permute))):
        v = v.permute(*permute)
    if requires_view:
        v = v.view(*view)
    return v"
_positional_no_permute;"def _positional_no_permute(self, dim, expand_dim=False):
    from . import Tensor

    ptensor, levels = self._tensor, llist(self._levels)
    try:
        idx = levels.index(dim)
    except ValueError:
        if not expand_dim:
            raise
        idx = 0
        ptensor = ptensor.expand(dim.size, *ptensor.size())
        levels.insert(0, 0)
    idx_batched = 0
    for i in range(idx):
        if isinstance(levels[i], int):
            levels[i] -= 1
            idx_batched += 1
    levels[idx] = -idx_batched - 1
    return Tensor.from_positional(ptensor, levels, self._has_device), idx_batched"
patched_make_field;"def patched_make_field(self, types, domain, items, **kw):
    # `kw` catches `env=None` needed for newer sphinx while maintaining
    #  backwards compatibility when passed along further down!

    # (List, unicode, Tuple) -> nodes.field
    def handle_item(fieldarg, content):
        par = nodes.paragraph()
        par += addnodes.literal_strong("""", fieldarg)  # Patch: this line added
        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,
        #                           addnodes.literal_strong))
        if fieldarg in types:
            par += nodes.Text("" ("")
            # NOTE: using .pop() here to prevent a single type node to be
            # inserted twice into the doctree, which leads to
            # inconsistencies later when references are resolved
            fieldtype = types.pop(fieldarg)
            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):
                typename = """".join(n.astext() for n in fieldtype)
                typename = typename.replace(""int"", ""python:int"")
                typename = typename.replace(""long"", ""python:long"")
                typename = typename.replace(""float"", ""python:float"")
                typename = typename.replace(""bool"", ""python:bool"")
                typename = typename.replace(""type"", ""python:type"")
                par.extend(
                    self.make_xrefs(
                        self.typerolename,
                        domain,
                        typename,
                        addnodes.literal_emphasis,
                        **kw,
                    )
                )
            else:
                par += fieldtype
            par += nodes.Text("")"")
        par += nodes.Text("" -- "")
        par += content
        return par

    fieldname = nodes.field_name("""", self.label)
    if len(items) == 1 and self.can_collapse:
        fieldarg, content = items[0]
        bodynode = handle_item(fieldarg, content)
    else:
        bodynode = self.list_type()
        for fieldarg, content in items:
            bodynode += nodes.list_item("""", handle_item(fieldarg, content))
    fieldbody = nodes.field_body("""", bodynode)
    return nodes.field("""", fieldname, fieldbody)"
handle_item;"def handle_item(fieldarg, content):
        par = nodes.paragraph()
        par += addnodes.literal_strong("""", fieldarg)  # Patch: this line added
        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,
        #                           addnodes.literal_strong))
        if fieldarg in types:
            par += nodes.Text("" ("")
            # NOTE: using .pop() here to prevent a single type node to be
            # inserted twice into the doctree, which leads to
            # inconsistencies later when references are resolved
            fieldtype = types.pop(fieldarg)
            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):
                typename = """".join(n.astext() for n in fieldtype)
                typename = typename.replace(""int"", ""python:int"")
                typename = typename.replace(""long"", ""python:long"")
                typename = typename.replace(""float"", ""python:float"")
                typename = typename.replace(""bool"", ""python:bool"")
                typename = typename.replace(""type"", ""python:type"")
                par.extend(
                    self.make_xrefs(
                        self.typerolename,
                        domain,
                        typename,
                        addnodes.literal_emphasis,
                        **kw,
                    )
                )
            else:
                par += fieldtype
            par += nodes.Text("")"")
        par += nodes.Text("" -- "")
        par += content
        return par"
_create_rearrange_callable;"def _create_rearrange_callable(
    tensor_ndim: int, pattern: str, **axes_lengths: int
) -> Callable[[torch.Tensor], torch.Tensor]:
    r""""""Translate an `einops`-style pattern into a callable that performs the rearrange using first-class dimensions.

    Since the an equivalent result is computed for tensors with the same number of dimensions, with the same pattern and
    specified axes lengths, this function can be memoized.

    Args:
        tensor_ndim (int): the number of dimensions in the tensor to rearrange
        pattern (str): the `einops`-style rearrangement pattern
        axes_lengths (int): any additional length specifications for dimensions

    Returns:
        Callable[[torch.Tensor], torch.Tensor]: a callable that performs the rearrangement
    """"""
    left, right = parse_pattern(pattern, axes_lengths)
    validate_rearrange_expressions(left, right, axes_lengths)

    n_anon_dims = sum(not dim for dim in left.composition)
    if left.has_ellipsis:
        n_ellipsis_dims = tensor_ndim - (len(left.composition) - 1)
        n_named_dims = len(left.identifiers) - 1

        if (pattern_ndim := n_anon_dims + n_named_dims) > tensor_ndim:
            raise ValueError(
                f""Number of dimensions in pattern ({pattern_ndim}) must be less than or equal to the number of ""
                f""dimensions in the tensor ({tensor_ndim})""
            )
    else:
        n_ellipsis_dims = 0
        n_named_dims = len(left.identifiers)

        if (pattern_ndim := len(left.composition)) != tensor_ndim:
            raise ValueError(
                f""Number of dimensions in pattern ({pattern_ndim}) must be equal to the number of dimensions in ""
                f""the tensor ({tensor_ndim})""
            )
    n_dims = n_named_dims + n_ellipsis_dims + n_anon_dims

    if n_dims == 0:
        # an identity rearrangement on a 0-dimension tensor
        return lambda tensor: tensor

    first_class_dims: tuple[str, ...] = tuple(f""d{i}"" for i in range(n_dims))
    identifier_dim_map: dict[Union[str, AnonymousAxis], tuple[str, ...]] = {}
    anon_axes: list[AnonymousAxis] = []

    # map the left-hand side identifiers to strings representing first class dims
    dims_i = 0
    for dimension in left.composition:
        if isinstance(dimension, list):
            for identifier in dimension:
                # non-unitary anon axes are not allowed in rearrange & unitary anon axes are represented as empty lists
                assert isinstance(identifier, str)
                identifier_dim_map[identifier] = (first_class_dims[dims_i],)
                dims_i += 1
            if not dimension:
                # unitary anonymous axis
                anon_axis = AnonymousAxis(""1"")
                identifier_dim_map[anon_axis] = (first_class_dims[dims_i],)
                anon_axes.append(anon_axis)
                dimension.append(anon_axis)
                dims_i += 1
        elif dimension == _ellipsis:
            identifier = _ellipsis
            identifier_dim_map[identifier] = tuple(
                first_class_dims[dims_i + j] for j in range(n_ellipsis_dims)
            )
            dims_i += n_ellipsis_dims
        else:
            raise ValueError(f""Unexpected dimension: {dimension}"")

    def composition_to_dims(
        composition: Sequence[Union[list[Union[str, AnonymousAxis]], str]],
    ) -> list[Union[str, tuple[str, ...]]]:
        """"""Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first
        class dims.""""""
        dim_composition: list[Union[str, tuple[str, ...]]] = []
        for dimension in composition:
            if isinstance(dimension, list):
                dim_composition.append(
                    tuple(
                        dim
                        for identifier in dimension
                        for dim in identifier_dim_map[identifier]
                    )
                )
            elif dimension == _ellipsis:
                dim_composition.extend(identifier_dim_map[_ellipsis])
            else:
                raise ValueError(f""Unexpected dimension: {dimension}"")
        return dim_composition

    left_dims = composition_to_dims(left.composition)
    right_dims = composition_to_dims(right.composition)
    anon_dims = tuple(identifier_dim_map[axis][0] for axis in anon_axes)
    specified_lengths = tuple(
        (identifier_dim_map[axis][0], length) for axis, length in axes_lengths.items()
    )

    custom_rearrange_callable_name = ""do_rearrange""
    custom_rearrange_callable_code = (
        (
            f""def {custom_rearrange_callable_name}(tensor):\n""
            f""    {comma_separate(first_class_dims)} = dims({n_dims})\n""
        )
        + (
            """".join(
                f""    {dim}.size = {length}\n"" for (dim, length) in specified_lengths
            )
            if specified_lengths
            else """"
        )
        + f""    tensor = tensor[{comma_separate(left_dims)}].order({comma_separate(right_dims)})\n""
        + (
            f""    return tensor.sum({comma_separate([anon_dims])}, keepdim=False)\n""
            if anon_dims
            else ""    return tensor\n""
        )
    )

    exec(custom_rearrange_callable_code)
    return locals()[custom_rearrange_callable_name]"
rearrange;"def rearrange(
    tensor: Union[torch.Tensor, list[torch.Tensor], tuple[torch.Tensor, ...]],
    pattern: str,
    **axes_lengths: int,
) -> torch.Tensor:
    r""""""A native implementation of `einops.rearrange`, a reader-friendly smart element reordering for multidimensional
    tensors. This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,
    stack, concatenate and other operations.

    See: https://einops.rocks/api/rearrange/

    Args:
        tensor (Tensor or sequence of Tensor): the tensor(s) to rearrange
        pattern (str): the rearrangement pattern
        axes_lengths (int): any additional length specifications for dimensions

    Returns:
        Tensor: the rearranged tensor

    Examples:
        >>> # suppose we have a set of 32 images in ""h w c"" format (height-width-channel)
        >>> images = torch.randn((32, 30, 40, 3))

        >>> # stack along first (batch) axis, output is a single array
        >>> rearrange(images, ""b h w c -> b h w c"").shape
        torch.Size([32, 30, 40, 3])

        >>> # concatenate images along height (vertical axis), 960 = 32 * 30
        >>> rearrange(images, ""b h w c -> (b h) w c"").shape
        torch.Size([960, 40, 3])

        >>> # concatenated images along horizontal axis, 1280 = 32 * 40
        >>> rearrange(images, ""b h w c -> h (b w) c"").shape
        torch.Size([30, 1280, 3])

        >>> # reordered axes to ""b c h w"" format for deep learning
        >>> rearrange(images, ""b h w c -> b c h w"").shape
        torch.Size([32, 3, 30, 40])

        >>> # flattened each image into a vector, 3600 = 30 * 40 * 3
        >>> rearrange(images, ""b h w c -> b (c h w)"").shape
        torch.Size([32, 3600])

        >>> # split each image into 4 smaller (top-left, top-right, bottom-left, bottom-right), 128 = 32 * 2 * 2
        >>> rearrange(images, ""b (h1 h) (w1 w) c -> (b h1 w1) h w c"", h1=2, w1=2).shape
        torch.Size([128, 15, 20, 3])

        >>> # space-to-depth operation
        >>> rearrange(images, ""b (h h1) (w w1) c -> b h w (c h1 w1)"", h1=2, w1=2).shape
        torch.Size([32, 15, 20, 12])
    """"""
    if not isinstance(tensor, torch.Tensor):
        tensor = torch.stack(tensor)

    rearrange_callable = _create_rearrange_callable(
        tensor.ndim, pattern, **axes_lengths
    )

    return rearrange_callable(tensor)"
composition_to_dims;"def composition_to_dims(
        composition: Sequence[Union[list[Union[str, AnonymousAxis]], str]],
    ) -> list[Union[str, tuple[str, ...]]]:
        """"""Convert a `ParsedExpression.composition` into a `Tensor.__getitem__` index of strings representing first
        class dims.""""""
        dim_composition: list[Union[str, tuple[str, ...]]] = []
        for dimension in composition:
            if isinstance(dimension, list):
                dim_composition.append(
                    tuple(
                        dim
                        for identifier in dimension
                        for dim in identifier_dim_map[identifier]
                    )
                )
            elif dimension == _ellipsis:
                dim_composition.extend(identifier_dim_map[_ellipsis])
            else:
                raise ValueError(f""Unexpected dimension: {dimension}"")
        return dim_composition"
parse_pattern;"def parse_pattern(
    pattern: str, axes_lengths: Mapping[str, int]
) -> tuple[ParsedExpression, ParsedExpression]:
    """"""Parse an `einops`-style pattern into a left-hand side and right-hand side `ParsedExpression` object.

    Args:
        pattern (str): the `einops`-style rearrangement pattern
        axes_lengths (Mapping[str, int]): any additional length specifications for dimensions

    Returns:
       tuple[ParsedExpression, ParsedExpression]: a tuple containing the left-hand side and right-hand side expressions
    """"""
    # adapted from einops.einops._prepare_transformation_recipe
    # https://github.com/arogozhnikov/einops/blob/230ac1526c1f42c9e1f7373912c7f8047496df11/einops/einops.py
    try:
        left_str, right_str = pattern.split(""->"")
    except ValueError:
        raise ValueError(""Pattern must contain a single '->' separator"") from None

    if _ellipsis in axes_lengths:
        raise ValueError(f""'{_ellipsis}' is not an allowed axis identifier"")

    left = ParsedExpression(left_str)
    right = ParsedExpression(right_str)

    if not left.has_ellipsis and right.has_ellipsis:
        raise ValueError(
            f""Ellipsis found in right side, but not left side of a pattern {pattern}""
        )
    if left.has_ellipsis and left.has_ellipsis_parenthesized:
        raise ValueError(
            f""Ellipsis is parenthesis in the left side is not allowed: {pattern}""
        )

    return left, right"
validate_rearrange_expressions;"def validate_rearrange_expressions(
    left: ParsedExpression, right: ParsedExpression, axes_lengths: Mapping[str, int]
) -> None:
    """"""Perform expression validations that are specific to the `rearrange` operation.

    Args:
        left (ParsedExpression): left-hand side expression
        right (ParsedExpression): right-hand side expression
        axes_lengths (Mapping[str, int]): any additional length specifications for dimensions
    """"""
    for length in axes_lengths.values():
        if (length_type := type(length)) is not int:
            raise TypeError(
                f""rearrange axis lengths must be integers, got: {length_type}""
            )

    if left.has_non_unitary_anonymous_axes or right.has_non_unitary_anonymous_axes:
        raise ValueError(""rearrange only supports unnamed axes of size 1"")

    difference = set.symmetric_difference(left.identifiers, right.identifiers)
    if len(difference) > 0:
        raise ValueError(
            f""Identifiers only on one side of rearrange expression (should be on both): {difference}""
        )

    unmatched_axes = axes_lengths.keys() - left.identifiers
    if len(unmatched_axes) > 0:
        raise ValueError(
            f""Identifiers not found in rearrange expression: {unmatched_axes}""
        )"
comma_separate;"def comma_separate(collection: Collection[Union[str, Collection[str]]]) -> str:
    """"""Convert a collection of strings representing first class dims into a comma-separated string.

    Args:
        collection (Collection[Union[str, Collection[str]]]): the collection of strings to convert

    Returns:
        str: the comma-separated string

    Examples:
        >>> comma_separate((""d0"",))
        'd0'

        >>> comma_separate((""d0"", ""d1"", ""d2"", ""d3""))
        'd0, d1, d2, d3'

        >>> comma_separate([(""d1"", ""d4"")])
        '(d1, d4)'

        >>> comma_separate([(""d0"",), (), (""d1"",), (""d2"",), (""d3"", ""d4"")])
        '(d0,), (), (d1,), (d2,), (d3, d4)'
    """"""
    return "", "".join(
        item
        if isinstance(item, str)
        else f""({comma_separate(item)}{',' if len(item) == 1 else ''})""
        for item in collection
    )"
check_axis_name_return_reason;"def check_axis_name_return_reason(
        name: str, allow_underscore: bool = False
    ) -> tuple[bool, str]:
        """"""Check if the given axis name is valid, and a message explaining why if not.

        Valid axes names are python identifiers except keywords, and should not start or end with an underscore.

        Args:
            name (str): the axis name to check
            allow_underscore (bool): whether axis names are allowed to start with an underscore

        Returns:
            tuple[bool, str]: whether the axis name is valid, a message explaining why if not
        """"""
        if not str.isidentifier(name):
            return False, ""not a valid python identifier""
        elif name[0] == ""_"" or name[-1] == ""_"":
            if name == ""_"" and allow_underscore:
                return True, """"
            return False, ""axis name should should not start or end with underscore""
        else:
            if keyword.iskeyword(name):
                warnings.warn(
                    f""It is discouraged to use axes names that are keywords: {name}"",
                    RuntimeWarning,
                )
            if name in [""axis""]:
                warnings.warn(
                    ""It is discouraged to use 'axis' as an axis name and will raise an error in future"",
                    FutureWarning,
                )
            return True, """""
check_axis_name;"def check_axis_name(name: str) -> bool:
        """"""Check if the name is a valid axis name.

        Args:
            name (str): the axis name to check

        Returns:
            bool: whether the axis name is valid
        """"""
        is_valid, _ = ParsedExpression.check_axis_name_return_reason(name)
        return is_valid"
add_axis_name;"def add_axis_name(x: str) -> None:
            if x in self.identifiers:
                if not (allow_underscore and x == ""_"") and not allow_duplicates:
                    raise ValueError(
                        f""Indexing expression contains duplicate dimension '{x}'""
                    )
            if x == _ellipsis:
                self.identifiers.add(_ellipsis)
                if bracket_group is None:
                    self.composition.append(_ellipsis)
                    self.has_ellipsis_parenthesized = False
                else:
                    bracket_group.append(_ellipsis)
                    self.has_ellipsis_parenthesized = True
            else:
                is_number = str.isdecimal(x)
                if is_number and int(x) == 1:
                    # handling the case of anonymous axis of length 1
                    if bracket_group is None:
                        self.composition.append([])
                    else:
                        pass  # no need to think about 1s inside parenthesis
                    return
                is_axis_name, reason = self.check_axis_name_return_reason(
                    x, allow_underscore=allow_underscore
                )
                if not (is_number or is_axis_name):
                    raise ValueError(f""Invalid axis identifier: {x}\n{reason}"")
                axis_name: Union[str, AnonymousAxis] = (
                    AnonymousAxis(x) if is_number else x
                )
                self.identifiers.add(axis_name)
                if is_number:
                    self.has_non_unitary_anonymous_axes = True
                if bracket_group is None:
                    self.composition.append([axis_name])
                else:
                    bracket_group.append(axis_name)"
save_checkpoint;"def save_checkpoint(state, is_best, filename=""checkpoint.tar""):
    torch.save(state, filename)
    if is_best:
        shutil.copyfile(filename, ""model_best.pth.tar"")"
clip_and_accumulate_and_add_noise;"def clip_and_accumulate_and_add_noise(
    model, max_per_sample_grad_norm=1.0, noise_multiplier=1.0
):
    sample_grads = tuple(param.grad_sample for param in model.parameters())

    # step 0: compute the norms
    sample_norms, batch_size = compute_norms(sample_grads)

    # step 1: compute clipping factors
    clip_factor = max_per_sample_grad_norm / (sample_norms + 1e-6)
    clip_factor = clip_factor.clamp(max=1.0)

    # step 2: clip
    grads = tuple(
        torch.einsum(""i,i..."", clip_factor, sample_grad) for sample_grad in sample_grads
    )

    # step 3: add gaussian noise
    stddev = max_per_sample_grad_norm * noise_multiplier
    noises = tuple(
        torch.normal(0, stddev, grad_param.shape, device=grad_param.device)
        for grad_param in grads
    )
    grads = tuple(noise + grad_param for noise, grad_param in zip(noises, grads))

    # step 4: assign the new grads, delete the sample grads
    for param, param_grad in zip(model.parameters(), grads):
        param.grad = param_grad / batch_size
        del param.grad_sample"
compute_loss_and_output;"def compute_loss_and_output(weights, image, target):
            images = image.unsqueeze(0)
            targets = target.unsqueeze(0)
            output = functional_call(model, weights, images)
            loss = criterion(output, targets)
            return loss, output.squeeze(0)"
make_spirals;"def make_spirals(n_samples, noise_std=0.0, rotations=1.0):
    ts = torch.linspace(0, 1, n_samples, device=DEVICE)
    rs = ts**0.5
    thetas = rs * rotations * 2 * math.pi
    signs = torch.randint(0, 2, (n_samples,), device=DEVICE) * 2 - 1
    labels = (signs > 0).to(torch.long).to(DEVICE)

    xs = (
        rs * signs * torch.cos(thetas)
        + torch.randn(n_samples, device=DEVICE) * noise_std
    )
    ys = (
        rs * signs * torch.sin(thetas)
        + torch.randn(n_samples, device=DEVICE) * noise_std
    )
    points = torch.stack([xs, ys], dim=1)
    return points, labels"
train_step_fn;"def train_step_fn(weights, batch, targets, lr=0.2):
    def compute_loss(weights, batch, targets):
        output = functional_call(model, weights, batch)
        loss = loss_fn(output, targets)
        return loss

    grad_weights, loss = grad_and_value(compute_loss)(weights, batch, targets)

    # NB: PyTorch is missing a ""functional optimizer API"" (possibly coming soon)
    # so we are going to re-implement SGD here.
    new_weights = {}
    with torch.no_grad():
        for key in grad_weights:
            new_weights[key] = weights[key] - grad_weights[key] * lr

    return loss, new_weights"
compute_loss;"def compute_loss(weights, batch, targets):
        output = functional_call(model, weights, batch)
        loss = loss_fn(output, targets)
        return loss"
lennard_jones;"def lennard_jones(r):
    return epsilon * ((sigma / r) ** 12 - (sigma / r) ** 6)"
lennard_jones_force;"def lennard_jones_force(r):
    """"""Get magnitude of LJ force""""""
    return -epsilon * ((-12 * sigma**12 / r**13) + (6 * sigma**6 / r**7))"
make_prediction;"def make_prediction(model, drs):
    norms = torch.norm(drs, dim=1).reshape(-1, 1)
    energies = model(norms)

    network_derivs = vmap(jacrev(model))(norms).squeeze(-1)
    forces = -network_derivs * drs / norms
    return energies, forces"
loss_for_task;"def loss_for_task(net, n_inner_iter, x_spt, y_spt, x_qry, y_qry):
    params = dict(net.named_parameters())
    buffers = dict(net.named_buffers())
    querysz = x_qry.size(0)

    def compute_loss(new_params, buffers, x, y):
        logits = functional_call(net, (new_params, buffers), x)
        loss = F.cross_entropy(logits, y)
        return loss

    new_params = params
    for _ in range(n_inner_iter):
        grads = grad(compute_loss)(new_params, buffers, x_spt, y_spt)
        new_params = {k: new_params[k] - g * 1e-1 for k, g in grads.items()}

    # The final set of adapted parameters will induce some
    # final loss and accuracy on the query dataset.
    # These will be used to update the model's meta-parameters.
    qry_logits = functional_call(net, (new_params, buffers), x_qry)
    qry_loss = F.cross_entropy(qry_logits, y_qry)
    qry_acc = (qry_logits.argmax(dim=1) == y_qry).sum() / querysz

    return qry_loss, qry_acc"
compute_loss;"def compute_loss(new_params, buffers, x, y):
        logits = functional_call(net, (new_params, buffers), x)
        loss = F.cross_entropy(logits, y)
        return loss"
find_classes;"def find_classes(root_dir):
    retour = []
    for root, dirs, files in os.walk(root_dir):
        for f in files:
            if f.endswith(""png""):
                r = root.split(""/"")
                lr = len(r)
                retour.append((f, r[lr - 2] + ""/"" + r[lr - 1], root))
    print(f""== Found {len(retour)} items "")
    return retour"
index_classes;"def index_classes(items):
    idx = {}
    for i in items:
        if i[1] not in idx:
            idx[i[1]] = len(idx)
    print(f""== Found {len(idx)} classes"")
    return idx"
predict_with_output_summed;"def predict_with_output_summed(weight, bias, x):
    return predict(weight, bias, x).sum(0)"
loss_fn;"def loss_fn(predictions, targets):
    return F.nll_loss(predictions, targets)"
compute_grad;"def compute_grad(sample, target):
    sample = sample.unsqueeze(0)
    target = target.unsqueeze(0)
    prediction = model(sample)
    loss = loss_fn(prediction, target)
    return torch.autograd.grad(loss, list(model.parameters()))"
compute_sample_grads;"def compute_sample_grads(data, targets):
    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]
    sample_grads = zip(*sample_grads)
    sample_grads = [torch.stack(shards) for shards in sample_grads]
    return sample_grads"
compute_loss;"def compute_loss(params, buffers, sample, target):
    batch = sample.unsqueeze(0)
    targets = target.unsqueeze(0)
    predictions = fmodel(params, buffers, batch)
    loss = loss_fn(predictions, targets)
    return loss"
__init__;"def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)"
forward;"def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        output = x
        return output"
get_ops_for_key;"def get_ops_for_key(key):
    # Needs modified PyTorch C++ code to work
    if key is None:
        ops = torch._C._dispatch_get_registrations_for_dispatch_key()
    else:
        ops = torch._C._dispatch_get_registrations_for_dispatch_key(key)
    cleaned_ops = []
    for i in ops:
        if ""aten::"" not in i:
            continue
        cleaned_ops.append(i[6:].strip())
    return set(cleaned_ops)"
gen_data;"def gen_data(special_op_lists, analysis_name):
    all_ops = get_ops_for_key(None)
    composite_ops = get_ops_for_key(""CompositeImplicitAutograd"")
    noncomposite_ops = all_ops - composite_ops

    ops = yaml.load(
        open(""../../aten/src/ATen/native/native_functions.yaml"").read(),
        Loader=yaml.CLoader,
    )

    annotated_ops = {
        a.strip(): b.strip() for a, b in list(csv.reader(open(""annotated_ops"")))
    }
    from collections import defaultdict

    uniq_ops = []
    uniq_names = set()
    overload_types = defaultdict(list)
    cnt = 0
    for op in ops:
        func_str = op[""func""]
        name = func_str[: func_str.index(""("")]
        if ""."" in name:
            uniq_name = name[: name.index(""."")]
            overload_types[name[name.index(""."") + 1 :]].append(name)
        else:
            uniq_name = name
        op[""name""] = uniq_name
        full_name = func_str[: func_str.index(""("")]
        op[""full_name""] = full_name
        ret_type = func_str[func_str.index(""->"") + 3 :]
        op[""ret_type""] = ret_type
        cnt += 1
        if uniq_name in uniq_names:
            continue
        uniq_names.add(uniq_name)
        uniq_ops.append(op)

    def annotate_ops(ops, is_unique):
        categorization = defaultdict(int)
        for op in ops:
            if op[""name""][-1] == ""_"":
                categorization[""inplace""] += 1
                op[""meta""] = ""inplace""
                continue
            if not is_unique and ""a!"" in op[""func""].lower():
                categorization[""out""] += 1
                op[""meta""] = ""out""
                continue
            if ""conv"" in op[""name""]:
                categorization[""conv""] += 1
                op[""meta""] = ""conv""
                continue
            if ""pool"" in op[""name""]:
                categorization[""pool""] += 1
                op[""meta""] = ""pool""
                continue
            if ""backward"" in op[""name""]:
                categorization[""backward""] += 1
                op[""meta""] = ""backward""
                continue
            if op[""name""][0] == ""_"" and op[""name""][1] != ""_"":
                categorization[""private""] += 1
                op[""meta""] = ""private""
                continue
            if ""batch_norm"" in op[""name""]:
                categorization[""batch_norm""] += 1
                op[""meta""] = ""batch_norm""
                continue
            if ""Tensor"" not in op[""func""] or ""Tensor"" not in op[""ret_type""]:
                categorization[""non_tensor""] += 1
                op[""meta""] = ""non_tensor""
                continue
            if (
                ""cudnn"" in op[""name""]
                or ""mkldnn"" in op[""name""]
                or ""miopen"" in op[""name""]
                or ""native"" in op[""name""]
                or ""thnn"" in op[""name""]
                or ""slow"" in op[""name""]
            ):
                categorization[""backend""] += 1
                op[""meta""] = ""backend""
                continue
            if op[""name""] in annotated_ops:
                categorization[""core""] += 1
                op[""meta""] = ""core "" + annotated_ops[op[""name""]]
                continue
            categorization[""core""] += 1
            op[""meta""] = ""core unknown""
        return categorization

    annotate_ops(ops, is_unique=False)
    with open(f""{analysis_name}"", ""w"") as f:
        for op in ops:
            info = [
                op[""full_name""],
                op[""meta""],
                op[""full_name""] not in noncomposite_ops,
            ] + [check(op) for check in special_op_lists]
            f.write("","".join([str(i) for i in info]) + ""\n"")"
name_check;"def name_check(lst):
    return lambda x: x[""name""] in lst"
full_name_check;"def full_name_check(lst):
    return lambda x: x[""full_name""] in lst"
remove_suffix;"def remove_suffix(input_string, suffix):
    if suffix and input_string.endswith(suffix):
        return input_string[: -len(suffix)]
    return input_string"
remove_prefix;"def remove_prefix(input_string, prefix):
    if prefix and input_string.startswith(prefix):
        return input_string[len(prefix) :]
    return input_string"
annotate_ops;"def annotate_ops(ops, is_unique):
        categorization = defaultdict(int)
        for op in ops:
            if op[""name""][-1] == ""_"":
                categorization[""inplace""] += 1
                op[""meta""] = ""inplace""
                continue
            if not is_unique and ""a!"" in op[""func""].lower():
                categorization[""out""] += 1
                op[""meta""] = ""out""
                continue
            if ""conv"" in op[""name""]:
                categorization[""conv""] += 1
                op[""meta""] = ""conv""
                continue
            if ""pool"" in op[""name""]:
                categorization[""pool""] += 1
                op[""meta""] = ""pool""
                continue
            if ""backward"" in op[""name""]:
                categorization[""backward""] += 1
                op[""meta""] = ""backward""
                continue
            if op[""name""][0] == ""_"" and op[""name""][1] != ""_"":
                categorization[""private""] += 1
                op[""meta""] = ""private""
                continue
            if ""batch_norm"" in op[""name""]:
                categorization[""batch_norm""] += 1
                op[""meta""] = ""batch_norm""
                continue
            if ""Tensor"" not in op[""func""] or ""Tensor"" not in op[""ret_type""]:
                categorization[""non_tensor""] += 1
                op[""meta""] = ""non_tensor""
                continue
            if (
                ""cudnn"" in op[""name""]
                or ""mkldnn"" in op[""name""]
                or ""miopen"" in op[""name""]
                or ""native"" in op[""name""]
                or ""thnn"" in op[""name""]
                or ""slow"" in op[""name""]
            ):
                categorization[""backend""] += 1
                op[""meta""] = ""backend""
                continue
            if op[""name""] in annotated_ops:
                categorization[""core""] += 1
                op[""meta""] = ""core "" + annotated_ops[op[""name""]]
                continue
            categorization[""core""] += 1
            op[""meta""] = ""core unknown""
        return categorization"
count_fn;"def count_fn(x):
        return opinfo_counts[x[""full_name""]]"
has_ref_impl;"def has_ref_impl(x):
        name = x[""name""]
        for prefix in [""linalg_"", ""special_""]:
            name = remove_prefix(name, prefix)
        prefixes = [""nn.functional"", ""fft"", ""special"", ""linalg""]
        return (
            any(f""{prefix}.{name}"" in ref_api for prefix in prefixes) or name in ref_api
        )"
get_correct_mypy_version;"def get_correct_mypy_version():
    # there's probably a more elegant way to do this
    (match,) = re.finditer(
        r""mypy==(\d+(?:\.\d+)*)"",
        (
            Path(__file__).parent.parent / "".ci"" / ""docker"" / ""requirements-ci.txt""
        ).read_text(),
    )
    (version,) = match.groups()
    return version"
plugin;"def plugin(version: str):
    correct_version = get_correct_mypy_version()
    if version != correct_version:
        print(
            f""""""\
You are using mypy version {version}, which is not supported
in the PyTorch repo. Please switch to mypy version {correct_version}.

For example, if you installed mypy via pip, run this:

    pip install mypy=={correct_version}

Or if you installed mypy via conda, run this:

    conda install -c conda-forge mypy={correct_version}
"""""",
            file=sys.stderr,
        )
    return Plugin"
add_assumptions;"def add_assumptions(ctx) -> None:
    # Generated by list(sys.modules['sympy.core.assumptions']._assume_defined)
    # (do not import sympy to speedup mypy plugin load time)
    assumptions = [
        ""hermitian"",
        ""prime"",
        ""noninteger"",
        ""negative"",
        ""antihermitian"",
        ""infinite"",
        ""finite"",
        ""irrational"",
        ""extended_positive"",
        ""nonpositive"",
        ""odd"",
        ""algebraic"",
        ""integer"",
        ""rational"",
        ""extended_real"",
        ""nonnegative"",
        ""transcendental"",
        ""extended_nonzero"",
        ""extended_negative"",
        ""composite"",
        ""complex"",
        ""imaginary"",
        ""nonzero"",
        ""zero"",
        ""even"",
        ""positive"",
        ""polar"",
        ""extended_nonpositive"",
        ""extended_nonnegative"",
        ""real"",
        ""commutative"",
    ]
    for a in assumptions:
        add_attribute_to_class(
            ctx.api,
            ctx.cls,
            f""is_{a}"",
            UnionType([ctx.api.named_type(""builtins.bool""), NoneType()]),
        )"
plugin;"def plugin(version: str):
    return SympyPlugin"
get_base_class_hook;"def get_base_class_hook(self, fullname: str):
        # TODO: This apparently never worked
        if fullname == ""sympy.core.basic.Basic"":
            return add_assumptions
        return None"
get_attribute_hook;"def get_attribute_hook(self, fullname: str):
        if fullname == ""sympy.core.basic.Basic.free_symbols"":
            return lambda ctx: ctx.api.named_generic_type(
                ""builtins.set"", [ctx.api.named_type(""sympy.Symbol"")]
            )
        return None"
open_test_results;"def open_test_results(directory):
    xmls = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith("".xml""):
                tree = parse(f""{root}/{file}"")
                xmls.append(tree)
    return xmls"
get_testcases;"def get_testcases(xmls):
    testcases = []
    for xml in xmls:
        root = xml.getroot()
        testcases.extend(list(root.iter(""testcase"")))
    return testcases"
find;"def find(testcase, condition):
    children = list(testcase.iter())
    assert children[0] is testcase
    children = children[1:]
    return condition(children)"
skipped_test;"def skipped_test(testcase):
    def condition(children):
        return ""skipped"" in {child.tag for child in children}

    return find(testcase, condition)"
passed_test;"def passed_test(testcase):
    def condition(children):
        if len(children) == 0:
            return True
        tags = {child.tag for child in children}
        return ""skipped"" not in tags and ""failed"" not in tags

    return find(testcase, condition)"
key_find;"def key(testcase):
    file = testcase.attrib.get(""file"", ""UNKNOWN"")
    classname = testcase.attrib[""classname""]
    name = testcase.attrib[""name""]
    return ""::"".join([file, classname, name])"
get_passed_testcases;"def get_passed_testcases(xmls):
    testcases = get_testcases(xmls)
    passed_testcases = [testcase for testcase in testcases if passed_test(testcase)]
    return passed_testcases"
get_excluded_testcases;"def get_excluded_testcases(xmls):
    testcases = get_testcases(xmls)
    excluded_testcases = [t for t in testcases if excluded_testcase(t)]
    return excluded_testcases"
excluded_testcase;"def excluded_testcase(testcase):
    def condition(children):
        for child in children:
            if child.tag == ""skipped"":
                if ""Policy: we don't run"" in child.attrib[""message""]:
                    return True
        return False

    return find(testcase, condition)"
is_unexpected_success;"def is_unexpected_success(testcase):
    def condition(children):
        for child in children:
            if child.tag != ""failure"":
                continue
            is_unexpected_success = (
                ""unexpected success"" in child.attrib[""message""].lower()
            )
            if is_unexpected_success:
                return True
        return False

    return find(testcase, condition)"
is_passing_skipped_test;"def is_passing_skipped_test(testcase):
    def condition(children):
        for child in children:
            if child.tag != ""skipped"":
                continue
            has_passing_skipped_test_msg = MSG in child.attrib[""message""]
            if has_passing_skipped_test_msg:
                return True
        return False

    return find(testcase, condition)"
is_failure;"def is_failure(testcase):
    def condition(children):
        for child in children:
            if child.tag != ""failure"":
                continue
            is_unexpected_success = (
                ""unexpected success"" in child.attrib[""message""].lower()
            )
            if not is_unexpected_success:
                return True
        return False

    return find(testcase, condition)"
download_reports;"def download_reports(commit_sha, configs=(""dynamo39"", ""dynamo311"", ""eager311"")):
    log_dir = ""tmp_test_reports_"" + commit_sha

    def subdir_path(config):
        return f""{log_dir}/{config}""

    for config in configs:
        assert config in CONFIGS.keys(), config
    subdir_paths = [subdir_path(config) for config in configs]

    # See which configs we haven't downloaded logs for yet
    missing_configs = []
    for config, path in zip(configs, subdir_paths):
        if os.path.exists(path):
            continue
        missing_configs.append(config)
    if len(missing_configs) == 0:
        print(
            f""All required logs appear to exist, not downloading again. Run `rm -rf {log_dir}` if this is not the case""
        )
        return subdir_paths

    output = subprocess.check_output(
        [""gh"", ""run"", ""list"", ""-c"", commit_sha, ""-w"", ""pull"", ""--json"", ""databaseId""]
    ).decode()
    workflow_run_id = str(json.loads(output)[0][""databaseId""])
    output = subprocess.check_output([""gh"", ""run"", ""view"", workflow_run_id])
    workflow_jobs = parse_workflow_jobs(output)
    print(""found the following workflow jobs:"")
    pprint.pprint(workflow_jobs)

    # Figure out which jobs we need to download logs for
    required_jobs = []
    for config in configs:
        required_jobs.extend(list(CONFIGS[config]))
    for job in required_jobs:
        assert job in workflow_jobs, (
            f""{job} not found, is the commit_sha correct? has the job finished running? The GitHub API may take a couple minutes to update.""
        )

    # This page lists all artifacts.
    listings = requests.get(
        f""https://hud.pytorch.org/api/artifacts/s3/{workflow_run_id}""
    ).json()

    def download_report(job_name, subdir):
        job_id = workflow_jobs[job_name]
        for listing in listings:
            name = listing[""name""]
            if not name.startswith(""test-reports-""):
                continue
            if name.endswith(f""_{job_id}.zip""):
                url = listing[""url""]
                subprocess.run([""wget"", ""-P"", subdir, url], check=True)
                path_to_zip = f""{subdir}/{name}""
                dir_name = path_to_zip[:-4]
                subprocess.run([""unzip"", path_to_zip, ""-d"", dir_name], check=True)
                return
        raise AssertionError(""should not be hit"")

    if not os.path.exists(log_dir):
        os.mkdir(log_dir)

    for config in set(configs) - set(missing_configs):
        print(
            f""Logs for {config} already exist, not downloading again. Run `rm -rf {subdir_path(config)}` if this is not the case.""
        )
    for config in missing_configs:
        subdir = subdir_path(config)
        os.mkdir(subdir)
        job_names = CONFIGS[config]
        for job_name in job_names:
            download_report(job_name, subdir)

    return subdir_paths"
parse_workflow_jobs;"def parse_workflow_jobs(output):
    result = {}
    lines = output.decode().split(""\n"")
    for line in lines:
        match = re.search(r""(\S+ / .*) in .* \(ID (\d+)\)"", line)
        if match is None:
            continue
        result[match.group(1)] = match.group(2)
    return result"
subdir_path;"def subdir_path(config):
        return f""{log_dir}/{config}"""
download_report;"def download_report(job_name, subdir):
        job_id = workflow_jobs[job_name]
        for listing in listings:
            name = listing[""name""]
            if not name.startswith(""test-reports-""):
                continue
            if name.endswith(f""_{job_id}.zip""):
                url = listing[""url""]
                subprocess.run([""wget"", ""-P"", subdir, url], check=True)
                path_to_zip = f""{subdir}/{name}""
                dir_name = path_to_zip[:-4]
                subprocess.run([""unzip"", path_to_zip, ""-d"", dir_name], check=True)
                return
        raise AssertionError(""should not be hit"")"
skip_reason;"def skip_reason(testcase):
    for child in testcase.iter():
        if child.tag != ""skipped"":
            continue
        return child.attrib[""message""]
    raise AssertionError(""no message?"")"
skip_reason_normalized;"def skip_reason_normalized(testcase):
    for child in testcase.iter():
        if child.tag != ""skipped"":
            continue
        result = child.attrib[""message""].split(""\n"")[0]
        result = result.split("">"")[0]
        result = re.sub(r""0x\w+"", ""0xDEADBEEF"", result)
        result = re.sub(r""MagicMock id='\d+'"", ""MagicMock id='0000000000'"", result)
        result = re.sub(r""issues/\d+"", ""issues/XXX"", result)
        result = re.sub(r""torch.Size\(\[.*\]\)"", ""torch.Size([...])"", result)
        result = re.sub(
            r""Could not get qualified name for class '.*'"",
            ""Could not get qualified name for class"",
            result,
        )
        return result
    raise AssertionError(""no message?"")"
get_failures;"def get_failures(testcases):
    skipped = [t for t in testcases if skipped_test(t)]
    skipped_dict = {}
    for s in skipped:
        reason = skip_reason_normalized(s)
        if reason not in skipped_dict:
            skipped_dict[reason] = []
        skipped_dict[reason].append(s)
    result = []
    for s, v in skipped_dict.items():
        result.append((len(v), s, v))
    result.sort(reverse=True)
    return result"
